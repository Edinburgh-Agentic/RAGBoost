{"group_id": 0, "group_size": 29, "items": [{"qid": 2980, "question": "What are the differences between this dataset and pre-existing ones? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["no prior work has explored the target of the offensive language"], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 3316, 6285, 59], "orig_top_k_doc_id": [5146, 5144, 3310, 876, 6176, 4948, 3316, 5145, 3309, 3311, 1788, 4515, 6285, 59, 3314]}, {"qid": 2982, "question": "What is the size of the new dataset? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["No", "14,100 tweets", "Dataset contains total of 14100 annotations."], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 3316, 6285, 3315], "orig_top_k_doc_id": [5146, 5144, 1788, 3310, 6176, 4948, 3309, 876, 5145, 3311, 3316, 6285, 4515, 3314, 3315]}, {"qid": 2979, "question": "What models are used in the experiment? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 3316, 3315, 6532], "orig_top_k_doc_id": [5146, 5144, 876, 5145, 3310, 4948, 6176, 3309, 1788, 3316, 3314, 3311, 4515, 3315, 6532]}, {"qid": 2983, "question": "What kinds of offensive content are explored? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech", "Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.", "offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)"], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 3315, 5644, 245], "orig_top_k_doc_id": [5144, 6176, 5146, 3315, 3314, 3309, 3310, 5145, 876, 1788, 4948, 3311, 5644, 245, 4515]}, {"qid": 2984, "question": "What is the best performing model? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["CNN "], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 3316, 3313, 6519], "orig_top_k_doc_id": [5146, 5144, 3314, 1788, 6176, 3310, 5145, 4948, 876, 3309, 4515, 3316, 3311, 3313, 6519]}, {"qid": 2986, "question": "What is the definition of offensive language? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": [" Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 ."], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 3315, 6131, 3574], "orig_top_k_doc_id": [5144, 5146, 6176, 3310, 6131, 3309, 4948, 1788, 876, 5145, 3314, 3311, 4515, 3315, 3574]}, {"qid": 2162, "question": "What is the challenge for other language except English in Offensive Language and Hate Speech Detection for Danish", "answer": ["not researched as much as English"], "top_k_doc_id": [5144, 3309, 3310, 3574, 6176, 6177, 3314, 5168, 1788, 5976, 6131, 3315, 5291, 5977, 3586], "orig_top_k_doc_id": [3310, 3309, 6176, 5144, 3314, 3315, 1788, 6177, 6131, 5168, 3574, 5291, 5976, 5977, 3586]}, {"qid": 2163, "question": "How many categories of offensive language were there? in Offensive Language and Hate Speech Detection for Danish", "answer": ["3"], "top_k_doc_id": [5144, 3309, 3310, 3574, 6176, 6177, 3314, 5168, 1788, 5976, 6131, 5295, 6133, 4515, 876], "orig_top_k_doc_id": [3310, 3309, 6176, 6131, 5144, 5168, 3574, 6177, 5295, 6133, 3314, 4515, 876, 5976, 1788]}, {"qid": 3773, "question": "How many users does their dataset have? in Automated Hate Speech Detection and the Problem of Offensive Language", "answer": ["33,458", "33,458 Twitter users are orginally used, but than random sample of tweets is extracted resulting in smaller number or users in final dataset.", "33458"], "top_k_doc_id": [5144, 3309, 876, 4515, 5168, 6131, 6133, 6770, 6771, 5976, 6519, 412, 5169, 5295, 413], "orig_top_k_doc_id": [6770, 5168, 6131, 876, 4515, 412, 6133, 5976, 5169, 6771, 3309, 6519, 5144, 5295, 413]}, {"qid": 3774, "question": "How long is their dataset? in Automated Hate Speech Detection and the Problem of Offensive Language", "answer": ["85400000", "24,802 ", "24,802 labeled tweets"], "top_k_doc_id": [5144, 3309, 876, 4515, 5168, 6131, 6133, 6770, 6771, 5976, 6519, 4516, 1788, 3445, 6134], "orig_top_k_doc_id": [6770, 4515, 6519, 5168, 3309, 4516, 876, 5976, 6771, 5144, 6131, 1788, 3445, 6133, 6134]}, {"qid": 3817, "question": "What is the performance of the best model? in Offensive Language Identification in Greek", "answer": ["F1 Macro of 0.89", "LSTMs and GRU with attention which achieved 0.89 F1 score", "0.89 F1 score"], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 3313, 3314, 4948, 5146, 5275, 6180, 6153, 4515, 2618], "orig_top_k_doc_id": [6179, 6178, 6176, 6177, 5144, 5146, 7257, 6180, 3314, 3313, 2618, 5275, 6153, 4515, 4948]}, {"qid": 3818, "question": "What are the models tested on the dataset? in Offensive Language Identification in Greek", "answer": ["linear SVM, RBF SVM, linear classifier with SGDC, multinomial naive bayes, bernoulli naive bayes, pooled GRU, stacked LSTM with attention, LSTM and GRU with attention, 2d convolution with pooling, GRU with Capsule, LSTM with Capsule and attention, and BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning, The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features, Pooled GRU,  Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU , Stacked LSTM with Attention , LSTM and GRU with Attention, 2D Convolution with Pooling , GRU with Capsule,  LSTM with Capsule and Attention, BERT"], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 3313, 3314, 4948, 5146, 5275, 6180, 6153, 4515, 6151], "orig_top_k_doc_id": [6178, 6179, 6176, 6177, 5144, 7257, 6180, 6151, 5146, 6153, 3314, 4948, 3313, 5275, 4515]}, {"qid": 2164, "question": "How large was the dataset of Danish comments? in Offensive Language and Hate Speech Detection for Danish", "answer": ["3600 user-generated comments"], "top_k_doc_id": [5144, 3309, 3310, 3574, 6176, 6177, 3314, 5168, 3311, 3313, 3315, 4515, 3586, 3589, 3445], "orig_top_k_doc_id": [6176, 3310, 3309, 3311, 6177, 3314, 3313, 3315, 5144, 3574, 4515, 5168, 3586, 3589, 3445]}, {"qid": 2985, "question": "How many annotators participated? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["five annotators"], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3316, 5812, 1205, 5978], "orig_top_k_doc_id": [5146, 5144, 1788, 5145, 3309, 3311, 3310, 6176, 876, 5812, 3316, 4948, 1205, 5978, 4515]}, {"qid": 2987, "question": "What are the three layers of the annotation scheme? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["Level A: Offensive language Detection\n, Level B: Categorization of Offensive Language\n, Level C: Offensive Language Target Identification\n"], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3314, 6624, 4137, 6623], "orig_top_k_doc_id": [5146, 5144, 3310, 5145, 6624, 3309, 4948, 6176, 3311, 1788, 876, 4137, 3314, 4515, 6623]}, {"qid": 2988, "question": "How long is the dataset for each step of hierarchy? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 4515, 3316, 4784, 4516, 5582], "orig_top_k_doc_id": [4948, 5146, 5144, 3309, 876, 3310, 1788, 4515, 6176, 4784, 5145, 3316, 4516, 3311, 5582]}, {"qid": 3772, "question": "What type of model do they train? in Automated Hate Speech Detection and the Problem of Offensive Language", "answer": ["logistic regression, na\u00efve Bayes, decision trees, random forests, linear SVMs", "logistic regression, na\u00efve Bayes, decision trees, random forests, linear SVM", "logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs"], "top_k_doc_id": [5144, 3309, 876, 4515, 5168, 6131, 6133, 6770, 6771, 3445, 5295, 1788, 3574, 3310, 412], "orig_top_k_doc_id": [5144, 4515, 3309, 6133, 6770, 6131, 3445, 5295, 1788, 876, 5168, 3574, 6771, 3310, 412]}, {"qid": 3823, "question": "What models do they experiment on? in Offensive Language Identification in Greek", "answer": [" Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features, Pooled GRU, Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU , Stacked LSTM with Attention , LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), linear classifier with Stochastic Gradient Descent (SGDC) learning, Multinomial Na\u00efve Bayes, Bernoulli Na\u00efve Bayes, Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26, BERT BIBREF24"], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 3313, 3314, 4948, 5146, 5275, 6180, 6153, 3445, 3574], "orig_top_k_doc_id": [6179, 6178, 6176, 6177, 5144, 3313, 5146, 7257, 6153, 3314, 6180, 5275, 4948, 3445, 3574]}, {"qid": 2165, "question": "Who were the annotators? in Offensive Language and Hate Speech Detection for Danish", "answer": ["the author and the supervisor"], "top_k_doc_id": [5144, 3309, 3310, 3574, 6176, 6177, 5976, 6519, 412, 5295, 4137, 3045, 6131, 3989, 3046], "orig_top_k_doc_id": [3310, 6176, 3309, 5976, 3574, 5144, 6177, 6519, 412, 5295, 4137, 3045, 6131, 3989, 3046]}, {"qid": 3771, "question": "What is their definition of hate speech? in Automated Hate Speech Detection and the Problem of Offensive Language", "answer": ["language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group", "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group"], "top_k_doc_id": [5144, 3309, 876, 4515, 5168, 6131, 6133, 6770, 412, 6176, 770, 6134, 1788, 3045, 6519], "orig_top_k_doc_id": [6131, 6133, 4515, 6770, 412, 6176, 5144, 770, 3309, 876, 6134, 1788, 5168, 3045, 6519]}, {"qid": 3822, "question": "Is the dataset balanced? in Offensive Language Identification in Greek", "answer": ["No", "No", "No"], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 3313, 3314, 4948, 5146, 5275, 6180, 1788, 4949, 2618], "orig_top_k_doc_id": [6179, 6176, 6178, 6177, 7257, 6180, 3314, 3313, 5144, 5146, 1788, 4948, 4949, 5275, 2618]}, {"qid": 2981, "question": "In what language are the tweets? in Predicting the Type and Target of Offensive Posts in Social Media", "answer": ["English", "English ", "English"], "top_k_doc_id": [5144, 3309, 876, 1788, 3310, 3311, 4948, 5145, 5146, 6176, 6131, 245, 6285, 5582, 5168], "orig_top_k_doc_id": [5144, 5146, 4948, 6176, 5145, 1788, 3310, 6131, 3309, 876, 245, 6285, 5582, 5168, 3311]}, {"qid": 3411, "question": "What was the baseline? in Measuring Offensive Speech in Online Political Discourse", "answer": ["stochastic gradient descent, naive bayes, decision tree", "No"], "top_k_doc_id": [5144, 3309, 3574, 5145, 5641, 5643, 5644, 5645, 2409, 3046, 5168, 3312, 5169, 6131, 6176], "orig_top_k_doc_id": [5641, 5644, 5645, 3574, 5144, 5168, 3309, 5145, 5643, 3312, 5169, 6176, 2409, 3046, 6131]}, {"qid": 3412, "question": "What was their system's performance? in Measuring Offensive Speech in Online Political Discourse", "answer": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "top_k_doc_id": [5144, 3309, 3574, 5145, 5641, 5643, 5644, 5645, 2409, 3046, 5168, 3312, 5169, 6131, 5642], "orig_top_k_doc_id": [5644, 5641, 5645, 3574, 5144, 5168, 5643, 5145, 3309, 5642, 3312, 5169, 3046, 2409, 6131]}, {"qid": 3819, "question": "Which method best performs on the offensive language identification task? in Offensive Language Identification in Greek", "answer": ["LSTM and GRU with Attention can be considered as the best model trained for OGTD", "LSTMs and GRU with attention", " a system using LSTMs and GRU with attention"], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 3313, 3314, 4948, 5146, 4949, 3315, 3445, 4515, 3575], "orig_top_k_doc_id": [6176, 6178, 6179, 5144, 3313, 3314, 5146, 6177, 7257, 4948, 4949, 3315, 3445, 4515, 3575]}, {"qid": 3820, "question": "Did they use crowdsourcing for the annotations? in Offensive Language Identification in Greek", "answer": ["No", "Yes", "No"], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 1443, 3445, 5145, 6180, 5294, 414, 3314, 6153, 6154], "orig_top_k_doc_id": [6179, 6176, 6177, 6178, 5145, 7257, 3445, 6180, 5144, 5294, 414, 3314, 1443, 6153, 6154]}, {"qid": 3821, "question": "How many annotators did they have? in Offensive Language Identification in Greek", "answer": ["Three, plus 2 in case of disagreement below 66%.", "three", "three volunteers "], "top_k_doc_id": [5144, 6176, 6177, 6178, 6179, 7257, 1443, 3445, 5145, 6180, 5978, 5295, 6625, 5976, 6131], "orig_top_k_doc_id": [6179, 6176, 6177, 6178, 7257, 6180, 5144, 1443, 5145, 5978, 5295, 6625, 3445, 5976, 6131]}, {"qid": 3414, "question": "What classifier did they use? in Measuring Offensive Speech in Online Political Discourse", "answer": ["Random Forest"], "top_k_doc_id": [5144, 3309, 3574, 5145, 5641, 5643, 5644, 5645, 2409, 3046, 5168, 5642, 6079, 6176, 1788], "orig_top_k_doc_id": [5641, 5644, 5645, 5643, 5168, 5144, 3046, 5642, 6079, 3309, 5145, 2409, 6176, 1788, 3574]}, {"qid": 3413, "question": "What other political events are included in the database? in Measuring Offensive Speech in Online Political Discourse", "answer": ["US presidential primaries, Democratic and Republican National Conventions"], "top_k_doc_id": [5144, 3309, 3574, 5145, 5641, 5643, 5644, 5645, 7772, 2667, 7382, 5642, 5468, 7387, 7745], "orig_top_k_doc_id": [5641, 5644, 5645, 7772, 2667, 5145, 7382, 3309, 5643, 5642, 5468, 3574, 7387, 7745, 5144]}]}
{"group_id": 1, "group_size": 25, "items": [{"qid": 184, "question": "What is the previous work's model? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["Ternary Trans-CNN"], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 1066, 5275, 1069, 1488, 1492, 1493], "orig_top_k_doc_id": [243, 245, 244, 2137, 6396, 6399, 2138, 1067, 7172, 1069, 1493, 1066, 1488, 5275, 1492]}, {"qid": 185, "question": "What dataset is used? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["HEOT , A labelled dataset for a corresponding english tweets", "HEOT"], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 1066, 5275, 1069, 1488, 1492, 7173], "orig_top_k_doc_id": [243, 245, 244, 6396, 2137, 6399, 1067, 7172, 2138, 1066, 5275, 7173, 1069, 1488, 1492]}, {"qid": 191, "question": "What dataset is used? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["HEOT , A labelled dataset for a corresponding english tweets "], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 1066, 5275, 1069, 1488, 1492, 7173], "orig_top_k_doc_id": [243, 245, 244, 6396, 2137, 6399, 1067, 7172, 2138, 1066, 5275, 7173, 1069, 1488, 1492]}, {"qid": 3110, "question": "What type of system does the baseline classification use? in Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System", "answer": ["support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19", "Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers."], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 243, 5274, 5273, 1488, 1788, 1492, 7173, 6399, 7174, 6403], "orig_top_k_doc_id": [5272, 5275, 5274, 7172, 5273, 1067, 243, 7173, 1788, 1488, 3598, 6399, 7174, 1492, 6403]}, {"qid": 3113, "question": "Where did the texts in the corpus come from? in Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System", "answer": ["tweets from the past two years from domains like `sports', `politics', `entertainment'", "twitter"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 243, 5274, 5273, 1488, 1788, 1492, 5977, 6176, 3665, 1000], "orig_top_k_doc_id": [5272, 5275, 5274, 7172, 5273, 5977, 3598, 243, 1488, 6176, 1788, 1492, 1067, 3665, 1000]}, {"qid": 189, "question": "What models do previous work use? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM"], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 1066, 5275, 1069, 1488, 1068, 2730], "orig_top_k_doc_id": [243, 245, 244, 2137, 6396, 6399, 1067, 1069, 2138, 1068, 1066, 7172, 5275, 1488, 2730]}, {"qid": 1126, "question": "Which psycholinguistic and basic linguistic features are used? in A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "answer": ["Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 1488, 1489, 1492, 1066, 5274, 7174, 2292, 2774, 1490, 3504], "orig_top_k_doc_id": [1488, 1489, 1492, 5275, 7172, 7174, 1067, 1066, 2774, 5274, 2292, 1490, 3598, 5272, 3504]}, {"qid": 1129, "question": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages? in A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "answer": ["None"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 1488, 1489, 1492, 1066, 5274, 7174, 2292, 243, 6403, 5976], "orig_top_k_doc_id": [1488, 7172, 1489, 1067, 1492, 5275, 3598, 1066, 5272, 243, 5274, 6403, 5976, 7174, 2292]}, {"qid": 3112, "question": "How many annotators tagged each text? in Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System", "answer": ["three ", "three annotators"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 243, 5274, 5273, 1488, 1788, 7173, 5977, 2137, 1068, 3585], "orig_top_k_doc_id": [5272, 5274, 5275, 5273, 7172, 7173, 243, 3598, 1067, 5977, 1488, 2137, 1068, 3585, 1788]}, {"qid": 4598, "question": "how many data pairs were there for each dataset? in Sentiment Analysis of Code-Mixed Indian Languages: An Overview of SAIL_Code-Mixed Shared Task @ICON-2017", "answer": ["18461 for Hindi-English and 5538 for Bengali-English", "HI-EN dataset has total size of of 18461 while BN-EN has total size of 5538. "], "top_k_doc_id": [7172, 1067, 3598, 243, 1068, 1069, 3599, 6403, 7173, 7174, 5, 2789, 4510, 6399, 3600], "orig_top_k_doc_id": [7172, 3598, 7173, 7174, 1068, 6403, 1069, 243, 1067, 5, 3599, 4510, 3600, 6399, 2789]}, {"qid": 4599, "question": "how many systems were there? in Sentiment Analysis of Code-Mixed Indian Languages: An Overview of SAIL_Code-Mixed Shared Task @ICON-2017", "answer": ["Six", "nine"], "top_k_doc_id": [7172, 1067, 3598, 243, 1068, 1069, 3599, 6403, 7173, 7174, 5, 2789, 4510, 6399, 2795], "orig_top_k_doc_id": [7172, 7173, 3598, 7174, 1068, 6403, 1069, 1067, 243, 5, 3599, 4510, 6399, 2789, 2795]}, {"qid": 4600, "question": "what was the baseline? in Sentiment Analysis of Code-Mixed Indian Languages: An Overview of SAIL_Code-Mixed Shared Task @ICON-2017", "answer": ["Random labeling", " randomly assigning any of the sentiment values to each of the test instances"], "top_k_doc_id": [7172, 1067, 3598, 243, 1068, 1069, 3599, 6403, 7173, 7174, 5, 2789, 4510, 6399, 3600], "orig_top_k_doc_id": [7172, 7173, 3598, 7174, 1068, 1069, 5, 6403, 243, 1067, 3599, 2789, 3600, 4510, 6399]}, {"qid": 186, "question": "How big is the dataset? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["3189 rows of text messages", "Resulting dataset was 7934 messages for train and 700 messages for test."], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 1066, 5275, 1934, 1936, 3024, 1935], "orig_top_k_doc_id": [243, 245, 244, 6396, 2137, 6399, 1934, 1936, 7172, 2138, 1066, 1067, 3024, 5275, 1935]}, {"qid": 838, "question": "What are puns? in Automatic Target Recovery for Hindi-English Code Mixed Puns", "answer": ["a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 243, 5274, 1066, 1068, 1069, 2083, 5222, 7173, 5225, 5224], "orig_top_k_doc_id": [1066, 1067, 1069, 1068, 5222, 2083, 5225, 5224, 5272, 5275, 7172, 5274, 3598, 243, 7173]}, {"qid": 839, "question": "What are the categories of code-mixed puns? in Automatic Target Recovery for Hindi-English Code Mixed Puns", "answer": ["intra-sequential and intra-word"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 243, 5274, 1066, 1068, 1069, 2083, 5222, 7173, 1493, 1492], "orig_top_k_doc_id": [1066, 1067, 1069, 1068, 7172, 5275, 5272, 2083, 1493, 243, 7173, 5274, 3598, 5222, 1492]}, {"qid": 1125, "question": "What is English mixed with in the TRAC dataset? in A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "answer": ["Hindi"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 1488, 1489, 1492, 1066, 5274, 7174, 1491, 5144, 7173, 1493], "orig_top_k_doc_id": [1488, 1489, 1492, 7172, 5275, 1067, 7174, 1066, 1491, 5272, 3598, 5144, 7173, 5274, 1493]}, {"qid": 1127, "question": "How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem? in A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "answer": ["Systems do not perform well both in Facebook and Twitter texts"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 1488, 1489, 1492, 876, 5144, 6176, 7257, 1066, 5976, 3585], "orig_top_k_doc_id": [1488, 1489, 1492, 5272, 3598, 7172, 5144, 876, 7257, 1066, 5976, 6176, 3585, 5275, 1067]}, {"qid": 1128, "question": "What are the key differences in communication styles between Twitter and Facebook? in A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts", "answer": ["No"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 1488, 1489, 1492, 876, 5144, 6176, 7257, 5816, 243, 7174], "orig_top_k_doc_id": [1488, 1489, 1492, 5144, 5272, 3598, 7172, 7257, 876, 6176, 1067, 5816, 243, 5275, 7174]}, {"qid": 3111, "question": "What experiments were carried out on the corpus? in Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System", "answer": ["task of humor identification in social media texts is analyzed as a classification problem"], "top_k_doc_id": [7172, 1067, 3598, 5272, 5275, 243, 5274, 5273, 6399, 6403, 6396, 2137, 2329, 6176, 7173], "orig_top_k_doc_id": [5272, 5274, 5275, 7172, 6399, 5273, 6403, 1067, 243, 3598, 6396, 2137, 2329, 6176, 7173]}, {"qid": 4602, "question": "what datasets did they use? in Sentiment Analysis of Code-Mixed Indian Languages: An Overview of SAIL_Code-Mixed Shared Task @ICON-2017", "answer": ["Bengali-English and Hindi-English", "HI-EN, BN-EN"], "top_k_doc_id": [7172, 1067, 3598, 243, 1068, 1069, 3599, 6403, 7173, 7174, 5, 2789, 4510, 6640, 3600], "orig_top_k_doc_id": [7172, 7174, 3598, 7173, 5, 1068, 1069, 6403, 243, 1067, 6640, 3599, 3600, 4510, 2789]}, {"qid": 187, "question": "How is the dataset collected? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al"], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 1066, 7173, 3024, 1068, 5272, 7174], "orig_top_k_doc_id": [243, 245, 6396, 244, 2137, 6399, 7172, 1067, 7173, 3024, 1068, 2138, 1066, 5272, 7174]}, {"qid": 4601, "question": "what metrics did they use for evaluation? in Sentiment Analysis of Code-Mixed Indian Languages: An Overview of SAIL_Code-Mixed Shared Task @ICON-2017", "answer": ["precision, recall and f-score ", "The macro average precision, recall, and f-score"], "top_k_doc_id": [7172, 1067, 3598, 243, 1068, 1069, 3599, 6403, 7173, 7174, 5, 3600, 6640, 245, 3603], "orig_top_k_doc_id": [7172, 7173, 3598, 7174, 1069, 1068, 5, 243, 1067, 6403, 3599, 3600, 6640, 245, 3603]}, {"qid": 188, "question": "Was each text augmentation technique experimented individually? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["No"], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1067, 2138, 78, 1488, 5274, 2730, 1936, 79], "orig_top_k_doc_id": [243, 244, 245, 6399, 6396, 2137, 78, 1488, 5274, 2138, 7172, 2730, 1936, 79, 1067]}, {"qid": 4597, "question": "which social media platforms was the data collected from? in Sentiment Analysis of Code-Mixed Indian Languages: An Overview of SAIL_Code-Mixed Shared Task @ICON-2017", "answer": ["Twitter", "Twitter"], "top_k_doc_id": [7172, 1067, 3598, 243, 1068, 1069, 3599, 6403, 7173, 7174, 1488, 5272, 5275, 1489, 6396], "orig_top_k_doc_id": [7172, 3598, 7173, 243, 7174, 6403, 1068, 1488, 1067, 5272, 5275, 1069, 1489, 3599, 6396]}, {"qid": 190, "question": "Does the dataset contain content from various social media platforms? in \"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "answer": ["No"], "top_k_doc_id": [7172, 243, 244, 245, 2137, 6396, 6399, 1488, 5272, 876, 1725, 5275, 6525, 3311, 4136], "orig_top_k_doc_id": [243, 2137, 245, 1488, 6396, 7172, 5272, 876, 244, 6399, 1725, 5275, 6525, 3311, 4136]}]}
{"group_id": 2, "group_size": 23, "items": [{"qid": 1020, "question": "What features are used? in Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction", "answer": ["Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc."], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 59, 520, 1341, 60, 6005, 645, 5739, 3550], "orig_top_k_doc_id": [1339, 1340, 643, 1341, 644, 647, 5403, 520, 646, 59, 6005, 60, 645, 3550, 5739]}, {"qid": 1023, "question": "What is the dataset used? in Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction", "answer": ["EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA"], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 59, 520, 1341, 60, 6005, 645, 5739, 6209], "orig_top_k_doc_id": [1339, 1340, 643, 1341, 644, 647, 5403, 646, 520, 59, 6005, 645, 60, 5739, 6209]}, {"qid": 1021, "question": "Do they compare to previous models? in Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction", "answer": ["Yes"], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 59, 520, 1341, 60, 6005, 645, 3550, 6209], "orig_top_k_doc_id": [1339, 1340, 643, 1341, 644, 647, 520, 646, 5403, 645, 59, 6005, 60, 3550, 6209]}, {"qid": 3219, "question": "Is this dataset publicly available for commercial use? in A Corpus for Detecting High-Context Medical Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients", "answer": ["No", "Yes"], "top_k_doc_id": [5403, 5396, 2640, 5404, 6711, 7832, 7833, 643, 1339, 2496, 3076, 5131, 644, 6317, 2641], "orig_top_k_doc_id": [5403, 5404, 5396, 1339, 6711, 2640, 7832, 2496, 3076, 643, 2641, 5131, 7833, 644, 6317]}, {"qid": 3221, "question": "What are 10 other phenotypes that are annotated? in A Corpus for Detecting High-Context Medical Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients", "answer": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "top_k_doc_id": [5403, 5396, 2640, 5404, 6711, 7832, 7833, 643, 1339, 2496, 3076, 5131, 644, 6317, 1756], "orig_top_k_doc_id": [5403, 5404, 5396, 1339, 6711, 2640, 3076, 6317, 7832, 643, 5131, 2496, 7833, 644, 1756]}, {"qid": 526, "question": "What additional features are proposed for future work? in Analysis of Risk Factor Domains in Psychosis Patient Health Records", "answer": ["distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort"], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 1394, 1403, 3076, 6711, 3752, 6318, 5004, 1404], "orig_top_k_doc_id": [643, 644, 1339, 647, 1340, 1394, 1403, 5004, 646, 1404, 5403, 3752, 6711, 3076, 6318]}, {"qid": 527, "question": "What are their initial results on this task? in Analysis of Risk Factor Domains in Psychosis Patient Health Records", "answer": ["Achieved the highest per-domain scores on Substance (F1 \u2248 0.8) and the lowest scores on Interpersonal and Mood (F1 \u2248 0.5), and show consistency in per-domain performance rankings between MLP and RBF models."], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 1394, 1403, 3076, 6711, 3752, 6318, 4995, 6713], "orig_top_k_doc_id": [643, 644, 1339, 647, 1340, 5403, 1394, 6711, 646, 1403, 3076, 3752, 4995, 6318, 6713]}, {"qid": 1022, "question": "How do they incorporate sentiment analysis? in Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction", "answer": ["features per admission were extracted as inputs to the readmission risk classifier"], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 59, 520, 1341, 60, 6005, 343, 3730, 6640], "orig_top_k_doc_id": [1339, 1340, 643, 1341, 644, 647, 520, 343, 6005, 646, 5403, 3730, 59, 60, 6640]}, {"qid": 3213, "question": "what topics did they label? in Extractive Summarization of EHR Discharge Notes", "answer": ["Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.", "Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others"], "top_k_doc_id": [5403, 5396, 644, 1339, 4825, 5397, 5399, 643, 1340, 4826, 5398, 5404, 6025, 7832, 4382], "orig_top_k_doc_id": [5396, 5397, 644, 1339, 5403, 5399, 1340, 5398, 643, 4825, 5404, 6025, 7832, 4382, 4826]}, {"qid": 3215, "question": "what datasets were used? in Extractive Summarization of EHR Discharge Notes", "answer": ["MIMIC-III", "MIMIC-III"], "top_k_doc_id": [5403, 5396, 644, 1339, 4825, 5397, 5399, 643, 1340, 4826, 5398, 5404, 5542, 2334, 4828], "orig_top_k_doc_id": [5396, 5397, 1339, 5403, 644, 1340, 5399, 5398, 5404, 643, 5542, 2334, 4826, 4825, 4828]}, {"qid": 3220, "question": "How many different phenotypes are present in the dataset? in A Corpus for Detecting High-Context Medical Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients", "answer": ["15 clinical patient phenotypes", "Thirteen different phenotypes are present in the dataset."], "top_k_doc_id": [5403, 5396, 2640, 5404, 6711, 7832, 7833, 643, 1339, 2496, 3076, 5131, 2641, 1757, 1756], "orig_top_k_doc_id": [5403, 5404, 5396, 1339, 6711, 2640, 643, 3076, 2496, 7832, 7833, 2641, 5131, 1757, 1756]}, {"qid": 528, "question": "What datasets did the authors use? in Analysis of Risk Factor Domains in Psychosis Patient Health Records", "answer": [" a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)"], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 1394, 1403, 3076, 6711, 5004, 6206, 1404, 4995], "orig_top_k_doc_id": [643, 644, 1339, 647, 6711, 1394, 5403, 5004, 1340, 6206, 1403, 646, 1404, 4995, 3076]}, {"qid": 1024, "question": "How do they extract topics? in Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction", "answer": [" automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15"], "top_k_doc_id": [5403, 643, 644, 646, 647, 1339, 1340, 59, 520, 1341, 5396, 3550, 6026, 3730, 7528], "orig_top_k_doc_id": [1339, 643, 1340, 1341, 644, 647, 5403, 520, 5396, 646, 3550, 6026, 3730, 7528, 59]}, {"qid": 2838, "question": "where did they obtain the annotated clinical notes from? in Extracting clinical concepts from user queries", "answer": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "top_k_doc_id": [5403, 5396, 2640, 5404, 6711, 7832, 7833, 4970, 4967, 4971, 4969, 6603, 644, 6604, 6714], "orig_top_k_doc_id": [4970, 7832, 4967, 4971, 4969, 6603, 5396, 6711, 7833, 644, 5404, 5403, 6604, 2640, 6714]}, {"qid": 3214, "question": "did they compare with other extractive summarization methods? in Extractive Summarization of EHR Discharge Notes", "answer": ["No"], "top_k_doc_id": [5403, 5396, 644, 1339, 4825, 5397, 5399, 3715, 6496, 1132, 7137, 6840, 7242, 1253, 4828], "orig_top_k_doc_id": [5396, 5397, 5399, 1339, 3715, 6496, 5403, 1132, 7137, 6840, 7242, 4825, 644, 1253, 4828]}, {"qid": 2205, "question": "What evaluation metrics do they use? in A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation", "answer": ["Precision, Recall and INLINEFORM0 score"], "top_k_doc_id": [5403, 1339, 3431, 3432, 3434, 3436, 4610, 5757, 6603, 6604, 3076, 4646, 5399, 1340, 3724], "orig_top_k_doc_id": [3431, 3434, 6604, 3432, 3436, 1340, 5399, 1339, 4646, 3076, 5403, 5757, 4610, 6603, 3724]}, {"qid": 2206, "question": "What performance is achieved? in A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation", "answer": ["No"], "top_k_doc_id": [5403, 1339, 3431, 3432, 3434, 3436, 4610, 5757, 6603, 6604, 3076, 4646, 5399, 5398, 5404], "orig_top_k_doc_id": [3431, 3434, 3436, 3432, 6604, 5398, 5399, 5403, 4610, 6603, 3076, 5404, 1339, 4646, 5757]}, {"qid": 2207, "question": "Do they use BERT? in A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation", "answer": ["No"], "top_k_doc_id": [5403, 1339, 3431, 3432, 3434, 3436, 4610, 5757, 6603, 6604, 3076, 4646, 5399, 5398, 5404], "orig_top_k_doc_id": [3431, 3434, 3432, 4610, 6604, 3436, 5757, 3076, 5403, 5399, 6603, 1339, 4646, 5404, 5398]}, {"qid": 2208, "question": "What is their baseline? in A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation", "answer": ["Burckhardt et al. BIBREF22, Liu et al. BIBREF18, Dernoncourt et al. BIBREF9, Yang et al. BIBREF10"], "top_k_doc_id": [5403, 1339, 3431, 3432, 3434, 3436, 4610, 5757, 6603, 6604, 5404, 6605, 1340, 5397, 5399], "orig_top_k_doc_id": [3431, 3434, 6604, 3432, 5404, 5403, 6605, 1340, 3436, 6603, 4610, 5757, 1339, 5397, 5399]}, {"qid": 2209, "question": "Which two datasets is the system tested on? in A Deep Learning Architecture for De-identification of Patient Notes: Implementation and Evaluation", "answer": ["2014 i2b2 de-identification challenge data set BIBREF2, nursing notes corpus BIBREF3"], "top_k_doc_id": [5403, 1339, 3431, 3432, 3434, 3436, 4610, 5757, 6603, 6604, 5404, 6605, 3076, 1084, 5396], "orig_top_k_doc_id": [3431, 3434, 3432, 6604, 3436, 6603, 4610, 6605, 5403, 3076, 5404, 1339, 1084, 5396, 5757]}, {"qid": 2641, "question": "What is the baseline? in Medication Regimen Extraction From Clinical Conversations", "answer": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "top_k_doc_id": [5403, 644, 1339, 4646, 4647, 4649, 4651, 5258, 7832, 7833, 7834, 7837, 7838, 4648, 2127], "orig_top_k_doc_id": [4646, 4647, 4651, 4649, 7838, 7837, 7833, 7832, 7834, 5258, 4648, 644, 5403, 2127, 1339]}, {"qid": 2642, "question": "Is the data de-identified? in Medication Regimen Extraction From Clinical Conversations", "answer": ["Yes", "Yes"], "top_k_doc_id": [5403, 644, 1339, 4646, 4647, 4649, 4651, 5258, 7832, 7833, 7834, 7837, 7838, 1400, 1340], "orig_top_k_doc_id": [4646, 4647, 4651, 7834, 4649, 7838, 1339, 644, 7832, 7833, 7837, 5258, 5403, 1400, 1340]}, {"qid": 2643, "question": "What embeddings are used? in Medication Regimen Extraction From Clinical Conversations", "answer": [" simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13", "ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13"], "top_k_doc_id": [5403, 644, 1339, 4646, 4647, 4649, 4651, 5258, 7832, 7833, 7834, 7837, 7838, 4648, 2127], "orig_top_k_doc_id": [4646, 4647, 4651, 4649, 7838, 7837, 7833, 7832, 5258, 7834, 4648, 5403, 1339, 644, 2127]}]}
{"group_id": 3, "group_size": 22, "items": [{"qid": 2679, "question": "What languages are evaluated? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["German, English, Spanish, Finnish, French, Russian,  Swedish.", "No"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 657, 1773, 4437, 438, 7512, 7509], "orig_top_k_doc_id": [4706, 4708, 395, 1430, 398, 396, 659, 222, 655, 657, 1773, 438, 4437, 7512, 7509]}, {"qid": 2680, "question": "Does the model have attention? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["Yes", "Yes"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 657, 1773, 4437, 438, 1583, 7319], "orig_top_k_doc_id": [4706, 4708, 1430, 395, 398, 396, 659, 222, 657, 655, 1583, 4437, 438, 1773, 7319]}, {"qid": 2681, "question": "What architecture does the decoder have? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["LSTM", "LSTM"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 657, 1773, 4437, 1583, 656, 1774], "orig_top_k_doc_id": [4706, 4708, 1430, 398, 395, 396, 659, 655, 222, 657, 1583, 656, 1774, 1773, 4437]}, {"qid": 2682, "question": "What architecture does the encoder have? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["LSTM", "LSTM"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 657, 1773, 4437, 1583, 656, 1774], "orig_top_k_doc_id": [4706, 4708, 1430, 395, 398, 396, 655, 659, 222, 657, 1583, 656, 1773, 4437, 1774]}, {"qid": 1081, "question": "What system is used as baseline? in A Resource for Studying Chatino Verbal Morphology", "answer": ["DyNet"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 624, 657, 4864, 4224, 395, 5983, 4349, 650], "orig_top_k_doc_id": [1431, 1430, 1429, 4864, 398, 656, 659, 221, 657, 5983, 624, 4349, 650, 395, 4224]}, {"qid": 1083, "question": "How was the data collected? in A Resource for Studying Chatino Verbal Morphology", "answer": ["No"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 624, 657, 4864, 4224, 395, 5983, 4862, 2450], "orig_top_k_doc_id": [1431, 1430, 1429, 4864, 659, 656, 221, 398, 624, 5983, 657, 395, 4862, 2450, 4224]}, {"qid": 2678, "question": "How do they perform multilingual training? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["Multilingual training is performed by randomly alternating between languages for every new minibatch", "by randomly alternating between languages for every new minibatch"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 657, 1773, 4437, 7509, 6870, 4510], "orig_top_k_doc_id": [4706, 4708, 395, 398, 1430, 396, 659, 1773, 7509, 655, 4437, 222, 6870, 4510, 657]}, {"qid": 1077, "question": "How does morphological analysis differ from morphological inflection? in A Resource for Studying Chatino Verbal Morphology", "answer": ["Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 655, 627, 628, 657, 660, 395, 7509, 7512], "orig_top_k_doc_id": [1431, 1430, 1429, 659, 656, 398, 660, 655, 221, 395, 7509, 657, 628, 7512, 627]}, {"qid": 1079, "question": "What are the architectures used for the three tasks? in A Resource for Studying Chatino Verbal Morphology", "answer": ["DyNet"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 655, 627, 628, 657, 4864, 624, 4349, 3904], "orig_top_k_doc_id": [1431, 1430, 1429, 4864, 398, 656, 659, 627, 624, 221, 4349, 628, 655, 657, 3904]}, {"qid": 1082, "question": "How was annotation done? in A Resource for Studying Chatino Verbal Morphology", "answer": [" hand-curated collection of complete inflection tables for 198 lemmata"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 624, 657, 4864, 4224, 4013, 4349, 2806, 222], "orig_top_k_doc_id": [1431, 1430, 1429, 4864, 221, 659, 656, 624, 657, 4013, 4224, 4349, 2806, 398, 222]}, {"qid": 2684, "question": "What type of inflections are considered? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["No", "No"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 657, 7512, 7509, 7511, 5911, 221], "orig_top_k_doc_id": [4706, 4708, 395, 1430, 398, 655, 657, 7512, 7509, 396, 659, 7511, 5911, 222, 221]}, {"qid": 332, "question": "What were the non-neural baselines used for the task? in The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection", "answer": ["The Lemming model in BIBREF17"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 655, 627, 628, 397, 4029, 626, 1048], "orig_top_k_doc_id": [1430, 659, 398, 396, 395, 627, 628, 397, 4708, 4706, 222, 4029, 626, 1048, 655]}, {"qid": 539, "question": "How is the performance on the task evaluated? in Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge", "answer": ["Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors"], "top_k_doc_id": [659, 655, 656, 658, 660, 4473, 6165, 6279, 395, 6758, 398, 1430, 5344, 1773, 7318], "orig_top_k_doc_id": [659, 655, 660, 656, 6279, 4473, 6165, 5344, 658, 395, 6758, 1430, 398, 1773, 7318]}, {"qid": 540, "question": "What are the tree target languages studied in the paper? in Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge", "answer": ["English, Spanish and Zulu"], "top_k_doc_id": [659, 655, 656, 658, 660, 4473, 6165, 6279, 395, 6758, 398, 1430, 6755, 686, 6943], "orig_top_k_doc_id": [659, 655, 660, 656, 6165, 6279, 4473, 658, 6755, 1430, 398, 395, 686, 6758, 6943]}, {"qid": 1078, "question": "What was the criterion used for selecting the lemmata? in A Resource for Studying Chatino Verbal Morphology", "answer": ["No"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 655, 4864, 395, 7509, 7510, 7512, 4862, 4861], "orig_top_k_doc_id": [1431, 1430, 1429, 4864, 221, 655, 395, 656, 7509, 659, 7510, 7512, 398, 4862, 4861]}, {"qid": 1080, "question": "Which language family does Chatino belong to? in A Resource for Studying Chatino Verbal Morphology", "answer": ["the Otomanguean language family"], "top_k_doc_id": [659, 398, 1430, 221, 656, 1429, 1431, 624, 657, 4864, 4712, 2971, 4707, 2909, 5704], "orig_top_k_doc_id": [1431, 1430, 1429, 4864, 656, 657, 659, 4712, 2971, 221, 4707, 398, 624, 2909, 5704]}, {"qid": 538, "question": "What is an example of a prefixing language? in Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge", "answer": ["Zulu"], "top_k_doc_id": [659, 655, 656, 658, 660, 4473, 6165, 6279, 395, 6758, 657, 377, 1431, 6255, 1439], "orig_top_k_doc_id": [659, 655, 660, 658, 656, 657, 6165, 4473, 6279, 377, 395, 1431, 6758, 6255, 1439]}, {"qid": 2683, "question": "What is MSD prediction? in Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "answer": ["The task of predicting MSD tags: V, PST, V.PCTP, PASS.", "morphosyntactic descriptions (MSD)"], "top_k_doc_id": [659, 398, 1430, 222, 395, 396, 4706, 4708, 4432, 4436, 4431, 4707, 4435, 221, 4438], "orig_top_k_doc_id": [4706, 4708, 395, 1430, 222, 398, 4432, 396, 4436, 4431, 4707, 4435, 659, 221, 4438]}, {"qid": 537, "question": "Are agglutinative languages used in the prediction of both prefixing and suffixing languages? in Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge", "answer": ["Yes"], "top_k_doc_id": [659, 655, 656, 658, 660, 4473, 6165, 6279, 657, 6280, 4472, 4471, 1584, 648, 377], "orig_top_k_doc_id": [659, 660, 658, 655, 656, 657, 4473, 6280, 6279, 4472, 4471, 1584, 648, 6165, 377]}, {"qid": 4518, "question": "What are the baselines? in Multi-task Learning for Low-resource Second Language Acquisition Modeling", "answer": ["LR Here, GBDT Here, RNN Here, ours-MTL ", "GBDT, LR, RNN"], "top_k_doc_id": [659, 627, 655, 660, 1061, 3985, 7066, 3273, 5716, 7069, 7070, 3004, 626, 628, 2713], "orig_top_k_doc_id": [7066, 659, 7070, 627, 1061, 7069, 3004, 3985, 5716, 655, 660, 626, 628, 2713, 3273]}, {"qid": 4519, "question": "Which language learning datasets are used? in Multi-task Learning for Low-resource Second Language Acquisition Modeling", "answer": ["Duolingo SLA modeling shared datasets", "Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29"], "top_k_doc_id": [659, 627, 655, 660, 1061, 3985, 7066, 3273, 5716, 7069, 624, 3938, 3439, 656, 395], "orig_top_k_doc_id": [7066, 659, 1061, 7069, 660, 655, 627, 624, 3938, 3439, 5716, 3985, 656, 395, 3273]}, {"qid": 4517, "question": "What are the evaluation metrics used? in Multi-task Learning for Low-resource Second Language Acquisition Modeling", "answer": ["ROC curve (AUC) BIBREF32, $F_{1}$ score BIBREF33", "ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 "], "top_k_doc_id": [659, 627, 655, 660, 1061, 3985, 7066, 6673, 7070, 2225, 2818, 395, 5042, 1583, 656], "orig_top_k_doc_id": [7066, 659, 1061, 6673, 655, 627, 3985, 660, 7070, 2225, 2818, 395, 5042, 1583, 656]}]}
{"group_id": 4, "group_size": 20, "items": [{"qid": 2963, "question": "How did they obtain the dataset? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy", "public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 5553, 5784, 5551], "orig_top_k_doc_id": [5136, 5137, 5785, 2157, 6455, 5786, 5135, 5783, 1494, 5553, 1726, 6817, 1734, 5551, 5784]}, {"qid": 2964, "question": "What activation function do they use in their model? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["relu, selu, tanh", "Activation function is hyperparameter. Possible values: relu, selu, tanh."], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 5553, 5784, 6665], "orig_top_k_doc_id": [5136, 5137, 5785, 5135, 2157, 5783, 5786, 6455, 1494, 6817, 1734, 1726, 6665, 5553, 5784]}, {"qid": 2965, "question": "What baselines do they compare to? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets", "Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 5553, 5784, 1497], "orig_top_k_doc_id": [5136, 5137, 5135, 1494, 5785, 5783, 5786, 6455, 2157, 1497, 5553, 1734, 5784, 1726, 6817]}, {"qid": 2973, "question": "What baselines were used in this work? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets", "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 5553, 5784, 1497], "orig_top_k_doc_id": [5136, 5137, 5785, 5135, 1494, 6455, 2157, 5786, 5783, 1726, 1734, 6817, 5553, 1497, 5784]}, {"qid": 2966, "question": "How are chunks defined? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.", "sequence of $s$ tweets"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 2159, 6101, 5553], "orig_top_k_doc_id": [5136, 5137, 5135, 2157, 5785, 6455, 5786, 5783, 1726, 1494, 1734, 6817, 5553, 2159, 6101]}, {"qid": 2968, "question": "How many layers does their model have? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["No"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 2159, 6101, 1497], "orig_top_k_doc_id": [5136, 5137, 6455, 5135, 5785, 2157, 5786, 1494, 5783, 1726, 2159, 1734, 6817, 6101, 1497]}, {"qid": 2971, "question": "How big is the dataset used in this work? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["Total dataset size: 171 account (522967 tweets)", "212 accounts"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 6817, 5553, 3276, 5551], "orig_top_k_doc_id": [5136, 5137, 2157, 6455, 5785, 5135, 5786, 5783, 1726, 6817, 1734, 1494, 5553, 3276, 5551]}, {"qid": 3495, "question": "What were their distribution results? in Characterizing Political Fake News in Twitter by its Meta-Data", "answer": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "top_k_doc_id": [5783, 5786, 2157, 5785, 3542, 3926, 3927, 5784, 1494, 5321, 5322, 6107, 6455, 1734, 3015], "orig_top_k_doc_id": [5783, 5784, 3926, 5785, 3542, 1734, 6107, 5322, 5786, 1494, 6455, 3927, 2157, 5321, 3015]}, {"qid": 3496, "question": "How did they determine fake news tweets? in Characterizing Political Fake News in Twitter by its Meta-Data", "answer": ["an expert annotator determined if the tweet fell under a specific category", "Exposure, Characterization, Polarization"], "top_k_doc_id": [5783, 5786, 2157, 5785, 3542, 3926, 3927, 5784, 1494, 5321, 5322, 6107, 6455, 5135, 6667], "orig_top_k_doc_id": [5783, 5784, 5786, 3926, 2157, 5785, 6107, 3542, 3927, 6455, 5321, 5135, 1494, 5322, 6667]}, {"qid": 2967, "question": "What features are extracted? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["Sentiment, Morality, Style, Words embeddings", "15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 1734, 5553, 5551, 1731, 6456], "orig_top_k_doc_id": [5136, 5137, 2157, 5135, 5786, 5785, 6455, 5553, 5783, 1494, 5551, 1734, 1731, 1726, 6456]}, {"qid": 3497, "question": "What is their definition of tweets going viral? in Characterizing Political Fake News in Twitter by its Meta-Data", "answer": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "top_k_doc_id": [5783, 5786, 2157, 5785, 3542, 3926, 3927, 5784, 1494, 5321, 5322, 1726, 3277, 1734, 5324], "orig_top_k_doc_id": [5783, 5784, 5786, 5785, 3542, 3926, 1726, 1494, 3277, 5322, 2157, 3927, 5321, 1734, 5324]}, {"qid": 2969, "question": "Was the approach used in this work to detect fake news fully supervised? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["Yes"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 6817, 6816, 6101, 3927, 6056], "orig_top_k_doc_id": [5136, 2157, 5135, 5786, 6455, 5785, 5783, 5137, 1494, 6817, 6816, 1726, 6101, 3927, 6056]}, {"qid": 3498, "question": "What are the characteristics of the accounts that spread fake news? in Characterizing Political Fake News in Twitter by its Meta-Data", "answer": ["Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio", "have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only"], "top_k_doc_id": [5783, 5786, 2157, 5785, 3542, 3926, 3927, 5784, 1494, 6455, 1734, 5135, 3015, 6817, 7120], "orig_top_k_doc_id": [5783, 2157, 5786, 5784, 5785, 6455, 1734, 5135, 3926, 1494, 3015, 6817, 3542, 3927, 7120]}, {"qid": 2970, "question": "Based on this paper, what is the more predictive set of features to detect fake news? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 6101, 6665, 5553, 6102, 5321, 6817], "orig_top_k_doc_id": [5136, 5786, 5135, 2157, 6455, 1494, 5783, 5785, 6101, 5137, 6665, 5553, 6102, 5321, 6817]}, {"qid": 3500, "question": "How is the ground truth for fake news established? in Characterizing Political Fake News in Twitter by its Meta-Data", "answer": ["Ground truth is not established in the paper"], "top_k_doc_id": [5783, 5786, 2157, 5785, 3542, 3926, 3927, 5784, 6107, 6663, 3015, 5321, 3277, 6101, 7120], "orig_top_k_doc_id": [5783, 5784, 5785, 3926, 2157, 5786, 6107, 3927, 6663, 3015, 3542, 5321, 3277, 6101, 7120]}, {"qid": 2972, "question": "How is a \"chunk of posts\" defined in this work? in FacTweet: Profiling Fake News Twitter Accounts", "answer": ["chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", "sequence of $s$ tweets"], "top_k_doc_id": [5783, 5786, 2157, 5785, 1494, 5135, 5136, 5137, 6455, 1726, 5553, 6456, 1173, 1172, 1174], "orig_top_k_doc_id": [5136, 5137, 5135, 1726, 2157, 6455, 5785, 5783, 5786, 5553, 6456, 1173, 1172, 1494, 1174]}, {"qid": 3499, "question": "What is the threshold for determining that a tweet has gone viral? in Characterizing Political Fake News in Twitter by its Meta-Data", "answer": ["1000"], "top_k_doc_id": [5783, 5786, 2157, 5785, 3542, 3926, 3927, 5784, 5324, 2080, 3277, 6821, 5135, 3543, 3486], "orig_top_k_doc_id": [5783, 5786, 5784, 5785, 3542, 5324, 2080, 3277, 3926, 6821, 5135, 3543, 2157, 3927, 3486]}, {"qid": 4364, "question": "what are the other methods they compare to? in And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election", "answer": ["BIBREF1", "Naive Bayes Classifier"], "top_k_doc_id": [5783, 5786, 330, 331, 1726, 5644, 5879, 6455, 6873, 6875, 7411, 7413, 7414, 1957, 1494], "orig_top_k_doc_id": [6873, 6875, 7411, 7414, 1726, 6455, 5786, 5783, 7413, 5644, 5879, 1957, 330, 1494, 331]}, {"qid": 4365, "question": "what preprocessing method is introduced? in And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election", "answer": ["Tweets without candidate names are removed, URLs and pictures are removed from the tweets that remain.", "(1) removing URLs and pictures, (2) by filtering tweets which have candidates' name"], "top_k_doc_id": [5783, 5786, 330, 331, 1726, 5644, 5879, 6455, 6873, 6875, 7411, 7413, 7414, 7672, 7412], "orig_top_k_doc_id": [6873, 6875, 7411, 7414, 1726, 6455, 5786, 5644, 330, 331, 5783, 7413, 5879, 7672, 7412]}, {"qid": 2395, "question": "How accurate is their predictive model? in Why Do Urban Legends Go Viral?", "answer": ["No"], "top_k_doc_id": [5783, 5786, 3878, 3879, 3883, 3882, 999, 3881, 2689, 5784, 5785, 5628, 5729, 3880, 5627], "orig_top_k_doc_id": [3878, 3879, 3883, 3882, 999, 3881, 5786, 2689, 5784, 5785, 5783, 5628, 5729, 3880, 5627]}]}
{"group_id": 5, "group_size": 20, "items": [{"qid": 4252, "question": "where does their data come from? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["Snopes", "Snopes ", "Snopes fact-checking website"], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 3860, 1834, 3861, 3862, 6743, 3486, 3487, 2158, 7675, 2630], "orig_top_k_doc_id": [6740, 6745, 6741, 3860, 3861, 6468, 3287, 3487, 1834, 2158, 6743, 3486, 7675, 2630, 3862]}, {"qid": 4255, "question": "which architectures did they experiment with? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["For stance detection they used MLP, for evidence extraction they used Tf-idf and BiLSTM, for claim validation they used MLP, BiLSTM and SVM", "AtheneMLP, DecompAttent BIBREF20, USE+Attent"], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 3860, 1834, 3861, 3862, 6743, 3486, 2900, 589, 1148, 4993], "orig_top_k_doc_id": [6740, 6745, 6741, 3287, 3860, 3861, 6468, 3862, 2900, 589, 3486, 1148, 1834, 6743, 4993]}, {"qid": 2223, "question": "Do they report results only on English data? in Fully Automated Fact Checking Using External Sources", "answer": ["Yes"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3483, 3485, 3860, 3862, 6745, 7675, 123, 3486, 3861, 2158], "orig_top_k_doc_id": [6741, 6740, 3287, 3487, 3483, 3860, 3485, 3861, 3486, 3862, 6745, 123, 7671, 2158, 7675]}, {"qid": 2225, "question": "How are the potentially relevant text fragments identified? in Fully Automated Fact Checking Using External Sources", "answer": [" Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5\u201310 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable."], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3483, 3485, 3860, 3862, 6745, 7675, 123, 3486, 6985, 2400], "orig_top_k_doc_id": [7671, 6740, 3483, 3287, 3485, 7675, 3487, 6741, 3860, 6745, 3486, 3862, 6985, 2400, 123]}, {"qid": 4254, "question": "what is the size of their corpus? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["6,422", "Corpus has 6422 claims, 16509 ETSs, 8291 FGE sets and 14296 ODCs."], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 3860, 1834, 3861, 3862, 6743, 899, 7671, 6576, 6676, 589], "orig_top_k_doc_id": [6740, 6745, 6741, 6468, 3860, 3861, 3287, 899, 1834, 7671, 6743, 6576, 6676, 3862, 589]}, {"qid": 2224, "question": "Does this system improve on the SOTA? in Fully Automated Fact Checking Using External Sources", "answer": ["No"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3483, 3485, 3860, 3862, 6745, 7675, 3861, 3015, 1141, 3016], "orig_top_k_doc_id": [3483, 6740, 3487, 3485, 3862, 3287, 6741, 6745, 3860, 3861, 3015, 7671, 7675, 1141, 3016]}, {"qid": 4253, "question": "which existing corpora do they compare with? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["FEVER shared task BIBREF7 and the Fake News Challenge BIBREF8", "PolitiFact14, Emergent16, PolitiFact17, RumourEval17, Snopes17, CLEF-2018, FEVER18"], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 3860, 1834, 3861, 899, 898, 3145, 6576, 2618, 3486, 4008], "orig_top_k_doc_id": [6740, 6745, 6741, 6468, 3287, 1834, 3860, 899, 898, 3145, 6576, 3861, 2618, 3486, 4008]}, {"qid": 2152, "question": "What were the baselines? in MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims", "answer": ["a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3288, 3291, 3292, 3486, 6745, 3289, 3483, 3015, 322, 6746], "orig_top_k_doc_id": [3292, 3287, 3291, 3288, 6740, 3486, 6741, 6745, 7671, 3487, 3289, 3015, 322, 3483, 6746]}, {"qid": 2153, "question": "What metadata is included? in MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims", "answer": ["besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3288, 3291, 3292, 3486, 6745, 3289, 3483, 5950, 3290, 7675], "orig_top_k_doc_id": [3292, 3291, 3287, 3288, 6740, 6741, 3289, 6745, 7671, 5950, 3487, 3486, 3290, 7675, 3483]}, {"qid": 2227, "question": "What data is used to build the task-specific embeddings? in Fully Automated Fact Checking Using External Sources", "answer": ["embedding of the claim, Web evidence"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3483, 3485, 3860, 3862, 6745, 2158, 3861, 3051, 7672, 5642], "orig_top_k_doc_id": [3483, 6740, 3485, 3487, 3860, 3287, 7671, 6741, 2158, 3862, 6745, 3861, 3051, 7672, 5642]}, {"qid": 4251, "question": "did they crowdsource annotations? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 3860, 1834, 3608, 6742, 6743, 3486, 2900, 7671, 3145, 7185], "orig_top_k_doc_id": [6740, 6745, 6741, 3608, 3287, 1834, 6742, 6743, 3486, 6468, 3860, 2900, 7671, 3145, 7185]}, {"qid": 253, "question": "Do the authors report results only on English data? in Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks", "answer": ["No", "No"], "top_k_doc_id": [6740, 321, 322, 3860, 3861, 6743, 6745, 5928, 1499, 3287, 6741, 117, 6101, 5930, 2337], "orig_top_k_doc_id": [321, 3860, 3861, 6743, 6741, 5928, 3287, 1499, 117, 6740, 6101, 5930, 6745, 322, 2337]}, {"qid": 254, "question": "How is the accuracy of the system measured? in Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks", "answer": ["F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates, distances between duplicate and non-duplicate questions using different embedding systems"], "top_k_doc_id": [6740, 321, 322, 3860, 3861, 6743, 6745, 5928, 323, 5930, 7500, 1018, 982, 3926, 7411], "orig_top_k_doc_id": [321, 3860, 3861, 5930, 6743, 7500, 5928, 1018, 323, 6740, 982, 322, 3926, 6745, 7411]}, {"qid": 256, "question": "What existing corpus is used for comparison in these experiments? in Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks", "answer": ["Quora duplicate question dataset BIBREF22"], "top_k_doc_id": [6740, 321, 322, 3860, 3861, 6743, 6745, 5928, 1499, 3287, 6741, 6666, 323, 3926, 883], "orig_top_k_doc_id": [321, 3860, 6740, 6743, 3861, 6741, 322, 6745, 6666, 323, 3926, 883, 3287, 5928, 1499]}, {"qid": 257, "question": "What are the components in the factchecking algorithm?  in Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks", "answer": ["No"], "top_k_doc_id": [6740, 321, 322, 3860, 3861, 6743, 6745, 5928, 323, 5930, 3927, 6666, 2337, 2404, 3015], "orig_top_k_doc_id": [321, 322, 3860, 3861, 6743, 3927, 5928, 6666, 323, 2337, 5930, 6740, 6745, 2404, 3015]}, {"qid": 2154, "question": "How many expert journalists were there? in MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims", "answer": ["No"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3288, 3291, 3292, 3486, 6745, 6676, 6746, 6743, 3861, 6742], "orig_top_k_doc_id": [7671, 6745, 3287, 6740, 3486, 3292, 6741, 3291, 3288, 6676, 6746, 6743, 3861, 3487, 6742]}, {"qid": 2226, "question": "What algorithm and embedding dimensions are used to build the task-specific embeddings? in Fully Automated Fact Checking Using External Sources", "answer": [" task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN"], "top_k_doc_id": [6740, 3287, 6741, 3487, 7671, 3483, 3485, 3860, 3862, 5642, 3019, 5305, 123, 4324, 2158], "orig_top_k_doc_id": [3483, 6740, 7671, 5642, 3485, 3487, 3019, 3860, 3287, 5305, 123, 6741, 4324, 2158, 3862]}, {"qid": 4256, "question": "what domains are present in the corpus? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["corpus covers multiple domains, including discussion blogs, news, and social media", "discussion blogs, news, social media"], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 3860, 6743, 3893, 899, 898, 5950, 6676, 855, 3861, 2158], "orig_top_k_doc_id": [6740, 6745, 6741, 6468, 3287, 3860, 6743, 3893, 899, 898, 5950, 6676, 855, 3861, 2158]}, {"qid": 4250, "question": "what crowdsourcing platform did they use? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["Amazon Mechanical Turk", "Amazon Mechanical Turk", " Amazon Mechanical Turk"], "top_k_doc_id": [6740, 3287, 6741, 6468, 6745, 5038, 3486, 899, 2400, 1834, 7675, 6995, 6808, 3485, 2800], "orig_top_k_doc_id": [6741, 6745, 6740, 5038, 3486, 899, 6468, 2400, 1834, 7675, 6995, 6808, 3485, 2800, 3287]}, {"qid": 255, "question": "How is an incoming claim used to retrieve similar factchecked claims? in Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks", "answer": ["text clustering on the embeddings of texts"], "top_k_doc_id": [6740, 321, 322, 3860, 3861, 6743, 6745, 6741, 3287, 323, 1018, 3483, 1019, 7499, 1647], "orig_top_k_doc_id": [321, 3860, 6741, 6743, 3287, 6745, 323, 3861, 6740, 322, 1018, 3483, 1019, 7499, 1647]}]}
{"group_id": 6, "group_size": 19, "items": [{"qid": 3100, "question": "What labels do they create on their dataset? in Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring", "answer": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "top_k_doc_id": [5257, 2496, 5258, 5057, 5259, 5260, 6859, 7833, 1413, 7835, 7837, 824, 1168, 5399, 3362], "orig_top_k_doc_id": [5257, 5260, 5258, 5259, 7833, 5399, 7837, 3362, 2496, 824, 7835, 6859, 1413, 1168, 5057]}, {"qid": 3101, "question": "How do they select instances to their hold-out test set? in Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring", "answer": ["1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues", "held out from the simulated data"], "top_k_doc_id": [5257, 2496, 5258, 5057, 5259, 5260, 6859, 7833, 1413, 7835, 7837, 824, 1168, 1716, 575], "orig_top_k_doc_id": [5257, 5260, 5259, 5258, 7833, 6859, 1716, 575, 1413, 5057, 824, 7835, 7837, 2496, 1168]}, {"qid": 1743, "question": "How did they annotate the dataset? in MedDialog: A Large-scale Medical Dialogue Dataset", "answer": ["No"], "top_k_doc_id": [5257, 2496, 197, 1444, 4188, 4189, 5675, 5759, 1443, 6858, 6653, 1671, 1074, 6705, 1714], "orig_top_k_doc_id": [2496, 4188, 4189, 5257, 197, 5675, 5759, 1443, 6858, 6653, 1444, 1671, 1074, 6705, 1714]}, {"qid": 1744, "question": "What annotations are in the dataset? in MedDialog: A Large-scale Medical Dialogue Dataset", "answer": ["No"], "top_k_doc_id": [5257, 2496, 197, 1444, 4188, 3679, 5873, 196, 3363, 5874, 6860, 2277, 3681, 1655, 2276], "orig_top_k_doc_id": [2496, 3679, 5257, 5873, 196, 4188, 1444, 3363, 5874, 197, 6860, 2277, 3681, 1655, 2276]}, {"qid": 2487, "question": "Did they experiment on the proposed task? in An Annotation Scheme of A Large-scale Multi-party Dialogues Dataset for Discourse Parsing and Machine Comprehension", "answer": ["No"], "top_k_doc_id": [5257, 196, 4188, 4189, 5258, 5917, 7157, 3416, 6471, 7590, 5380, 5916, 2234, 6584, 736], "orig_top_k_doc_id": [4188, 4189, 5257, 3416, 196, 5380, 5917, 5916, 2234, 6584, 7157, 6471, 7590, 5258, 736]}, {"qid": 2489, "question": "How large is the proposed dataset? in An Annotation Scheme of A Large-scale Multi-party Dialogues Dataset for Discourse Parsing and Machine Comprehension", "answer": ["we obtain 52,053 dialogues and 460,358 utterances"], "top_k_doc_id": [5257, 196, 4188, 4189, 5258, 5917, 7157, 3416, 6471, 7590, 6793, 1743, 2264, 2268, 3143], "orig_top_k_doc_id": [4188, 4189, 5257, 196, 5258, 5917, 6793, 7590, 6471, 1743, 7157, 3416, 2264, 2268, 3143]}, {"qid": 3098, "question": "How big is their created dataset? in Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring", "answer": ["353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis"], "top_k_doc_id": [5257, 2496, 5258, 5057, 5259, 5260, 6859, 7833, 1413, 7835, 7837, 7832, 821, 6858, 6861], "orig_top_k_doc_id": [5257, 5260, 5259, 5258, 7833, 6859, 2496, 1413, 7832, 821, 7837, 7835, 6858, 6861, 5057]}, {"qid": 3978, "question": "Do they report results only on English data? in Human-like machine thinking: Language guided imagination", "answer": ["No", "Yes", "Yes"], "top_k_doc_id": [5257, 5258, 1284, 6438, 6439, 6440, 6441, 3554, 3789, 7064, 3553, 5870, 2915, 221, 231], "orig_top_k_doc_id": [6438, 6440, 6441, 6439, 1284, 5258, 5257, 7064, 3553, 5870, 3789, 2915, 3554, 221, 231]}, {"qid": 3980, "question": "Which 8 tasks has LGI learned? in Human-like machine thinking: Language guided imagination", "answer": ["move left, move right, this is \u2026, the size is big/small, give me a \u2026, enlarge/shrink, rotate \u2026", "move left, move right, this is \u2026, the size is big/small, the size is not small/big, give me a \u2026, enlarge/shrink, rotate \u2026", "move left, move right, this is \u2026, the size is big/small\u2019, the size is not small/big, give me a \u2026, enlarge/shrink, rotate \u2026\u2019"], "top_k_doc_id": [5257, 5258, 1284, 6438, 6439, 6440, 6441, 3554, 3789, 523, 1865, 5428, 234, 3679, 6583], "orig_top_k_doc_id": [6440, 6438, 6441, 6439, 1284, 5257, 5258, 523, 1865, 5428, 3789, 234, 3679, 6583, 3554]}, {"qid": 3982, "question": "In what way does an LSTM mimic the intra parietal sulcus? in Human-like machine thinking: Language guided imagination", "answer": [" mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "textizer to produce text symbols output, extract the quantity information from language text ", "It mimics the number processing functionality of human Intra-Parietal Sulcus."], "top_k_doc_id": [5257, 5258, 1284, 6438, 6439, 6440, 6441, 3800, 3799, 6583, 5911, 6612, 3789, 1081, 1082], "orig_top_k_doc_id": [6439, 6438, 6440, 6441, 5257, 5258, 1284, 3800, 5911, 3799, 6612, 3789, 1081, 6583, 1082]}, {"qid": 3983, "question": "How do the authors define imagination, or imagined scenarios? in Human-like machine thinking: Language guided imagination", "answer": ["Ability to change the answering contents by considering the consequence of the next few output sentences.", " transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image", "Continual thinking requires the capacity to generate mental imagination guided by language, and extract language representations from a real or imagined scenario"], "top_k_doc_id": [5257, 5258, 1284, 6438, 6439, 6440, 6441, 3800, 3799, 6583, 2041, 5987, 234, 6214, 3550], "orig_top_k_doc_id": [6440, 6438, 6441, 6439, 1284, 2041, 6583, 5987, 5257, 234, 3799, 3800, 5258, 6214, 3550]}, {"qid": 1741, "question": "Did they experiment on this dataset? in MedDialog: A Large-scale Medical Dialogue Dataset", "answer": ["No"], "top_k_doc_id": [5257, 2496, 6657, 1074, 824, 2234, 3679, 2276, 5675, 4430, 3225, 4667, 6584, 196, 111], "orig_top_k_doc_id": [2496, 5257, 6657, 1074, 824, 2234, 3679, 2276, 5675, 4430, 3225, 4667, 6584, 196, 111]}, {"qid": 2488, "question": "Is annotation done manually? in An Annotation Scheme of A Large-scale Multi-party Dialogues Dataset for Discourse Parsing and Machine Comprehension", "answer": ["Yes"], "top_k_doc_id": [5257, 196, 4188, 4189, 5258, 5917, 7157, 7327, 736, 7332, 1349, 1907, 3143, 7156, 3145], "orig_top_k_doc_id": [4188, 4189, 7327, 5257, 196, 736, 5258, 7332, 7157, 5917, 1349, 1907, 3143, 7156, 3145]}, {"qid": 3099, "question": "Which data do they use as a starting point for the dialogue dataset? in Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring", "answer": ["A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital", "recordings of nurse-initiated telephone conversations for congestive heart failure patients"], "top_k_doc_id": [5257, 2496, 5258, 5057, 5259, 5260, 6859, 7833, 575, 821, 6860, 6861, 3362, 824, 3191], "orig_top_k_doc_id": [5257, 5259, 5260, 5258, 6859, 575, 2496, 821, 7833, 6860, 6861, 3362, 824, 5057, 3191]}, {"qid": 3979, "question": "How do the authors measure the extent to which LGI has learned the task? in Human-like machine thinking: Language guided imagination", "answer": ["precision, accuracy", "classify figures in various morphology with correct identity (accuracy = 72.7%), demonstrates that LGI can understand the verbs and nouns"], "top_k_doc_id": [5257, 5258, 1284, 6438, 6439, 6440, 6441, 7040, 3839, 523, 5913, 5428, 6348, 1378, 3553], "orig_top_k_doc_id": [6440, 6438, 6439, 6441, 1284, 5257, 7040, 3839, 523, 5913, 5258, 5428, 6348, 1378, 3553]}, {"qid": 3981, "question": "In what was does an LSTM mimic the prefrontal cortex? in Human-like machine thinking: Language guided imagination", "answer": ["the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "No", "It combines language and vision streams similar to the human prefrontal cortex."], "top_k_doc_id": [5257, 5258, 1284, 6438, 6439, 6440, 6441, 3800, 2701, 6442, 2702, 7168, 2706, 2704, 3789], "orig_top_k_doc_id": [6441, 6438, 6440, 2701, 6439, 6442, 2702, 5258, 5257, 7168, 2706, 1284, 2704, 3789, 3800]}, {"qid": 1742, "question": "What language are the conversations in? in MedDialog: A Large-scale Medical Dialogue Dataset", "answer": ["The language is Chinese, which is not easy for non-Chinese-speaking researchers to work on."], "top_k_doc_id": [5257, 2496, 5258, 196, 7371, 7217, 1444, 1804, 6651, 6858, 5800, 1168, 4646, 7218, 7222], "orig_top_k_doc_id": [2496, 5257, 196, 7371, 5258, 7217, 1444, 1804, 6651, 6858, 5800, 1168, 4646, 7218, 7222]}, {"qid": 2474, "question": "What approach does this work propose for the new task? in Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine", "answer": ["We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. "], "top_k_doc_id": [5257, 5258, 2755, 2839, 3839, 3840, 3842, 3843, 3972, 4149, 4150, 4928, 1822, 2836, 1512], "orig_top_k_doc_id": [4149, 4928, 3839, 3843, 3842, 5257, 5258, 2839, 4150, 1822, 3972, 3840, 2836, 1512, 2755]}, {"qid": 2475, "question": "What is the new task proposed in this work? in Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine", "answer": [" listening comprehension task "], "top_k_doc_id": [5257, 5258, 2755, 2839, 3839, 3840, 3842, 3843, 3972, 4149, 4150, 4928, 2011, 7537, 5368], "orig_top_k_doc_id": [4149, 3839, 4928, 3843, 3842, 5257, 4150, 3840, 5258, 2839, 2011, 7537, 5368, 3972, 2755]}]}
{"group_id": 7, "group_size": 19, "items": [{"qid": 3132, "question": "Which publicly available datasets are used? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 412, 1725, 1726, 6176, 3581, 3736, 5292, 3445, 5168], "orig_top_k_doc_id": [5291, 3574, 6285, 3581, 5292, 6770, 5295, 412, 1726, 3736, 1725, 5085, 3445, 6176, 5168]}, {"qid": 3133, "question": "What baseline is used? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10", "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 412, 1725, 1726, 6176, 3581, 3736, 5292, 3309, 6131], "orig_top_k_doc_id": [5291, 6285, 3574, 5292, 5085, 6770, 3581, 1726, 3309, 5295, 3736, 1725, 412, 6131, 6176]}, {"qid": 699, "question": "What was the baseline? in VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination", "answer": ["No"], "top_k_doc_id": [5291, 6375, 3445, 3736, 4499, 4515, 4517, 6770, 6176, 7232, 6523, 6894, 3047, 3737, 876], "orig_top_k_doc_id": [5291, 4499, 4515, 3736, 4517, 6894, 6770, 3445, 7232, 3047, 6176, 6523, 3737, 6375, 876]}, {"qid": 704, "question": "What dataset do they use? in VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination", "answer": ["They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper)."], "top_k_doc_id": [5291, 6375, 3445, 3736, 4499, 4515, 4517, 6770, 6176, 7232, 6523, 6285, 412, 5295, 5168], "orig_top_k_doc_id": [5291, 4515, 3736, 4499, 3445, 4517, 6770, 6285, 6523, 412, 6375, 5295, 6176, 5168, 7232]}, {"qid": 3135, "question": "What are the existing biases? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["systematic and substantial racial biases, biases from data collection, rules of annotation", "sampling tweets from specific keywords create systematic and substancial racial biases in datasets"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 3581, 3587, 4140, 5292, 412, 3007, 6131, 3583, 3582], "orig_top_k_doc_id": [5291, 3581, 5295, 5292, 6285, 6770, 3587, 3007, 3583, 3582, 412, 3574, 4140, 5085, 6131]}, {"qid": 3136, "question": "What biases does their model capture? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 3581, 3587, 4140, 5292, 412, 3007, 6131, 3309, 5907], "orig_top_k_doc_id": [5291, 5292, 5295, 3581, 4140, 3309, 6285, 3574, 3587, 3007, 412, 5085, 5907, 6131, 6770]}, {"qid": 3137, "question": "What existing approaches do they compare to? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10", "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 412, 1725, 1726, 6176, 3581, 3045, 6131, 3445, 6519], "orig_top_k_doc_id": [5291, 412, 6770, 6285, 3581, 3574, 3045, 5085, 6131, 1725, 6176, 3445, 1726, 5295, 6519]}, {"qid": 700, "question": "Is the data all in Vietnamese? in VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination", "answer": ["Yes"], "top_k_doc_id": [5291, 6375, 3445, 3736, 4499, 4515, 4517, 6770, 6176, 7232, 5292, 5295, 876, 4516, 245], "orig_top_k_doc_id": [4517, 4515, 876, 4516, 5291, 4499, 245, 3736, 6770, 6375, 3445, 7232, 6176, 5295, 5292]}, {"qid": 701, "question": "What classifier do they use? in VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination", "answer": ["Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN"], "top_k_doc_id": [5291, 6375, 3445, 3736, 4499, 4515, 4517, 6770, 6176, 7232, 5292, 5295, 6519, 6376, 770], "orig_top_k_doc_id": [4499, 5291, 3736, 6375, 5295, 4515, 6176, 7232, 3445, 5292, 4517, 6519, 6376, 770, 6770]}, {"qid": 702, "question": "What is private dashboard? in VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination", "answer": ["Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set)."], "top_k_doc_id": [5291, 6375, 3445, 3736, 4499, 4515, 4517, 6770, 6176, 7232, 4516, 878, 227, 3047, 6285], "orig_top_k_doc_id": [4517, 5291, 4516, 878, 4499, 4515, 3736, 227, 6770, 6375, 3445, 7232, 3047, 6176, 6285]}, {"qid": 3130, "question": "Do they report results only on English data? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["No", "Yes"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 412, 1725, 1726, 6176, 5292, 5976, 3309, 6519, 1788], "orig_top_k_doc_id": [5291, 1726, 3574, 5295, 5292, 5976, 6770, 3309, 6519, 1725, 6285, 5085, 6176, 412, 1788]}, {"qid": 3131, "question": "What evidence do the authors present that the model can capture some biases in data annotation and collection? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 6770, 3581, 3587, 4140, 5292, 3309, 5907, 3583, 5976, 770], "orig_top_k_doc_id": [5295, 5291, 5292, 3587, 3309, 3574, 4140, 3581, 5907, 3583, 5976, 6285, 6770, 5085, 770]}, {"qid": 4377, "question": "What deep learning methods do they look at? in Deep Learning for Hate Speech Detection in Tweets", "answer": ["CNN, LSTM, FastText", "FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs)"], "top_k_doc_id": [5291, 6375, 5168, 5292, 6771, 6894, 6176, 6770, 5085, 5288, 6285, 3445, 7232, 447, 5295], "orig_top_k_doc_id": [5291, 6894, 3445, 6771, 6176, 5168, 7232, 5085, 6770, 6285, 5292, 447, 5288, 5295, 6375]}, {"qid": 4378, "question": "What is their baseline? in Deep Learning for Hate Speech Detection in Tweets", "answer": ["Char n-grams, TF-IDF, BoWV", "char n-grams, TF-IDF vectors, Bag of Words vectors (BoWV)"], "top_k_doc_id": [5291, 6375, 5168, 5292, 6771, 6894, 6176, 6770, 5085, 5288, 6285, 6131, 3007, 6523, 1077], "orig_top_k_doc_id": [6894, 5291, 6285, 5168, 5288, 6176, 5292, 6771, 6770, 6131, 3007, 5085, 6375, 6523, 1077]}, {"qid": 4380, "question": "Are pretrained embeddings used? in Deep Learning for Hate Speech Detection in Tweets", "answer": ["GloVe", "Yes"], "top_k_doc_id": [5291, 6375, 5168, 5292, 6771, 6894, 6176, 6770, 3736, 7234, 5294, 5976, 412, 3445, 6519], "orig_top_k_doc_id": [6894, 5292, 5168, 6176, 3736, 7234, 6770, 5291, 5294, 5976, 412, 3445, 6771, 6519, 6375]}, {"qid": 703, "question": "What is public dashboard? in VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination", "answer": ["Public dashboard where competitors can see their results during competition, on part of the test set (public test set)."], "top_k_doc_id": [5291, 6375, 3445, 3736, 4499, 4515, 4517, 6770, 4516, 3581, 878, 6894, 227, 1788, 6519], "orig_top_k_doc_id": [4517, 4515, 5291, 4516, 6770, 3581, 6375, 3445, 4499, 878, 3736, 6894, 227, 1788, 6519]}, {"qid": 3134, "question": "What new fine-tuning methods are presented? in A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media", "answer": ["BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer", "BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer"], "top_k_doc_id": [5291, 5085, 3574, 5295, 6285, 5292, 394, 5293, 245, 3581, 1735, 7256, 6176, 5976, 3309], "orig_top_k_doc_id": [5291, 5292, 394, 5295, 5293, 6285, 5085, 3574, 245, 3581, 1735, 7256, 6176, 5976, 3309]}, {"qid": 4379, "question": "Which architectures do they experiment with? in Deep Learning for Hate Speech Detection in Tweets", "answer": ["CNN, LSTM, FastText", "FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs)"], "top_k_doc_id": [5291, 6375, 5168, 5292, 6771, 6894, 5977, 3574, 416, 1077, 5293, 5976, 7232, 3445, 5295], "orig_top_k_doc_id": [6894, 5977, 5291, 3574, 416, 5168, 5292, 1077, 5293, 6375, 5976, 6771, 7232, 3445, 5295]}, {"qid": 2569, "question": "What social media platform does the data come from? in Hate Speech Detection on Vietnamese Social Media Text using the Bidirectional-LSTM Model", "answer": ["No"], "top_k_doc_id": [5291, 5085, 4515, 4517, 876, 4516, 245, 1788, 5906, 1726, 3587, 3045, 5292, 3581, 3309], "orig_top_k_doc_id": [4515, 4517, 876, 4516, 245, 1788, 5906, 5085, 1726, 3587, 3045, 5291, 5292, 3581, 3309]}]}
{"group_id": 8, "group_size": 19, "items": [{"qid": 3443, "question": "What subtasks did they participate in? in UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish", "answer": ["Answer with content missing: (Subscript 1: \"We did not participate in subtask 5 (E-c)\") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks."], "top_k_doc_id": [5007, 7009, 5696, 5698, 5006, 5008, 5011, 7008, 448, 5010, 5423, 1330, 3622, 3623, 5697], "orig_top_k_doc_id": [5698, 5007, 5696, 5008, 5006, 7009, 3623, 5011, 5423, 5697, 448, 5010, 1330, 3622, 7008]}, {"qid": 3446, "question": "What dataset did they use? in UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish", "answer": [" Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.", "Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)"], "top_k_doc_id": [5007, 7009, 5696, 5698, 5006, 5008, 5011, 7008, 448, 5010, 5423, 1330, 3622, 3623, 2968], "orig_top_k_doc_id": [5698, 5007, 5006, 7009, 5008, 5696, 5423, 1330, 5011, 5010, 3622, 2968, 448, 7008, 3623]}, {"qid": 3444, "question": "What were the scores of their system? in UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish", "answer": ["column Ens Test in Table TABREF19"], "top_k_doc_id": [5007, 7009, 5696, 5698, 5006, 5008, 5011, 7008, 448, 5010, 5423, 5009, 2968, 3750, 5697], "orig_top_k_doc_id": [5698, 5007, 5008, 5006, 5011, 5696, 7009, 5009, 5010, 5423, 448, 2968, 3750, 7008, 5697]}, {"qid": 3448, "question": "What semi-supervised learning is applied? in UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish", "answer": ["first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained"], "top_k_doc_id": [5007, 7009, 5696, 5698, 5006, 5008, 5011, 7008, 448, 5697, 3750, 451, 2968, 3622, 3931], "orig_top_k_doc_id": [5698, 5697, 5006, 5007, 7009, 5008, 5011, 5696, 3750, 451, 448, 2968, 3622, 7008, 3931]}, {"qid": 3234, "question": "what dataset was used? in Seernet at EmoInt-2017: Tweet Emotion Intensity Estimator", "answer": ["WASSA-2017 Shared Task on Emotion Intensity"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 5696, 5697, 5975, 2968], "orig_top_k_doc_id": [5423, 5422, 5424, 3623, 5696, 3622, 5973, 5008, 3615, 5007, 5697, 7009, 2968, 5975, 5010]}, {"qid": 3236, "question": "what pretrained word embeddings were used? in Seernet at EmoInt-2017: Tweet Emotion Intensity Estimator", "answer": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 5696, 5697, 5975, 3543], "orig_top_k_doc_id": [5423, 5422, 5424, 5696, 3623, 5973, 5007, 5008, 3615, 5697, 3622, 7009, 5010, 5975, 3543]}, {"qid": 3445, "question": "How was the training data translated? in UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish", "answer": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "top_k_doc_id": [5007, 7009, 5696, 5698, 5006, 5008, 5011, 7008, 3750, 5697, 5423, 5974, 2968, 1330, 3749], "orig_top_k_doc_id": [5698, 5007, 5696, 7009, 5006, 5008, 3750, 5697, 5011, 7008, 5423, 5974, 2968, 1330, 3749]}, {"qid": 3637, "question": "did the top teams experiment with lexicons? in EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "answer": ["No", "No", "No", "No"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 371, 5696, 5974, 5009], "orig_top_k_doc_id": [5973, 5423, 5422, 7009, 5008, 5424, 3615, 3623, 5010, 5696, 5974, 3622, 371, 5009, 5007]}, {"qid": 3638, "question": "did they experiment with lexicons? in EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "answer": ["No", "No", "No"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 371, 5696, 5974, 370], "orig_top_k_doc_id": [5973, 5423, 5422, 5424, 7009, 3615, 3623, 5008, 5696, 3622, 5974, 5010, 371, 5007, 370]}, {"qid": 3639, "question": "what was the baseline? in EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "answer": ["Weka baseline BIBREF5", "Weka baseline BIBREF5", "Weka", " Weka baseline BIBREF5"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 5009, 5011, 5975, 5974], "orig_top_k_doc_id": [5973, 5423, 3623, 5424, 5422, 3622, 3615, 7009, 5975, 5008, 5007, 5011, 5010, 5974, 5009]}, {"qid": 3640, "question": "what was their result? in EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "answer": ["Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.", "0.689 on development and 0.522 on test set", "For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100., In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25., On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively., on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25", "No"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 5009, 5011, 5975, 5006], "orig_top_k_doc_id": [5973, 5423, 5424, 3623, 3622, 5422, 3615, 5007, 7009, 5008, 5010, 5975, 5011, 5009, 5006]}, {"qid": 3641, "question": "what dataset was used? in EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity", "answer": [" training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger", "datasets provided for the shared task BIBREF5", "Dataset of tweets provided for the shared task.", "Dataset from shared task BIBREF5"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 5009, 5011, 5975, 5974], "orig_top_k_doc_id": [5973, 5423, 3623, 5424, 5422, 3622, 3615, 5007, 5008, 7009, 5975, 5010, 5011, 5974, 5009]}, {"qid": 4264, "question": "How is the data labeled? in EiTAKA at SemEval-2018 Task 1: An Ensemble of N-Channels ConvNet and XGboost Regressors for Emotion Analysis of Tweets", "answer": ["No", "No", "No"], "top_k_doc_id": [5007, 7009, 5696, 5698, 1330, 5422, 5423, 5697, 7008, 87, 3543, 5408, 1319, 5006, 7020], "orig_top_k_doc_id": [5423, 7009, 7008, 5698, 5007, 5422, 5696, 5408, 1330, 1319, 5006, 87, 5697, 7020, 3543]}, {"qid": 4265, "question": "What is the best performing model? in EiTAKA at SemEval-2018 Task 1: An Ensemble of N-Channels ConvNet and XGboost Regressors for Emotion Analysis of Tweets", "answer": ["An ensemble of N-Channels ConvNet and XGboost regressor model", "Ensemble Model"], "top_k_doc_id": [5007, 7009, 5696, 5698, 1330, 5422, 5423, 5697, 7008, 87, 3543, 2110, 758, 1320, 756], "orig_top_k_doc_id": [5423, 5698, 1330, 5422, 7009, 5696, 3543, 5697, 7008, 2110, 758, 87, 1320, 756, 5007]}, {"qid": 3235, "question": "how many total combined features were there? in Seernet at EmoInt-2017: Tweet Emotion Intensity Estimator", "answer": ["Fourteen ", "No"], "top_k_doc_id": [5007, 7009, 3615, 3622, 3623, 5008, 5010, 5422, 5423, 5424, 5973, 5696, 5697, 5255, 3543], "orig_top_k_doc_id": [5423, 5422, 5424, 5008, 5973, 3623, 5696, 5007, 5697, 3615, 3622, 7009, 5255, 3543, 5010]}, {"qid": 3447, "question": "What other languages did they translate the data from? in UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish", "answer": ["English ", "English"], "top_k_doc_id": [5007, 7009, 5696, 5698, 5006, 5008, 5011, 3750, 438, 448, 3622, 5423, 6006, 1044, 2968], "orig_top_k_doc_id": [5698, 5696, 5007, 5006, 7009, 5008, 3750, 438, 448, 5011, 3622, 5423, 6006, 1044, 2968]}, {"qid": 4266, "question": "How long is the dataset? in EiTAKA at SemEval-2018 Task 1: An Ensemble of N-Channels ConvNet and XGboost Regressors for Emotion Analysis of Tweets", "answer": ["No", "No"], "top_k_doc_id": [5007, 7009, 5696, 5698, 1330, 5422, 5423, 5697, 7008, 756, 1488, 2968, 5006, 5175, 1320], "orig_top_k_doc_id": [5423, 7008, 7009, 5698, 1330, 756, 5007, 1488, 5422, 5696, 2968, 5697, 5006, 5175, 1320]}, {"qid": 2854, "question": "Which race and gender are given higher sentiment intensity predictions? in Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "answer": ["Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.\nAfrican American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.", " the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names"], "top_k_doc_id": [5007, 3988, 5006, 5008, 5010, 5011, 5765, 5768, 6558, 6559, 6700, 6735, 6560, 5009, 6966], "orig_top_k_doc_id": [5006, 5011, 5008, 5007, 5010, 6558, 3988, 6560, 5009, 6559, 6700, 5768, 6735, 6966, 5765]}, {"qid": 2855, "question": "What criteria are used to select the 8,640 English sentences? in Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "answer": ["Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.", "generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race"], "top_k_doc_id": [5007, 3988, 5006, 5008, 5010, 5011, 5765, 5768, 6558, 6559, 6700, 6735, 41, 5291, 7269], "orig_top_k_doc_id": [5006, 5007, 5011, 5008, 6558, 5010, 5765, 41, 3988, 6735, 6559, 5291, 5768, 6700, 7269]}]}
{"group_id": 9, "group_size": 18, "items": [{"qid": 679, "question": "Do they compare to previous work? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["No"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 2296, 4863, 4875, 1061, 6300, 2473, 3650, 2474], "orig_top_k_doc_id": [844, 847, 845, 594, 846, 593, 595, 1061, 2296, 6300, 4863, 4875, 3650, 2473, 2474]}, {"qid": 680, "question": "How many instances does their dataset have? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["10700"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 2296, 4863, 4875, 1061, 6300, 2473, 5217, 5218], "orig_top_k_doc_id": [844, 847, 845, 594, 846, 593, 595, 4875, 1061, 5217, 2296, 4863, 6300, 2473, 5218]}, {"qid": 675, "question": "Do they report results only on English data? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["No"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 2296, 4863, 4875, 1061, 6300, 3650, 2351, 2297], "orig_top_k_doc_id": [844, 847, 845, 594, 846, 593, 595, 4875, 2296, 4863, 1061, 3650, 2351, 6300, 2297]}, {"qid": 500, "question": "Do they compare their neural network against any other model? in Synchronising audio and ultrasound by learning cross-modal embeddings", "answer": ["No"], "top_k_doc_id": [844, 593, 594, 595, 596, 575, 847, 2938, 2927, 2930, 5966, 4122, 5970, 5968, 7138], "orig_top_k_doc_id": [593, 594, 595, 596, 4122, 847, 575, 2930, 2927, 2938, 5970, 844, 5968, 7138, 5966]}, {"qid": 503, "question": "What kind of neural network architecture do they use? in Synchronising audio and ultrasound by learning cross-modal embeddings", "answer": ["CNN"], "top_k_doc_id": [844, 593, 594, 595, 596, 575, 847, 2938, 2927, 2930, 5966, 2149, 2418, 2928, 6405], "orig_top_k_doc_id": [593, 594, 595, 2149, 575, 596, 5966, 847, 2930, 2418, 2938, 2927, 2928, 6405, 844]}, {"qid": 677, "question": "What are the characteristics of the dataset? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 2296, 1061, 2351, 6351, 6300, 2473, 6301, 2352], "orig_top_k_doc_id": [844, 845, 847, 594, 846, 595, 593, 6300, 1061, 2296, 2473, 6351, 2351, 6301, 2352]}, {"qid": 678, "question": "What type of models are used for classification? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["feedforward neural networks (DNNs), convolutional neural networks (CNNs)"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 2296, 1061, 2351, 6351, 5217, 5219, 1062, 7325], "orig_top_k_doc_id": [844, 847, 845, 594, 846, 595, 593, 1061, 6351, 5217, 2296, 5219, 1062, 2351, 7325]}, {"qid": 682, "question": "How many speakers do they have in the dataset? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["58"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 2296, 4863, 4875, 2473, 3922, 6351, 5219, 3648], "orig_top_k_doc_id": [844, 847, 845, 594, 846, 595, 593, 4875, 2296, 2473, 3922, 6351, 4863, 5219, 3648]}, {"qid": 501, "question": "Do they annotate their own dataset or use an existing one? in Synchronising audio and ultrasound by learning cross-modal embeddings", "answer": ["Use an existing one"], "top_k_doc_id": [844, 593, 594, 595, 596, 575, 847, 2938, 2927, 2930, 845, 846, 1836, 1837, 2413], "orig_top_k_doc_id": [593, 594, 595, 2938, 845, 575, 846, 847, 596, 844, 1836, 1837, 2930, 2927, 2413]}, {"qid": 502, "question": "Does their neural network predict a single offset in a recording? in Synchronising audio and ultrasound by learning cross-modal embeddings", "answer": ["Yes"], "top_k_doc_id": [844, 593, 594, 595, 596, 575, 847, 2938, 5217, 7642, 620, 7138, 3939, 1297, 7139], "orig_top_k_doc_id": [593, 594, 595, 596, 844, 5217, 847, 2938, 575, 7642, 620, 7138, 3939, 1297, 7139]}, {"qid": 667, "question": "How is performance of this system measured? in Automatic Reminiscence Therapy for Dementia.", "answer": ["using the BLEU score as a quantitative metric and human evaluation for quality"], "top_k_doc_id": [844, 821, 822, 823, 824, 847, 593, 594, 1413, 4096, 5056, 6172, 6418, 6321, 846], "orig_top_k_doc_id": [821, 824, 844, 822, 823, 6418, 593, 1413, 847, 6321, 6172, 5056, 594, 4096, 846]}, {"qid": 670, "question": "How big dataset is used for training this system? in Automatic Reminiscence Therapy for Dementia.", "answer": ["For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues."], "top_k_doc_id": [844, 821, 822, 823, 824, 847, 593, 594, 1413, 4096, 5056, 6172, 6418, 7858, 7128], "orig_top_k_doc_id": [821, 824, 822, 844, 823, 6418, 593, 6172, 594, 4096, 5056, 1413, 7858, 847, 7128]}, {"qid": 676, "question": "Do they propose any further additions that could be made to improve generalisation to unseen speakers? in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["Yes"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 4367, 7793, 3922, 4863, 1061, 3650, 1163, 2473], "orig_top_k_doc_id": [847, 844, 845, 846, 594, 595, 4367, 7793, 593, 3922, 4863, 1061, 3650, 1163, 2473]}, {"qid": 1957, "question": "How big are datasets used in experiments? in Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours", "answer": ["2000 images"], "top_k_doc_id": [844, 593, 594, 595, 596, 845, 846, 2119, 2689, 2930, 2931, 2932, 3735, 1417, 483], "orig_top_k_doc_id": [2930, 2931, 593, 844, 594, 2932, 595, 3735, 845, 596, 2119, 1417, 2689, 483, 846]}, {"qid": 1958, "question": "What previously annotated databases are available? in Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours", "answer": ["the UBC database BIBREF14"], "top_k_doc_id": [844, 593, 594, 595, 596, 845, 846, 2119, 2689, 2930, 2931, 2932, 2922, 6405, 5800], "orig_top_k_doc_id": [2930, 2931, 844, 2932, 593, 594, 595, 845, 596, 2689, 2922, 846, 2119, 6405, 5800]}, {"qid": 668, "question": "How many questions per image on average are available in dataset? in Automatic Reminiscence Therapy for Dementia.", "answer": ["5 questions per image"], "top_k_doc_id": [844, 821, 822, 823, 824, 847, 593, 594, 3799, 3800, 5730, 3801, 495, 520, 870], "orig_top_k_doc_id": [821, 824, 822, 823, 844, 3799, 594, 3800, 5730, 3801, 593, 495, 520, 870, 847]}, {"qid": 681, "question": "What model do they use to classify phonetic segments?  in Speaker-independent classification of phonetic segments from raw ultrasound in child speech", "answer": ["feedforward neural networks, convolutional neural networks"], "top_k_doc_id": [844, 593, 594, 595, 845, 846, 847, 6300, 4863, 2297, 2769, 2296, 7621, 6370, 2294], "orig_top_k_doc_id": [847, 844, 845, 594, 6300, 846, 593, 595, 4863, 2297, 2769, 2296, 7621, 6370, 2294]}, {"qid": 669, "question": "Is machine learning system underneath similar to image caption ML systems? in Automatic Reminiscence Therapy for Dementia.", "answer": ["Yes"], "top_k_doc_id": [844, 821, 822, 823, 824, 847, 2901, 4744, 7085, 2648, 5664, 2922, 5735, 111, 7086], "orig_top_k_doc_id": [821, 822, 824, 844, 2901, 4744, 823, 7085, 2648, 5664, 2922, 5735, 847, 111, 7086]}]}
{"group_id": 10, "group_size": 18, "items": [{"qid": 4165, "question": "How long is the test dataset for Dutch? in Automatic Detection of Cyberbullying in Social Media Text", "answer": ["Random 10 percent out of 78381 posts.", "sample ( INLINEFORM2 ) of all data", "78387"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 3309, 6622, 6624, 6627, 6625, 6626, 3893], "orig_top_k_doc_id": [6628, 6624, 6621, 6623, 6622, 3309, 6627, 6626, 5812, 6625, 6074, 5813, 4948, 6080, 3893]}, {"qid": 4166, "question": "How long is the training dataset for English? in Automatic Detection of Cyberbullying in Social Media Text", "answer": ["Random 90 percent out of 113698 posts.", "113698"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 3309, 6622, 6624, 6627, 6625, 6626, 6075], "orig_top_k_doc_id": [6623, 6628, 5812, 6624, 5813, 6621, 3309, 6074, 4948, 6622, 6080, 6626, 6075, 6627, 6625]}, {"qid": 3733, "question": "What agreement measure is used? in Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining Representations for Cyberbullying Classification", "answer": ["Fleiss's Kappa", "Fleiss's Kappa ", "Fleiss's Kappa"], "top_k_doc_id": [6074, 6080, 6623, 3312, 6075, 6076, 6077, 6079, 6622, 5813, 6627, 6628, 6078, 7807, 6625], "orig_top_k_doc_id": [6622, 6077, 6076, 6080, 6079, 6075, 6078, 6623, 6074, 3312, 6625, 5813, 6628, 6627, 7807]}, {"qid": 3736, "question": "What social-network features are used? in Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining Representations for Cyberbullying Classification", "answer": ["Relative positions of the author and target accounts in the directed following network by\ncomputing modified versions of Jaccard\u2019s similarity index, friends count, followers count, verified status, number of tweets posted within 6 months.", "Downward overlap, upward overlap, inward overlap, outward overlap, bidirectional overlap, count of friends of each user, count of followers of each user, users verified status, number of tweets posted within six-month snapshots", "Neighborhood Overlap,  count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines"], "top_k_doc_id": [6074, 6080, 6623, 3312, 6075, 6076, 6077, 6079, 6622, 5813, 6627, 6628, 6078, 7807, 6626], "orig_top_k_doc_id": [6622, 6075, 6623, 6077, 6080, 6074, 6076, 6078, 5813, 6079, 3312, 7807, 6628, 6627, 6626]}, {"qid": 3737, "question": "What are the five factors considered? in Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining Representations for Cyberbullying Classification", "answer": ["Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance", "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance", "Aggressive language, Repetition, Harmful intent, Visibility among peers, Power imbalance"], "top_k_doc_id": [6074, 6080, 6623, 3312, 6075, 6076, 6077, 6079, 6622, 5813, 6627, 6628, 3309, 6625, 6078], "orig_top_k_doc_id": [6076, 6622, 6075, 6077, 6074, 6080, 6623, 3312, 6079, 3309, 6078, 6627, 6628, 5813, 6625]}, {"qid": 3738, "question": "How is cyberbullying defined? in Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining Representations for Cyberbullying Classification", "answer": ["They define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance", "cyberbullying is a complex social phenomenon, different from the simpler notion of cyberaggression", "A public display of intention to \u201cinflict injury or discomfort\u201d upon a weaker victim through repeated acts of aggression."], "top_k_doc_id": [6074, 6080, 6623, 3312, 6075, 6076, 6077, 6079, 6622, 5813, 6627, 6628, 3309, 6625, 6626], "orig_top_k_doc_id": [6076, 6622, 6080, 6075, 6077, 6623, 6074, 6628, 3309, 6627, 6626, 5813, 6079, 6625, 3312]}, {"qid": 4163, "question": "What are their baselines? in Automatic Detection of Cyberbullying in Social Media Text", "answer": ["an unoptimised linear-kernel SVM, a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms", "unoptimised linear-kernel SVM, keyword-based system", "Linear-kernel SVM based on word n-grams, vocabulary-based classifier."], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 3309, 6622, 6624, 6627, 412, 6075, 1205], "orig_top_k_doc_id": [6623, 6624, 6621, 6622, 6628, 6074, 6080, 5813, 5812, 4948, 3309, 6627, 6075, 1205, 412]}, {"qid": 4164, "question": "Do they report the annotation agreement? in Automatic Detection of Cyberbullying in Social Media Text", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 3309, 6622, 6624, 6627, 6625, 6075, 3574], "orig_top_k_doc_id": [6624, 6621, 6623, 6074, 6625, 6622, 6080, 4948, 3309, 6628, 6075, 5812, 6627, 3574, 5813]}, {"qid": 4167, "question": "What features are used? in Automatic Detection of Cyberbullying in Social Media Text", "answer": ["Word INLINEFORM0 -gram bag-of-words, Character INLINEFORM0 -gram bag-of-words, Term lists, Subjectivity lexicon features, Topic model features", "Topic model features, Subjectivity lexicon features, Term lists, Character INLINEFORM0 -gram bag-of-words, Word INLINEFORM0 -gram bag-of-words"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 3309, 6622, 6624, 6627, 412, 6075, 6626], "orig_top_k_doc_id": [6623, 6621, 6622, 6074, 5813, 6624, 5812, 6628, 4948, 3309, 6080, 6075, 412, 6627, 6626]}, {"qid": 4168, "question": "What is the source of the data? in Automatic Detection of Cyberbullying in Social Media Text", "answer": ["social networking site ASKfm", " social networking site ASKfm"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 3309, 6622, 6624, 6627, 412, 6075, 6626], "orig_top_k_doc_id": [6623, 6624, 6622, 6621, 6628, 6074, 5813, 5812, 3309, 4948, 6080, 6075, 6627, 6626, 412]}, {"qid": 3521, "question": "What were their performance results? in Deep Learning for Detecting Cyberbullying Across Multiple Social Media Platforms", "answer": ["best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 412, 1876, 5814, 6075, 6176, 6624, 3309], "orig_top_k_doc_id": [5813, 5812, 6628, 4948, 6074, 6623, 6080, 6621, 412, 6624, 6176, 5814, 6075, 3309, 1876]}, {"qid": 3522, "question": "What cyberbulling topics did they address? in Deep Learning for Detecting Cyberbullying Across Multiple Social Media Platforms", "answer": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "top_k_doc_id": [6074, 6080, 6623, 4948, 5812, 5813, 6621, 6628, 412, 1876, 5814, 6075, 6176, 1205, 5144], "orig_top_k_doc_id": [5812, 5813, 6074, 6176, 4948, 6628, 6080, 412, 1205, 6623, 6621, 5814, 6075, 5144, 1876]}, {"qid": 3734, "question": "Do they report the annotation agreement? in Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining Representations for Cyberbullying Classification", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6074, 6080, 6623, 3312, 6075, 6076, 6077, 6079, 6622, 5976, 6078, 6625, 6627, 3582, 3309], "orig_top_k_doc_id": [6076, 6622, 6075, 3312, 6077, 6625, 6079, 6074, 6080, 6623, 5976, 6627, 6078, 3582, 3309]}, {"qid": 3735, "question": "How many annotators participated? in Aggressive, Repetitive, Intentional, Visible, and Imbalanced: Refining Representations for Cyberbullying Classification", "answer": ["170", "three ", "No"], "top_k_doc_id": [6074, 6080, 6623, 3312, 6075, 6076, 6077, 6079, 6622, 5976, 6078, 6625, 5813, 5498, 5812], "orig_top_k_doc_id": [6076, 6077, 6075, 6074, 6622, 5813, 6080, 5498, 6623, 3312, 5976, 6625, 6079, 6078, 5812]}, {"qid": 4781, "question": "which network community detection dataset was used? in The Power of Communities: A Text Classification Model with Automated Labeling Process Using Network Community Detection", "answer": ["Text data from Pypestream", "The data set obtained from Pypestream"], "top_k_doc_id": [6074, 6080, 6519, 7449, 7451, 7452, 1397, 4998, 7450, 4886, 7807, 4995, 4893, 3671, 4999], "orig_top_k_doc_id": [7449, 7451, 6080, 6074, 7452, 4886, 6519, 1397, 7807, 7450, 4998, 4995, 4893, 3671, 4999]}, {"qid": 4783, "question": "how many classes are they classifying? in The Power of Communities: A Text Classification Model with Automated Labeling Process Using Network Community Detection", "answer": ["18 ", "19 "], "top_k_doc_id": [6074, 6080, 6519, 7449, 7451, 7452, 1397, 4998, 7450, 4902, 6770, 412, 4579, 1398, 4580], "orig_top_k_doc_id": [7449, 7451, 6080, 7452, 6074, 7450, 4902, 6770, 412, 6519, 4579, 4998, 1398, 4580, 1397]}, {"qid": 4782, "question": "did they collect the human labeled data? in The Power of Communities: A Text Classification Model with Automated Labeling Process Using Network Community Detection", "answer": ["Yes", "No"], "top_k_doc_id": [6074, 6080, 6519, 7449, 7451, 7452, 3445, 2396, 4893, 5112, 7807, 6075, 2390, 4999, 482], "orig_top_k_doc_id": [7449, 6074, 7452, 7451, 6080, 3445, 2396, 4893, 5112, 7807, 6075, 2390, 4999, 482, 6519]}, {"qid": 4780, "question": "which had better results, the svm or the random forest model? in The Power of Communities: A Text Classification Model with Automated Labeling Process Using Network Community Detection", "answer": ["SVM", "SVM"], "top_k_doc_id": [6074, 6080, 6519, 7449, 7451, 7452, 1402, 1486, 4780, 6081, 4886, 6107, 6399, 6632, 4999], "orig_top_k_doc_id": [7452, 6080, 1402, 6519, 7449, 7451, 6074, 1486, 4780, 6081, 4886, 6107, 6399, 6632, 4999]}]}
{"group_id": 11, "group_size": 17, "items": [{"qid": 603, "question": "What was their result on Stance Sentiment Emotion Corpus? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["F1 score of 66.66%"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5007, 5008, 3623, 5037, 5038, 5006, 5549, 448], "orig_top_k_doc_id": [758, 756, 759, 757, 760, 7008, 3623, 5038, 5007, 5008, 7009, 5006, 5037, 5549, 448]}, {"qid": 607, "question": "What are the datasets used for training? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5007, 5008, 3623, 5037, 5406, 3622, 5011, 5], "orig_top_k_doc_id": [758, 756, 757, 759, 760, 7008, 3623, 5406, 7009, 3622, 5007, 5008, 5037, 5011, 5]}, {"qid": 604, "question": "What performance did they obtain on the SemEval dataset? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["F1 score of 82.10%"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5007, 5008, 5409, 5006, 448, 5255, 5406, 3622], "orig_top_k_doc_id": [758, 759, 756, 7008, 757, 7009, 5409, 760, 5007, 5006, 448, 5255, 5008, 5406, 3622]}, {"qid": 605, "question": "What are the state-of-the-art systems? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5037, 5256, 589, 5007, 6005, 5008, 5409, 1489], "orig_top_k_doc_id": [759, 758, 756, 757, 760, 6005, 5007, 5037, 7009, 7008, 5256, 5008, 589, 5409, 1489]}, {"qid": 606, "question": "How is multi-tasking performed? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks., Each of the shared representations is then fed to the primary attention mechanism"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5037, 5256, 589, 5007, 5038, 5, 2119, 88], "orig_top_k_doc_id": [756, 759, 757, 758, 760, 7008, 7009, 5037, 5038, 5, 5256, 589, 2119, 88, 5007]}, {"qid": 3660, "question": "Did they pre-train on existing sentiment corpora? in A system for the 2019 Sentiment, Emotion and Cognitive State Task of DARPAs LORELEI project", "answer": ["Yes", "Yes", "Yes", "No, they used someone else's pretrained model. "], "top_k_doc_id": [759, 756, 3623, 6005, 6006, 758, 760, 6640, 5406, 7008, 5008, 230, 6007, 5409, 1957], "orig_top_k_doc_id": [6005, 6006, 7008, 756, 5008, 5406, 759, 758, 3623, 230, 6007, 5409, 6640, 1957, 760]}, {"qid": 3662, "question": "How many languages are in the dataset? in A system for the 2019 Sentiment, Emotion and Cognitive State Task of DARPAs LORELEI project", "answer": ["2", "2", "2 (Spanish and English)"], "top_k_doc_id": [759, 756, 3623, 6005, 6006, 758, 760, 6640, 5406, 7008, 6208, 3124, 5255, 6441, 589], "orig_top_k_doc_id": [6005, 6006, 756, 760, 6208, 3623, 3124, 7008, 5406, 5255, 6441, 6640, 758, 759, 589]}, {"qid": 310, "question": "What are the baseline benchmarks? in DENS: A Dataset for Multi-class Emotion Analysis", "answer": ["TF-IDF + SVM, Depeche + SVM, NRC + SVM, TF-NRC + SVM, Doc2Vec + SVM,  Hierarchical RNN, BiRNN + Self-Attention, ELMo + BiRNN,  Fine-tuned BERT"], "top_k_doc_id": [759, 370, 371, 372, 760, 3622, 3623, 758, 1980, 3627, 7009, 5549, 2120, 5195, 6644], "orig_top_k_doc_id": [372, 371, 370, 3622, 3627, 759, 5549, 760, 3623, 2120, 7009, 5195, 1980, 758, 6644]}, {"qid": 311, "question": "What is the size of this dataset? in DENS: A Dataset for Multi-class Emotion Analysis", "answer": ["9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words"], "top_k_doc_id": [759, 370, 371, 372, 760, 3622, 3623, 758, 1980, 3627, 7009, 10, 6606, 1319, 1979], "orig_top_k_doc_id": [372, 371, 370, 759, 3622, 760, 758, 3627, 7009, 1980, 3623, 10, 6606, 1319, 1979]}, {"qid": 608, "question": "How many parameters does the model have? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["No"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5007, 5105, 5256, 5406, 5037, 3306, 1319, 5006], "orig_top_k_doc_id": [758, 756, 757, 760, 759, 7009, 7008, 5105, 5007, 5256, 5406, 5037, 3306, 1319, 5006]}, {"qid": 609, "question": "What is the previous state-of-the-art model? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5037, 5256, 5409, 1489, 5406, 2120, 88, 5255], "orig_top_k_doc_id": [758, 759, 756, 757, 760, 7008, 7009, 5256, 5409, 1489, 5406, 5037, 2120, 88, 5255]}, {"qid": 610, "question": "What is the previous state-of-the-art performance? in Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis", "answer": ["No"], "top_k_doc_id": [759, 756, 757, 758, 760, 7008, 7009, 5037, 5256, 5409, 1489, 5406, 2120, 88, 5255], "orig_top_k_doc_id": [759, 758, 756, 757, 760, 7008, 7009, 5256, 5409, 5406, 1489, 2120, 5037, 88, 5255]}, {"qid": 3663, "question": "Did the system perform well on low-resource languages? in A system for the 2019 Sentiment, Emotion and Cognitive State Task of DARPAs LORELEI project", "answer": ["No", "No", "No"], "top_k_doc_id": [759, 756, 3623, 6005, 6006, 758, 760, 6640, 3124, 1430, 2826, 1596, 5423, 5903, 67], "orig_top_k_doc_id": [6005, 3124, 6006, 1430, 3623, 756, 758, 759, 6640, 2826, 760, 1596, 5423, 5903, 67]}, {"qid": 308, "question": "Which tested technique was the worst performer? in DENS: A Dataset for Multi-class Emotion Analysis", "answer": ["Depeche + SVM"], "top_k_doc_id": [759, 370, 371, 372, 760, 3622, 3623, 758, 1980, 6609, 1489, 1497, 6174, 3543, 3439], "orig_top_k_doc_id": [372, 371, 370, 759, 6609, 3622, 3623, 1489, 760, 1497, 758, 6174, 3543, 1980, 3439]}, {"qid": 309, "question": "How many emotions do they look at? in DENS: A Dataset for Multi-class Emotion Analysis", "answer": ["9"], "top_k_doc_id": [759, 370, 371, 372, 760, 3622, 3623, 10, 9, 5, 4, 4740, 756, 6606, 1496], "orig_top_k_doc_id": [372, 370, 371, 3622, 760, 9, 3623, 5, 4, 4740, 756, 6606, 1496, 10, 759]}, {"qid": 312, "question": "How many annotators were there? in DENS: A Dataset for Multi-class Emotion Analysis", "answer": ["3 "], "top_k_doc_id": [759, 370, 371, 372, 760, 3622, 3623, 10, 3625, 5041, 5040, 3624, 7, 6, 592], "orig_top_k_doc_id": [370, 371, 372, 3625, 759, 5041, 5040, 3622, 3623, 10, 3624, 760, 7, 6, 592]}, {"qid": 3661, "question": "What were the most salient features extracted by the models? in A system for the 2019 Sentiment, Emotion and Cognitive State Task of DARPAs LORELEI project", "answer": ["unigrams and bigrams, word2vec, manually constructed lexica, sentiment embeddings", "No", "No", "No"], "top_k_doc_id": [759, 756, 3623, 6005, 6006, 5406, 523, 1129, 6208, 5409, 2042, 5900, 5423, 6441, 5904], "orig_top_k_doc_id": [6005, 6006, 3623, 5406, 523, 756, 1129, 6208, 5409, 2042, 5900, 5423, 759, 6441, 5904]}]}
{"group_id": 12, "group_size": 17, "items": [{"qid": 908, "question": "What kinds of neural networks did they use in this paper? in An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation", "answer": ["LSTMs"], "top_k_doc_id": [2056, 1165, 5455, 2725, 2501, 6616, 424, 4731, 6661, 7363, 6190, 4766, 7434, 1325, 6951], "orig_top_k_doc_id": [7363, 2725, 6190, 2056, 4766, 5455, 424, 1165, 4731, 7434, 6661, 1325, 6951, 2501, 6616]}, {"qid": 909, "question": "How did they use the domain tags? in An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation", "answer": ["Appending the domain tag \u201c<2domain>\" to the source sentences of the respective corpora"], "top_k_doc_id": [2056, 1165, 5455, 2725, 2501, 6616, 424, 4731, 6661, 6619, 6620, 5031, 7544, 1166, 4046], "orig_top_k_doc_id": [6616, 1165, 6619, 5455, 2501, 6620, 5031, 6661, 7544, 2056, 1166, 2725, 424, 4046, 4731]}, {"qid": 1748, "question": "How many examples do they have in the target domain? in Fast Domain Adaptation for Neural Machine Translation", "answer": ["Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)"], "top_k_doc_id": [2056, 1165, 5455, 2725, 2501, 6616, 6671, 1720, 6656, 4038, 4727, 4688, 7268, 1048, 4728], "orig_top_k_doc_id": [6671, 2501, 5455, 2056, 6616, 1720, 1165, 6656, 4038, 4727, 4688, 7268, 1048, 4728, 2725]}, {"qid": 546, "question": "what are the baselines? in Dense Information Flow for Neural Machine Translation", "answer": [" 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256"], "top_k_doc_id": [2056, 663, 666, 2922, 665, 667, 5238, 5411, 6943, 4212, 4185, 1498, 5472, 4184, 3416], "orig_top_k_doc_id": [666, 2922, 5238, 667, 663, 665, 6943, 2056, 4212, 5411, 4185, 1498, 5472, 4184, 3416]}, {"qid": 549, "question": "what datasets were used? in Dense Information Flow for Neural Machine Translation", "answer": ["IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German"], "top_k_doc_id": [2056, 663, 666, 2922, 665, 667, 5238, 5411, 2048, 4846, 3409, 3359, 2053, 7645, 1231], "orig_top_k_doc_id": [666, 665, 2056, 2922, 5238, 667, 663, 2048, 4846, 3409, 3359, 2053, 7645, 5411, 1231]}, {"qid": 907, "question": "How much improvement does their method get over the fine tuning baseline? in An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation", "answer": ["0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE."], "top_k_doc_id": [2056, 1165, 5455, 2725, 5031, 7266, 6619, 1476, 1166, 2013, 7270, 5026, 2268, 6095, 6943], "orig_top_k_doc_id": [5031, 7266, 6619, 1165, 5455, 2725, 1476, 1166, 2056, 2013, 7270, 5026, 2268, 6095, 6943]}, {"qid": 1863, "question": "How many examples are there in the source domain? in Domain Adaptation for Neural Networks by Parameter Augmentation", "answer": ["78,976"], "top_k_doc_id": [2056, 1325, 2725, 2726, 2727, 2729, 2862, 4727, 6671, 2861, 4728, 6882, 7268, 4038, 7266], "orig_top_k_doc_id": [2725, 2726, 2862, 2727, 2729, 6671, 2056, 2861, 7268, 4727, 6882, 4038, 1325, 7266, 4728]}, {"qid": 1864, "question": "How many examples are there in the target domain? in Domain Adaptation for Neural Networks by Parameter Augmentation", "answer": ["the food dataset has 3,806 images for training "], "top_k_doc_id": [2056, 1325, 2725, 2726, 2727, 2729, 2862, 4727, 6671, 2861, 4728, 6882, 7268, 2306, 5992], "orig_top_k_doc_id": [2725, 2726, 2862, 2727, 2729, 6671, 2056, 2861, 7268, 4727, 6882, 2306, 4728, 5992, 1325]}, {"qid": 5022, "question": "what is the size of the dataset? in Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "answer": ["80 plane crash events", "80 plane crash events, each paired with a set of related news articles"], "top_k_doc_id": [2056, 2048, 5847, 6731, 7371, 7820, 2752, 3634, 4755, 6555, 2840, 3395, 3247, 7821, 7814], "orig_top_k_doc_id": [7820, 2048, 5847, 7371, 2056, 6731, 2840, 3634, 3395, 2752, 6555, 3247, 4755, 7821, 7814]}, {"qid": 5023, "question": "what dataset did they use? in Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "answer": ["Event dataset with news articles", "Stanford Plane Crash Dataset BIBREF15"], "top_k_doc_id": [2056, 2048, 5847, 6731, 7371, 7820, 2752, 3634, 4755, 6555, 2661, 575, 5914, 2910, 490], "orig_top_k_doc_id": [2048, 2661, 7820, 3634, 5847, 7371, 6731, 6555, 2752, 4755, 2056, 575, 5914, 2910, 490]}, {"qid": 548, "question": "what language pairs are explored? in Dense Information Flow for Neural Machine Translation", "answer": ["German-English, Turkish-English, English-German"], "top_k_doc_id": [2056, 663, 666, 2922, 665, 4212, 4184, 6943, 1041, 5432, 4033, 2048, 564, 495, 2053], "orig_top_k_doc_id": [663, 4212, 665, 4184, 2922, 2056, 6943, 1041, 5432, 4033, 2048, 666, 564, 495, 2053]}, {"qid": 1469, "question": "To which systems do they compare their results against? in Non-Parametric Adaptation for Neural Machine Translation", "answer": ["standard Transformer Base model"], "top_k_doc_id": [2056, 1165, 5455, 2055, 2053, 1244, 2284, 1048, 111, 1245, 7271, 7269, 2619, 6616, 1246], "orig_top_k_doc_id": [2056, 2055, 2053, 1244, 2284, 1048, 111, 1245, 7271, 1165, 5455, 7269, 2619, 6616, 1246]}, {"qid": 1865, "question": "Did they only experiment with captioning task? in Domain Adaptation for Neural Networks by Parameter Augmentation", "answer": ["Yes"], "top_k_doc_id": [2056, 1325, 2725, 2726, 2727, 2729, 2862, 4727, 6671, 2728, 7266, 2556, 5850, 5992, 1267], "orig_top_k_doc_id": [2729, 2725, 2728, 2726, 2862, 2727, 7266, 6671, 2556, 5850, 5992, 2056, 1325, 4727, 1267]}, {"qid": 5021, "question": "what are the baselines? in Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "answer": ["Reschke CRF, Reschke Noisy-OR, Reschke Best", "Reschke CRF, Reschke Noisy-OR, Reschke Best"], "top_k_doc_id": [2056, 2048, 5847, 6731, 7371, 7820, 2752, 3634, 2840, 4277, 3004, 2661, 6955, 352, 4915], "orig_top_k_doc_id": [6731, 7820, 7371, 2840, 4277, 3004, 2048, 2661, 2752, 5847, 2056, 6955, 352, 4915, 3634]}, {"qid": 547, "question": "did they outperform previous methods? in Dense Information Flow for Neural Machine Translation", "answer": ["Yes"], "top_k_doc_id": [2056, 663, 666, 2922, 7372, 667, 5595, 2619, 4812, 3207, 2607, 6190, 3298, 1364, 5411], "orig_top_k_doc_id": [2922, 663, 666, 7372, 667, 5595, 2619, 2056, 4812, 3207, 2607, 6190, 3298, 1364, 5411]}, {"qid": 5020, "question": "what metrics are used to evaluate the models? in Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "answer": ["modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014, mean reciprocal rank (MRR)", "precision, recall, mean reciprocal rank, F INLINEFORM0"], "top_k_doc_id": [2056, 2048, 5847, 6731, 7371, 7820, 5854, 2910, 2002, 6555, 4910, 3247, 1920, 4825, 3175], "orig_top_k_doc_id": [7820, 5854, 2910, 2002, 2056, 6555, 7371, 4910, 3247, 1920, 6731, 2048, 5847, 4825, 3175]}, {"qid": 2537, "question": "Can the approach be generalized to other technical domains as well?  in Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "answer": ["There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable."], "top_k_doc_id": [2056, 4387, 4391, 4389, 4388, 4390, 1165, 643, 5841, 2998, 6658, 5838, 7149, 5837, 6616], "orig_top_k_doc_id": [4387, 4391, 4389, 4388, 4390, 1165, 643, 5841, 2998, 2056, 6658, 5838, 7149, 5837, 6616]}]}
{"group_id": 13, "group_size": 17, "items": [{"qid": 3012, "question": "what state of the art methods did they compare with? in Improving Neural Text Simplification Model with Simplified Corpora", "answer": ["OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress"], "top_k_doc_id": [5165, 5166, 7619, 7620, 153, 3181, 4593, 4595, 5167, 6597, 7617, 4594, 2433, 3182, 6295], "orig_top_k_doc_id": [5165, 3181, 5166, 4593, 6597, 3182, 5167, 7619, 7617, 2433, 6295, 153, 4595, 4594, 7620]}, {"qid": 3013, "question": "what are the sizes of both datasets? in Improving Neural Text Simplification Model with Simplified Corpora", "answer": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "top_k_doc_id": [5165, 5166, 7619, 7620, 153, 3181, 4593, 4595, 5167, 6597, 7617, 4594, 2433, 2434, 6598], "orig_top_k_doc_id": [5165, 5166, 5167, 4593, 6597, 3181, 4595, 4594, 7619, 2433, 153, 2434, 6598, 7617, 7620]}, {"qid": 3009, "question": "what language does this paper focus on? in Improving Neural Text Simplification Model with Simplified Corpora", "answer": ["English", "Simple English"], "top_k_doc_id": [5165, 5166, 7619, 7620, 153, 3181, 4593, 4595, 5167, 6597, 7617, 4594, 6295, 564, 7662], "orig_top_k_doc_id": [4593, 5165, 5167, 5166, 6597, 3181, 4595, 4594, 7617, 6295, 7620, 564, 7619, 7662, 153]}, {"qid": 3011, "question": "by how much did their model improve? in Improving Neural Text Simplification Model with Simplified Corpora", "answer": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "top_k_doc_id": [5165, 5166, 7619, 7620, 153, 3181, 4593, 4595, 5167, 6597, 7617, 3182, 7137, 3299, 564], "orig_top_k_doc_id": [5165, 5166, 3181, 4593, 6597, 5167, 4595, 7619, 7617, 153, 7620, 3182, 7137, 3299, 564]}, {"qid": 4133, "question": "what language was the data in? in Sentence Simplification with Memory-Augmented Neural Networks", "answer": ["English", "English ", "English"], "top_k_doc_id": [5165, 5166, 7619, 6597, 6598, 6599, 7374, 7375, 7617, 7618, 1155, 2752, 510, 7367, 1154], "orig_top_k_doc_id": [6597, 7619, 7375, 7617, 6599, 1155, 7374, 5165, 6598, 1154, 7367, 7618, 2752, 510, 5166]}, {"qid": 4136, "question": "how do humans judge the simplified sentences? in Sentence Simplification with Memory-Augmented Neural Networks", "answer": ["Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.", "We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.", "By fluency, adequacy, and simplicity using a five point Likert scale."], "top_k_doc_id": [5165, 5166, 7619, 7620, 153, 3181, 4593, 4595, 5167, 6597, 7617, 6599, 2701, 6295, 6928], "orig_top_k_doc_id": [6597, 7619, 5165, 5166, 7617, 6599, 5167, 4593, 2701, 4595, 153, 6295, 6928, 7620, 3181]}, {"qid": 4137, "question": "what datasets were used? in Sentence Simplification with Memory-Augmented Neural Networks", "answer": ["Newsela BIBREF22, WikiSmall BIBREF10, WikiLarge BIBREF15", "Newsela, WikiSmall, WikiLarge"], "top_k_doc_id": [5165, 5166, 7619, 6597, 6598, 6599, 7374, 7375, 7617, 7618, 1155, 2752, 510, 7367, 6825], "orig_top_k_doc_id": [6597, 7619, 7375, 6598, 7617, 6599, 1155, 7374, 5165, 510, 7618, 6825, 5166, 7367, 2752]}, {"qid": 4912, "question": "How large is the test set? in Controllable Sentence Simplification", "answer": ["359 samples", "359 samples"], "top_k_doc_id": [5165, 5166, 7619, 7620, 3183, 6598, 6957, 7617, 7618, 6295, 6597, 3184, 6955, 1235, 3182], "orig_top_k_doc_id": [7617, 6598, 3184, 5166, 7619, 7618, 6957, 5165, 3183, 6295, 6955, 1235, 7620, 3182, 6597]}, {"qid": 4914, "question": "What are the baseline models? in Controllable Sentence Simplification", "answer": ["PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS", "BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16"], "top_k_doc_id": [5165, 5166, 7619, 7620, 3183, 6598, 6957, 7617, 7618, 6295, 6597, 5167, 517, 4593, 6599], "orig_top_k_doc_id": [7617, 7619, 5166, 7620, 3183, 5165, 7618, 5167, 6598, 6957, 517, 6295, 6597, 4593, 6599]}, {"qid": 3010, "question": "what evaluation metrics did they use? in Improving Neural Text Simplification Model with Simplified Corpora", "answer": ["BLEU , FKGL , SARI ", "BLEU, FKGL, SARI, Simplicity"], "top_k_doc_id": [5165, 5166, 7619, 7620, 153, 3181, 4593, 4595, 5167, 6597, 3182, 4796, 3184, 6956, 6957], "orig_top_k_doc_id": [3181, 5166, 5165, 5167, 6597, 4593, 3182, 4595, 7619, 4796, 3184, 153, 6956, 6957, 7620]}, {"qid": 4134, "question": "what was the baseline? in Sentence Simplification with Memory-Augmented Neural Networks", "answer": ["Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari", "Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari"], "top_k_doc_id": [5165, 5166, 7619, 6597, 6598, 6599, 7374, 7375, 7617, 7618, 1155, 2752, 6928, 1154, 3298], "orig_top_k_doc_id": [6597, 7375, 7619, 7374, 7617, 6599, 1155, 6598, 6928, 5166, 2752, 1154, 5165, 3298, 7618]}, {"qid": 4913, "question": "What does SARI measure? in Controllable Sentence Simplification", "answer": ["SARI compares the predicted simplification with both the source and the target references", "the predicted simplification with both the source and the target references"], "top_k_doc_id": [5165, 5166, 7619, 7620, 3183, 6598, 6957, 7617, 7618, 6599, 5167, 3181, 3182, 4796, 4595], "orig_top_k_doc_id": [7617, 7619, 7618, 6598, 7620, 6599, 5167, 5166, 3181, 3182, 4796, 6957, 3183, 5165, 4595]}, {"qid": 4135, "question": "which automatic metrics were used in evaluation? in Sentence Simplification with Memory-Augmented Neural Networks", "answer": ["BLEU, SARI", "BLEU , SARI ", "BLEU, SARI"], "top_k_doc_id": [5165, 5166, 7619, 6597, 6598, 6599, 7374, 7375, 7617, 7618, 6320, 3181, 6928, 3184, 4796], "orig_top_k_doc_id": [6597, 6599, 6598, 7619, 6320, 5165, 7617, 7375, 3181, 5166, 7374, 7618, 6928, 3184, 4796]}, {"qid": 132, "question": "Is the semantic hierarchy representation used for any task? in DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "answer": ["Yes, Open IE", "Yes"], "top_k_doc_id": [5165, 5166, 153, 154, 736, 4593, 7023, 7617, 7618, 5701, 6255, 6295, 737, 7024, 7278], "orig_top_k_doc_id": [153, 154, 4593, 736, 737, 5165, 7023, 6255, 7618, 5701, 7617, 5166, 6295, 7024, 7278]}, {"qid": 134, "question": "Is the model evaluated? in DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "answer": ["the English version is evaluated. The German version evaluation is in progress "], "top_k_doc_id": [5165, 5166, 153, 154, 736, 4593, 7023, 7617, 7618, 5701, 6255, 6295, 7662, 4595, 7348], "orig_top_k_doc_id": [153, 154, 4593, 7618, 5165, 5166, 7662, 7617, 6295, 4595, 7023, 5701, 7348, 6255, 736]}, {"qid": 133, "question": "What are the corpora used for the task? in DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "answer": ["For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains, The evaluation of the German version is in progress."], "top_k_doc_id": [5165, 5166, 153, 154, 736, 4593, 7023, 7617, 7618, 1905, 7662, 6181, 5, 6853, 7024], "orig_top_k_doc_id": [153, 154, 4593, 5165, 7023, 5166, 1905, 7662, 736, 7617, 6181, 5, 6853, 7618, 7024]}, {"qid": 2096, "question": "what approaches are compared? in Reference-less Quality Estimation of Text Simplification Systems", "answer": ["MT metrics, Readability metrics and other sentence-level features, Metrics based on the baseline QuEst features, Metrics based on other features"], "top_k_doc_id": [5165, 5166, 3181, 3182, 3184, 3183, 6955, 6957, 6598, 4796, 6599, 3086, 6956, 2225, 2437], "orig_top_k_doc_id": [3181, 3182, 3184, 3183, 6955, 5166, 6957, 6598, 4796, 6599, 5165, 3086, 6956, 2225, 2437]}]}
{"group_id": 14, "group_size": 17, "items": [{"qid": 3216, "question": "what levels of document preprocessing are looked at? in How Document Pre-processing affects Keyphrase Extraction Performance", "answer": ["raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document", "Level 1, Level 2 and Level 3."], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5637, 460, 2411, 5402, 6235, 5632, 1507, 3300], "orig_top_k_doc_id": [5400, 5401, 5402, 6227, 6234, 460, 6226, 6236, 6228, 6235, 2411, 1507, 5637, 3300, 5632]}, {"qid": 3217, "question": "what keyphrase extraction models were reassessed? in How Document Pre-processing affects Keyphrase Extraction Performance", "answer": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5637, 460, 2411, 5402, 6235, 5632, 5636, 461], "orig_top_k_doc_id": [5400, 5401, 6234, 5402, 6227, 6226, 6236, 6228, 5632, 5637, 460, 5636, 6235, 461, 2411]}, {"qid": 3218, "question": "how many articles are in the dataset? in How Document Pre-processing affects Keyphrase Extraction Performance", "answer": ["244", "244 "], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5637, 460, 2411, 5402, 6235, 6715, 4740, 7280], "orig_top_k_doc_id": [5400, 6226, 6227, 6228, 6236, 5401, 6234, 6235, 5402, 5637, 2411, 6715, 4740, 460, 7280]}, {"qid": 3403, "question": "How is keyphrase diversity measured? in Generating Diverse Numbers of Diverse Keyphrases", "answer": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5637, 5632, 5633, 5634, 5635, 5636, 6235, 3227], "orig_top_k_doc_id": [5636, 5637, 5633, 5400, 5632, 6227, 5635, 6226, 6228, 5401, 5634, 6235, 6234, 6236, 3227]}, {"qid": 3846, "question": "Do they report results only on English data? in KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents", "answer": ["No", "No", "Yes"], "top_k_doc_id": [6226, 6227, 6228, 5632, 5635, 5719, 6492, 5400, 5637, 5718, 6234, 6236, 5636, 6955, 6496], "orig_top_k_doc_id": [6226, 6227, 6228, 5635, 5719, 5637, 5636, 5632, 6236, 5400, 6955, 6496, 5718, 6492, 6234]}, {"qid": 3848, "question": "What baseline is used for this task? in KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents", "answer": ["FirstPhrases baseline, MultipartiteRank BIBREF17", " FirstPhrases baseline, MultipartiteRank", "FirstPhrase, MultipartiteRank"], "top_k_doc_id": [6226, 6227, 6228, 5632, 5635, 5719, 6492, 5400, 5637, 5718, 6234, 6236, 3467, 5401, 5044], "orig_top_k_doc_id": [6226, 6227, 6228, 5719, 5632, 5637, 5718, 5635, 5400, 3467, 6234, 5401, 6236, 5044, 6492]}, {"qid": 3849, "question": "What type of nerual keyphrase generation models are trained? in KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents", "answer": ["CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes)", "encoder-decoder model", "CopyRNN BIBREF2"], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5637, 5632, 5633, 5634, 5635, 5636, 5402, 5719], "orig_top_k_doc_id": [6226, 6228, 6227, 5632, 5637, 5635, 5636, 5634, 6234, 5400, 5633, 6236, 5401, 5402, 5719]}, {"qid": 3855, "question": "what dataset did they use? in WikiRank: Improving Keyphrase Extraction Based on Background Knowledge", "answer": ["DUC-2001 dataset BIBREF6, Inspec dataset, NUS Keyphrase Corpus BIBREF10, ICSI Meeting Corpus", "DUC-2001, Inspec ,  NUS Keyphrase Corpus,  ICSI Meeting Corpus ", "DUC-2001 dataset, Inspec dataset, NUS Keyphrase Corpus, ICSI Meeting Corpus"], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5402, 5632, 4154, 6050, 460, 4075, 2086, 5719], "orig_top_k_doc_id": [6236, 6234, 5400, 6226, 6228, 5401, 6227, 4154, 5402, 5632, 460, 6050, 4075, 2086, 5719]}, {"qid": 3857, "question": "what are the state of the art models? in WikiRank: Improving Keyphrase Extraction Based on Background Knowledge", "answer": [" SingleRank and Topical PageRank", "SingleRank and Topical PageRank", "SingleRank, Topical PageRank"], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5402, 5632, 4154, 6050, 4159, 1798, 2176, 6575], "orig_top_k_doc_id": [6234, 6236, 5400, 5401, 6228, 6226, 5632, 6227, 5402, 4154, 4159, 1798, 2176, 6575, 6050]}, {"qid": 3406, "question": "What is the size of the StackExchange dataset? in Generating Diverse Numbers of Diverse Keyphrases", "answer": ["No", "around 332k questions"], "top_k_doc_id": [6226, 6227, 5633, 5635, 5636, 5637, 1670, 5400, 6658, 2063, 3227, 5632, 18, 6505, 609], "orig_top_k_doc_id": [5636, 5635, 5637, 18, 5633, 6505, 5400, 5632, 6226, 6227, 6658, 3227, 2063, 609, 1670]}, {"qid": 3407, "question": "What were the baselines? in Generating Diverse Numbers of Diverse Keyphrases", "answer": ["CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)", "CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*"], "top_k_doc_id": [6226, 6227, 5633, 5635, 5636, 5637, 1670, 5400, 6658, 2063, 3227, 5632, 3572, 6228, 5401], "orig_top_k_doc_id": [5637, 5636, 5635, 5400, 6227, 5633, 3572, 6228, 6658, 5401, 2063, 5632, 3227, 6226, 1670]}, {"qid": 3850, "question": "How do the editors' annotations differ from those in existing datasets? in KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents", "answer": ["Existing datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors", " news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm, provide additional tags which will be used by a taxonomy team to improve the algorithm", "Exper annotators use a smaller, more controlled indexing vocabulary."], "top_k_doc_id": [6226, 6227, 6228, 5632, 5635, 5719, 6492, 5400, 5637, 5718, 6793, 3961, 6493, 5636, 6474], "orig_top_k_doc_id": [6226, 6227, 6228, 6492, 5719, 5632, 5635, 5637, 6793, 3961, 6493, 5718, 5636, 5400, 6474]}, {"qid": 3856, "question": "what was their model's f1 score? in WikiRank: Improving Keyphrase Extraction Based on Background Knowledge", "answer": ["On DUC 27.53, on Inspec 27.01, on ICSI 4.30, and on Nus 9.10", "27.53, 27.01, 4.30 and 9.10 for DUC, Inspec, ICSI and Nus datasets respectively.", "F1 score their system achieved is 27.53, 27.01, 4.30 and 9.10 on DUC, Inspec, ICSI and NUS dataset respectively."], "top_k_doc_id": [6226, 6227, 6228, 5400, 5401, 6234, 6236, 5402, 5632, 1673, 2022, 100, 6738, 5878, 5635], "orig_top_k_doc_id": [6234, 6236, 5400, 5632, 1673, 6226, 5401, 6227, 6228, 5402, 2022, 100, 6738, 5878, 5635]}, {"qid": 3404, "question": "How was the StackExchange dataset collected? in Generating Diverse Numbers of Diverse Keyphrases", "answer": ["they obtained computer science related topics by looking at titles and user-assigned tags", "No"], "top_k_doc_id": [6226, 6227, 5633, 5635, 5636, 5637, 1672, 2910, 5632, 5400, 18, 6505, 5918, 6206, 6290], "orig_top_k_doc_id": [5636, 5635, 5637, 6227, 6226, 5400, 18, 6505, 5633, 5918, 6206, 6290, 5632, 1672, 2910]}, {"qid": 3405, "question": "What does the TextWorld ACG dataset contain? in Generating Diverse Numbers of Diverse Keyphrases", "answer": ["No"], "top_k_doc_id": [6226, 6227, 5633, 5635, 5636, 5637, 1670, 5400, 6658, 560, 3572, 5368, 6235, 3807, 6234], "orig_top_k_doc_id": [5637, 5636, 5635, 560, 6226, 5400, 6658, 3572, 5368, 5633, 6227, 1670, 6235, 3807, 6234]}, {"qid": 3408, "question": "What two metrics are proposed? in Generating Diverse Numbers of Diverse Keyphrases", "answer": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "top_k_doc_id": [6226, 6227, 5633, 5635, 5636, 5637, 1672, 2910, 5632, 3239, 3807, 3572, 1669, 1673, 2064], "orig_top_k_doc_id": [5637, 5636, 5635, 5632, 6227, 3239, 3807, 3572, 5633, 2910, 6226, 1669, 1673, 1672, 2064]}, {"qid": 3847, "question": "Where do the news texts come from? in KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents", "answer": ["online news websites, New York Times, Japan Times", "the New York Times", "New York Times, Japan Times"], "top_k_doc_id": [6226, 6227, 6228, 5632, 5635, 5719, 6492, 5998, 6236, 6235, 6234, 6433, 6955, 2011, 3157], "orig_top_k_doc_id": [6226, 6227, 6228, 5632, 5719, 5998, 6492, 6236, 6235, 6234, 6433, 6955, 2011, 3157, 5635]}]}
{"group_id": 15, "group_size": 17, "items": [{"qid": 3711, "question": "What word embeddings are used? in Automatic Argumentative-Zoning Using Word2vec", "answer": ["INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 )", "Sentiment-Specific Word Embedding, word2vec", "word2vec, Sentiment-Specific Word Embedding"], "top_k_doc_id": [5376, 977, 6419, 5375, 6053, 6054, 6055, 6423, 7185, 5386, 978, 979, 980, 6422, 6417], "orig_top_k_doc_id": [6419, 6055, 6054, 6053, 5375, 5376, 6423, 977, 7185, 978, 979, 6422, 980, 5386, 6417]}, {"qid": 3714, "question": "What is argumentative zoning? in Automatic Argumentative-Zoning Using Word2vec", "answer": [" Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences", "process of assigning rhetorical status to the extracted sentences", "a process of assigning rhetorical status to the extracted sentences"], "top_k_doc_id": [5376, 977, 6419, 5375, 6053, 6054, 6055, 6423, 7185, 5386, 978, 979, 980, 5377, 742], "orig_top_k_doc_id": [6419, 5375, 6055, 5376, 6054, 6053, 977, 6423, 7185, 978, 980, 5386, 979, 5377, 742]}, {"qid": 786, "question": "How do they demonstrate the robustness of their results? in Dissecting Content and Context in Argumentative Relation Analysis", "answer": ["performances of a purely content-based model naturally stays stable"], "top_k_doc_id": [5376, 977, 6419, 978, 979, 980, 981, 5380, 5386, 3143, 742, 4942, 5351, 6616, 4810], "orig_top_k_doc_id": [980, 981, 977, 979, 978, 4942, 6419, 3143, 5376, 5380, 5386, 5351, 6616, 4810, 742]}, {"qid": 789, "question": "How are elementary argumentative units defined? in Dissecting Content and Context in Argumentative Relation Analysis", "answer": ["No"], "top_k_doc_id": [5376, 977, 6419, 978, 979, 980, 981, 5380, 5386, 3143, 742, 238, 7185, 5385, 5375], "orig_top_k_doc_id": [977, 980, 979, 978, 981, 5376, 3143, 6419, 5386, 238, 5380, 7185, 5385, 742, 5375]}, {"qid": 3201, "question": "Do they report results only on English data? in Argumentation Mining in User-Generated Web Discourse", "answer": ["Yes", "Yes"], "top_k_doc_id": [5376, 5373, 5374, 5375, 5377, 5378, 5382, 5385, 5386, 5390, 980, 5379, 5380, 5387, 5384], "orig_top_k_doc_id": [5373, 5390, 5378, 5374, 5376, 5379, 5386, 5375, 5377, 5382, 5385, 5387, 5384, 5380, 980]}, {"qid": 3203, "question": "Which machine learning methods are used in experiments? in Argumentation Mining in User-Generated Web Discourse", "answer": ["Structural Support Vector Machine", "SVMhmm "], "top_k_doc_id": [5376, 5373, 5374, 5375, 5377, 5378, 5382, 5385, 5386, 5390, 980, 5379, 5380, 5387, 977], "orig_top_k_doc_id": [5373, 5390, 5374, 5377, 5378, 5376, 5375, 5386, 5385, 5387, 5379, 5382, 977, 5380, 980]}, {"qid": 3713, "question": "How are the sentence embeddings generated? in Automatic Argumentative-Zoning Using Word2vec", "answer": ["sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors", "Averaging the vectors of the words in a sentence, directly learning paragraph vectors using PV-DM, taking average of the SSWE of the words in a sentence.", " average the vectors in word sequence, training paragraph vectors, Sentiment-Specific Word Embedding"], "top_k_doc_id": [5376, 977, 6419, 5375, 6053, 6054, 6055, 6423, 7185, 5386, 978, 5387, 3143, 6422, 5380], "orig_top_k_doc_id": [6055, 6419, 6054, 6053, 5376, 5375, 977, 6423, 978, 5387, 7185, 3143, 6422, 5386, 5380]}, {"qid": 785, "question": "Do they report results only on English data? in Dissecting Content and Context in Argumentative Relation Analysis", "answer": ["No"], "top_k_doc_id": [5376, 977, 6419, 978, 979, 980, 981, 5380, 5386, 3143, 5389, 5387, 1726, 4942, 5379], "orig_top_k_doc_id": [977, 980, 979, 981, 978, 5376, 6419, 5380, 3143, 5389, 5387, 1726, 5386, 4942, 5379]}, {"qid": 3205, "question": "What argumentation phenomena encounter in actual data are now accounted for by this work? in Argumentation Mining in User-Generated Web Discourse", "answer": ["No"], "top_k_doc_id": [5376, 5373, 5374, 5375, 5377, 5378, 5382, 5385, 5386, 5390, 980, 5379, 5380, 5384, 977], "orig_top_k_doc_id": [5373, 5374, 5378, 5390, 5386, 5385, 5375, 5379, 5376, 5384, 5377, 977, 5382, 5380, 980]}, {"qid": 3709, "question": "What metric is considered? in Automatic Argumentative-Zoning Using Word2vec", "answer": ["Precision, recall and F-measure.", "precision, recall, F-measure", "precision, recall and F-measure"], "top_k_doc_id": [5376, 977, 6419, 5375, 6053, 6054, 6055, 6423, 7185, 5386, 980, 6417, 242, 6858, 238], "orig_top_k_doc_id": [6419, 5375, 5376, 6055, 6054, 6053, 6423, 980, 977, 6417, 5386, 7185, 242, 6858, 238]}, {"qid": 788, "question": "How are the EAU text spans annotated? in Dissecting Content and Context in Argumentative Relation Analysis", "answer": ["Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level."], "top_k_doc_id": [5376, 977, 6419, 978, 979, 980, 981, 5380, 5386, 3143, 1906, 1907, 3623, 7639, 5381], "orig_top_k_doc_id": [980, 981, 979, 978, 977, 6419, 5376, 5380, 1906, 1907, 5386, 3143, 3623, 7639, 5381]}, {"qid": 3204, "question": "How is the data in the new corpus come sourced? in Argumentation Mining in User-Generated Web Discourse", "answer": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "top_k_doc_id": [5376, 5373, 5374, 5375, 5377, 5378, 5382, 5385, 5386, 5390, 980, 5379, 977, 2409, 3143], "orig_top_k_doc_id": [5373, 5374, 5378, 5390, 5376, 5375, 5386, 5385, 5379, 5377, 5382, 977, 980, 2409, 3143]}, {"qid": 3712, "question": "Do they annotate their own dataset? in Automatic Argumentative-Zoning Using Word2vec", "answer": ["No", "No", "No"], "top_k_doc_id": [5376, 977, 6419, 5375, 6053, 6054, 6055, 6423, 7185, 5380, 6858, 5377, 6417, 3312, 1645], "orig_top_k_doc_id": [6055, 6419, 5376, 6054, 6053, 5375, 5380, 6423, 6858, 5377, 6417, 977, 3312, 1645, 7185]}, {"qid": 787, "question": "What baseline and classification systems are used in experiments? in Dissecting Content and Context in Argumentative Relation Analysis", "answer": ["BIBREF13, majority baseline"], "top_k_doc_id": [5376, 977, 6419, 978, 979, 980, 981, 5380, 5386, 329, 1920, 5389, 746, 7185, 236], "orig_top_k_doc_id": [980, 977, 979, 981, 978, 5376, 329, 6419, 5380, 1920, 5389, 746, 5386, 7185, 236]}, {"qid": 3206, "question": "What challenges do different registers and domains pose to this task? in Argumentation Mining in User-Generated Web Discourse", "answer": ["linguistic variability"], "top_k_doc_id": [5376, 5373, 5374, 5375, 5377, 5378, 5382, 5385, 5386, 5390, 980, 5379, 5383, 5387, 3145], "orig_top_k_doc_id": [5373, 5378, 5374, 5382, 5379, 5390, 5375, 5386, 5376, 5385, 5377, 5383, 5387, 3145, 980]}, {"qid": 3710, "question": "What hand-crafted features are used? in Automatic Argumentative-Zoning Using Word2vec", "answer": ["position of sentence, sentence length, tense, qualifying adjectives, meta-discourse features", " sentences with their rhetorical status ", "No"], "top_k_doc_id": [5376, 977, 6419, 5375, 6053, 6054, 6055, 6423, 5377, 5417, 2160, 5407, 3200, 5406, 2161], "orig_top_k_doc_id": [6055, 6053, 6419, 5375, 5376, 6054, 977, 5377, 5417, 2160, 5407, 3200, 5406, 2161, 6423]}, {"qid": 3202, "question": "What argument components do the ML methods aim to identify? in Argumentation Mining in User-Generated Web Discourse", "answer": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "top_k_doc_id": [5376, 5373, 5374, 5375, 5377, 5378, 5382, 5385, 5386, 5390, 5384, 5387, 5380, 5388, 3145], "orig_top_k_doc_id": [5373, 5374, 5386, 5390, 5375, 5376, 5385, 5378, 5384, 5382, 5387, 5377, 5380, 5388, 3145]}]}
{"group_id": 16, "group_size": 17, "items": [{"qid": 4506, "question": "What empirical evaluation was used? in Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues", "answer": ["coherence metrics", "Empirical  evaluation  has done using 10 fold cross-validation considering semantic representation with BERT and  measuring   differences between fake news and satire  using coherence metric."], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 1494, 1495, 3945, 7049, 7488, 1329, 6457, 3861, 3276, 34], "orig_top_k_doc_id": [7049, 7048, 7047, 7545, 7488, 1495, 1494, 3944, 3945, 1329, 3861, 3276, 7121, 34, 6457]}, {"qid": 4507, "question": "What is the baseline? in Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues", "answer": ["Naive Bayes Multinomial algorithm", "model using the Naive Bayes Multinomial algorithm"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 1494, 1495, 3945, 7049, 7488, 1329, 6457, 3861, 3276, 7546], "orig_top_k_doc_id": [7049, 7047, 7048, 7545, 7488, 1495, 3944, 1494, 3945, 6457, 3861, 3276, 7121, 1329, 7546]}, {"qid": 4509, "question": "What contextual language model is used? in Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues", "answer": ["BERT", "BERT "], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 1494, 1495, 3945, 7049, 7488, 1329, 6457, 3861, 3277, 1500], "orig_top_k_doc_id": [7049, 7047, 7048, 7545, 7488, 1495, 3944, 1494, 3945, 1329, 3861, 6457, 7121, 3277, 1500]}, {"qid": 4807, "question": "Did they release their dataset? in Reverse-Engineering Satire, or\"Paper on Computational Humor Accepted Despite Making Serious Advances\"", "answer": ["Yes", "Yes"], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 7047, 7049, 7486, 1501, 3945, 7048, 7262, 5272, 7483, 1500], "orig_top_k_doc_id": [7488, 7486, 7487, 7481, 7047, 7482, 7049, 7262, 7048, 3944, 5272, 3945, 1501, 1500, 7483]}, {"qid": 4809, "question": "Did they use The Onion as their dataset? in Reverse-Engineering Satire, or\"Paper on Computational Humor Accepted Despite Making Serious Advances\"", "answer": ["Yes", "Yes"], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 7047, 7049, 7486, 1501, 3945, 7048, 7262, 5272, 7483, 7484], "orig_top_k_doc_id": [7486, 7481, 7488, 3944, 7487, 3945, 7047, 7483, 7484, 7482, 7049, 7262, 7048, 1501, 5272]}, {"qid": 4859, "question": "What out of domain scenarios did they evaluate on? in Do Sentence Interactions Matter? Leveraging Sentence Level Representations for Fake News Classification", "answer": ["In 2-way classification they used LUN-train for training, LUN-test for development and the entire SLN dataset for testing. In 4-way classification they used LUN-train for training and development and LUN-test for testing.", "entire SLN dataset,  LUN-test as our out of domain test set"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 3273, 5321, 6104, 6663, 6817, 7120, 7546, 2160, 4963, 4834], "orig_top_k_doc_id": [3273, 6104, 7545, 7546, 6663, 7048, 7047, 5321, 6817, 3944, 7121, 7120, 2160, 4963, 4834]}, {"qid": 4861, "question": "Which datasets did they use? in Do Sentence Interactions Matter? Leveraging Sentence Level Representations for Fake News Classification", "answer": ["Satirical and Legitimate News Database, Random Political News Dataset, Labeled Unreliable News Dataset", "Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10, LUN: Labeled Unreliable News Dataset BIBREF0"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 3273, 5321, 6104, 6663, 6817, 7120, 7546, 3926, 3928, 3860], "orig_top_k_doc_id": [6663, 7545, 3273, 5321, 7121, 6104, 7048, 6817, 3926, 7047, 3944, 3928, 3860, 7546, 7120]}, {"qid": 2413, "question": "How large is the dataset? in Satirical News Detection with Semantic Feature Extraction and Game-theoretic Rough Sets", "answer": ["8757 news records"], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 3945, 3946, 3947, 3948, 7121, 7483, 7545, 5135, 7242, 7047], "orig_top_k_doc_id": [3944, 3947, 3946, 3945, 3948, 7488, 7481, 7483, 7482, 7121, 7545, 7487, 7242, 7047, 5135]}, {"qid": 2414, "question": "What features do they extract? in Satirical News Detection with Semantic Feature Extraction and Game-theoretic Rough Sets", "answer": ["Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF"], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 3945, 3946, 3947, 3948, 7121, 7483, 7545, 5135, 7097, 4019], "orig_top_k_doc_id": [3944, 3947, 3946, 3945, 3948, 7482, 7483, 7488, 7481, 7121, 5135, 7545, 7487, 7097, 4019]}, {"qid": 4508, "question": "Which linguistic features are used? in Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues", "answer": ["First person singular pronoun incidence\nSentence length, number of words, \nEstimates of hypernymy for nouns \n...\nAgentless passive voice density,\nAverage word frequency for content words ,\nAdverb incidence\n\n...", "Coh-Metrix indices"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 1494, 1495, 3945, 7049, 7488, 1329, 6457, 6456, 5783, 6663], "orig_top_k_doc_id": [7049, 7048, 7047, 7545, 1495, 3944, 7488, 3945, 1494, 6457, 1329, 6456, 5783, 7121, 6663]}, {"qid": 4806, "question": "Where can I access the dataset? in Reverse-Engineering Satire, or\"Paper on Computational Humor Accepted Despite Making Serious Advances\"", "answer": ["BIBREF9", "BIBREF9"], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 7047, 7049, 7486, 1501, 3945, 7048, 7262, 1500, 7485, 7484], "orig_top_k_doc_id": [7488, 7486, 7487, 7481, 7482, 7047, 7049, 7048, 7262, 3944, 1501, 1500, 3945, 7485, 7484]}, {"qid": 4858, "question": "What other evaluation metrics are reported? in Do Sentence Interactions Matter? Leveraging Sentence Level Representations for Fake News Classification", "answer": ["Precision and recall for 2-way classification and F1 for 4-way classification.", "Macro-averaged F1-score, macro-averaged precision, macro-averaged recall"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 3273, 5321, 6104, 6663, 3926, 7049, 5325, 5784, 3945, 135], "orig_top_k_doc_id": [7048, 6663, 7049, 3926, 3273, 5325, 3944, 7047, 5321, 7545, 6104, 7121, 5784, 3945, 135]}, {"qid": 4860, "question": "What was their state of the art accuracy score? in Do Sentence Interactions Matter? Leveraging Sentence Level Representations for Fake News Classification", "answer": ["In 2-way classification precision score was 88% and recall 82%. In 4-way classification on LUN-dev F1-score was 91% and on LUN-test F1-score was 65%.", "accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 3273, 5321, 6104, 6663, 3926, 7049, 3860, 713, 6101, 1329], "orig_top_k_doc_id": [3273, 6663, 7048, 3944, 3860, 713, 7545, 7121, 6104, 3926, 7049, 5321, 7047, 6101, 1329]}, {"qid": 4862, "question": "What are the neural baselines mentioned? in Do Sentence Interactions Matter? Leveraging Sentence Level Representations for Fake News Classification", "answer": ["CNN, LSTM, BERT", "CNN, LSTM, BERT"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 3273, 5321, 6104, 6663, 6817, 3928, 6260, 6666, 2160, 6730], "orig_top_k_doc_id": [3273, 6663, 7545, 7048, 6104, 3944, 3928, 6817, 5321, 7047, 6260, 6666, 2160, 6730, 7121]}, {"qid": 2412, "question": "How much improvement do they get? in Satirical News Detection with Semantic Feature Extraction and Game-theoretic Rough Sets", "answer": ["Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak."], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 3945, 3946, 3947, 3948, 7121, 7483, 7545, 4019, 6743, 7546], "orig_top_k_doc_id": [3944, 3947, 3946, 3945, 3948, 7481, 7483, 7488, 7482, 7545, 7487, 7121, 4019, 6743, 7546]}, {"qid": 4505, "question": "What nuances between fake news and satire were discovered? in Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues", "answer": ["semantic and linguistic differences between,  satire articles are more sophisticated, or less easy to read, than fake news articles", "satire articles are more sophisticated, or less easy to read, than fake news articles"], "top_k_doc_id": [3944, 7047, 7048, 7121, 7545, 1494, 1495, 3945, 7049, 7488, 3861, 7546, 7487, 3277, 1501], "orig_top_k_doc_id": [7049, 7047, 7048, 7545, 7488, 3944, 1494, 1495, 3945, 3861, 7121, 7546, 7487, 3277, 1501]}, {"qid": 4808, "question": "Did they use Amazon Mechanical Turk to collect data? in Reverse-Engineering Satire, or\"Paper on Computational Humor Accepted Despite Making Serious Advances\"", "answer": ["No", "No"], "top_k_doc_id": [3944, 7481, 7482, 7487, 7488, 7047, 7049, 7486, 2169, 6830, 5978, 6620, 6074, 171, 3872], "orig_top_k_doc_id": [7486, 7488, 7487, 7481, 2169, 3944, 6830, 5978, 6620, 7047, 6074, 7482, 171, 7049, 3872]}]}
{"group_id": 17, "group_size": 16, "items": [{"qid": 290, "question": "Do the authors offer any hypothesis about why the dense mode outperformed the sparse one? in An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages", "answer": ["Yes"], "top_k_doc_id": [68, 349, 350, 4475, 4477, 351, 6791, 3207, 5305, 4476, 3112, 7669, 4695, 3208, 2154], "orig_top_k_doc_id": [351, 349, 68, 4475, 4476, 350, 4477, 3207, 6791, 5305, 3112, 7669, 4695, 3208, 2154]}, {"qid": 291, "question": "What evaluation is conducted? in An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages", "answer": ["Word Sense Induction & Disambiguation"], "top_k_doc_id": [68, 349, 350, 4475, 4477, 351, 6791, 73, 4695, 4738, 4014, 5757, 69, 4476, 72], "orig_top_k_doc_id": [68, 351, 349, 4695, 4477, 6791, 73, 4738, 4014, 5757, 350, 69, 4476, 4475, 72]}, {"qid": 292, "question": "Which corpus of synsets are used? in An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages", "answer": ["Wiktionary"], "top_k_doc_id": [68, 349, 350, 4475, 4477, 351, 6791, 73, 4695, 6787, 6788, 1096, 4734, 4341, 4342], "orig_top_k_doc_id": [349, 351, 6791, 68, 350, 6787, 4477, 6788, 1096, 4734, 4341, 73, 4342, 4475, 4695]}, {"qid": 293, "question": "What measure of semantic similarity is used? in An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages", "answer": ["cosine similarity"], "top_k_doc_id": [68, 349, 350, 4475, 4477, 351, 6791, 3207, 5305, 69, 5716, 73, 986, 6787, 4737], "orig_top_k_doc_id": [68, 349, 351, 6791, 350, 69, 4475, 5716, 4477, 73, 5305, 986, 6787, 3207, 4737]}, {"qid": 66, "question": "Is the method described in this work a clustering-based method? in Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "answer": ["Yes", "Yes"], "top_k_doc_id": [68, 349, 70, 71, 72, 73, 4476, 6791, 69, 4475, 350, 7490, 4860, 351, 6787], "orig_top_k_doc_id": [68, 73, 71, 69, 72, 70, 350, 349, 6791, 4475, 7490, 4860, 351, 4476, 6787]}, {"qid": 67, "question": "How are the different senses annotated/labeled?  in Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "answer": ["The contexts are manually labelled with WordNet senses of the target words"], "top_k_doc_id": [68, 349, 70, 71, 72, 73, 4476, 6791, 69, 4475, 6331, 6229, 3371, 5344, 5759], "orig_top_k_doc_id": [68, 73, 71, 6331, 4475, 4476, 72, 6791, 69, 70, 349, 6229, 3371, 5344, 5759]}, {"qid": 68, "question": "Was any extrinsic evaluation carried out? in Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "answer": ["Yes"], "top_k_doc_id": [68, 349, 70, 71, 72, 73, 4476, 6791, 351, 5341, 1067, 4932, 350, 987, 3786], "orig_top_k_doc_id": [68, 73, 71, 351, 5341, 72, 349, 1067, 4476, 4932, 350, 70, 6791, 987, 3786]}, {"qid": 2558, "question": "Do they use a neural model for their task? in Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation", "answer": ["No"], "top_k_doc_id": [68, 349, 350, 4475, 4477, 3207, 4476, 4734, 73, 2023, 5305, 3208, 7687, 5341, 7688], "orig_top_k_doc_id": [4475, 4477, 68, 3207, 4476, 349, 4734, 73, 2023, 5305, 3208, 7687, 5341, 7688, 350]}, {"qid": 3357, "question": "What is the state of the art system mentioned? in GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge", "answer": ["Two knowledge-based systems,\ntwo traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system."], "top_k_doc_id": [68, 69, 5586, 4341, 4700, 4837, 5587, 5588, 945, 1560, 2746, 4475, 3371, 4476, 6787], "orig_top_k_doc_id": [5586, 68, 5587, 5588, 69, 4475, 3371, 945, 4476, 6787, 4341, 4837, 1560, 4700, 2746]}, {"qid": 3358, "question": "Do they incoprorate WordNet into the model? in GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge", "answer": ["construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem", "construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word"], "top_k_doc_id": [68, 69, 5586, 4341, 4700, 4837, 5587, 5588, 73, 349, 2241, 6787, 6791, 4349, 4734], "orig_top_k_doc_id": [5586, 5587, 68, 5588, 69, 4700, 4837, 4341, 6791, 6787, 2241, 4349, 349, 4734, 73]}, {"qid": 3359, "question": "Is SemCor3.0 reflective of English language data in general? in GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge", "answer": ["Yes", "No"], "top_k_doc_id": [68, 69, 5586, 4341, 4700, 4837, 5587, 5588, 73, 349, 2241, 6787, 6791, 5757, 5341], "orig_top_k_doc_id": [5587, 5586, 68, 5588, 69, 4837, 4700, 2241, 4341, 349, 6787, 5757, 6791, 5341, 73]}, {"qid": 3360, "question": "Do they use large or small BERT? in GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge", "answer": ["small BERT", "small BERT"], "top_k_doc_id": [68, 69, 5586, 4341, 4700, 4837, 5587, 5588, 945, 1560, 2746, 2241, 1819, 947, 6791], "orig_top_k_doc_id": [5586, 5587, 68, 5588, 69, 2241, 1819, 947, 4700, 945, 1560, 6791, 4341, 4837, 2746]}, {"qid": 1596, "question": "Which language(s) are found in the WSD datasets? in Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations", "answer": [" WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese"], "top_k_doc_id": [68, 69, 5586, 351, 2238, 2239, 2241, 2985, 5341, 7490, 349, 2240, 2242, 4475, 4476], "orig_top_k_doc_id": [2238, 68, 2241, 5586, 7490, 349, 2240, 2242, 5341, 2239, 4475, 351, 4476, 69, 2985]}, {"qid": 1597, "question": "What datasets are used for testing? in Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations", "answer": ["Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15), OntoNotes Release 5.0"], "top_k_doc_id": [68, 69, 5586, 351, 2238, 2239, 2241, 2985, 5341, 5757, 1560, 5759, 3371, 4603, 5305], "orig_top_k_doc_id": [5757, 2238, 68, 2241, 1560, 5341, 5759, 3371, 5586, 4603, 69, 2239, 351, 2985, 5305]}, {"qid": 2703, "question": "How do they deal with unknown distribution senses? in How big is big enough? Unsupervised word sense disambiguation using a very large corpus", "answer": ["The N\u00e4ive-Bayes classifier is corrected so it is not biased to most frequent classes", "Bayesian classifier has been modified, removing the bias towards frequent labels in the training data"], "top_k_doc_id": [68, 73, 2984, 4475, 4476, 4477, 4734, 4738, 4737, 4735, 5759, 3880, 6791, 6229, 72], "orig_top_k_doc_id": [4738, 4734, 4476, 68, 4475, 4737, 4735, 5759, 73, 2984, 4477, 3880, 6791, 6229, 72]}, {"qid": 3361, "question": "How does the neural network architecture accomodate an unknown amount of senses per word? in GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge", "answer": ["converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge"], "top_k_doc_id": [68, 73, 2984, 4475, 4476, 4477, 4734, 4738, 5586, 5587, 5588, 69, 2985, 6787, 2241], "orig_top_k_doc_id": [5586, 68, 5587, 5588, 69, 4476, 4475, 2985, 6787, 73, 4734, 4738, 2241, 4477, 2984]}]}
{"group_id": 18, "group_size": 16, "items": [{"qid": 646, "question": "Do the other multilingual baselines make use of the same amount of training data? in Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [4568, 4571, 4030, 6060, 5621, 5868, 783, 786, 6031, 6034, 7409, 7410, 247, 4027, 6036], "orig_top_k_doc_id": [4030, 7409, 6060, 4568, 786, 6036, 6034, 4571, 5868, 7410, 247, 783, 4027, 6031, 5621]}, {"qid": 648, "question": "What data were they used to train the multilingual encoder? in Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation", "answer": ["WMT 2014 En-Fr parallel corpus"], "top_k_doc_id": [4568, 4571, 4030, 6060, 5621, 5868, 783, 786, 6031, 6034, 7409, 7410, 247, 4027, 7339], "orig_top_k_doc_id": [6034, 6060, 783, 4568, 5868, 4030, 7339, 7410, 6031, 247, 5621, 786, 4027, 7409, 4571]}, {"qid": 647, "question": "How big is the impact of training data size on the performance of the multilingual encoder? in Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [4568, 4571, 4030, 6060, 5621, 5868, 783, 786, 6031, 6034, 7409, 7410, 7339, 4031, 6036], "orig_top_k_doc_id": [4030, 4571, 6060, 6034, 5868, 7339, 6031, 7409, 5621, 786, 4568, 783, 7410, 4031, 6036]}, {"qid": 1371, "question": "What multilingual parallel data is used for training proposed model? in Zero-Shot Paraphrase Generation with Multilingual Language Models", "answer": ["MultiUN BIBREF20, OpenSubtitles BIBREF21"], "top_k_doc_id": [4568, 4571, 4030, 6060, 5621, 5868, 1884, 1886, 1887, 1888, 4027, 7339, 786, 247, 783], "orig_top_k_doc_id": [1887, 1884, 7339, 5868, 4027, 786, 1886, 6060, 247, 783, 5621, 4568, 4571, 4030, 1888]}, {"qid": 1372, "question": "How much better are results of proposed model compared to pivoting method? in Zero-Shot Paraphrase Generation with Multilingual Language Models", "answer": ["our method outperforms the baseline in both relevance and fluency significantly."], "top_k_doc_id": [4568, 4571, 4030, 6060, 5621, 5868, 1884, 1886, 1887, 1888, 4027, 7339, 2894, 4031, 1885], "orig_top_k_doc_id": [1887, 1884, 1888, 4571, 4027, 1886, 4568, 4030, 2894, 4031, 6060, 1885, 7339, 5621, 5868]}, {"qid": 2593, "question": "which multilingual approaches do they compare with? in Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "answer": ["BIBREF19, BIBREF20", "multilingual NMT (MNMT) BIBREF19"], "top_k_doc_id": [4568, 4571, 4030, 6060, 5621, 5868, 783, 786, 6031, 6034, 7409, 4027, 4031, 4752, 4569], "orig_top_k_doc_id": [4571, 4568, 4030, 6034, 5868, 6060, 4027, 5621, 4031, 4752, 783, 786, 4569, 6031, 7409]}, {"qid": 2595, "question": "which datasets did they experiment with? in Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "answer": ["Europarl, MultiUN", "Europarl BIBREF31, MultiUN BIBREF32"], "top_k_doc_id": [4568, 4571, 4030, 6060, 4027, 4031, 4569, 4572, 6034, 6063, 4752, 5621, 5868, 6033, 783], "orig_top_k_doc_id": [4568, 5868, 6034, 4752, 6060, 4571, 4572, 4569, 4030, 5621, 4031, 4027, 6033, 6063, 783]}, {"qid": 2596, "question": "what language pairs are explored? in Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "answer": ["De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru", "French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation"], "top_k_doc_id": [4568, 4571, 4030, 6060, 4027, 4031, 4569, 4572, 6034, 6063, 4752, 5621, 5868, 6870, 247], "orig_top_k_doc_id": [4569, 4568, 6034, 5868, 6060, 4571, 4031, 4027, 4572, 4030, 6063, 6870, 4752, 247, 5621]}, {"qid": 2446, "question": "Are experiments performed with any other pair of languages, how did proposed method perform compared to other models? in Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages", "answer": ["No"], "top_k_doc_id": [4568, 4571, 4030, 4027, 4028, 4029, 4031, 4569, 4570, 5026, 3604, 7339, 4572, 626, 627], "orig_top_k_doc_id": [4030, 4568, 5026, 4027, 4570, 4031, 4571, 4569, 3604, 4572, 4028, 7339, 4029, 626, 627]}, {"qid": 2448, "question": "What are multilingual models that were outperformed in performed experiment? in Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages", "answer": ["Direct source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target, Multilingual: A single, shared NMT model for multiple translation directions, Many-to-many: Trained for all possible directions among source, target, and pivot languages, Many-to-one: Trained for only the directions to target language"], "top_k_doc_id": [4568, 4571, 4030, 4027, 4028, 4029, 4031, 4569, 4570, 5026, 3604, 7339, 1777, 5869, 6034], "orig_top_k_doc_id": [4027, 5026, 4568, 4030, 4031, 4029, 7339, 4571, 4570, 1777, 5869, 4569, 3604, 6034, 4028]}, {"qid": 2594, "question": "what are the pivot-based baselines? in Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "answer": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "top_k_doc_id": [4568, 4571, 4030, 6060, 4027, 4031, 4569, 4572, 6034, 6063, 4570, 4028, 6064, 7339, 2839], "orig_top_k_doc_id": [4571, 4568, 4030, 4031, 4027, 4570, 4569, 4572, 4028, 6063, 6034, 6064, 7339, 6060, 2839]}, {"qid": 2716, "question": "What is the model performance on target language reading comprehension? in Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model", "answer": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "top_k_doc_id": [4568, 4571, 247, 2837, 2839, 2840, 4752, 6034, 2836, 4572, 4415, 4570, 2838, 4414, 4569], "orig_top_k_doc_id": [2840, 2837, 2839, 2836, 4752, 4415, 2838, 4414, 4571, 247, 4568, 6034, 4572, 4569, 4570]}, {"qid": 2718, "question": "What model is used as a baseline?   in Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model", "answer": ["pre-trained multi-BERT", "QANet , BIBREF14,  fine-tuned a BERT model"], "top_k_doc_id": [4568, 4571, 247, 2837, 2839, 2840, 4752, 6034, 2836, 4572, 4415, 4570, 4030, 6870, 4031], "orig_top_k_doc_id": [4752, 2839, 2837, 2840, 4571, 247, 2836, 4572, 4415, 4030, 6034, 4568, 6870, 4031, 4570]}, {"qid": 2447, "question": "Is pivot language used in experiments English or some other language? in Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages", "answer": ["Yes"], "top_k_doc_id": [4568, 4571, 4030, 4027, 4028, 4029, 4031, 4569, 4570, 5026, 4572, 5027, 1721, 6424, 1884], "orig_top_k_doc_id": [4027, 4568, 4031, 4570, 4028, 4029, 4030, 5026, 4569, 4572, 4571, 5027, 1721, 6424, 1884]}, {"qid": 2717, "question": "What source-target language pairs were used in this work?  in Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model", "answer": ["En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean", "English , Chinese", "English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate"], "top_k_doc_id": [4568, 4571, 247, 2837, 2839, 2840, 4752, 6034, 2836, 4572, 4031, 4030, 4569, 6031, 2838], "orig_top_k_doc_id": [4752, 2840, 2839, 2837, 4031, 4568, 4571, 4572, 2836, 247, 4030, 4569, 6034, 6031, 2838]}, {"qid": 2719, "question": "what does the model learn in zero-shot setting? in Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model", "answer": ["we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged"], "top_k_doc_id": [4568, 4571, 247, 2837, 2839, 2840, 4752, 6034, 4569, 6870, 4031, 6063, 4570, 786, 6033], "orig_top_k_doc_id": [4752, 2839, 4568, 4571, 2837, 247, 2840, 4569, 6034, 6870, 4031, 6063, 4570, 786, 6033]}]}
{"group_id": 19, "group_size": 16, "items": [{"qid": 1807, "question": "What accuracy is achieved by the speech recognition system? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["Accuracy not available: WER results are reported 42.6 German, 35.9 English"], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 2450, 4862, 659, 844, 3126, 3286, 6584, 1790, 3807], "orig_top_k_doc_id": [2630, 2633, 2634, 2631, 2632, 5385, 3286, 2450, 659, 6584, 844, 4862, 3126, 1790, 3807]}, {"qid": 1808, "question": "How is the speech recognition system evaluated? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["Speech recognition system is evaluated using WER metric."], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 2450, 4862, 659, 844, 3126, 2958, 3173, 1286, 7162], "orig_top_k_doc_id": [2630, 2633, 2631, 2634, 844, 2632, 659, 2958, 5385, 3173, 2450, 3126, 4862, 1286, 7162]}, {"qid": 3996, "question": "what classifiers did they train? in A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity", "answer": ["a multinomial logistic regression model with ridge estimator, a multilayer perceptron, a support vector machine learner, Sequential Minimal Optimization (SMO), a decision tree (J48)", "(1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), (4) a decision tree (J48)", "multinomial logistic regression model with ridge estimator, multilayer perceptron, support vector machine learner, Sequential Minimal Optimization, decision tree"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3883, 5589, 7638, 6005, 2270, 2721, 4902, 59, 2969, 3319, 3574], "orig_top_k_doc_id": [7638, 4593, 6451, 4902, 59, 6005, 2969, 4595, 5589, 3883, 3319, 2721, 2270, 2630, 3574]}, {"qid": 4000, "question": "what is the state of the art in English? in A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity", "answer": ["BIBREF9 , BIBREF12", "BIBREF9, BIBREF12"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3883, 5589, 7638, 6005, 2270, 2721, 2720, 1329, 2547, 5590, 7661], "orig_top_k_doc_id": [4593, 6451, 7638, 2630, 5589, 2720, 2270, 6005, 1329, 2721, 4595, 2547, 5590, 7661, 3883]}, {"qid": 1804, "question": "Are any of the utterances ungrammatical? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["Yes"], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 594, 2958, 5209, 5273, 6621, 7162, 7327, 6584, 6321], "orig_top_k_doc_id": [2630, 2631, 2633, 2634, 2632, 5385, 7327, 6584, 6321, 5209, 6621, 2958, 7162, 594, 5273]}, {"qid": 1809, "question": "How many of the utterances are transcribed? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned)"], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 2450, 4862, 230, 6584, 3402, 4149, 7156, 7162, 594], "orig_top_k_doc_id": [2630, 2633, 2631, 2634, 2632, 2450, 5385, 230, 6584, 3402, 4149, 7156, 7162, 4862, 594]}, {"qid": 1810, "question": "How many utterances are in the corpus? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["Total number of utterances available is: 70607 (37344 ENG + 33263 GER)"], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 594, 2958, 5209, 5273, 6621, 7162, 7156, 1286, 659], "orig_top_k_doc_id": [2630, 2631, 2633, 2634, 2632, 5385, 5209, 7162, 7156, 2958, 594, 5273, 6621, 1286, 659]}, {"qid": 3998, "question": "what combination of features helped improve the classification? in A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity", "answer": ["Using all the 61 features helped them improve the classification", "a combination of all features for the document level", "length-based, lexical, morphological, syntactic and semantic features"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3883, 5589, 7638, 6005, 3319, 6454, 2720, 5327, 5590, 4865, 59], "orig_top_k_doc_id": [3883, 7638, 4593, 6451, 5589, 3319, 6454, 4595, 2720, 5327, 5590, 6005, 4865, 2630, 59]}, {"qid": 1805, "question": "How is the proficiency score calculated? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert."], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 861, 2720, 2721, 3807, 5209, 5379, 6451, 6454, 659], "orig_top_k_doc_id": [2630, 2631, 2634, 2633, 5209, 2632, 6451, 2720, 5385, 5379, 2721, 6454, 659, 861, 3807]}, {"qid": 1806, "question": "What proficiency indicators are used to the score the utterances? in TLT-school: a Corpus of Non Native Children Speech", "answer": ["6 indicators:\n- lexical richness\n- pronunciation and fluency\n- syntactical correctness\n- fulfillment of delivery\n- coherence and cohesion\n- communicative, descriptive, narrative skills"], "top_k_doc_id": [2630, 2631, 2632, 2633, 2634, 5385, 861, 2720, 2721, 3807, 5209, 5379, 6451, 6454, 7162], "orig_top_k_doc_id": [2630, 2631, 2634, 2633, 2632, 5209, 6451, 5385, 2721, 2720, 3807, 5379, 6454, 861, 7162]}, {"qid": 3997, "question": "what dataset did they use? in A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity", "answer": ["subset of COCTAILL", "a subset of COCTAILL", "a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1)"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3883, 5589, 7638, 2270, 2969, 2704, 2703, 2970, 1899, 6454, 6595], "orig_top_k_doc_id": [7638, 4593, 6451, 3883, 2270, 2969, 4595, 5589, 2630, 2704, 2703, 2970, 1899, 6454, 6595]}, {"qid": 1861, "question": "What standard large speaker verification corpora is used for evaluation? in Non-native Speaker Verification for Spoken Language Assessment", "answer": ["non-native speech from the BULATS test "], "top_k_doc_id": [2630, 2631, 2632, 2720, 2721, 2722, 2723, 2724, 5392, 5393, 5391, 483, 5395, 3422, 3423], "orig_top_k_doc_id": [2720, 2724, 2721, 2722, 2723, 5392, 5393, 5391, 483, 5395, 3422, 2630, 2631, 3423, 2632]}, {"qid": 1862, "question": "What systems are tested? in Non-native Speaker Verification for Spoken Language Assessment", "answer": ["BULATS i-vector/PLDA\nBULATS x-vector/PLDA\nVoxCeleb x-vector/PLDA\nPLDA adaptation (X1)\n Extractor fine-tuning (X2) "], "top_k_doc_id": [2630, 2631, 2632, 2720, 2721, 2722, 2723, 2724, 5392, 5393, 2634, 1899, 1247, 2473, 6351], "orig_top_k_doc_id": [2720, 2724, 2721, 2723, 2722, 2630, 2631, 2632, 5393, 2634, 1899, 5392, 1247, 2473, 6351]}, {"qid": 2607, "question": "Which information about text structure is included in the corpus? in A Corpus for Automatic Readability Assessment and Text Simplification of German", "answer": ["paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation", "paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3182, 4594, 4865, 5152, 6110, 6320, 6321, 6955, 6956, 6957, 153], "orig_top_k_doc_id": [4593, 4595, 4594, 6957, 2630, 3182, 6110, 6321, 6451, 6320, 5152, 4865, 153, 6956, 6955]}, {"qid": 2608, "question": "Which information about typography is included in the corpus? in A Corpus for Automatic Readability Assessment and Text Simplification of German", "answer": ["font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page", "font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3182, 4594, 4865, 5152, 6110, 6320, 6321, 6955, 6956, 6957, 1286], "orig_top_k_doc_id": [4593, 4595, 4594, 2630, 6957, 3182, 6321, 6110, 1286, 6955, 6320, 6956, 4865, 6451, 5152]}, {"qid": 3999, "question": "what linguistics features did they apply? in A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity", "answer": ["length-based, lexical, morphological, syntactic, semantic", "Sentence length\nModal verbs to verbs\nAverage token length\nParticle IncSc\nExtra-long words\nSG pronoun IncSc\nNumber of characters\nPunctuation IncSc\nLIX\nSubjunction IncSc\nS-verb IncSc\nA1 lemma IncSc\nS-verbs to verbs\nA2 lemma IncSc\nAdjective IncSc\nB1 lemma IncSc\nAdjective variation\nB2 lemma IncSc\nAdverb IncSc\nC1 lemma IncSc\nAdverb variation\nC2 lemma IncSc\nNoun IncSc\nDifficult word IncSc\nNoun variation\nDifficult noun and verb IncSc\nVerb IncSc\nOut-of-Kelly IncSc\nVerb variation\nMissing lemma form IncSc\nNominal ratio\nAvg. Kelly log frequency\nNouns to verbs\nFunction word IncSc\nAverage dependency length\nLexical words to non-lexical words\nDependency arcs longer than\nLexical words to all tokens\nLongest dependency from root node\nNeuter gender noun IncSc\nRatio of right dependency arcs\nCon- and subjunction IncSc\nRatio of left dependency arcs\nPast participles to verbs\nModifier variation\nPresent participles to verbs\nPre-modifier IncSc\nPast verbs to verbs\nPost-modifier IncSc\nPresent verbs to verbs\nSubordinate IncSc\nSupine verbs to verbs\nRelative clause IncSc\nRelative structure IncSc\nPrepositional complement IncSc\nBilog type-token ratio\nSquare root type-token ratio\nAvg. nr. of senses per token\nPronouns to nouns\nNoun senses per noun\nPronouns to prepositions", "lexical, morphological, syntactic and semantic features"], "top_k_doc_id": [2630, 4593, 4595, 6451, 3883, 5589, 7638, 2086, 5590, 4470, 2388, 3207, 2969, 7562, 3122], "orig_top_k_doc_id": [7638, 4593, 6451, 5589, 2086, 5590, 4595, 3883, 4470, 2388, 3207, 2630, 2969, 7562, 3122]}]}
{"group_id": 20, "group_size": 16, "items": [{"qid": 2522, "question": "Do they use pretrained word representations in their neural network models? in Neural Network Translation Models for Grammatical Error Correction", "answer": ["No"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 4316, 7664, 5351, 5846, 3830, 1469, 1987, 6688], "orig_top_k_doc_id": [4312, 3173, 4316, 6265, 1469, 5841, 5845, 6443, 1987, 1468, 7664, 3830, 5846, 6688, 5351]}, {"qid": 3879, "question": "Which neural machine translation model was used? in The CUED's Grammatical Error Correction Systems for BEA-2019", "answer": ["SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8", "No", "SGNMT"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 4316, 7664, 5351, 5846, 3830, 6267, 6266, 5162], "orig_top_k_doc_id": [6265, 6267, 6266, 4312, 3173, 6443, 5845, 5841, 7664, 1468, 4316, 3830, 5846, 5162, 5351]}, {"qid": 2524, "question": "Which dataset do they evaluate grammatical error correction on? in Neural Network Translation Models for Grammatical Error Correction", "answer": ["CoNLL 2014"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 4316, 7664, 5351, 5846, 5842, 6447, 7667, 5162], "orig_top_k_doc_id": [4312, 6443, 3173, 5841, 5845, 6265, 1468, 7664, 4316, 5842, 5846, 6447, 7667, 5351, 5162]}, {"qid": 3880, "question": "What position did this entry finish in, in the overall shared task? in The CUED's Grammatical Error Correction Systems for BEA-2019", "answer": ["No", "No", "No"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 6266, 6267, 3174, 4316, 4613, 5842, 7489, 5602], "orig_top_k_doc_id": [6265, 6267, 6266, 3173, 4312, 4613, 5845, 5841, 3174, 1468, 4316, 6443, 5842, 7489, 5602]}, {"qid": 3882, "question": "What does BEA stand for? in The CUED's Grammatical Error Correction Systems for BEA-2019", "answer": ["No", "No", "No"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 6266, 6267, 3174, 4316, 6447, 5844, 7664, 5846], "orig_top_k_doc_id": [6267, 6265, 6266, 5845, 4312, 1468, 6443, 5841, 6447, 3173, 5844, 3174, 7664, 4316, 5846]}, {"qid": 1108, "question": "What solutions are proposed for error detection and context awareness? in Evaluation of basic modules for isolated spelling error correction in Polish texts", "answer": ["No"], "top_k_doc_id": [5845, 1468, 1469, 1470, 6443, 5843, 1987, 6600, 5351, 803, 133, 806, 4280, 6265, 2766], "orig_top_k_doc_id": [1468, 1469, 1470, 6443, 1987, 5351, 803, 6600, 5845, 133, 806, 4280, 6265, 2766, 5843]}, {"qid": 1109, "question": "How is PIEWi annotated? in Evaluation of basic modules for isolated spelling error correction in Polish texts", "answer": ["[error, correction] pairs"], "top_k_doc_id": [5845, 1468, 1469, 1470, 6443, 5843, 6447, 6896, 7330, 7327, 5846, 5842, 5210, 2765, 132], "orig_top_k_doc_id": [1468, 1469, 1470, 6443, 5845, 5843, 7330, 7327, 5846, 6896, 6447, 5842, 5210, 2765, 132]}, {"qid": 1110, "question": "What methods are tested in PIEWi? in Evaluation of basic modules for isolated spelling error correction in Polish texts", "answer": ["Levenshtein distance metric BIBREF8, diacritical swapping, Levenshtein distance is used in a weighted sum to cosine distance between word vectors, ELMo-augmented LSTM"], "top_k_doc_id": [5845, 1468, 1469, 1470, 6443, 5843, 6447, 6896, 7330, 133, 5351, 4737, 7742, 5841, 580], "orig_top_k_doc_id": [1468, 1469, 1470, 133, 6443, 6896, 5845, 5351, 6447, 4737, 5843, 7742, 7330, 5841, 580]}, {"qid": 1111, "question": "Which specific error correction solutions have been proposed for specialized corpora in the past? in Evaluation of basic modules for isolated spelling error correction in Polish texts", "answer": ["spellchecking mammography reports and tweets BIBREF7 , BIBREF4"], "top_k_doc_id": [5845, 1468, 1469, 1470, 6443, 5843, 1987, 6600, 6479, 4312, 6468, 2765, 5352, 5841, 7742], "orig_top_k_doc_id": [1468, 1470, 1469, 6479, 4312, 1987, 6443, 5845, 6468, 6600, 2765, 5352, 5843, 5841, 7742]}, {"qid": 2523, "question": "How do they combine the two proposed neural network models? in Neural Network Translation Models for Grammatical Error Correction", "answer": ["ncorporating NNGLM and NNJM both independently and jointly into, baseline system"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 4316, 7664, 3830, 1987, 6479, 7054, 1469, 3686], "orig_top_k_doc_id": [4312, 3173, 6265, 4316, 5845, 6443, 3830, 1468, 1987, 7664, 5841, 6479, 7054, 1469, 3686]}, {"qid": 3537, "question": "How do they measure style transfer success? in Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation", "answer": ["No"], "top_k_doc_id": [5845, 5841, 4568, 5842, 5846, 247, 5621, 5844, 6395, 6871, 6870, 514, 6853, 6034, 6060], "orig_top_k_doc_id": [5841, 5845, 5846, 247, 4568, 5844, 5842, 6395, 6871, 6870, 514, 6853, 6034, 5621, 6060]}, {"qid": 3538, "question": "Do they introduce errors in the data or does the data already contain them? in Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation", "answer": [" all three languages have error-corrected corpora for testing purposes", "Data already contain errors"], "top_k_doc_id": [5845, 5841, 4568, 5842, 5846, 4312, 5843, 6060, 6265, 6443, 1468, 5162, 4569, 4030, 6266], "orig_top_k_doc_id": [5841, 5846, 5845, 5842, 4312, 6443, 6060, 1468, 4568, 6265, 5162, 4569, 5843, 4030, 6266]}, {"qid": 3539, "question": "What error types is their model more reliable for? in Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation", "answer": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "top_k_doc_id": [5845, 5841, 4568, 5842, 5846, 4312, 5843, 6060, 6265, 5713, 6063, 1885, 247, 6064, 6034], "orig_top_k_doc_id": [5841, 5845, 5846, 5842, 4312, 5843, 5713, 6063, 4568, 1885, 6060, 247, 6265, 6064, 6034]}, {"qid": 3540, "question": "How does their parallel data differ in terms of style? in Grammatical Error Correction and Style Transfer via Zero-shot Monolingual Translation", "answer": ["No"], "top_k_doc_id": [5845, 5841, 4568, 5842, 5846, 247, 5621, 5844, 4535, 4031, 4571, 4312, 5843, 5622, 4569], "orig_top_k_doc_id": [5841, 5845, 5846, 5844, 4535, 5842, 247, 4031, 4571, 5621, 4312, 4568, 5843, 5622, 4569]}, {"qid": 3881, "question": "What are the restrictions of the restricted track? in The CUED's Grammatical Error Correction Systems for BEA-2019", "answer": ["explore the potential of purely neural models for grammatical error correction", "The organizers provided a dataset allowed to use for training", "goal on the restricted track was to explore the potential of purely neural models for grammatical error correction"], "top_k_doc_id": [5845, 5841, 1468, 3173, 4312, 6265, 6443, 6266, 6267, 3317, 6447, 6321, 6479, 5842, 5351], "orig_top_k_doc_id": [6267, 6265, 6266, 5845, 4312, 1468, 3317, 6443, 6447, 3173, 6321, 5841, 6479, 5842, 5351]}, {"qid": 1107, "question": "What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system? in Evaluation of basic modules for isolated spelling error correction in Polish texts", "answer": ["Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818."], "top_k_doc_id": [5845, 1468, 1469, 1470, 6443, 5341, 1082, 1987, 7264, 1091, 5447, 1093, 5844, 4946, 580], "orig_top_k_doc_id": [1468, 1469, 1470, 5341, 1082, 1987, 7264, 5845, 1091, 5447, 6443, 1093, 5844, 4946, 580]}]}
{"group_id": 21, "group_size": 16, "items": [{"qid": 3687, "question": "What are the five downstream tasks? in Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation", "answer": ["These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.", "NLI (XNLI dataset), document classification (MLDoc dataset), intent classification, POS tagging, NER", "NLI (XNLI dataset), document classification (MLDoc dataset),  intent classification, sequence tagging tasks: POS tagging, NER", "NLI, document classification, intent classification, POS tagging, NER"], "top_k_doc_id": [6035, 3617, 5713, 6031, 6034, 5714, 5715, 5716, 3618, 3621, 6032, 247, 6060, 783, 3620], "orig_top_k_doc_id": [6035, 6031, 6034, 5713, 3617, 6032, 5716, 5714, 247, 6060, 5715, 783, 3618, 3620, 3621]}, {"qid": 3688, "question": "Is this more effective for low-resource than high-resource languages? in Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation", "answer": ["Yes", "Yes", "Yes", "we see that the gains are more pronounced in low resource languages"], "top_k_doc_id": [6035, 3617, 5713, 6031, 6034, 5714, 5715, 5716, 3618, 3621, 6032, 4568, 2836, 1991, 4027], "orig_top_k_doc_id": [3617, 6035, 6031, 5714, 5713, 5716, 5715, 6032, 4568, 6034, 3621, 3618, 2836, 1991, 4027]}, {"qid": 3552, "question": "What are the languages they use in their experiment? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "top_k_doc_id": [6035, 5868, 5872, 4571, 5871, 627, 6060, 247, 3617, 6034, 4568, 5713, 4569, 6031, 628], "orig_top_k_doc_id": [5872, 5868, 6035, 5871, 6034, 3617, 4568, 6060, 5713, 627, 247, 4569, 4571, 6031, 628]}, {"qid": 3557, "question": "What languages do they use in their experiments? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["English, Spanish, Finnish"], "top_k_doc_id": [6035, 5868, 5872, 4571, 5871, 627, 6060, 247, 3617, 6034, 1040, 633, 1048, 3620, 5622], "orig_top_k_doc_id": [5872, 5868, 5871, 6035, 247, 6060, 3617, 1040, 6034, 627, 4571, 633, 1048, 3620, 5622]}, {"qid": 3690, "question": "How did they select the 50 languages they test? in Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation", "answer": ["These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model", "For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.", "intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model"], "top_k_doc_id": [6035, 3617, 5713, 6031, 6034, 5714, 5715, 5716, 2836, 4186, 6036, 247, 5872, 5868, 5708], "orig_top_k_doc_id": [6035, 3617, 6031, 5713, 5715, 5714, 5716, 6034, 2836, 4186, 6036, 247, 5872, 5868, 5708]}, {"qid": 3551, "question": "What are examples of these artificats? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["the degree of lexical overlap between them, presence of negation words", "hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap"], "top_k_doc_id": [6035, 5868, 5872, 4571, 5871, 2155, 6031, 6034, 4752, 7409, 4568, 2154, 6040, 633, 783], "orig_top_k_doc_id": [5872, 5868, 6035, 5871, 6034, 4752, 4571, 7409, 2155, 4568, 2154, 6040, 6031, 633, 783]}, {"qid": 3553, "question": "Does the professional translation or the machine translation introduce the artifacts? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["Yes"], "top_k_doc_id": [6035, 5868, 5872, 4571, 5871, 627, 6060, 5869, 4569, 1040, 4568, 633, 1053, 6036, 6137], "orig_top_k_doc_id": [5868, 5872, 5871, 5869, 6060, 4569, 1040, 4568, 627, 4571, 633, 1053, 6036, 6035, 6137]}, {"qid": 3555, "question": "Is the improvement over state-of-the-art statistically significant? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["Yes"], "top_k_doc_id": [6035, 5868, 5872, 4571, 5871, 2155, 6031, 6034, 3617, 7410, 3621, 5713, 4030, 3140, 2472], "orig_top_k_doc_id": [5872, 5868, 5871, 3617, 6035, 6034, 7410, 6031, 2155, 4571, 3621, 5713, 4030, 3140, 2472]}, {"qid": 3689, "question": "Is mBERT fine-tuned for each language? in Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation", "answer": ["No", "Yes"], "top_k_doc_id": [6035, 3617, 5713, 6031, 6034, 6032, 249, 3620, 3618, 247, 248, 3621, 5621, 5623, 5624], "orig_top_k_doc_id": [3617, 6031, 6034, 6035, 6032, 249, 3620, 3618, 247, 248, 3621, 5713, 5621, 5623, 5624]}, {"qid": 2486, "question": "What neural machine translation models can learn in terms of transfer learning? in Emerging Language Spaces Learned From Massively Multilingual Corpora", "answer": ["Multilingual Neural Machine Translation Models"], "top_k_doc_id": [6035, 3617, 5713, 6031, 6034, 4186, 4185, 5715, 4184, 3746, 7339, 3641, 4568, 5841, 633], "orig_top_k_doc_id": [6035, 4186, 3617, 4185, 6031, 5715, 4184, 5713, 3746, 6034, 7339, 3641, 4568, 5841, 633]}, {"qid": 1287, "question": "What languages did they experiment with? in Empirical Gaussian priors for cross-lingual transfer learning", "answer": ["No"], "top_k_doc_id": [6035, 5868, 5872, 1766, 1767, 7408, 628, 6034, 627, 629, 626, 4186, 659, 1053, 5621], "orig_top_k_doc_id": [1766, 1767, 7408, 628, 6035, 6034, 627, 629, 626, 4186, 659, 1053, 5868, 5621, 5872]}, {"qid": 2284, "question": "asdfasdaf in Unsupervised Cross-lingual Representation Learning at Scale", "answer": ["No"], "top_k_doc_id": [6035, 3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 4569, 2149, 7828, 4568, 2713, 1040], "orig_top_k_doc_id": [3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 6035, 4569, 2149, 7828, 4568, 2713, 1040]}, {"qid": 2285, "question": "asdfasdf in Unsupervised Cross-lingual Representation Learning at Scale", "answer": ["No"], "top_k_doc_id": [6035, 3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 4569, 2149, 7828, 4568, 2713, 1040], "orig_top_k_doc_id": [3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 6035, 4569, 2149, 7828, 4568, 2713, 1040]}, {"qid": 2286, "question": "asdfasd in Unsupervised Cross-lingual Representation Learning at Scale", "answer": ["No"], "top_k_doc_id": [6035, 3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 4569, 2149, 7828, 4568, 2713, 1040], "orig_top_k_doc_id": [3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 6035, 4569, 2149, 7828, 4568, 2713, 1040]}, {"qid": 2287, "question": "asdf in Unsupervised Cross-lingual Representation Learning at Scale", "answer": ["No"], "top_k_doc_id": [6035, 3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 4569, 2149, 7828, 4568, 2713, 1040], "orig_top_k_doc_id": [3617, 5714, 3618, 2162, 5716, 6424, 5715, 5709, 6035, 4569, 2149, 7828, 4568, 2713, 1040]}, {"qid": 2345, "question": "What are the tasks that this method has shown improvements? in Improving Cross-Lingual Word Embeddings by Meeting in the Middle", "answer": ["bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery"], "top_k_doc_id": [6035, 3748, 6854, 5713, 3747, 7409, 5715, 3996, 6426, 3903, 3617, 6060, 783, 1048, 5711], "orig_top_k_doc_id": [3748, 6854, 5713, 3747, 7409, 5715, 3996, 6426, 3903, 3617, 6060, 783, 1048, 5711, 6035]}]}
{"group_id": 22, "group_size": 16, "items": [{"qid": 4502, "question": "Which datasets do they experiment on? in Learning variable length units for SMT between related languages via Byte Pair Encoding", "answer": ["Indian Language Corpora Initiative (ILCI) corpus, OpenSubtitles2016 section of the OPUS corpus", "multilingual Indian Language Corpora Initiative (ILCI) corpus BIBREF25, OpenSubtitles2016 section of the OPUS corpus collection BIBREF26"], "top_k_doc_id": [5835, 7042, 648, 1370, 5836, 7043, 7045, 7046, 4180, 7435, 7669, 6311, 2770, 3861, 4851], "orig_top_k_doc_id": [7042, 7043, 5835, 7045, 4180, 1370, 7669, 7046, 648, 5836, 6311, 2770, 3861, 4851, 7435]}, {"qid": 4504, "question": "How many steps of BPE do they experiment with? in Learning variable length units for SMT between related languages via Byte Pair Encoding", "answer": ["from 1000 to 4000", "No"], "top_k_doc_id": [5835, 7042, 648, 1370, 5836, 7043, 7045, 7046, 4180, 7435, 7669, 3274, 1691, 1411, 4029], "orig_top_k_doc_id": [7042, 7043, 5835, 7045, 4180, 1370, 648, 7046, 5836, 3274, 7669, 1691, 7435, 1411, 4029]}, {"qid": 4503, "question": "Which other units of text do they experiment with (apart from BPE and ortographic syllables)? in Learning variable length units for SMT between related languages via Byte Pair Encoding", "answer": ["character,  morpheme, word", "character, morpheme, word"], "top_k_doc_id": [5835, 7042, 648, 1370, 5836, 7043, 7045, 7046, 4180, 7435, 2452, 7044, 4951, 3274, 6311], "orig_top_k_doc_id": [7042, 7043, 7045, 7046, 5835, 648, 7435, 1370, 2452, 4180, 7044, 4951, 5836, 3274, 6311]}, {"qid": 5024, "question": "How is the quality of the translation evaluated? in Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring", "answer": ["They report the scores of several evaluation methods for every step of their approach.", "The performances of our final model and other baseline models are illustrated in Table TABREF34."], "top_k_doc_id": [5835, 7042, 7825, 7828, 648, 5837, 6215, 7043, 7669, 650, 231, 7827, 7687, 1119, 649], "orig_top_k_doc_id": [7825, 7828, 7042, 5835, 650, 231, 7827, 648, 7669, 7043, 7687, 5837, 6215, 1119, 649]}, {"qid": 5025, "question": "What are the post-processing approaches applied to the output? in Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring", "answer": ["Special Token Replacement, Quotes Fixing, Recaser,  Patch-up", "unknown words replacement"], "top_k_doc_id": [5835, 7042, 7825, 7828, 231, 7686, 7687, 7827, 1116, 7043, 2487, 1061, 2486, 6943, 7853], "orig_top_k_doc_id": [7825, 7828, 7827, 5835, 7042, 1116, 7043, 2487, 7686, 7687, 1061, 2486, 231, 6943, 7853]}, {"qid": 5026, "question": "Is the MUSE alignment independently evaluated? in Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring", "answer": ["No", "No"], "top_k_doc_id": [5835, 7042, 7825, 7828, 231, 7686, 7687, 7827, 3747, 1044, 661, 648, 4389, 6782, 7669], "orig_top_k_doc_id": [7825, 7828, 7687, 5835, 3747, 231, 7827, 1044, 7686, 661, 648, 4389, 7042, 6782, 7669]}, {"qid": 5027, "question": "How does byte-pair encoding work? in Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring", "answer": ["No", "No"], "top_k_doc_id": [5835, 7042, 7825, 7828, 648, 5837, 6215, 7043, 7669, 1370, 5836, 6554, 1374, 3274, 4695], "orig_top_k_doc_id": [7669, 7825, 7042, 648, 7828, 1370, 5835, 5836, 6554, 5837, 1374, 7043, 6215, 3274, 4695]}, {"qid": 2131, "question": "Which language family does Mboshi belong to? in Controlling Utterance Length in NMT-based Word Segmentation with Attention", "answer": ["Bantu"], "top_k_doc_id": [5835, 3250, 3252, 3253, 5836, 661, 662, 5241, 6960, 7043, 6782, 1117, 6961, 2948, 7044], "orig_top_k_doc_id": [661, 3253, 3252, 3250, 662, 5241, 6960, 7043, 6782, 5836, 5835, 1117, 6961, 2948, 7044]}, {"qid": 2132, "question": "Does the paper report any alignment-only baseline? in Controlling Utterance Length in NMT-based Word Segmentation with Attention", "answer": ["Yes"], "top_k_doc_id": [5835, 3250, 3252, 3253, 5836, 5791, 4766, 4692, 231, 2491, 3918, 3919, 663, 5788, 4768], "orig_top_k_doc_id": [3253, 3250, 5835, 5791, 4766, 5836, 4692, 231, 2491, 3918, 3919, 663, 5788, 3252, 4768]}, {"qid": 2133, "question": "What is the dataset used in the paper? in Controlling Utterance Length in NMT-based Word Segmentation with Attention", "answer": ["French-Mboshi 5K corpus"], "top_k_doc_id": [5835, 3250, 1117, 1371, 2257, 6334, 2253, 5791, 29, 4190, 783, 1116, 4692, 5839, 1024], "orig_top_k_doc_id": [5835, 1117, 2253, 5791, 2257, 3250, 1371, 29, 4190, 6334, 783, 1116, 4692, 5839, 1024]}, {"qid": 2134, "question": "How is the word segmentation task evaluated? in Controlling Utterance Length in NMT-based Word Segmentation with Attention", "answer": ["precision, recall, and F-measure on boundaries (BP, BR, BF), and tokens (WP, WR, WF),  exact-match (X) metric"], "top_k_doc_id": [5835, 3250, 1117, 1371, 2257, 6334, 650, 5836, 1370, 649, 920, 648, 1345, 6782, 6293], "orig_top_k_doc_id": [650, 5836, 1370, 649, 920, 1117, 3250, 5835, 648, 2257, 6334, 1345, 1371, 6782, 6293]}, {"qid": 4501, "question": "Which translation model do they employ? in Learning variable length units for SMT between related languages via Byte Pair Encoding", "answer": ["BPE level, PBSMT models at morpheme and OS levels", "subword level phrase-based SMT model"], "top_k_doc_id": [5835, 7042, 648, 1370, 5836, 7043, 7045, 7046, 4693, 4692, 7669, 799, 4851, 2760, 4570], "orig_top_k_doc_id": [7042, 7043, 7045, 5835, 1370, 4693, 7046, 4692, 7669, 648, 799, 4851, 5836, 2760, 4570]}, {"qid": 2309, "question": "What is their baseline? in Neural Machine Translation System of Indic Languages -- An Attention based Approach", "answer": ["Google's Neural Machine Translation"], "top_k_doc_id": [5835, 4615, 4692, 4766, 6943, 3685, 4848, 4849, 7046, 1309, 7044, 7045, 1308, 2839, 3686], "orig_top_k_doc_id": [3685, 4848, 4849, 7046, 4766, 1309, 7044, 7045, 6943, 1308, 2839, 4692, 3686, 5835, 4615]}, {"qid": 2749, "question": "What languages pairs are used in machine translation? in Quasi-Recurrent Neural Networks", "answer": ["German\u2013English", "German\u2013English"], "top_k_doc_id": [5835, 4615, 4692, 4766, 6943, 4814, 4811, 24, 5165, 2607, 4212, 7847, 7434, 7108, 6551], "orig_top_k_doc_id": [4814, 4811, 24, 5165, 4615, 5835, 4692, 2607, 6943, 4212, 7847, 7434, 7108, 4766, 6551]}, {"qid": 4772, "question": "What morphological features are considered? in Linguistic Input Features Improve Neural Machine Translation", "answer": ["case, number, gender, person, tense, aspect", "nouns have case, number and gender, verbs have person, number, tense and aspect, features may be underspecified"], "top_k_doc_id": [5835, 2760, 4851, 6216, 7434, 7437, 1374, 7435, 397, 1583, 1889, 7436, 6283, 6282, 7687], "orig_top_k_doc_id": [6216, 1374, 7437, 7435, 397, 4851, 7434, 1583, 5835, 1889, 7436, 6283, 6282, 2760, 7687]}, {"qid": 4773, "question": "What type of attention do they use in the decoder? in Linguistic Input Features Improve Neural Machine Translation", "answer": ["Generalized attention", "weighted sum of the annotations"], "top_k_doc_id": [5835, 2760, 4851, 6216, 7434, 7437, 4692, 6943, 493, 3686, 4756, 1237, 4766, 7246, 3655], "orig_top_k_doc_id": [5835, 7434, 4692, 6943, 2760, 493, 7437, 3686, 6216, 4756, 1237, 4766, 4851, 7246, 3655]}]}
{"group_id": 23, "group_size": 16, "items": [{"qid": 4557, "question": "How are the three different forms defined in this work? in A Transformer-based approach to Irony and Sarcasm detection", "answer": ["Irony, sarcasm and metaphor are figurative language form. Irony and sarcasm are considered as a way of indirect denial.", "We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial."], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 1331, 2114, 2115, 6328, 2104, 5405, 2109, 7114, 1318], "orig_top_k_doc_id": [7115, 2103, 2114, 1329, 2329, 1318, 5405, 2115, 1967, 2109, 6328, 2104, 1330, 7114, 1331]}, {"qid": 4560, "question": "Does this work differentiate metaphor(technique) from irony and sarcasm (purpose)?  in A Transformer-based approach to Irony and Sarcasm detection", "answer": ["Yes", "No"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 1331, 2114, 2115, 6328, 2104, 5405, 2109, 7114, 1655], "orig_top_k_doc_id": [2103, 7115, 6328, 2329, 2115, 2114, 1329, 1967, 2104, 2109, 7114, 1330, 1655, 1331, 5405]}, {"qid": 1423, "question": "What is the size of the dataset? in IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "answer": ["a balanced dataset of 2,396 ironic and 2,396 non-ironic tweets is provided"], "top_k_doc_id": [1329, 1330, 1967, 1331, 2109, 2110, 4379, 5175, 1968, 2112, 1319, 2329, 1318, 1320, 2111], "orig_top_k_doc_id": [5175, 1330, 1967, 1329, 2110, 4379, 2329, 2109, 1968, 1331, 1319, 1318, 2112, 2111, 1320]}, {"qid": 1424, "question": "What was the baseline model? in IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "answer": ["a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs"], "top_k_doc_id": [1329, 1330, 1967, 1331, 2109, 2110, 4379, 5175, 1968, 2112, 1319, 2329, 1318, 1320, 2115], "orig_top_k_doc_id": [1330, 5175, 1329, 1967, 4379, 2110, 2329, 1331, 2109, 1968, 1319, 2115, 2112, 1318, 1320]}, {"qid": 4558, "question": "What datasets are used for training and testing? in A Transformer-based approach to Irony and Sarcasm detection", "answer": ["SemEval-2018,  Riloff\u2019s high quality sarcastic unbalanced dataset,  a large dataset containing political comments from Reddit, SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d ", "dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d,  ironic tweets BIBREF95, Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96,  a large dataset containing political comments from Reddit BIBREF97, SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 1331, 2114, 2115, 6328, 2104, 5405, 2109, 2330, 3589], "orig_top_k_doc_id": [1329, 2103, 2329, 7115, 1967, 6328, 2114, 2330, 2115, 1330, 2104, 3589, 1331, 2109, 5405]}, {"qid": 1010, "question": "Do they evaluate only on English? in Deep contextualized word representations for detecting sarcasm and irony", "answer": ["Yes"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 2105, 2109, 7114, 2104, 2110, 2114, 3589, 6328, 5177], "orig_top_k_doc_id": [1329, 1967, 7115, 2103, 1330, 2109, 2105, 7114, 3589, 6328, 5177, 2329, 2114, 2104, 2110]}, {"qid": 1011, "question": "What are the 7 different datasets? in Deep contextualized word representations for detecting sarcasm and irony", "answer": ["SemEval 2018 Task 3, BIBREF20, BIBREF4, SARC 2.0, SARC 2.0 pol, Sarcasm Corpus V1 (SC-V1), Sarcasm Corpus V2 (SC-V2)"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 2105, 2109, 7114, 2104, 2110, 2114, 3589, 1331, 2112], "orig_top_k_doc_id": [1329, 1967, 2103, 1331, 7115, 1330, 2105, 7114, 2112, 2109, 2329, 2104, 2114, 2110, 3589]}, {"qid": 1012, "question": "What are the three different sources of data? in Deep contextualized word representations for detecting sarcasm and irony", "answer": ["Twitter, Reddit, Online Dialogues"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 2105, 2109, 7114, 1331, 2112, 6328, 1494, 1500, 2104], "orig_top_k_doc_id": [1329, 1967, 2103, 7114, 1330, 1331, 2112, 7115, 1494, 2105, 1500, 2104, 2109, 2329, 6328]}, {"qid": 1014, "question": "Which morphosyntactic features are thought to indicate irony or sarcasm? in Deep contextualized word representations for detecting sarcasm and irony", "answer": ["all caps, quotation marks, emoticons, emojis, hashtags"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 2105, 2109, 7114, 1331, 2112, 6328, 2114, 1655, 2115], "orig_top_k_doc_id": [1329, 2103, 1967, 7115, 1330, 7114, 2114, 2112, 2109, 1331, 1655, 2105, 2329, 6328, 2115]}, {"qid": 1421, "question": "Did they experiment with pre-training schemes? in IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "answer": ["No"], "top_k_doc_id": [1329, 1330, 1967, 1331, 2109, 2110, 4379, 5175, 1968, 2112, 1319, 2329, 1318, 1320, 2114], "orig_top_k_doc_id": [1330, 5175, 1967, 2110, 4379, 2109, 1968, 1319, 1329, 1331, 2329, 2112, 1318, 1320, 2114]}, {"qid": 4556, "question": "What are the baseline models? in A Transformer-based approach to Irony and Sarcasm detection", "answer": ["ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model", "ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa "], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 1331, 2114, 2115, 6328, 2104, 5405, 1318, 1655, 2110], "orig_top_k_doc_id": [2103, 1329, 7115, 2115, 2329, 1967, 2114, 1330, 1331, 2104, 1318, 1655, 2110, 6328, 5405]}, {"qid": 1422, "question": "What were their results on the test set? in IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "answer": ["an official F1-score of 0.2905 on the test set"], "top_k_doc_id": [1329, 1330, 1967, 1331, 2109, 2110, 4379, 5175, 1968, 2112, 1319, 2329, 7115, 87, 2111], "orig_top_k_doc_id": [1330, 5175, 2110, 4379, 1967, 1329, 2329, 2109, 1319, 1968, 1331, 2112, 7115, 87, 2111]}, {"qid": 4559, "question": "Does approach handle overlapping forms (e.g., metaphor and irony)? in A Transformer-based approach to Irony and Sarcasm detection", "answer": ["Yes", "Yes"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 2329, 1331, 2114, 2115, 6328, 7114, 4378, 5176, 1318, 5177], "orig_top_k_doc_id": [7115, 6328, 2329, 2103, 7114, 1967, 1329, 2115, 4378, 2114, 5176, 1331, 1318, 5177, 1330]}, {"qid": 2534, "question": "What baseline system is used? in NIHRIO at SemEval-2018 Task 3: A Simple and Accurate Neural Network Model for Irony Detection in Twitter", "answer": ["No"], "top_k_doc_id": [1329, 1330, 1967, 1331, 2109, 2110, 4379, 5175, 1968, 2112, 2115, 1318, 7118, 2114, 7115], "orig_top_k_doc_id": [4379, 5175, 1330, 1331, 1329, 2115, 1967, 1318, 2110, 7118, 2109, 1968, 2114, 7115, 2112]}, {"qid": 1013, "question": "What type of model are the ELMo representations used in? in Deep contextualized word representations for detecting sarcasm and irony", "answer": ["A bi-LSTM with max-pooling on top of it"], "top_k_doc_id": [1329, 1330, 1967, 2103, 7115, 1331, 4603, 6697, 4606, 7114, 2105, 7116, 4209, 4604, 2109], "orig_top_k_doc_id": [1329, 1967, 1330, 1331, 2103, 4603, 7115, 6697, 4606, 7114, 2105, 7116, 4209, 4604, 2109]}, {"qid": 2535, "question": "What type of lexical, syntactic, semantic and polarity features are used? in NIHRIO at SemEval-2018 Task 3: A Simple and Accurate Neural Network Model for Irony Detection in Twitter", "answer": ["Our lexical features include 1-, 2-, and 3-grams in both word and character levels., number of characters and the number of words, POS tags, 300-dimensional pre-trained word embeddings from GloVe, latent semantic indexing, tweet representation by applying the Brown clustering algorithm, positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon, boolean features that check whether or not a negation word is in a tweet"], "top_k_doc_id": [1329, 1330, 1967, 1331, 2109, 2110, 4379, 5175, 4378, 7115, 7131, 7116, 5177, 7114, 5405], "orig_top_k_doc_id": [4379, 1329, 5175, 4378, 7115, 1331, 1967, 1330, 7131, 7116, 5177, 2110, 7114, 5405, 2109]}]}
{"group_id": 24, "group_size": 15, "items": [{"qid": 421, "question": "What is the latest paper covered by this survey? in Recent Advances in Neural Question Generation", "answer": ["Kim et al. (2019)"], "top_k_doc_id": [490, 1322, 3175, 491, 2083, 3069, 3070, 6590, 6543, 815, 854, 7382, 277, 2818, 2733], "orig_top_k_doc_id": [490, 3070, 6543, 3175, 3069, 491, 6590, 815, 854, 7382, 1322, 277, 2083, 2818, 2733]}, {"qid": 423, "question": "Do they survey multilingual aspects? in Recent Advances in Neural Question Generation", "answer": ["No"], "top_k_doc_id": [490, 1322, 3175, 491, 276, 1924, 3798, 7262, 7382, 6847, 6566, 3293, 3244, 2075, 6590], "orig_top_k_doc_id": [490, 491, 3798, 7262, 7382, 6847, 6566, 3293, 3175, 1322, 3244, 276, 2075, 6590, 1924]}, {"qid": 424, "question": "What learning paradigms do they cover in this survey? in Recent Advances in Neural Question Generation", "answer": ["Considering \"What\" and \"How\" separately versus jointly optimizing for both."], "top_k_doc_id": [490, 1322, 3175, 491, 2083, 3069, 3070, 6590, 7163, 273, 3798, 4719, 495, 276, 4235], "orig_top_k_doc_id": [490, 3070, 3175, 3069, 491, 7163, 1322, 273, 3798, 4719, 2083, 6590, 495, 276, 4235]}, {"qid": 426, "question": "Do they survey non-neural methods for question generation? in Recent Advances in Neural Question Generation", "answer": ["No"], "top_k_doc_id": [490, 1322, 3175, 491, 276, 1924, 3798, 2910, 2911, 2083, 6676, 3034, 7664, 3805, 5430], "orig_top_k_doc_id": [490, 491, 2910, 3175, 2911, 1322, 2083, 3798, 6676, 3034, 7664, 1924, 276, 3805, 5430]}, {"qid": 4587, "question": "How quickly is this hybrid model trained?   in Visual Question Answering using Deep Learning: A Survey and Performance Analysis", "answer": ["No", "No"], "top_k_doc_id": [490, 7163, 5729, 491, 511, 2900, 3175, 495, 7520, 2380, 7514, 7164, 6441, 4908, 2379], "orig_top_k_doc_id": [7520, 7163, 2380, 5729, 7514, 490, 3175, 2900, 511, 7164, 6441, 495, 4908, 491, 2379]}, {"qid": 4588, "question": "What are the new deep learning models discussed in the paper?   in Visual Question Answering using Deep Learning: A Survey and Performance Analysis", "answer": ["Vanilla VQA, Stacked Attention Networks, Teney et al. Model, Neural-Symbolic VQA, Focal Visual Text Attention (FVTA), Pythia v1.0, Differential Networks", "Stacked Attention Networks BIBREF11, Teney et al. Model BIBREF13, Neural-Symbolic VQA BIBREF23, Focal Visual Text Attention (FVTA) BIBREF24, Pythia v1.0 BIBREF27, Differential Networks BIBREF19:"], "top_k_doc_id": [490, 7163, 5729, 491, 511, 2900, 3175, 495, 7520, 2733, 7148, 7138, 5727, 5728, 276], "orig_top_k_doc_id": [491, 7163, 2733, 490, 7148, 495, 2900, 5729, 7138, 5727, 5728, 511, 3175, 276, 7520]}, {"qid": 422, "question": "Do they survey visual question generation work? in Recent Advances in Neural Question Generation", "answer": ["Yes"], "top_k_doc_id": [490, 1322, 3175, 491, 495, 575, 7138, 3798, 4033, 276, 285, 2900, 3034, 7164, 4267], "orig_top_k_doc_id": [490, 3175, 3798, 491, 4033, 276, 575, 7138, 285, 495, 1322, 2900, 3034, 7164, 4267]}, {"qid": 425, "question": "What are all the input modalities considered in prior work in question generation? in Recent Advances in Neural Question Generation", "answer": ["Textual inputs, knowledge bases, and images."], "top_k_doc_id": [490, 1322, 3175, 491, 495, 575, 7138, 6723, 7148, 494, 2519, 3807, 1922, 2083, 6936], "orig_top_k_doc_id": [490, 491, 7138, 575, 6723, 7148, 1322, 494, 2519, 3175, 495, 3807, 1922, 2083, 6936]}, {"qid": 2093, "question": "What are least important components identified in the the training of VQA models? in Component Analysis for Visual Question Answering Architectures", "answer": [" some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant"], "top_k_doc_id": [490, 7163, 510, 3175, 3176, 3177, 7147, 7804, 507, 575, 788, 3180, 7148, 7164, 512], "orig_top_k_doc_id": [3175, 7163, 510, 3177, 7164, 7148, 3176, 7147, 3180, 490, 788, 512, 507, 7804, 575]}, {"qid": 2095, "question": "What components are identified as core components for training VQA models? in Component Analysis for Visual Question Answering Architectures", "answer": ["pre-trained text representations, transformer-based encoders together with GRU models, attention mechanisms are paramount for learning top performing networks, Top-Down is the preferred attention method"], "top_k_doc_id": [490, 7163, 510, 3175, 3176, 3177, 7147, 7804, 507, 575, 788, 3180, 7148, 7164, 508], "orig_top_k_doc_id": [3175, 510, 7164, 7163, 7148, 3180, 3177, 507, 508, 7147, 3176, 490, 7804, 788, 575]}, {"qid": 4586, "question": "What are remaining challenges in VQA? in Visual Question Answering using Deep Learning: A Survey and Performance Analysis", "answer": ["develop better deep learning models,  more challenging datasets for VQA", " object level details, segmentation masks, and sentiment of the question"], "top_k_doc_id": [490, 7163, 5729, 788, 4267, 5728, 7147, 7164, 7800, 3175, 791, 575, 867, 7165, 7804], "orig_top_k_doc_id": [7163, 5729, 7800, 490, 3175, 7164, 5728, 791, 575, 867, 7147, 788, 7165, 7804, 4267]}, {"qid": 4589, "question": "What was the architecture of the 2017 Challenge Winner model? in Visual Question Answering using Deep Learning: A Survey and Performance Analysis", "answer": ["Region-based CNN", "R-CNN architecture"], "top_k_doc_id": [490, 7163, 5729, 491, 511, 2900, 3175, 7164, 7165, 871, 5736, 575, 911, 4278, 7572], "orig_top_k_doc_id": [7164, 7165, 871, 511, 7163, 5736, 3175, 491, 575, 5729, 911, 2900, 4278, 490, 7572]}, {"qid": 4590, "question": "What is an example of a common sense question? in Visual Question Answering using Deep Learning: A Survey and Performance Analysis", "answer": ["How many giraffes are drinking water?", "Can you park here?\nIs something under the sink broken?\nDoes this man have children?"], "top_k_doc_id": [490, 7163, 5729, 788, 4267, 5728, 7147, 7164, 7800, 7520, 491, 495, 7801, 690, 3799], "orig_top_k_doc_id": [5729, 7164, 7163, 7520, 491, 7800, 495, 490, 7147, 788, 7801, 4267, 690, 3799, 5728]}, {"qid": 2094, "question": "What type of experiments are performed? in Component Analysis for Visual Question Answering Architectures", "answer": ["pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13, transformer-based sentence encoders BIBREF14, distinct convolutional neural networks, standard fusion strategies,  two main attention mechanisms BIBREF18, BIBREF19"], "top_k_doc_id": [490, 7163, 510, 3175, 3176, 3177, 7147, 7804, 3178, 1217, 3799, 3179, 511, 2734, 1140], "orig_top_k_doc_id": [3175, 510, 3178, 3177, 7163, 1217, 490, 3799, 7804, 3179, 511, 3176, 2734, 1140, 7147]}, {"qid": 420, "question": "Do they cover data augmentation papers? in Recent Advances in Neural Question Generation", "answer": ["No"], "top_k_doc_id": [490, 1322, 3175, 1924, 1925, 2442, 4698, 2746, 4841, 3798, 286, 3034, 6492, 2397, 4637], "orig_top_k_doc_id": [3175, 1924, 1925, 2442, 4698, 2746, 4841, 3798, 1322, 490, 286, 3034, 6492, 2397, 4637]}]}
{"group_id": 25, "group_size": 15, "items": [{"qid": 443, "question": "Do they report results only on English data? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["No"], "top_k_doc_id": [520, 521, 6207, 6209, 2413, 7514, 7147, 343, 6206, 6208, 1527, 1528, 1701, 3657, 3123], "orig_top_k_doc_id": [520, 521, 6207, 2413, 6209, 7514, 6208, 6206, 343, 3657, 1527, 7147, 1701, 1528, 3123]}, {"qid": 448, "question": "How is the data annotated? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["The data are self-reported by Twitter users and then verified by two human experts."], "top_k_doc_id": [520, 521, 6207, 6209, 2413, 7514, 7147, 343, 6206, 6208, 1527, 1528, 1701, 6014, 3874], "orig_top_k_doc_id": [520, 521, 2413, 6207, 7514, 1701, 6209, 343, 6208, 7147, 6014, 6206, 1527, 1528, 3874]}, {"qid": 447, "question": "What types of features are used from each data type? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns"], "top_k_doc_id": [520, 521, 6207, 6209, 2413, 7514, 7147, 343, 6206, 6208, 6014, 6013, 3550, 6011, 59], "orig_top_k_doc_id": [520, 521, 6014, 6207, 7147, 7514, 6013, 6209, 2413, 3550, 6206, 6208, 6011, 59, 343]}, {"qid": 451, "question": "What is the source of the textual data?  in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["Users' tweets"], "top_k_doc_id": [520, 521, 6207, 6209, 2413, 7514, 7147, 3658, 4267, 4428, 523, 343, 6206, 1701, 7557], "orig_top_k_doc_id": [520, 521, 2413, 7514, 6207, 4428, 6209, 7147, 4267, 523, 3658, 343, 6206, 1701, 7557]}, {"qid": 452, "question": "What is the source of the visual data?  in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["Profile pictures from the Twitter users' profiles."], "top_k_doc_id": [520, 521, 6207, 6209, 2413, 7514, 7147, 3658, 4267, 4428, 1527, 1528, 6014, 4230, 3657], "orig_top_k_doc_id": [520, 521, 2413, 7514, 7147, 1527, 1528, 6014, 6207, 4267, 4230, 3658, 4428, 6209, 3657]}, {"qid": 446, "question": "How do this framework facilitate demographic inference from social media? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["Demographic information is predicted using weighted lexicon of terms."], "top_k_doc_id": [520, 521, 6207, 6209, 1701, 6206, 6208, 343, 522, 5258, 523, 60, 1735, 7029, 6714], "orig_top_k_doc_id": [520, 521, 6209, 6208, 6207, 522, 1701, 6206, 523, 60, 343, 1735, 5258, 7029, 6714]}, {"qid": 450, "question": "What is the source of the user interaction data?  in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["Sociability from ego-network on Twitter"], "top_k_doc_id": [520, 521, 6207, 6209, 1701, 6206, 6208, 343, 522, 5258, 7514, 5056, 2413, 5739, 4721], "orig_top_k_doc_id": [520, 521, 7514, 6207, 6209, 5056, 6206, 2413, 343, 5739, 522, 1701, 4721, 6208, 5258]}, {"qid": 2833, "question": "Do they evaluate only on English datasets? in Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health", "answer": ["Yes", "Yes"], "top_k_doc_id": [520, 521, 522, 6206, 59, 643, 1192, 1757, 1758, 4964, 4965, 4966, 6155, 644, 6207], "orig_top_k_doc_id": [4964, 4966, 4965, 643, 520, 521, 1757, 522, 1758, 6155, 59, 6206, 6207, 1192, 644]}, {"qid": 2834, "question": "What are the three steps to feature elimination? in Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health", "answer": ["Reduction, Selection, Rank", "reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features."], "top_k_doc_id": [520, 521, 522, 6206, 59, 643, 1192, 1757, 1758, 4964, 4965, 4966, 6155, 1190, 1340], "orig_top_k_doc_id": [4964, 4966, 4965, 59, 520, 643, 1757, 521, 1758, 522, 1192, 1340, 6155, 6206, 1190]}, {"qid": 2835, "question": "How is the dataset annotated? in Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health", "answer": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "top_k_doc_id": [520, 521, 522, 6206, 59, 643, 1192, 1757, 1758, 4964, 4965, 4966, 6155, 644, 3300], "orig_top_k_doc_id": [4964, 4966, 4965, 520, 521, 643, 1757, 522, 1758, 1192, 59, 6155, 644, 3300, 6206]}, {"qid": 2836, "question": "What dataset is used for this study? in Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health", "answer": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "top_k_doc_id": [520, 521, 522, 6206, 59, 643, 1192, 1757, 1758, 4964, 4965, 4966, 6155, 1190, 1193], "orig_top_k_doc_id": [4964, 4966, 4965, 1757, 643, 520, 521, 1758, 522, 1192, 59, 1190, 6155, 6206, 1193]}, {"qid": 444, "question": "What insights into the relationship between demographics and mental health are provided? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression"], "top_k_doc_id": [520, 521, 6207, 6209, 1701, 6206, 6208, 6210, 4139, 60, 64, 5056, 3550, 59, 1190], "orig_top_k_doc_id": [520, 521, 6207, 6209, 6206, 1701, 6210, 4139, 60, 64, 5056, 3550, 59, 6208, 1190]}, {"qid": 445, "question": "What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["Random Forest classifier"], "top_k_doc_id": [520, 521, 522, 6206, 523, 524, 3550, 3551, 3553, 6207, 4964, 59, 6209, 60, 4965], "orig_top_k_doc_id": [520, 521, 3550, 6206, 4964, 3553, 59, 522, 3551, 523, 6207, 6209, 60, 524, 4965]}, {"qid": 449, "question": "Where does the information on individual-level demographics come from? in Fusing Visual, Textual and Connectivity Clues for Studying Mental Health", "answer": ["From Twitter profile descriptions of the users."], "top_k_doc_id": [520, 521, 6207, 6209, 2413, 7514, 60, 523, 4139, 1701, 1527, 934, 522, 7557, 1528], "orig_top_k_doc_id": [520, 521, 60, 523, 4139, 2413, 1701, 6209, 7514, 1527, 6207, 934, 522, 7557, 1528]}, {"qid": 2261, "question": "Do they report results only on English datasets? in Depressed individuals express more distorted thinking on social media", "answer": ["Yes"], "top_k_doc_id": [520, 521, 522, 6206, 523, 524, 3550, 3551, 3553, 6207, 3554, 3552, 2343, 5524, 4140], "orig_top_k_doc_id": [3553, 3554, 3550, 3552, 3551, 520, 523, 521, 522, 524, 6206, 2343, 5524, 6207, 4140]}]}
{"group_id": 26, "group_size": 15, "items": [{"qid": 776, "question": "Does their model also take the expected answer style as input? in Multi-style Generative Reading Comprehension", "answer": ["Yes"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1961, 2012, 3827, 1512, 4637, 2210, 4910, 1005, 5681], "orig_top_k_doc_id": [1370, 968, 2759, 4910, 4637, 2011, 972, 1512, 3827, 2012, 1005, 2210, 5681, 2234, 1961]}, {"qid": 781, "question": "What is an \"answer style\"? in Multi-style Generative Reading Comprehension", "answer": ["well-formed sentences vs concise answers"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1961, 2012, 3827, 1512, 4637, 2210, 3416, 3972, 5157], "orig_top_k_doc_id": [1370, 968, 2759, 2011, 972, 3827, 1512, 4637, 2012, 3416, 1961, 2234, 2210, 3972, 5157]}, {"qid": 778, "question": "Is there exactly one \"answer style\" per dataset? in Multi-style Generative Reading Comprehension", "answer": ["Yes"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1961, 2012, 3827, 1512, 4637, 3972, 2836, 352, 4910], "orig_top_k_doc_id": [2011, 1370, 2012, 968, 4637, 2759, 1512, 1961, 3972, 3827, 972, 2234, 2836, 352, 4910]}, {"qid": 2379, "question": "Are there some results better than state of the art on these tasks? in Contextual Recurrent Units for Cloze-style Reading Comprehension", "answer": ["Yes"], "top_k_doc_id": [1370, 2011, 3827, 3825, 3826, 3829, 2759, 2836, 3416, 1822, 2012, 2013, 355, 352, 3828], "orig_top_k_doc_id": [3827, 3825, 3829, 3826, 1370, 3416, 2011, 1822, 2012, 2836, 2013, 355, 352, 2759, 3828]}, {"qid": 2381, "question": "What datasets are used for testing sentiment classification and reading comprehension? in Contextual Recurrent Units for Cloze-style Reading Comprehension", "answer": ["CBT NE/CN, MR Movie reviews, IMDB Movie reviews, SUBJ"], "top_k_doc_id": [1370, 2011, 3827, 3825, 3826, 3829, 2759, 2836, 3416, 1822, 2012, 1961, 1512, 3972, 2004], "orig_top_k_doc_id": [3827, 3825, 2011, 1370, 3829, 3826, 2012, 1822, 1961, 2759, 3416, 1512, 3972, 2836, 2004]}, {"qid": 775, "question": "How do they measure the quality of summaries? in Multi-style Generative Reading Comprehension", "answer": ["Rouge-L, Bleu-1"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1512, 2836, 4637, 4257, 6957, 2064, 6955, 2837, 4307], "orig_top_k_doc_id": [1370, 4637, 4257, 6957, 2011, 2759, 972, 2836, 1512, 968, 2064, 6955, 2837, 2234, 4307]}, {"qid": 777, "question": "What do they mean by answer styles? in Multi-style Generative Reading Comprehension", "answer": ["well-formed sentences vs concise answers"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1512, 2836, 4637, 5157, 3972, 5798, 7727, 7728, 7572], "orig_top_k_doc_id": [968, 972, 5157, 1370, 2234, 2011, 3972, 2759, 5798, 1512, 7727, 4637, 7728, 2836, 7572]}, {"qid": 779, "question": "What are the baselines that Masque is compared against? in Multi-style Generative Reading Comprehension", "answer": ["BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1961, 2012, 3827, 3416, 5970, 352, 355, 2839, 5473], "orig_top_k_doc_id": [968, 972, 3416, 2011, 2759, 2234, 3827, 5970, 352, 1370, 355, 1961, 2839, 2012, 5473]}, {"qid": 1047, "question": "which public datasets were used? in Subword-augmented Embedding for Cloze Reading Comprehension", "answer": ["CMRC-2017, People's Daily (PD), Children Fairy Tales (CFT) , Children's Book Test (CBT)"], "top_k_doc_id": [1370, 2011, 3827, 352, 1372, 1373, 1374, 1512, 2012, 2013, 2759, 3416, 2836, 3829, 4910], "orig_top_k_doc_id": [1370, 1374, 2011, 1372, 1373, 3827, 1512, 2012, 2836, 2759, 3416, 2013, 352, 3829, 4910]}, {"qid": 1048, "question": "what are the baselines? in Subword-augmented Embedding for Cloze Reading Comprehension", "answer": ["AS Reader, GA Reader, CAS Reader"], "top_k_doc_id": [1370, 2011, 3827, 352, 1372, 1373, 1374, 1512, 2012, 2013, 2759, 3416, 2004, 1822, 355], "orig_top_k_doc_id": [1370, 1374, 2011, 2013, 2012, 3827, 1373, 1512, 1372, 3416, 352, 2004, 1822, 2759, 355]}, {"qid": 2377, "question": "What supports the claim that injected CNN into recurent units will enhance ability of the model to catch local context and reduce ambiguities? in Contextual Recurrent Units for Cloze-style Reading Comprehension", "answer": ["word embeddings to generate a new feature, i.e., summarizing a local context"], "top_k_doc_id": [1370, 2011, 3827, 3825, 3826, 3829, 355, 1372, 1374, 3973, 6823, 3972, 1822, 3828, 160], "orig_top_k_doc_id": [3826, 3827, 3825, 3829, 1370, 2011, 1374, 3972, 1822, 6823, 3973, 355, 1372, 3828, 160]}, {"qid": 2378, "question": "How is CNN injected into recurent units? in Contextual Recurrent Units for Cloze-style Reading Comprehension", "answer": ["The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward."], "top_k_doc_id": [1370, 2011, 3827, 3825, 3826, 3829, 355, 1372, 1374, 3973, 6823, 2759, 2012, 352, 1961], "orig_top_k_doc_id": [3827, 3826, 3825, 1370, 3829, 2011, 355, 2759, 6823, 1374, 1372, 2012, 352, 1961, 3973]}, {"qid": 2380, "question": "Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models? in Contextual Recurrent Units for Cloze-style Reading Comprehension", "answer": ["Yes"], "top_k_doc_id": [1370, 2011, 3827, 3825, 3826, 3829, 2759, 2836, 3416, 3828, 1609, 1961, 5367, 355, 2096], "orig_top_k_doc_id": [3825, 3826, 3827, 2011, 3829, 3828, 1370, 3416, 1609, 2836, 1961, 5367, 355, 2759, 2096]}, {"qid": 1046, "question": "how are rare words defined? in Subword-augmented Embedding for Cloze Reading Comprehension", "answer": ["low-frequency words"], "top_k_doc_id": [1370, 2011, 3827, 352, 1372, 1373, 1374, 1512, 2012, 2013, 2759, 1371, 293, 1822, 1961], "orig_top_k_doc_id": [1370, 1374, 1373, 1372, 1512, 2011, 3827, 2012, 2013, 2759, 1371, 352, 293, 1822, 1961]}, {"qid": 780, "question": "What is the performance achieved on NarrativeQA? in Multi-style Generative Reading Comprehension", "answer": ["Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87"], "top_k_doc_id": [1370, 2011, 968, 972, 2234, 2759, 1146, 1630, 1145, 2096, 2264, 3416, 2607, 2752, 5473], "orig_top_k_doc_id": [1146, 1630, 1145, 1370, 2096, 2759, 972, 2264, 2011, 968, 3416, 2607, 2234, 2752, 5473]}]}
{"group_id": 27, "group_size": 15, "items": [{"qid": 1414, "question": "How are the topics embedded in the #MeToo tweets extracted? in #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media", "answer": ["Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 4138, 4139, 934, 5783, 6131, 447, 1480, 1193, 5085], "orig_top_k_doc_id": [1957, 1960, 4136, 1959, 1958, 4138, 934, 4139, 4137, 5783, 447, 1193, 6131, 5085, 1480]}, {"qid": 1415, "question": "How many tweets are explored in this paper? in #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media", "answer": ["60,000 "], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 4138, 4139, 934, 5783, 6131, 447, 1480, 6140, 5144], "orig_top_k_doc_id": [1957, 1960, 4136, 1959, 934, 1958, 4138, 4137, 4139, 5783, 447, 1480, 6140, 6131, 5144]}, {"qid": 1412, "question": "Which major geographical regions are studied? in #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media", "answer": ["Northeast U.S, South U.S., West U.S. and Midwest U.S."], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 4138, 4139, 934, 5783, 85, 1193, 6896, 1725, 2534], "orig_top_k_doc_id": [1957, 1959, 1960, 4136, 1958, 934, 4137, 4138, 4139, 5783, 1193, 85, 1725, 6896, 2534]}, {"qid": 1416, "question": "Which geographical regions correlate to the trend? in #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media", "answer": ["Northeast U.S., West U.S. and South U.S."], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 4138, 4139, 934, 5783, 85, 1193, 6896, 2077, 521], "orig_top_k_doc_id": [1957, 1960, 1959, 4136, 1958, 934, 4137, 4138, 4139, 1193, 85, 2077, 6896, 521, 5783]}, {"qid": 1417, "question": "How many followers did they analyze? in #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media", "answer": ["51,104"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 4138, 4139, 934, 5783, 6131, 5463, 5468, 5469, 521], "orig_top_k_doc_id": [1957, 1960, 4136, 1959, 934, 1958, 4138, 4137, 4139, 5463, 5783, 5468, 6131, 5469, 521]}, {"qid": 4837, "question": "How many tweets did they look at? in SentiBubbles: Topic Modeling and Sentiment Visualization of Entity-centric Tweets", "answer": ["No", "No"], "top_k_doc_id": [1958, 4280, 7528, 7529, 7752, 447, 881, 1732, 4137, 333, 1957, 330, 332, 951, 6132], "orig_top_k_doc_id": [7528, 7529, 881, 333, 1732, 447, 1957, 330, 1958, 332, 7752, 4280, 4137, 951, 6132]}, {"qid": 4838, "question": "What language are the tweets in? in SentiBubbles: Topic Modeling and Sentiment Visualization of Entity-centric Tweets", "answer": ["Portuguese ", "portuguese and english"], "top_k_doc_id": [1958, 4280, 7528, 7529, 7752, 447, 881, 1732, 4137, 1731, 879, 7534, 3300, 2405, 2828], "orig_top_k_doc_id": [7528, 7529, 4280, 7752, 881, 1731, 4137, 1958, 447, 1732, 879, 7534, 3300, 2405, 2828]}, {"qid": 748, "question": "What is the size of the dataset? in Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization", "answer": [" 9,892 stories of sexual harassment incidents"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 934, 938, 939, 5085, 935, 937, 936, 3584, 7256], "orig_top_k_doc_id": [934, 935, 938, 1957, 937, 939, 936, 1960, 1959, 1958, 4136, 3584, 7256, 5085, 4137]}, {"qid": 749, "question": "What model did they use? in Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization", "answer": ["joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 934, 938, 939, 5085, 935, 937, 936, 3584, 7256], "orig_top_k_doc_id": [934, 935, 938, 1957, 937, 939, 1959, 1958, 1960, 936, 4136, 3584, 4137, 7256, 5085]}, {"qid": 750, "question": "What patterns were discovered from the stories? in Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization", "answer": ["we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers."], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 934, 938, 939, 5085, 935, 937, 936, 3584, 7256], "orig_top_k_doc_id": [934, 938, 935, 939, 937, 1957, 1959, 936, 1958, 1960, 4136, 3584, 7256, 4137, 5085]}, {"qid": 751, "question": "Did they use a crowdsourcing platform? in Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization", "answer": ["No"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 934, 938, 939, 5085, 935, 937, 936, 3584, 7256], "orig_top_k_doc_id": [934, 935, 938, 1957, 937, 939, 4136, 1959, 1960, 5085, 936, 1958, 3584, 4137, 7256]}, {"qid": 1413, "question": "How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]? in #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media", "answer": ["0.9098 correlation"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 934, 938, 939, 5085, 4139, 4138, 5953, 6624, 4140], "orig_top_k_doc_id": [1960, 1957, 4136, 1959, 4139, 4138, 934, 1958, 4137, 938, 939, 5085, 5953, 6624, 4140]}, {"qid": 2467, "question": "Do the tweets come from a specific region? in #MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo Movement", "answer": ["No"], "top_k_doc_id": [1958, 1957, 1959, 1960, 4136, 4137, 4138, 4139, 5901, 5976, 5900, 5977, 449, 762, 1310], "orig_top_k_doc_id": [4136, 4139, 1959, 4137, 4138, 1960, 1958, 5901, 5976, 5900, 1957, 5977, 449, 762, 1310]}, {"qid": 4836, "question": "What model was used for sentiment analysis? in SentiBubbles: Topic Modeling and Sentiment Visualization of Entity-centric Tweets", "answer": ["A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words", "Lexicon based word-level  SA."], "top_k_doc_id": [1958, 4280, 7528, 7529, 7752, 447, 2828, 3731, 5879, 448, 3528, 1731, 2405, 756, 3795], "orig_top_k_doc_id": [7528, 7529, 447, 7752, 2828, 3731, 1958, 5879, 448, 3528, 1731, 2405, 4280, 756, 3795]}, {"qid": 4835, "question": "What is the timeframe of the current events? in SentiBubbles: Topic Modeling and Sentiment Visualization of Entity-centric Tweets", "answer": ["from January 2014 to December 2015", "January 2014 to December 2015"], "top_k_doc_id": [1958, 4280, 7528, 7529, 7752, 2405, 2404, 4359, 2403, 5189, 2080, 1256, 330, 495, 954], "orig_top_k_doc_id": [7528, 7529, 2405, 2404, 4359, 2403, 4280, 5189, 2080, 1958, 1256, 330, 495, 954, 7752]}]}
{"group_id": 28, "group_size": 15, "items": [{"qid": 3354, "question": "What is the word-level baseline? in Tweet2Vec: Character-Based Distributed Representations for Social Media", "answer": ["a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.", "The encoder is essentially the same as tweet2vec, with the input as words instead of characters."], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 5584, 6057, 6458, 6886, 1329, 6240, 5288, 7259], "orig_top_k_doc_id": [5582, 5584, 6534, 6532, 6884, 6057, 5583, 6458, 5314, 6533, 6886, 6240, 5288, 7259, 1329]}, {"qid": 3355, "question": "What other tasks do they test their method on? in Tweet2Vec: Character-Based Distributed Representations for Social Media", "answer": ["None"], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 5584, 6057, 6458, 6886, 1329, 6240, 6056, 4781], "orig_top_k_doc_id": [5582, 6534, 5584, 6532, 6057, 5314, 6884, 6886, 6240, 5583, 6458, 6533, 1329, 6056, 4781]}, {"qid": 3356, "question": "what is the word level baseline they compare to? in Tweet2Vec: Character-Based Distributed Representations for Social Media", "answer": ["a simple word-level encoder, with the input as words instead of characters", "The encoder is essentially the same as tweet2vec, with the input as words instead of characters"], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 5584, 6057, 6458, 6886, 1329, 6240, 5288, 7259], "orig_top_k_doc_id": [5582, 5584, 6534, 6532, 5583, 6884, 6057, 6240, 5288, 5314, 6458, 6533, 6886, 1329, 7259]}, {"qid": 4370, "question": "did they experiment with other languages? in Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder", "answer": ["No", "No"], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 6885, 6886, 462, 5136, 6070, 5584, 6057, 7587], "orig_top_k_doc_id": [5314, 6884, 6886, 6532, 6534, 6533, 5584, 5583, 6885, 5582, 5136, 6057, 462, 6070, 7587]}, {"qid": 4373, "question": "what are the previous state of the art for tweet semantic similarity? in Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder", "answer": ["nnfeats, ikr, linearsvm and svckernel.", "nnfeats, ikr, linearsvm and svckernel."], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 6885, 6886, 462, 5136, 6070, 6512, 4842, 6058], "orig_top_k_doc_id": [5314, 6886, 6884, 6532, 6534, 6885, 6533, 5583, 5582, 462, 6070, 5136, 6512, 4842, 6058]}, {"qid": 3352, "question": "Does the paper clearly establish that the challenges listed here exist in this dataset and task? in Tweet2Vec: Character-Based Distributed Representations for Social Media", "answer": ["Yes"], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 5584, 6057, 6458, 6886, 6834, 241, 4948, 6519], "orig_top_k_doc_id": [5582, 5584, 6532, 6534, 6057, 6533, 5583, 5314, 6834, 6884, 241, 4948, 6458, 6886, 6519]}, {"qid": 4372, "question": "what are the previous state of the art for sentiment categorization? in Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder", "answer": ["INESC-ID,  lsislif, unitn and  Webis.", "INESC-ID, lsislif, unitn and Webis."], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 6885, 6886, 462, 5136, 6867, 5584, 4844, 4845], "orig_top_k_doc_id": [5314, 6884, 6886, 6532, 6534, 6885, 6867, 6533, 462, 5583, 5136, 5582, 5584, 4844, 4845]}, {"qid": 3353, "question": "Is this hashtag prediction task an established task, or something new? in Tweet2Vec: Character-Based Distributed Representations for Social Media", "answer": ["established task", "Hashtag prediction for social media has been addressed earlier"], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 5584, 6057, 6458, 2587, 2588, 6818, 1517, 5252], "orig_top_k_doc_id": [5582, 6533, 6532, 5584, 6534, 5583, 5314, 2587, 2588, 6057, 6884, 6818, 1517, 6458, 5252]}, {"qid": 4072, "question": "Do they report results only for English data? in Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks", "answer": ["No", "Yes", "No"], "top_k_doc_id": [6533, 6534, 6532, 6520, 7625, 1935, 2701, 3183, 6158, 6073, 5470, 4328, 5240, 3554, 4284], "orig_top_k_doc_id": [6532, 6533, 6534, 7625, 6073, 6158, 3183, 5470, 1935, 6520, 4328, 5240, 2701, 3554, 4284]}, {"qid": 4073, "question": "What conclusions do the authors draw from their experiments? in Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks", "answer": ["among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models, Tweet length affects the task prediction accuracies, LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive", "CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models, Tweet length affects the task prediction accuracies,, CNN, LSTM and BLSTM are extremely sensitive to word order", "Supervised models CNN, LSTM and BLSTM and unsupervised models BOW, DSSM, STV and T2V can  encapsulate most of the syntactic and social properties. Tweet length affects the task prediction accuracies for all models. LDA is insensitive to input word order, but, CNN, LSTM\nand BLSTM are not."], "top_k_doc_id": [6533, 6534, 6532, 6520, 7625, 1935, 2701, 3183, 6158, 5907, 1273, 252, 5180, 1290, 3184], "orig_top_k_doc_id": [6532, 6533, 6534, 7625, 5907, 1273, 1935, 6158, 3183, 6520, 252, 5180, 2701, 1290, 3184]}, {"qid": 4371, "question": "by how much did their system outperform previous tasks? in Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder", "answer": ["Sentiment classification task by 0,008  F1, and semantic similarity task by 0,003 F1.", "On paraphrase and semantic similarity proposed model has F1 score of 0.677 compared to best previous model result of 0.674, while on sentiment classification it has 0.656 compared to 0.648 of best previous result."], "top_k_doc_id": [6533, 6534, 6532, 5314, 5582, 5583, 6884, 6885, 6886, 5584, 4755, 6057, 4972, 4842, 4841], "orig_top_k_doc_id": [5314, 6884, 6886, 6532, 6533, 6534, 6885, 5584, 5582, 5583, 4755, 6057, 4972, 4842, 4841]}, {"qid": 4074, "question": "In what way does each classifier evaluate one of the syntactic or social properties which are salient for a tweet? in Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks", "answer": [" if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation", "Through 8 different property prediction tasks"], "top_k_doc_id": [6533, 6534, 6532, 6520, 7625, 3300, 4322, 4781, 5470, 6131, 6329, 344, 4328, 4896, 4378], "orig_top_k_doc_id": [6532, 6533, 6534, 7625, 6520, 3300, 4322, 4781, 5470, 6131, 6329, 344, 4328, 4896, 4378]}, {"qid": 4763, "question": "How do they perform semi-supervised learning? in AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification", "answer": ["On each step, a generative network is used to generate samples, then a classifier labels them to an extra class. A mix of generated data and real data is combined into a batch, then a gradient descent is performed on the batch,  and the parameters are updated.", "At each iteration of the training process, we first generate INLINEFORM6 samples from the generator G then we draw INLINEFORM7 samples from the real dataset., We then perform gradient descent on the AC-BLSTM and generative net G and finally update the parameters of both nets."], "top_k_doc_id": [6533, 6534, 382, 1662, 5213, 6232, 7420, 7421, 7422, 7423, 4211, 4210, 6229, 6532, 7116], "orig_top_k_doc_id": [7423, 7420, 7422, 7421, 6534, 382, 4211, 6232, 6533, 4210, 6229, 6532, 7116, 5213, 1662]}, {"qid": 4764, "question": "What are the five evaluated tasks? in AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification", "answer": ["Model is evaluated on six tasks: TREC, MR, SST-1, SST-2, SUBJ and YELP13.", "TREC, MR, SST, SUBJ, YELP13"], "top_k_doc_id": [6533, 6534, 382, 1662, 5213, 6232, 7420, 7421, 7422, 7423, 3825, 4945, 1345, 6231, 6262], "orig_top_k_doc_id": [7423, 7420, 7422, 7421, 6534, 3825, 6232, 1662, 5213, 4945, 6533, 1345, 382, 6231, 6262]}, {"qid": 2161, "question": "Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion? in SOC: hunting the underground inside story of the ethereum Social-network Opinion and Comment", "answer": ["Yes"], "top_k_doc_id": [6533, 3306, 2119, 5192, 936, 4131, 119, 6520, 3530, 2874, 2873, 7016, 3731, 6056, 3307], "orig_top_k_doc_id": [3306, 2119, 5192, 936, 4131, 6533, 119, 6520, 3530, 2874, 2873, 7016, 3731, 6056, 3307]}]}
{"group_id": 29, "group_size": 15, "items": [{"qid": 4022, "question": "what is the size of the introduced dataset? in Controversy in Context", "answer": ["608 controversial Wikipedia concepts, 3561 concepts", "About 1216 in dataset II, 3561 in dataset III.", "Dataset I  -  480 concepts, 240  controversial examples, and  240 not-controversial examples.\nDataset II -  608 controversial concepts\nDataset III -  3561 controversial concepts"], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 6474, 6475, 6476, 2081, 5381], "orig_top_k_doc_id": [6016, 446, 6476, 2080, 442, 6474, 443, 6017, 445, 6018, 441, 444, 6475, 2081, 5381]}, {"qid": 4026, "question": "what are the baselines? in Controversy in Context", "answer": ["Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN)", "Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016)."], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 6474, 6475, 6476, 2081, 7416], "orig_top_k_doc_id": [6016, 6018, 446, 6017, 442, 2080, 6474, 441, 443, 445, 6476, 444, 7416, 2081, 6475]}, {"qid": 4021, "question": "what are the existing datasets for this task? in Controversy in Context", "answer": ["480 concepts previously analyzed in BIBREF1, BIBREF4", "Dataset I created and analyzed in BIBREF1, BIBREF4", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives, Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017)., Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. "], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 6474, 6475, 6476, 7416, 7572], "orig_top_k_doc_id": [6474, 442, 2080, 446, 441, 6018, 6016, 445, 6017, 6476, 443, 444, 6475, 7416, 7572]}, {"qid": 4024, "question": "how was labeling done? in Controversy in Context", "answer": ["The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10", "10 or more annotators marked whether a topic was controversial or not. The score was then normalized on an integer scale of 0-10.", "As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia., For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random, The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10."], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 6474, 6475, 2081, 7416, 2621], "orig_top_k_doc_id": [443, 6018, 6474, 442, 441, 444, 2080, 446, 6017, 6016, 445, 2081, 7416, 6475, 2621]}, {"qid": 373, "question": "What are the state of the art measures? in Vocabulary-based Method for Quantifying Controversy in Social Media", "answer": ["Randomwalk, Walktrap, Louvain clustering"], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 2079, 3316, 7416, 1798, 6005], "orig_top_k_doc_id": [441, 446, 442, 7416, 6016, 1798, 444, 2080, 445, 443, 6018, 3316, 6017, 6005, 2079]}, {"qid": 4025, "question": "where does their dataset come from? in Controversy in Context", "answer": ["Wikipedia list of controversial issues, concepts whose Wikipedia pages are under edit protection", "Wikipedia ", "The topics from Wikipedia list of controversial issues that appear more than 50 times in Wikipedia, topics with their Wikipedia pages under edit protection."], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 2079, 3316, 7416, 6474, 6475], "orig_top_k_doc_id": [446, 442, 6474, 443, 6018, 441, 6016, 445, 6475, 444, 6017, 2080, 2079, 7416, 3316]}, {"qid": 4027, "question": "what tools did they use? in Controversy in Context", "answer": ["nearest-neighbor estimator, Naive Bayes model, bidirectional RNN", "No"], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 443, 6017, 6018, 6474, 6475, 2081, 5390, 2970], "orig_top_k_doc_id": [2080, 442, 6018, 446, 2081, 6016, 441, 443, 444, 6474, 5390, 2970, 6475, 445, 6017]}, {"qid": 374, "question": "What controversial topics are experimented with? in Vocabulary-based Method for Quantifying Controversy in Social Media", "answer": ["political events such as elections, corruption cases or justice decisions"], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 803, 2079, 2081, 6474, 7416, 6018, 6017, 6476], "orig_top_k_doc_id": [441, 446, 6016, 7416, 445, 444, 442, 6018, 2079, 2080, 6474, 6017, 2081, 6476, 803]}, {"qid": 375, "question": "What datasets did they use? in Vocabulary-based Method for Quantifying Controversy in Social Media", "answer": ["BIBREF32, BIBREF23, BIBREF33, discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. "], "top_k_doc_id": [441, 442, 446, 6016, 2080, 444, 445, 803, 2079, 2081, 6474, 7416, 5322, 443, 1798], "orig_top_k_doc_id": [441, 446, 442, 2080, 6016, 444, 2081, 7416, 5322, 2079, 445, 6474, 443, 803, 1798]}, {"qid": 4020, "question": "is this the first dataset with a grading scaling rather than binary? in Controversy in Context", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [441, 442, 446, 6016, 2080, 6474, 6475, 443, 6018, 2964, 566, 7416, 6017, 5379, 6476], "orig_top_k_doc_id": [6474, 446, 6475, 6016, 443, 6018, 442, 2964, 566, 441, 7416, 6017, 5379, 6476, 2080]}, {"qid": 4023, "question": "did they crowdsource annotations? in Controversy in Context", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [441, 442, 446, 6016, 2080, 3608, 6018, 6474, 6475, 2081, 5721, 444, 1646, 443, 5390], "orig_top_k_doc_id": [3608, 6018, 6474, 2080, 6475, 2081, 441, 442, 5721, 444, 446, 1646, 443, 5390, 6016]}, {"qid": 377, "question": "How many languages do they experiment with? in Vocabulary-based Method for Quantifying Controversy in Social Media", "answer": ["four different languages: English, Portuguese, Spanish and French"], "top_k_doc_id": [441, 442, 446, 6016, 444, 7416, 803, 5524, 6558, 6833, 6018, 5168, 443, 6474, 445], "orig_top_k_doc_id": [441, 446, 442, 444, 7416, 6016, 803, 5524, 6558, 6833, 6018, 5168, 443, 6474, 445]}, {"qid": 376, "question": "What social media platform is observed? in Vocabulary-based Method for Quantifying Controversy in Social Media", "answer": ["Twitter"], "top_k_doc_id": [441, 442, 446, 6016, 2079, 7416, 5812, 3316, 5191, 3730, 7028, 5102, 4279, 5322, 3553], "orig_top_k_doc_id": [441, 446, 6016, 2079, 7416, 442, 5812, 3316, 5191, 3730, 7028, 5102, 4279, 5322, 3553]}, {"qid": 1486, "question": "How is sentiment polarity measured? in Empirical Study on Detecting Controversy in Social Media", "answer": ["For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets"], "top_k_doc_id": [441, 442, 446, 7416, 554, 2081, 7114, 5180, 6474, 6770, 447, 448, 7752, 5927, 5906], "orig_top_k_doc_id": [7416, 441, 442, 554, 446, 2081, 7114, 5180, 6474, 6770, 447, 448, 7752, 5927, 5906]}, {"qid": 1485, "question": "How does the method measure the impact of the event on market prices? in Empirical Study on Detecting Controversy in Social Media", "answer": ["We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . "], "top_k_doc_id": [441, 442, 446, 2081, 2079, 4392, 4989, 4393, 3730, 2080, 3731, 2696, 6250, 3735, 7743], "orig_top_k_doc_id": [2081, 441, 2079, 4392, 4989, 4393, 3730, 2080, 442, 3731, 446, 2696, 6250, 3735, 7743]}]}
{"group_id": 30, "group_size": 15, "items": [{"qid": 4644, "question": "What datasets were used in this work? in Tackling Online Abuse: A Survey of Automated Abuse Detection Methods", "answer": ["DATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F,  DATA-WIKI-ATT,  DATA-WIKI-AGG,  DATA-WIKI-TOX,  DATA-FOX-NEWS,  DATA-GAZZETTA, DATA-FACEBOOK, Arabic News,  GermEval,  Ask.fm.", "DATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F, DATA-WIKI-ATT, DATA-WIKI-AGG, DATA-WIKI-TOX, DATA-FOX-NEWS, DATA-GAZZETTA, DATA-FACEBOOK, Arabic News, GermEval, Ask.fun"], "top_k_doc_id": [3581, 3583, 7256, 7262, 3582, 3585, 3587, 5085, 7261, 7807, 3584, 7258, 3588, 4140, 3593], "orig_top_k_doc_id": [7256, 7262, 7261, 3581, 7258, 3587, 3583, 3582, 3588, 3584, 7807, 5085, 3585, 3593, 4140]}, {"qid": 4645, "question": "How is abuse defined for the purposes of this research? in Tackling Online Abuse: A Survey of Automated Abuse Detection Methods", "answer": ["we define abuse as any expression that is meant to denigrate or offend a particular person or group.", "we define abuse as any expression that is meant to denigrate or offend a particular person or group."], "top_k_doc_id": [3581, 3583, 7256, 7262, 3582, 3585, 3587, 5085, 7261, 7807, 3584, 7258, 3588, 4140, 3586], "orig_top_k_doc_id": [7256, 7262, 7261, 3581, 3587, 3584, 3585, 3582, 3583, 7258, 7807, 3588, 4140, 5085, 3586]}, {"qid": 1305, "question": "What is the performance of the model for the German sub-task A? in HateMonitors: Language Agnostic Abuse Detection in Social Media", "answer": ["macro F1 score of 0.62"], "top_k_doc_id": [3581, 3583, 7256, 7262, 1788, 5144, 7259, 3574, 3585, 3587, 7261, 3575, 4137, 4948, 446], "orig_top_k_doc_id": [1788, 5144, 3574, 3575, 3581, 7256, 3583, 4137, 7262, 7261, 7259, 4948, 446, 3587, 3585]}, {"qid": 1306, "question": "Is the model tested for language identification? in HateMonitors: Language Agnostic Abuse Detection in Social Media", "answer": ["No"], "top_k_doc_id": [3581, 3583, 7256, 7262, 1788, 5144, 7259, 1725, 3575, 4140, 5085, 7261, 4136, 3584, 7807], "orig_top_k_doc_id": [1788, 7262, 5144, 5085, 7256, 7261, 4136, 3581, 3583, 3584, 3575, 7807, 7259, 1725, 4140]}, {"qid": 1307, "question": "Is the model compared to a baseline model? in HateMonitors: Language Agnostic Abuse Detection in Social Media", "answer": ["No"], "top_k_doc_id": [3581, 3583, 7256, 7262, 1788, 5144, 7259, 1725, 3575, 4140, 5085, 1701, 3587, 3736, 3586], "orig_top_k_doc_id": [1788, 7262, 7256, 1701, 3575, 3581, 3587, 7259, 5085, 3583, 3736, 1725, 4140, 5144, 3586]}, {"qid": 1308, "question": "What are the languages used to test the model? in HateMonitors: Language Agnostic Abuse Detection in Social Media", "answer": ["Hindi, English and German (German task won)"], "top_k_doc_id": [3581, 3583, 7256, 7262, 1788, 5144, 7259, 3574, 3585, 3587, 7261, 3582, 5085, 3588, 3586], "orig_top_k_doc_id": [1788, 3583, 3581, 7256, 7262, 3582, 5085, 5144, 3585, 7261, 7259, 3574, 3587, 3588, 3586]}, {"qid": 4643, "question": "Is deep learning the state-of-the-art method in automated abuse detection in Tackling Online Abuse: A Survey of Automated Abuse Detection Methods", "answer": ["Yes", "No"], "top_k_doc_id": [3581, 3583, 7256, 7262, 3582, 3585, 3587, 5085, 7261, 7807, 3584, 7258, 7260, 3586, 7259], "orig_top_k_doc_id": [7256, 7261, 7262, 7258, 5085, 3581, 7807, 3583, 3587, 3582, 3585, 3584, 7260, 3586, 7259]}, {"qid": 5014, "question": "What fusion methods are applied? in Abusive Language Detection in Online Conversations by Combining Content-and Graph-based Features", "answer": ["Early fusion, late fusion, hybrid fusion.", "Early Fusion, Late Fusion, Hybrid Fusion"], "top_k_doc_id": [3581, 3583, 1205, 3582, 3586, 5085, 5291, 6771, 7807, 7808, 7809, 7811, 1735, 7260, 7810], "orig_top_k_doc_id": [7807, 7811, 7809, 7808, 3581, 5291, 3583, 7810, 1205, 3582, 3586, 5085, 6771, 1735, 7260]}, {"qid": 5015, "question": "What graph-based features are considered? in Abusive Language Detection in Online Conversations by Combining Content-and Graph-based Features", "answer": ["Coreness Score, PageRank Centrality, Strength Centrality, Vertex Count, Closeness Centrality, Authority Score, Hub Score, Reciprocity, Closeness Centrality", "Top graph based features are: Coreness Score, PageRank Centrality, Strength Centrality, Vertex Count, Closeness Centrality, Closeness Centrality, Authority Score, Hub Score, Reciprocity and Closeness Centrality."], "top_k_doc_id": [3581, 3583, 1205, 3582, 3586, 5085, 5291, 6771, 7807, 7808, 7809, 7811, 1735, 7260, 6816], "orig_top_k_doc_id": [7807, 7811, 7809, 7808, 3581, 5291, 3583, 7260, 1205, 6816, 3582, 3586, 6771, 5085, 1735]}, {"qid": 4642, "question": "Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms? in Tackling Online Abuse: A Survey of Automated Abuse Detection Methods", "answer": ["Yes", "Yes"], "top_k_doc_id": [3581, 3583, 7256, 7262, 3582, 3585, 3587, 5085, 7261, 7807, 1725, 3586, 3588, 3593, 3592], "orig_top_k_doc_id": [7256, 5085, 7262, 3587, 7261, 3581, 3583, 3582, 7807, 1725, 3586, 3588, 3585, 3593, 3592]}, {"qid": 5012, "question": "What is the proposed algorithm or model architecture? in Abusive Language Detection in Online Conversations by Combining Content-and Graph-based Features", "answer": ["They combine content- and graph-based methods in new ways.", "Hybrid Fusion, Late Fusion, Early Fusion"], "top_k_doc_id": [3581, 3583, 1205, 3582, 3586, 5085, 5291, 6771, 7807, 7808, 7809, 7811, 1735, 1788, 3588], "orig_top_k_doc_id": [7807, 7811, 5291, 3581, 3583, 7808, 7809, 3582, 1205, 3586, 5085, 1788, 1735, 3588, 6771]}, {"qid": 2272, "question": "Have any baseline model been trained on this abusive language dataset? in Directions in Abusive Language Training Data: Garbage In, Garbage Out", "answer": ["No"], "top_k_doc_id": [3581, 3583, 3586, 3591, 3593, 6558, 6560, 3582, 3592, 7256, 5288, 5289, 3587, 3988, 1788], "orig_top_k_doc_id": [3593, 3581, 5288, 6558, 5289, 6560, 3587, 3988, 1788, 3586, 3583, 3582, 7256, 3592, 3591]}, {"qid": 2274, "question": "What is open website for cataloguing abusive language data? in Directions in Abusive Language Training Data: Garbage In, Garbage Out", "answer": ["hatespeechdata.com"], "top_k_doc_id": [3581, 3583, 3586, 3591, 3593, 6558, 6560, 3582, 3592, 7256, 3590, 2386, 7257, 3588, 3589], "orig_top_k_doc_id": [3581, 3593, 3592, 3582, 3590, 2386, 7256, 3591, 3583, 7257, 6560, 6558, 3588, 3589, 3586]}, {"qid": 5013, "question": "Do they attain state-of-the-art performance? in Abusive Language Detection in Online Conversations by Combining Content-and Graph-based Features", "answer": ["No", "No"], "top_k_doc_id": [3581, 3583, 1205, 3582, 3586, 5085, 5291, 6771, 7807, 7808, 7809, 7811, 7261, 7260, 3585], "orig_top_k_doc_id": [7807, 7811, 5291, 1205, 3581, 3583, 7261, 7809, 7260, 7808, 3582, 5085, 3586, 3585, 6771]}, {"qid": 2273, "question": "How big are this dataset and catalogue? in Directions in Abusive Language Training Data: Garbage In, Garbage Out", "answer": [" from 469 posts to 17 million"], "top_k_doc_id": [3581, 3583, 3586, 3591, 3593, 6558, 6560, 2386, 4329, 4629, 3635, 3963, 776, 2991, 775], "orig_top_k_doc_id": [3593, 3581, 2386, 4329, 4629, 3635, 3586, 3963, 776, 2991, 775, 6560, 3591, 3583, 6558]}]}
{"group_id": 31, "group_size": 14, "items": [{"qid": 24, "question": "What is the perWhat are the tasks evaluated? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["No"], "top_k_doc_id": [19, 20, 1147, 23, 1256, 1560, 3743, 4610, 5739, 5257, 4609, 6448, 5231, 5738, 5757], "orig_top_k_doc_id": [19, 20, 23, 5739, 1256, 3743, 4610, 5257, 4609, 6448, 5231, 5738, 1147, 1560, 5757]}, {"qid": 29, "question": "What are strong baseline models in specific tasks? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26"], "top_k_doc_id": [19, 20, 1147, 23, 1256, 1560, 3743, 4610, 5739, 1258, 1257, 1340, 3175, 2416, 6368], "orig_top_k_doc_id": [19, 23, 20, 1256, 1258, 5739, 1257, 1560, 3743, 1340, 4610, 3175, 1147, 2416, 6368]}, {"qid": 21, "question": "What are the specific tasks being unified? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": [" three types of questions, namely tumor size, proximal resection margin and distal resection margin", "No"], "top_k_doc_id": [19, 20, 1147, 23, 1256, 945, 2661, 5739, 490, 4609, 4610, 6441, 4646, 946, 1258], "orig_top_k_doc_id": [19, 20, 23, 490, 945, 5739, 1256, 4609, 4610, 6441, 4646, 946, 2661, 1147, 1258]}, {"qid": 23, "question": "How many questions are in the dataset? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["2,714 "], "top_k_doc_id": [19, 20, 1147, 23, 1256, 945, 2661, 5739, 3851, 3175, 5738, 1560, 5368, 3805, 2464], "orig_top_k_doc_id": [19, 23, 20, 5739, 1256, 945, 3851, 3175, 5738, 1147, 2661, 1560, 5368, 3805, 2464]}, {"qid": 26, "question": "How they introduce domain-specific features into pre-trained language model? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["integrate clinical named entity information into pre-trained language model"], "top_k_doc_id": [19, 20, 1147, 23, 1256, 1560, 3743, 4610, 7002, 21, 2306, 2307, 7351, 2413, 3175], "orig_top_k_doc_id": [19, 20, 23, 7002, 21, 2306, 2307, 1256, 3743, 7351, 1147, 1560, 2413, 3175, 4610]}, {"qid": 22, "question": "Is all text in this dataset a question, or are there unrelated sentences in between questions? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences "], "top_k_doc_id": [19, 20, 1147, 23, 5739, 5738, 945, 5258, 2234, 5231, 5735, 21, 2442, 5368, 4610], "orig_top_k_doc_id": [19, 20, 23, 1147, 5739, 5738, 945, 5258, 2234, 5231, 5735, 21, 2442, 5368, 4610]}, {"qid": 894, "question": "What baselines did they consider? in Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks", "answer": ["LSTM, SCIBERT"], "top_k_doc_id": [19, 20, 1147, 945, 1148, 1149, 1256, 3743, 3744, 4609, 5739, 6135, 6162, 5231, 4610], "orig_top_k_doc_id": [5739, 19, 3744, 20, 3743, 4609, 5231, 945, 1148, 6135, 4610, 1147, 1256, 6162, 1149]}, {"qid": 895, "question": "What are the problems related to ambiguity in PICO sentence prediction tasks? in Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks", "answer": ["Some sentences are associated to ambiguous dimensions in the hidden state output"], "top_k_doc_id": [19, 20, 1147, 945, 1148, 1149, 1256, 3743, 3744, 4609, 5739, 6135, 6162, 1152, 1150], "orig_top_k_doc_id": [1147, 1149, 1152, 1148, 1150, 19, 20, 3744, 4609, 6162, 5739, 945, 6135, 1256, 3743]}, {"qid": 20, "question": "How is the clinical text structuring task defined? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ", "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text."], "top_k_doc_id": [19, 20, 23, 3743, 5739, 4609, 5257, 5757, 6603, 1256, 21, 3054, 7832, 6423, 2621], "orig_top_k_doc_id": [19, 20, 23, 1256, 21, 3054, 7832, 6603, 5757, 6423, 5257, 3743, 2621, 5739, 4609]}, {"qid": 25, "question": "Are there privacy concerns with clinical data? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["No"], "top_k_doc_id": [19, 20, 23, 3743, 5739, 4609, 5257, 5757, 6603, 6711, 6714, 4646, 6712, 6713, 4613], "orig_top_k_doc_id": [19, 20, 23, 6711, 6714, 4646, 6712, 5739, 5257, 6713, 5757, 4609, 3743, 6603, 4613]}, {"qid": 18, "question": "What data is the language model pretrained on? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["Chinese general corpus", "No"], "top_k_doc_id": [19, 20, 23, 3743, 5739, 5737, 6656, 1256, 4688, 4901, 5231, 3617, 4649, 4646, 1340], "orig_top_k_doc_id": [19, 20, 23, 5737, 3743, 6656, 1256, 4688, 4901, 5231, 5739, 3617, 4649, 4646, 1340]}, {"qid": 19, "question": "What baselines is the proposed model compared against? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["BERT-Base, QANet", "QANet BIBREF39, BERT-Base BIBREF26"], "top_k_doc_id": [19, 20, 23, 1256, 5970, 5968, 3744, 1257, 1258, 5739, 2661, 2234, 4614, 4610, 2268], "orig_top_k_doc_id": [19, 20, 23, 1256, 5970, 5968, 3744, 1257, 1258, 5739, 2661, 2234, 4614, 4610, 2268]}, {"qid": 27, "question": "How big is QA-CTS task dataset? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["17,833 sentences, 826,987 characters and 2,714 question-answer pairs"], "top_k_doc_id": [19, 20, 23, 7804, 4649, 6449, 3805, 945, 5739, 1141, 4609, 3839, 5735, 4535, 7351], "orig_top_k_doc_id": [19, 23, 20, 7804, 4649, 6449, 3805, 945, 5739, 1141, 4609, 3839, 5735, 4535, 7351]}, {"qid": 28, "question": "How big is dataset of pathology reports collected from Ruijing Hospital? in Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "answer": ["17,833 sentences, 826,987 characters and 2,714 question-answer pairs"], "top_k_doc_id": [19, 20, 23, 21, 5489, 6603, 5490, 5004, 22, 4610, 5491, 4613, 5396, 5403, 7832], "orig_top_k_doc_id": [23, 19, 21, 5489, 6603, 5490, 20, 5004, 22, 4610, 5491, 4613, 5396, 5403, 7832]}]}
{"group_id": 32, "group_size": 14, "items": [{"qid": 341, "question": "How large is the dataset? in Exploring Hate Speech Detection in Multimodal Publications", "answer": [" $150,000$ tweets"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 3582, 3583, 6176, 3587, 6770, 5015, 3586], "orig_top_k_doc_id": [416, 412, 415, 413, 414, 3582, 3737, 3583, 3736, 3585, 3587, 6770, 6176, 5015, 3586]}, {"qid": 344, "question": "What metrics are used to benchmark the results? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 3582, 3583, 5015, 3738, 1726, 3587, 3588], "orig_top_k_doc_id": [416, 415, 412, 413, 414, 3737, 3736, 3582, 3583, 1726, 5015, 3738, 3587, 3585, 3588]}, {"qid": 346, "question": "How many tweats does MMHS150k contains, 150000? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["$150,000$ tweets"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 3582, 3583, 6176, 3587, 6770, 3309, 6131], "orig_top_k_doc_id": [416, 412, 415, 413, 414, 3582, 3736, 3737, 3583, 3587, 3585, 6176, 3309, 6770, 6131]}, {"qid": 348, "question": "What different models for multimodal detection were proposed? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 3582, 3583, 5015, 3738, 767, 7121, 6285], "orig_top_k_doc_id": [416, 412, 415, 413, 414, 3736, 3737, 5015, 3582, 3583, 3738, 3585, 767, 7121, 6285]}, {"qid": 339, "question": "What models do they propose? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 3582, 3583, 5015, 6285, 3588, 3586, 3589], "orig_top_k_doc_id": [412, 416, 413, 415, 414, 3582, 3736, 3737, 3583, 6285, 3588, 5015, 3586, 3585, 3589]}, {"qid": 347, "question": "What unimodal detection models were used? in Exploring Hate Speech Detection in Multimodal Publications", "answer": [" single layer LSTM with a 150-dimensional hidden state for hate / not hate classification"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 3582, 3583, 6176, 1726, 5015, 6131, 6285], "orig_top_k_doc_id": [412, 415, 416, 414, 413, 3582, 3736, 3583, 3737, 1726, 3585, 6176, 5015, 6131, 6285]}, {"qid": 340, "question": "Are all tweets in English? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["No"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3011, 3582, 5976, 6771, 6176, 1726, 3988, 5168], "orig_top_k_doc_id": [416, 412, 413, 415, 414, 6176, 1726, 3736, 5976, 6771, 3582, 3011, 3737, 3988, 5168]}, {"qid": 343, "question": "What is author's opinion on why current multimodal models cannot outperform models analyzing only text? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3585, 5015, 5907, 230, 1726, 276, 3738, 6770], "orig_top_k_doc_id": [416, 412, 415, 413, 414, 3736, 5015, 3737, 5907, 230, 1726, 276, 3738, 3585, 6770]}, {"qid": 349, "question": "What annotations are available in the dataset - tweat used hate speach or not? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities"], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 3011, 3582, 5976, 6771, 3588, 3587, 3589, 3583], "orig_top_k_doc_id": [416, 412, 413, 414, 415, 3582, 3736, 3588, 3737, 3587, 3589, 3583, 3011, 6771, 5976]}, {"qid": 342, "question": "What is the results of multimodal compared to unimodal models? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 "], "top_k_doc_id": [416, 412, 413, 414, 415, 3736, 3737, 5015, 930, 7141, 2900, 3738, 2899, 2904, 3582], "orig_top_k_doc_id": [412, 415, 416, 414, 413, 3736, 3737, 5015, 930, 7141, 2900, 3738, 2899, 2904, 3582]}, {"qid": 2336, "question": "What is the source of memes? in Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation", "answer": ["Google Images, Reddit Memes Dataset"], "top_k_doc_id": [416, 876, 3007, 3309, 3736, 3737, 5144, 415, 3738, 4137, 6519, 3585, 3045, 6131, 3582], "orig_top_k_doc_id": [3736, 3737, 416, 3738, 5144, 876, 3309, 3585, 3007, 3045, 6131, 4137, 3582, 415, 6519]}, {"qid": 2337, "question": "Is the dataset multimodal? in Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation", "answer": ["Yes"], "top_k_doc_id": [416, 876, 3007, 3309, 3736, 3737, 5144, 415, 3738, 4137, 6519, 414, 412, 413, 6771], "orig_top_k_doc_id": [3736, 416, 3737, 5144, 414, 412, 3738, 3309, 876, 413, 3007, 415, 6771, 6519, 4137]}, {"qid": 345, "question": "How is data collected, manual collection or Twitter api? in Exploring Hate Speech Detection in Multimodal Publications", "answer": ["Twitter API"], "top_k_doc_id": [416, 412, 413, 414, 415, 3585, 3586, 6176, 1727, 3583, 3582, 3007, 6286, 6770, 3988], "orig_top_k_doc_id": [413, 412, 414, 416, 3585, 3586, 6176, 1727, 415, 3583, 3582, 3007, 6286, 6770, 3988]}, {"qid": 2338, "question": "How is each instance of the dataset annotated? in Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation", "answer": ["weakly labeled into hate or non-hate memes, depending on their source"], "top_k_doc_id": [416, 876, 3007, 3309, 3736, 3737, 5144, 5976, 3574, 5977, 6771, 3045, 3585, 1788, 414], "orig_top_k_doc_id": [3736, 5144, 3309, 5976, 876, 3574, 416, 3007, 3737, 5977, 6771, 3045, 3585, 1788, 414]}]}
{"group_id": 33, "group_size": 14, "items": [{"qid": 469, "question": "How do they quantify moral relevance? in Text-based inference of moral sentiment change", "answer": ["By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 5526, 5529, 5524, 5525, 5535], "orig_top_k_doc_id": [557, 554, 556, 5528, 555, 5529, 5536, 5526, 1798, 1801, 5524, 1800, 1799, 5525, 5535]}, {"qid": 471, "question": "Which dataset sources to they use to demonstrate moral sentiment through history? in Text-based inference of moral sentiment change", "answer": ["No"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 5526, 5529, 5524, 5525, 1802], "orig_top_k_doc_id": [554, 557, 556, 5536, 5528, 1801, 555, 1802, 1800, 5529, 1798, 5526, 5525, 5524, 1799]}, {"qid": 466, "question": "Does the paper discuss previous models which have been applied to the same task? in Text-based inference of moral sentiment change", "answer": ["Yes"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 5524, 5525, 5526, 5529, 1798, 5535, 5, 330], "orig_top_k_doc_id": [557, 556, 554, 5536, 5529, 5524, 5, 5528, 1801, 5535, 1798, 5526, 5525, 555, 330]}, {"qid": 468, "question": "How does the parameter-free model work? in Text-based inference of moral sentiment change", "answer": ["A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Na\u00efve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 5524, 5525, 5526, 5529, 1798, 5535, 6135, 2886], "orig_top_k_doc_id": [554, 556, 557, 555, 5536, 1801, 6135, 2886, 5524, 1798, 5526, 5535, 5529, 5528, 5525]}, {"qid": 470, "question": "Which fine-grained moral dimension examples do they showcase? in Text-based inference of moral sentiment change", "answer": ["Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 5526, 5529, 5, 1802, 5535], "orig_top_k_doc_id": [557, 556, 554, 555, 5536, 5529, 1801, 1798, 5528, 1799, 1800, 5, 1802, 5526, 5535]}, {"qid": 1311, "question": "Do they report results only on English data? in BERT has a Moral Compass: Improvements of ethical and moral values of machines", "answer": ["Yes"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 1802, 1803, 5524, 5535, 5529], "orig_top_k_doc_id": [1801, 1802, 1798, 1799, 1803, 554, 5524, 1800, 557, 556, 555, 5528, 5536, 5529, 5535]}, {"qid": 1312, "question": "What is the Moral Choice Machine? in BERT has a Moral Compass: Improvements of ethical and moral values of machines", "answer": ["Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 1802, 1803, 5524, 5535, 5529], "orig_top_k_doc_id": [1801, 1802, 1798, 1799, 1803, 1800, 554, 555, 5524, 557, 556, 5536, 5535, 5528, 5529]}, {"qid": 1313, "question": "How is moral bias measured? in BERT has a Moral Compass: Improvements of ethical and moral values of machines", "answer": ["Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) \u2212 cos(b, q)\nBias is calculated as substraction of cosine similarities of question and some answer for two opposite answers."], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 1802, 1803, 5524, 5535, 5529], "orig_top_k_doc_id": [1801, 1802, 1798, 1799, 1803, 554, 1800, 5524, 5536, 557, 556, 555, 5528, 5535, 5529]}, {"qid": 1314, "question": "What sentence embeddings were used in the previous Jentzsch paper? in BERT has a Moral Compass: Improvements of ethical and moral values of machines", "answer": ["No"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 1802, 1803, 5524, 5535, 5529], "orig_top_k_doc_id": [1801, 1802, 1798, 1799, 1803, 1800, 554, 5524, 557, 556, 555, 5535, 5528, 5536, 5529]}, {"qid": 1315, "question": "How do the authors define deontological ethical reasoning? in BERT has a Moral Compass: Improvements of ethical and moral values of machines", "answer": ["These ask which choices are morally required, forbidden, or permitted, norms are understood as universal rules of what to do and what not to do"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 1798, 1799, 1800, 1802, 1803, 5524, 5535, 3591], "orig_top_k_doc_id": [1801, 1802, 1798, 1799, 554, 1803, 5524, 3591, 1800, 556, 557, 5536, 555, 5528, 5535]}, {"qid": 467, "question": "Which datasets are used in the paper? in Text-based inference of moral sentiment change", "answer": ["Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n"], "top_k_doc_id": [5536, 554, 555, 556, 557, 1801, 5528, 5524, 5525, 5526, 5529, 5, 1800, 6135, 926], "orig_top_k_doc_id": [554, 557, 556, 1801, 5536, 5528, 5524, 555, 5, 1800, 5525, 5526, 5529, 6135, 926]}, {"qid": 3306, "question": "Do they model semantics  in A Framework for the Computational Linguistic Analysis of Dehumanization", "answer": ["Yes", "Yes"], "top_k_doc_id": [5536, 5524, 5525, 5526, 5531, 5532, 5537, 5538, 2287, 5528, 6208, 2701, 2702, 2506, 7097], "orig_top_k_doc_id": [5524, 5537, 5525, 5538, 5536, 5526, 5528, 5531, 5532, 6208, 2287, 2701, 2702, 2506, 7097]}, {"qid": 3308, "question": "Do they analyze specific derogatory words? in A Framework for the Computational Linguistic Analysis of Dehumanization", "answer": ["Yes", "Yes"], "top_k_doc_id": [5536, 5524, 5525, 5526, 5531, 5532, 5537, 5538, 2287, 5528, 5976, 5977, 521, 6380, 523], "orig_top_k_doc_id": [5525, 5537, 5538, 5524, 5536, 5526, 5528, 5532, 5976, 5977, 5531, 2287, 521, 6380, 523]}, {"qid": 3307, "question": "How do they identify discussions of LGBTQ people in the New York Times? in A Framework for the Computational Linguistic Analysis of Dehumanization", "answer": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "top_k_doc_id": [5536, 5524, 5525, 5526, 5531, 5532, 5537, 5538, 5530, 5529, 5534, 5535, 5533, 4140, 1272], "orig_top_k_doc_id": [5524, 5537, 5536, 5538, 5525, 5531, 5526, 5530, 5532, 5529, 5534, 5535, 5533, 4140, 1272]}]}
{"group_id": 34, "group_size": 14, "items": [{"qid": 634, "question": "what was their system's f1 performance? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively."], "top_k_doc_id": [412, 1725, 7232, 7234, 1727, 6176, 1726, 5291, 6285, 771, 6131, 6894, 3989, 1077, 770], "orig_top_k_doc_id": [7232, 771, 5291, 6176, 1725, 3989, 412, 6894, 6285, 1077, 770, 1727, 7234, 6131, 1726]}, {"qid": 635, "question": "what was the baseline? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["logistic regression"], "top_k_doc_id": [412, 1725, 7232, 7234, 1727, 6176, 1726, 5291, 6285, 771, 6131, 6894, 3007, 3047, 6523], "orig_top_k_doc_id": [7232, 7234, 1725, 771, 6894, 1727, 6176, 6285, 6131, 412, 5291, 1726, 3007, 3047, 6523]}, {"qid": 631, "question": "how much was the parameter difference between their model and previous methods? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["our model requires 100k parameters , while BIBREF8 requires 250k parameters"], "top_k_doc_id": [412, 1725, 7232, 7234, 1727, 6176, 771, 3989, 5976, 6131, 6770, 1726, 5291, 876, 7235], "orig_top_k_doc_id": [7232, 7234, 1725, 6131, 1727, 412, 6770, 3989, 1726, 6176, 771, 5291, 5976, 876, 7235]}, {"qid": 632, "question": "how many parameters did their model use? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["Excluding the embedding weights, our model requires 100k parameters"], "top_k_doc_id": [412, 1725, 7232, 7234, 1727, 6176, 771, 3989, 5976, 6131, 414, 5292, 6285, 1788, 772], "orig_top_k_doc_id": [7232, 1725, 6131, 412, 414, 1727, 771, 5976, 5292, 3989, 6285, 7234, 6176, 1788, 772]}, {"qid": 633, "question": "which datasets were used? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["Sexist/Racist (SR) data set, HATE dataset, HAR"], "top_k_doc_id": [412, 1725, 7232, 7234, 1727, 6176, 1726, 5291, 6285, 5292, 1077, 3989, 3988, 1788, 3007], "orig_top_k_doc_id": [7232, 412, 5292, 1725, 6285, 5291, 7234, 6176, 1077, 3989, 1727, 3988, 1788, 1726, 3007]}, {"qid": 844, "question": "How do they combine the models? in Detecting Online Hate Speech Using Context Aware Models", "answer": ["maximum of two scores assigned by the two separate models, average score"], "top_k_doc_id": [412, 1725, 1077, 1080, 1726, 5168, 6285, 5905, 5906, 1735, 3581, 413, 414, 6770, 4140], "orig_top_k_doc_id": [1077, 413, 412, 6285, 1725, 5906, 1080, 414, 5905, 1726, 5168, 1735, 6770, 4140, 3581]}, {"qid": 846, "question": "What context do they use? in Detecting Online Hate Speech Using Context Aware Models", "answer": ["title of the news article, screen name of the user"], "top_k_doc_id": [412, 1725, 1077, 1080, 1726, 5168, 6285, 5905, 5906, 1735, 3581, 413, 414, 3445, 6286], "orig_top_k_doc_id": [1077, 6285, 412, 1080, 1725, 414, 1726, 413, 1735, 5905, 5906, 3581, 3445, 5168, 6286]}, {"qid": 627, "question": "Do they report results only on English data? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["Yes"], "top_k_doc_id": [412, 1725, 7232, 7234, 1727, 6176, 5976, 1726, 5977, 5291, 3312, 5292, 1788, 6770, 6894], "orig_top_k_doc_id": [7232, 5976, 1726, 5977, 1727, 1725, 5291, 7234, 3312, 412, 6176, 5292, 1788, 6770, 6894]}, {"qid": 629, "question": "What embedding algorithm and dimension size are used? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["300 Dimensional Glove"], "top_k_doc_id": [412, 1725, 7232, 7234, 414, 770, 771, 3312, 5291, 5292, 6285, 6894, 5293, 3045, 5288], "orig_top_k_doc_id": [7234, 7232, 771, 414, 6285, 5291, 6894, 1725, 5292, 5293, 3312, 412, 3045, 5288, 770]}, {"qid": 630, "question": "What data are the embeddings trained on? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["Common Crawl "], "top_k_doc_id": [412, 1725, 7232, 7234, 414, 770, 771, 3312, 5291, 5292, 6285, 6894, 6176, 3989, 6770], "orig_top_k_doc_id": [7232, 5292, 6176, 7234, 412, 771, 5291, 1725, 6894, 414, 3989, 6285, 770, 6770, 3312]}, {"qid": 845, "question": "What is their baseline? in Detecting Online Hate Speech Using Context Aware Models", "answer": ["Logistic regression model with character-level n-gram features"], "top_k_doc_id": [412, 1725, 1077, 1080, 1726, 5168, 6285, 5905, 5906, 1735, 3581, 6770, 3007, 5288, 6176], "orig_top_k_doc_id": [1077, 6285, 1080, 412, 1726, 1735, 1725, 5905, 3581, 5906, 5168, 6770, 3007, 5288, 6176]}, {"qid": 847, "question": "What is their definition of hate speech? in Detecting Online Hate Speech Using Context Aware Models", "answer": ["language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation"], "top_k_doc_id": [412, 1725, 1077, 1080, 1726, 5168, 6285, 5905, 5906, 6176, 6131, 413, 414, 6286, 6133], "orig_top_k_doc_id": [1077, 412, 6285, 6176, 6131, 413, 414, 1080, 6286, 1726, 5905, 5906, 5168, 1725, 6133]}, {"qid": 628, "question": "Which publicly available datasets are used? in Predictive Embeddings for Hate Speech Detection on Twitter", "answer": ["BIBREF3, BIBREF4, BIBREF9"], "top_k_doc_id": [412, 1725, 7232, 5291, 6285, 3989, 1077, 3574, 6894, 5168, 6286, 772, 5292, 5976, 3581], "orig_top_k_doc_id": [5291, 6285, 3989, 1077, 3574, 7232, 1725, 412, 6894, 5168, 6286, 772, 5292, 5976, 3581]}, {"qid": 848, "question": "What architecture has the neural network? in Detecting Online Hate Speech Using Context Aware Models", "answer": ["three parallel LSTM BIBREF21 layers"], "top_k_doc_id": [412, 1725, 1077, 1080, 1726, 5168, 6285, 6770, 414, 6176, 4499, 4515, 1735, 6375, 770], "orig_top_k_doc_id": [1077, 1080, 6285, 6770, 5168, 412, 1726, 414, 6176, 4499, 4515, 1735, 6375, 770, 1725]}]}
{"group_id": 35, "group_size": 14, "items": [{"qid": 3368, "question": "Which languages do they use? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["English", "English"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 80, 3414, 2690, 7046, 1245, 84, 5592, 1040, 81], "orig_top_k_doc_id": [6765, 5589, 5590, 5594, 5593, 80, 5591, 2690, 7046, 1245, 84, 5592, 3414, 1040, 81]}, {"qid": 3369, "question": "How large is their data set? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 80, 3414, 230, 7140, 4902, 6405, 7138, 5219, 7083], "orig_top_k_doc_id": [5589, 6765, 5590, 5594, 5593, 5591, 230, 7140, 4902, 6405, 7138, 80, 5219, 7083, 3414]}, {"qid": 3362, "question": "Which fonts are the best indicators of high quality? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["No"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 80, 4595, 4869, 2544, 2714, 5147, 230, 1244, 5592], "orig_top_k_doc_id": [5589, 5594, 5593, 5590, 5591, 4595, 6765, 4869, 2544, 2714, 5147, 80, 230, 1244, 5592]}, {"qid": 3363, "question": "What kind of model do they use? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 1244, 3175, 6405, 1941, 4756, 2804, 1247, 2124, 7215], "orig_top_k_doc_id": [5590, 5589, 6765, 5594, 5593, 6405, 5591, 1941, 4756, 2804, 1244, 3175, 1247, 2124, 7215]}, {"qid": 3366, "question": "What is their system's absolute accuracy? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 1244, 3175, 3861, 6936, 59, 3414, 2233, 5592, 6826], "orig_top_k_doc_id": [5594, 5589, 5590, 5593, 6765, 1244, 3861, 6936, 5591, 59, 3175, 3414, 2233, 5592, 6826]}, {"qid": 3754, "question": "what model is used? in Lexical Bias In Essay Level Prediction", "answer": ["gradient boosted trees", "Light Gradient Boosting Machine", "gradient boosted trees"], "top_k_doc_id": [5590, 5210, 6108, 6109, 7078, 3547, 3549, 7039, 973, 1934, 2755, 7077, 775, 1920, 3949], "orig_top_k_doc_id": [5590, 6108, 5210, 3547, 6109, 7078, 3549, 2755, 1934, 7077, 775, 7039, 1920, 3949, 973]}, {"qid": 3755, "question": "what future work is described? in Lexical Bias In Essay Level Prediction", "answer": ["the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not", "Investigate the effectiveness of LDA to capture the subject of the essay.", "investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used"], "top_k_doc_id": [5590, 5210, 6108, 6109, 7078, 3547, 3549, 7039, 973, 1934, 5537, 6270, 1273, 1283, 3332], "orig_top_k_doc_id": [5590, 6108, 5210, 3549, 7039, 3547, 5537, 6270, 1273, 973, 1283, 3332, 6109, 1934, 7078]}, {"qid": 3364, "question": "Did they release their data set of academic papers? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["No", "No"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 2388, 2875, 5592, 6461, 6462, 2046, 2386, 2394, 2400], "orig_top_k_doc_id": [6461, 6462, 5589, 5591, 5590, 2388, 6765, 2875, 2046, 2386, 2394, 5594, 5592, 2400, 5593]}, {"qid": 3365, "question": "Do the methods that work best on academic papers also work best on Wikipedia? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["Yes", "No"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 6765, 2388, 2875, 5592, 6461, 6462, 4870, 5045, 5044, 3487], "orig_top_k_doc_id": [5589, 6462, 5591, 5590, 6461, 5592, 5594, 4870, 5593, 5045, 5044, 6765, 2388, 2875, 3487]}, {"qid": 3756, "question": "what was the baseline? in Lexical Bias In Essay Level Prediction", "answer": ["No", "No", "No"], "top_k_doc_id": [5590, 5210, 6108, 6109, 7078, 3547, 3549, 7039, 5011, 3548, 6453, 5010, 3949, 3615, 7297], "orig_top_k_doc_id": [5590, 6108, 5210, 3547, 6109, 5011, 7078, 3548, 6453, 5010, 3549, 7039, 3949, 3615, 7297]}, {"qid": 3367, "question": "Which is more useful, visual or textual features? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. "], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 2899, 2900, 413, 2904, 5592, 7143, 416, 3658, 3175, 5015], "orig_top_k_doc_id": [5594, 5590, 5589, 5593, 5591, 2899, 413, 2904, 5592, 7143, 2900, 416, 3658, 3175, 5015]}, {"qid": 3370, "question": "Where do they get their ground truth quality judgments? in A Joint Model for Multimodal Document Quality Assessment", "answer": ["Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (\u201cFA\u201d), Good Article (\u201cGA\u201d), B-class Article (\u201cB\u201d), C-class Article (\u201cC\u201d), Start Article (\u201cStart\u201d), and Stub Article (\u201cStub\u201d)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). ", "quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI"], "top_k_doc_id": [5590, 5589, 5591, 5593, 5594, 2899, 2900, 6765, 5128, 7085, 3182, 4794, 5152, 936, 1943], "orig_top_k_doc_id": [6765, 5128, 5589, 7085, 3182, 5590, 4794, 5152, 5591, 5594, 936, 5593, 2899, 2900, 1943]}, {"qid": 3752, "question": "what features of the essays are extracted? in Lexical Bias In Essay Level Prediction", "answer": ["Following groups of features are extracted:\n- Numerical Features\n- Language Models\n- Clusters\n- Latent Dirichlet Allocation\n- Part-Of-Speech\n- Bag-of-words", "Numerical features, language models features, clusters, latent Dirichlet allocation, Part-of-Speech tags, Bag-of-words.", "Numerical features, Language Models, Clusters, Latent Dirichlet Allocation, Part-Of-Speech tags, Bag-of-words"], "top_k_doc_id": [5590, 5210, 6108, 6109, 7078, 7077, 5376, 7855, 3615, 7297, 980, 861, 3614, 3949, 4581], "orig_top_k_doc_id": [6108, 5590, 6109, 7077, 7078, 5376, 7855, 3615, 7297, 980, 861, 3614, 3949, 5210, 4581]}, {"qid": 3753, "question": "what were the evaluation metrics? in Lexical Bias In Essay Level Prediction", "answer": ["Accuracy metric", "accuracy", "Accuracy"], "top_k_doc_id": [5590, 5210, 6108, 3719, 3335, 6452, 6270, 566, 3615, 788, 6453, 3547, 5011, 1920, 3614], "orig_top_k_doc_id": [6108, 5590, 3719, 3335, 6452, 6270, 566, 3615, 788, 5210, 6453, 3547, 5011, 1920, 3614]}]}
{"group_id": 36, "group_size": 13, "items": [{"qid": 323, "question": "What approach did previous models use for multi-span questions? in Tag-based Multi-Span Extraction in Reading Comprehension", "answer": ["Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span"], "top_k_doc_id": [2837, 384, 388, 1512, 1853, 1961, 385, 2838, 4640, 2234, 4075, 3416, 2839, 4076, 1966], "orig_top_k_doc_id": [384, 388, 4075, 2837, 1961, 1512, 1853, 2234, 2838, 385, 3416, 4640, 2839, 4076, 1966]}, {"qid": 327, "question": "What is the previous model that attempted to tackle multi-span questions as a part of its design? in Tag-based Multi-Span Extraction in Reading Comprehension", "answer": ["MTMSN BIBREF4"], "top_k_doc_id": [2837, 384, 388, 1512, 1853, 1961, 385, 2838, 4640, 2234, 2755, 1965, 2759, 4188, 5259], "orig_top_k_doc_id": [384, 388, 1512, 1961, 1853, 2755, 385, 2838, 2234, 1965, 2837, 4640, 2759, 4188, 5259]}, {"qid": 1910, "question": "How big are the datasets used? in Cross-Lingual Machine Reading Comprehension", "answer": ["Evaluation datasets used:\nCMRC 2018 - 18939 questions, 10 answers\nDRCD - 33953 questions, 5 answers\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\n\nSource language train data:\nSQuAD - Not specified"], "top_k_doc_id": [2837, 2836, 2838, 2839, 2840, 4189, 1370, 3805, 4791, 4415, 4752, 4790, 4788, 4414, 1052], "orig_top_k_doc_id": [2836, 2840, 2839, 2837, 4415, 2838, 1370, 4790, 4752, 4788, 4791, 4414, 3805, 4189, 1052]}, {"qid": 1912, "question": "Are the contexts in a language different from the questions? in Cross-Lingual Machine Reading Comprehension", "answer": ["No"], "top_k_doc_id": [2837, 2836, 2838, 2839, 2840, 4189, 1370, 3805, 4791, 4415, 4752, 7589, 7590, 2234, 2442], "orig_top_k_doc_id": [2836, 2840, 2837, 2839, 4415, 7589, 4791, 1370, 3805, 7590, 2234, 2442, 2838, 4189, 4752]}, {"qid": 324, "question": "How they use sequence tagging to answer multi-span questions? in Tag-based Multi-Span Extraction in Reading Comprehension", "answer": ["To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span"], "top_k_doc_id": [2837, 384, 388, 1512, 1853, 1961, 385, 2838, 4640, 1966, 1965, 386, 387, 1964, 4189], "orig_top_k_doc_id": [388, 385, 1961, 384, 1512, 1966, 1965, 2837, 386, 387, 4640, 2838, 1964, 4189, 1853]}, {"qid": 325, "question": "What is difference in peformance between proposed model and state-of-the art on other question types? in Tag-based Multi-Span Extraction in Reading Comprehension", "answer": ["For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1."], "top_k_doc_id": [2837, 384, 388, 1512, 1853, 1961, 2234, 2755, 4075, 2840, 2836, 1965, 2839, 7727, 7728], "orig_top_k_doc_id": [1512, 2234, 1961, 4075, 2840, 2836, 1965, 2839, 7727, 388, 2755, 384, 2837, 7728, 1853]}, {"qid": 326, "question": "What is the performance of proposed model on entire DROP dataset? in Tag-based Multi-Span Extraction in Reading Comprehension", "answer": ["The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev"], "top_k_doc_id": [2837, 384, 388, 1512, 1853, 1961, 2234, 2755, 4075, 352, 7573, 5260, 4188, 2048, 5259], "orig_top_k_doc_id": [388, 384, 2837, 1961, 4075, 352, 1853, 2234, 7573, 1512, 2755, 5260, 4188, 2048, 5259]}, {"qid": 1418, "question": "What two components are included in their proposed framework? in S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension", "answer": ["evidence extraction and answer synthesis"], "top_k_doc_id": [2837, 2836, 4075, 4637, 5226, 352, 1961, 1964, 1965, 1966, 2442, 4910, 2519, 1357, 2012], "orig_top_k_doc_id": [1964, 1966, 1961, 4075, 1965, 4637, 5226, 2837, 352, 2519, 1357, 4910, 2836, 2442, 2012]}, {"qid": 1419, "question": "Which framework they propose in this paper? in S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension", "answer": [" extraction-then-synthesis framework"], "top_k_doc_id": [2837, 2836, 4075, 4637, 5226, 352, 1961, 1964, 1965, 1966, 2442, 4910, 5266, 1512, 3805], "orig_top_k_doc_id": [1966, 1961, 1965, 1964, 5226, 2836, 2837, 4637, 2442, 4910, 5266, 1512, 3805, 352, 4075]}, {"qid": 1911, "question": "Is this a span-based (extractive) QA task? in Cross-Lingual Machine Reading Comprehension", "answer": ["Yes"], "top_k_doc_id": [2837, 2836, 2838, 2839, 2840, 4189, 1370, 3805, 4791, 2519, 1141, 7459, 2442, 7359, 1146], "orig_top_k_doc_id": [2836, 2519, 2837, 2840, 2839, 1141, 4189, 7459, 3805, 2442, 2838, 7359, 1146, 4791, 1370]}, {"qid": 1349, "question": "How much better does this baseline neural model do? in Learning Open Information Extraction of Implicit Relations from Reading Comprehension Datasets", "answer": ["The model outperforms at every point in the\nimplicit-tuples PR curve reaching almost 0.8 in recall"], "top_k_doc_id": [2837, 2836, 4075, 4637, 5226, 1853, 4074, 1533, 1422, 2519, 3825, 1512, 1920, 1822, 2839], "orig_top_k_doc_id": [1853, 4074, 4075, 2837, 1533, 1422, 2519, 3825, 4637, 1512, 1920, 2836, 1822, 5226, 2839]}, {"qid": 2199, "question": "What is the data selection paper in machine translation in Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension", "answer": ["BIBREF7, BIBREF26 "], "top_k_doc_id": [2837, 2836, 2838, 2839, 2840, 4189, 3416, 490, 1808, 4188, 3421, 7727, 4788, 4535, 1743], "orig_top_k_doc_id": [3416, 2839, 2837, 2840, 2836, 2838, 490, 1808, 4188, 3421, 7727, 4788, 4535, 1743, 4189]}, {"qid": 2453, "question": "How is the input triple translated to a slot-filling task? in Zero-Shot Relation Extraction via Reading Comprehension", "answer": ["The relation R(x,y) is mapped onto a question q whose answer is y"], "top_k_doc_id": [2837, 2836, 2838, 2839, 2840, 4075, 4074, 4077, 6395, 4076, 3480, 4078, 7138, 3481, 1979], "orig_top_k_doc_id": [4075, 4074, 4077, 6395, 2837, 4076, 3480, 2836, 2840, 2838, 4078, 7138, 2839, 3481, 1979]}]}
{"group_id": 37, "group_size": 13, "items": [{"qid": 432, "question": "Why is supporting fact supervision necessary for DMN? in Dynamic Memory Networks for Visual and Textual Question Answering", "answer": ["First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU."], "top_k_doc_id": [7803, 7804, 507, 510, 511, 160, 7800, 7801, 512, 508, 509, 1154, 2752, 513, 164], "orig_top_k_doc_id": [7803, 507, 511, 510, 7801, 7804, 512, 508, 7800, 2752, 513, 160, 1154, 509, 164]}, {"qid": 433, "question": "What does supporting fact supervision mean? in Dynamic Memory Networks for Visual and Textual Question Answering", "answer": [" the facts that are relevant for answering a particular question) are labeled during training."], "top_k_doc_id": [7803, 7804, 507, 510, 511, 160, 7800, 7801, 512, 164, 2752, 3175, 1154, 2733, 2371], "orig_top_k_doc_id": [1154, 7800, 2752, 7803, 7804, 507, 7801, 160, 510, 511, 3175, 512, 164, 2733, 2371]}, {"qid": 435, "question": "What improvements they did for DMN? in Dynamic Memory Networks for Visual and Textual Question Answering", "answer": ["the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training., In addition, we introduce a new input module to represent images."], "top_k_doc_id": [7803, 7804, 507, 510, 511, 160, 7800, 7801, 512, 508, 509, 1154, 3175, 7805, 7142], "orig_top_k_doc_id": [507, 510, 7803, 511, 7801, 7804, 508, 3175, 7800, 512, 7805, 160, 1154, 509, 7142]}, {"qid": 436, "question": "How does the model circumvent the lack of supporting facts during training? in Dynamic Memory Networks for Visual and Textual Question Answering", "answer": ["the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. "], "top_k_doc_id": [7803, 7804, 507, 510, 511, 160, 7800, 7801, 512, 164, 2752, 3175, 4914, 508, 7802], "orig_top_k_doc_id": [507, 7800, 510, 7804, 511, 7803, 2752, 7801, 160, 4914, 3175, 508, 512, 164, 7802]}, {"qid": 437, "question": "Does the DMN+ model establish state-of-the-art ? in Dynamic Memory Networks for Visual and Textual Question Answering", "answer": ["Yes"], "top_k_doc_id": [7803, 7804, 507, 510, 511, 160, 7800, 7801, 512, 508, 509, 1154, 3175, 7805, 7142], "orig_top_k_doc_id": [507, 511, 510, 7803, 7801, 7804, 512, 7800, 3175, 160, 508, 509, 1154, 7805, 7142]}, {"qid": 4054, "question": "What text classification tasks are considered? in Episodic Memory in Lifelong Language Learning", "answer": ["news classification, sentiment analysis, Wikipedia article classification, questions and answers categorization ", " AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes)", "news classification, sentiment analysis, Wikipedia article classification"], "top_k_doc_id": [7803, 7804, 507, 510, 511, 509, 512, 6508, 6509, 6510, 6511, 7805, 4719, 4720, 4721], "orig_top_k_doc_id": [6508, 6511, 6509, 6510, 7804, 510, 4720, 7803, 507, 511, 7805, 4721, 509, 512, 4719]}, {"qid": 4055, "question": "Do they compare against other models? in Episodic Memory in Lifelong Language Learning", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [7803, 7804, 507, 510, 511, 509, 512, 6508, 6509, 6510, 6511, 7805, 4719, 4720, 7459], "orig_top_k_doc_id": [6508, 6509, 6511, 510, 6510, 7804, 4720, 7803, 511, 7805, 509, 507, 7459, 4719, 512]}, {"qid": 434, "question": "What changes they did on input module? in Dynamic Memory Networks for Visual and Textual Question Answering", "answer": ["For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, The second component is the input fusion layer"], "top_k_doc_id": [7803, 7804, 507, 510, 511, 160, 7800, 7801, 3175, 1154, 161, 2737, 7148, 2752, 2733], "orig_top_k_doc_id": [160, 507, 510, 511, 3175, 1154, 161, 7800, 2737, 7148, 7803, 7801, 2752, 7804, 2733]}, {"qid": 4056, "question": "What is episodic memory? in Episodic Memory in Lifelong Language Learning", "answer": ["module that stores previously seen examples throughout its lifetime, used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer", "It is a memory that stores previously seen examples throughout its lifetime"], "top_k_doc_id": [7803, 7804, 507, 510, 511, 509, 512, 6508, 6509, 6510, 6511, 7805, 7459, 7142, 7373], "orig_top_k_doc_id": [6508, 6511, 6509, 7803, 7804, 6510, 510, 509, 507, 7805, 511, 7459, 512, 7142, 7373]}, {"qid": 5008, "question": "What are the baselines for this paper? in Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks", "answer": ["LSTM-Att BIBREF7 , a LSTM model with spatial attention, MemAUG BIBREF33 : a memory-augmented model for VQA, MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling, MLAN BIBREF11 : an advanced multi-level attention model", "Ablated versions of the full model (without external knowledge, without memory network); alternative VQA methods: LSTM-Att, MemAUG, MCB+Att, MLAN", "LSTM with attention, memory augmented model, "], "top_k_doc_id": [7803, 7804, 7800, 7801, 7802, 7805, 7806, 2737, 3175, 7163, 7514, 1154, 495, 2759, 5793], "orig_top_k_doc_id": [7805, 7801, 7800, 7804, 7806, 7803, 7802, 7514, 2737, 3175, 1154, 495, 2759, 7163, 5793]}, {"qid": 5009, "question": "What VQA datasets are used for evaluating this task?  in Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks", "answer": ["Visual7W, a collection of open-domain visual question-answer pairs", "Visual7W and an automatically constructed open-domain VQA dataset"], "top_k_doc_id": [7803, 7804, 7800, 7801, 7802, 7805, 7806, 2737, 3175, 7163, 7147, 510, 4270, 4267, 7164], "orig_top_k_doc_id": [7801, 7800, 7805, 7804, 7806, 7803, 7802, 7163, 3175, 2737, 7147, 510, 4270, 4267, 7164]}, {"qid": 5010, "question": "How do they model external knowledge?  in Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks", "answer": ["Word embeddings from knowledge triples (subject, rel, object) from ConceptNet are fed to an RNN"], "top_k_doc_id": [7803, 7804, 7800, 7801, 7802, 7805, 7806, 1154, 4278, 4637, 7147, 7514, 7518, 4638, 4640], "orig_top_k_doc_id": [7805, 7800, 7801, 7806, 7804, 7803, 7514, 1154, 4637, 4278, 7802, 7147, 4638, 7518, 4640]}, {"qid": 5011, "question": "What type of external knowledge has been used for this paper?  in Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks", "answer": ["ConceptNet, which contains common-sense relationships between daily words", "an open multilingual knowledge graph containing common-sense relationships between daily words"], "top_k_doc_id": [7803, 7804, 7800, 7801, 7802, 7805, 7806, 1154, 4278, 4637, 7147, 7514, 7518, 495, 3175], "orig_top_k_doc_id": [7805, 7801, 7806, 7800, 7804, 7803, 7514, 1154, 7802, 7518, 4637, 7147, 4278, 495, 3175]}]}
{"group_id": 38, "group_size": 13, "items": [{"qid": 757, "question": "What is the source of the \"control\" corpus? in What we write about when we write about causality: Features of causal statements across large-scale social discourse", "answer": ["Randomly selected from a Twitter dump, temporally matched to causal documents"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 3161, 3162, 3163, 5950, 953, 5908, 4825], "orig_top_k_doc_id": [954, 951, 4780, 955, 4781, 953, 4783, 3161, 4784, 4782, 3163, 3162, 5908, 5950, 4825]}, {"qid": 761, "question": "How do they collect the control corpus? in What we write about when we write about causality: Features of causal statements across large-scale social discourse", "answer": ["Randomly from Twitter"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 3161, 3162, 3163, 5950, 953, 4577, 1082], "orig_top_k_doc_id": [951, 954, 4780, 955, 4783, 4784, 4781, 4782, 3161, 953, 3162, 4577, 1082, 5950, 3163]}, {"qid": 760, "question": "how do they collect the comparable corpus? in What we write about when we write about causality: Features of causal statements across large-scale social discourse", "answer": ["Randomly from a Twitter dump"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 3161, 3162, 3163, 5950, 953, 6805, 6417], "orig_top_k_doc_id": [4780, 951, 954, 4783, 4784, 4781, 4782, 3161, 955, 3162, 5950, 3163, 6805, 953, 6417]}, {"qid": 756, "question": "How do they extract causality from text? in What we write about when we write about causality: Features of causal statements across large-scale social discourse", "answer": ["They identify documents that contain the unigrams 'caused', 'causing', or 'causes'"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 3161, 3162, 3163, 5950, 3164, 3165, 4825], "orig_top_k_doc_id": [4780, 951, 4783, 3161, 3162, 4782, 4784, 954, 4781, 3163, 3164, 3165, 955, 5950, 4825]}, {"qid": 758, "question": "What are the selection criteria for \"causal statements\"? in What we write about when we write about causality: Features of causal statements across large-scale social discourse", "answer": ["Presence of only the exact unigrams 'caused', 'causing', or 'causes'"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 3161, 3162, 3163, 953, 3875, 2661, 3164], "orig_top_k_doc_id": [951, 954, 4780, 955, 4781, 953, 4784, 4783, 4782, 3161, 3162, 3163, 3875, 2661, 3164]}, {"qid": 759, "question": "Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora? in What we write about when we write about causality: Features of causal statements across large-scale social discourse", "answer": ["Only automatic methods"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 3161, 3162, 6805, 5718, 3588, 5913, 3587], "orig_top_k_doc_id": [4780, 951, 954, 6805, 3161, 4783, 4784, 5718, 955, 3588, 4781, 3162, 5913, 3587, 4782]}, {"qid": 2081, "question": "How efective is MCDN for ambiguous and implicit causality inference compared to state-of-the-art? in A Multi-level Neural Network for Implicit Causality Detection in Web Texts", "answer": ["No"], "top_k_doc_id": [4780, 4781, 4783, 3161, 3162, 3163, 3165, 3164, 3166, 4782, 4784, 2004, 5945, 7073, 709], "orig_top_k_doc_id": [3162, 3161, 3163, 3166, 3165, 3164, 4783, 4780, 4781, 2004, 4782, 4784, 5945, 7073, 709]}, {"qid": 2082, "question": "What performance did proposed method achieve, how much better is than previous state-of-the-art? in A Multi-level Neural Network for Implicit Causality Detection in Web Texts", "answer": ["increases F1-score by 10.2% and 3% compared with the existing best systems $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$"], "top_k_doc_id": [4780, 4781, 4783, 3161, 3162, 3163, 3165, 709, 1256, 3416, 7111, 6666, 4303, 713, 4782], "orig_top_k_doc_id": [3162, 3161, 4783, 4780, 3416, 4781, 3163, 6666, 4303, 1256, 709, 7111, 3165, 713, 4782]}, {"qid": 2083, "question": "What was previous state-of-the-art approach? in A Multi-level Neural Network for Implicit Causality Detection in Web Texts", "answer": ["TextCNN, TextRNN, SASE, DPCNN, and BERT, $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$ are the best systems with the highest recall and F1-score respectively"], "top_k_doc_id": [4780, 4781, 4783, 3161, 3162, 3163, 3165, 709, 1256, 3416, 7111, 3164, 2004, 6016, 4784], "orig_top_k_doc_id": [3162, 3161, 3163, 4780, 4783, 4781, 7111, 3416, 3164, 2004, 3165, 709, 1256, 6016, 4784]}, {"qid": 2084, "question": "How is Relation network used to infer causality at segment level? in A Multi-level Neural Network for Implicit Causality Detection in Web Texts", "answer": ["we integrate object-pairs with global representation and make a pair-wise inference to detect the relationship among the segments"], "top_k_doc_id": [4780, 4781, 4783, 3161, 3162, 3163, 3165, 3164, 3166, 4782, 4784, 7578, 4632, 951, 4633], "orig_top_k_doc_id": [3162, 3161, 3163, 3164, 4780, 4783, 3166, 4781, 4782, 3165, 4784, 7578, 4632, 951, 4633]}, {"qid": 2735, "question": "What baselines did they consider? in Causal Explanation Analysis on Social Media", "answer": ["state-of-the-art PDTB taggers", "Linear SVM, RBF SVM, and Random Forest"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 953, 1481, 4002, 236, 5472, 882, 5908], "orig_top_k_doc_id": [4780, 4784, 4781, 951, 4783, 954, 4782, 955, 953, 4002, 1481, 236, 5472, 882, 5908]}, {"qid": 2736, "question": "What types of social media did they consider? in Causal Explanation Analysis on Social Media", "answer": ["Facebook status update messages", "Facebook status update messages"], "top_k_doc_id": [4780, 4781, 4783, 951, 954, 955, 4782, 4784, 953, 1481, 4002, 441, 5906, 3553, 447], "orig_top_k_doc_id": [4780, 4781, 4784, 951, 4782, 955, 954, 1481, 953, 4002, 441, 4783, 5906, 3553, 447]}, {"qid": 2394, "question": "What are causal attribution networks? in Inferring the size of the causal universe: features and fusion of causal attribution networks", "answer": ["networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans"], "top_k_doc_id": [4780, 3877, 3876, 3875, 3874, 3872, 954, 5241, 3161, 3162, 3163, 955, 236, 951, 3164], "orig_top_k_doc_id": [3877, 3876, 3875, 3874, 3872, 954, 5241, 4780, 3161, 3162, 3163, 955, 236, 951, 3164]}]}
{"group_id": 39, "group_size": 13, "items": [{"qid": 810, "question": "What are new best results on standard benchmark? in Duality Regularization for Unsupervised Bilingual Lexicon Induction", "answer": ["New best results of accuracy (P@1) on Vecmap:\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 1014, 1015, 1016, 4511, 2972, 1044, 2164, 83, 2163, 4221], "orig_top_k_doc_id": [1015, 1016, 1014, 2972, 2164, 2162, 84, 80, 2163, 5714, 1040, 4511, 1044, 83, 4221]}, {"qid": 813, "question": "What 6 language pairs is experimented on? in Duality Regularization for Unsupervised Bilingual Lexicon Induction", "answer": ["EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 1014, 1015, 1016, 4511, 2972, 1044, 2164, 83, 3748, 5715], "orig_top_k_doc_id": [1015, 1014, 3748, 2162, 1016, 2972, 5714, 80, 4511, 2164, 84, 1040, 5715, 83, 1044]}, {"qid": 812, "question": "How big is data used in experiments? in Duality Regularization for Unsupervised Bilingual Lexicon Induction", "answer": ["No"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 1014, 1015, 1016, 4511, 2972, 1044, 2164, 3748, 3746, 4222], "orig_top_k_doc_id": [1015, 1014, 1016, 2162, 1040, 80, 2164, 5714, 2972, 84, 4511, 1044, 3748, 3746, 4222]}, {"qid": 811, "question": "How better is performance compared to competitive baselines? in Duality Regularization for Unsupervised Bilingual Lexicon Induction", "answer": ["Proposed method vs best baseline result on Vecmap (Accuracy P@1):\nEN-IT: 50 vs 50\nIT-EN: 42.67 vs 42.67\nEN-DE: 51.6 vs 51.47\nDE-EN: 47.22 vs 46.96\nEN-FI: 35.88 vs 36.24\nFI-EN: 39.62 vs 39.57\nEN-ES: 39.47 vs 39.30\nES-EN: 36.43 vs 36.06"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 1014, 1015, 1016, 4511, 2972, 2971, 3748, 2948, 1046, 83], "orig_top_k_doc_id": [1015, 1014, 1016, 2162, 84, 2972, 5714, 4511, 2971, 3748, 80, 2948, 1040, 1046, 83]}, {"qid": 72, "question": "Which vision-based approaches does this approach outperform? in Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data", "answer": ["CNN-mean, CNN-avgmax"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 81, 82, 83, 1044, 2164, 2972, 5715, 2971, 4712, 3746], "orig_top_k_doc_id": [80, 84, 83, 81, 2162, 1040, 5714, 82, 3746, 1044, 5715, 2164, 2972, 2971, 4712]}, {"qid": 74, "question": "Which languages are used in the multi-lingual caption model? in Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data", "answer": ["German-English, French-English, and Japanese-English", "multiple language pairs including German-English, French-English, and Japanese-English."], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 81, 82, 83, 1044, 2164, 2972, 5715, 2971, 4712, 5711], "orig_top_k_doc_id": [80, 81, 84, 83, 82, 5714, 2162, 5715, 1040, 2972, 4712, 5711, 2164, 2971, 1044]}, {"qid": 814, "question": "What are current state-of-the-art methods that consider the two tasks independently? in Duality Regularization for Unsupervised Bilingual Lexicon Induction", "answer": ["Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 1014, 1015, 1016, 4511, 3747, 2163, 83, 81, 6782, 4221], "orig_top_k_doc_id": [1015, 1014, 2162, 1016, 5714, 80, 84, 4511, 3747, 1040, 2163, 83, 81, 6782, 4221]}, {"qid": 1549, "question": "How are seed dictionaries obtained by fully unsupervised methods? in Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?", "answer": ["the latest CLWE developments almost exclusively focus on fully unsupervised approaches BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 : they fully abandon any source of (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces"], "top_k_doc_id": [2162, 5714, 3746, 3750, 5715, 2163, 2164, 2165, 2166, 6424, 7828, 2972, 2971, 3747, 500], "orig_top_k_doc_id": [2162, 5714, 2164, 5715, 2166, 2972, 2165, 6424, 2163, 2971, 3746, 3750, 3747, 7828, 500]}, {"qid": 1551, "question": "What methods were used for unsupervised CLWE? in Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?", "answer": ["Unsupervised CLWEs. These methods first induce a seed dictionary $D^{(1)}$ leveraging only two unaligned monolingual spaces (C1). While the algorithms for unsupervised seed dictionary induction differ, they all strongly rely on the assumption of similar topological structure between the two pretrained monolingual spaces. Once the seed dictionary is obtained, the two-step iterative self-learning procedure (C2) takes place: 1) a dictionary $D^{(k)}$ is first used to learn the joint space $\\mathbf {Y}^{(k)} = \\mathbf {X{W}}^{(k)}_x \\cup \\mathbf {Z{W}}^{(k)}_z$ ; 2) the nearest neighbours in $\\mathbf {Y}^{(k)}$ then form the new dictionary $D^{(k+1)}$ . We illustrate the general structure in Figure 1 ."], "top_k_doc_id": [2162, 5714, 3746, 3750, 5715, 2163, 2164, 2165, 2166, 6424, 7828, 6426, 5711, 1014, 6035], "orig_top_k_doc_id": [2162, 5714, 2166, 5715, 2165, 2164, 2163, 3750, 6424, 6426, 7828, 5711, 1014, 6035, 3746]}, {"qid": 73, "question": "What baseline is used for the experimental setup? in Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data", "answer": ["CNN-mean, CNN-avgmax"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 81, 82, 83, 1044, 2164, 2972, 5715, 1045, 1014, 3746], "orig_top_k_doc_id": [80, 84, 83, 81, 2162, 1044, 2972, 1040, 82, 2164, 5714, 1045, 1014, 5715, 3746]}, {"qid": 809, "question": "What regularizers were used to encourage consistency in back translation cycles? in Duality Regularization for Unsupervised Bilingual Lexicon Induction", "answer": ["an adversarial loss ($\\ell _{adv}$) for each model as in the baseline, a cycle consistency loss ($\\ell _{cycle}$) on each side"], "top_k_doc_id": [2162, 5714, 80, 84, 1040, 1014, 1015, 1016, 4309, 1766, 3746, 1453, 2164, 2972, 7847], "orig_top_k_doc_id": [1014, 1015, 1016, 2162, 4309, 1766, 3746, 80, 1453, 1040, 84, 2164, 2972, 5714, 7847]}, {"qid": 1550, "question": "How does BLI measure alignment quality? in Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?", "answer": ["we use mean average precision (MAP) as the main evaluation metric"], "top_k_doc_id": [2162, 5714, 3746, 3750, 5715, 2163, 2164, 2165, 2166, 6424, 247, 2971, 248, 7511, 7510], "orig_top_k_doc_id": [2162, 2164, 5714, 2166, 247, 2163, 2165, 6424, 3746, 3750, 2971, 248, 7511, 7510, 5715]}, {"qid": 2346, "question": "Why does the model improve in monolingual spaces as well?  in Improving Cross-Lingual Word Embeddings by Meeting in the Middle", "answer": ["because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space"], "top_k_doc_id": [2162, 5714, 3746, 3750, 5715, 3748, 3747, 5713, 1051, 1040, 3749, 501, 1041, 1052, 497], "orig_top_k_doc_id": [3748, 3746, 3747, 5713, 1051, 5714, 2162, 1040, 3749, 501, 1041, 5715, 3750, 1052, 497]}]}
{"group_id": 40, "group_size": 13, "items": [{"qid": 1985, "question": "What data do they train the language models on? in Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling", "answer": [" BABEL speech corpus "], "top_k_doc_id": [1784, 2995, 436, 4375, 6035, 6036, 381, 3617, 6031, 783, 2996, 5841, 7688, 4371, 7687], "orig_top_k_doc_id": [2995, 6035, 436, 6031, 3617, 4371, 6036, 1784, 381, 5841, 783, 7688, 7687, 4375, 2996]}, {"qid": 1987, "question": "What languages do they use? in Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling", "answer": ["Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages."], "top_k_doc_id": [1784, 2995, 436, 4375, 6035, 6036, 381, 3617, 6031, 783, 2996, 5841, 7688, 659, 6060], "orig_top_k_doc_id": [2995, 6035, 1784, 6031, 5841, 436, 3617, 6036, 381, 659, 4375, 7688, 783, 2996, 6060]}, {"qid": 1302, "question": "How much training data is required for each low-resource language? in Multilingual Graphemic Hybrid ASR with Massive Data Augmentation", "answer": ["No"], "top_k_doc_id": [1784, 2995, 1785, 1786, 1787, 2996, 6310, 6312, 381, 5566, 2451, 380, 3617, 5991, 3126], "orig_top_k_doc_id": [1784, 1787, 1785, 1786, 6310, 2451, 380, 2996, 6312, 381, 2995, 5566, 3617, 5991, 3126]}, {"qid": 1304, "question": "How much of the ASR grapheme set is shared between languages? in Multilingual Graphemic Hybrid ASR with Massive Data Augmentation", "answer": ["Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script."], "top_k_doc_id": [1784, 2995, 1785, 1786, 1787, 2996, 6310, 6312, 381, 5566, 4209, 1812, 4972, 3401, 2452], "orig_top_k_doc_id": [1784, 1785, 1787, 1786, 2995, 6312, 2996, 6310, 4209, 1812, 4972, 3401, 2452, 381, 5566]}, {"qid": 1986, "question": "Do they report BLEU scores? in Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling", "answer": ["No"], "top_k_doc_id": [1784, 2995, 436, 4375, 6035, 6036, 381, 3617, 6031, 6034, 4592, 4371, 7687, 5566, 6032], "orig_top_k_doc_id": [6034, 2995, 4592, 1784, 4371, 7687, 436, 5566, 6031, 4375, 6035, 3617, 381, 6036, 6032]}, {"qid": 2619, "question": "how is model compactness measured? in Massively Multilingual Neural Grapheme-to-Phoneme Conversion", "answer": ["Using file size on disk", "15.4 MB"], "top_k_doc_id": [1784, 4618, 1267, 1268, 1287, 1288, 1785, 4615, 4616, 4617, 4972, 6035, 4973, 6031, 2995], "orig_top_k_doc_id": [4615, 4616, 4618, 1287, 4617, 1784, 1785, 4972, 6035, 1268, 1267, 4973, 1288, 6031, 2995]}, {"qid": 2620, "question": "what was the baseline? in Massively Multilingual Neural Grapheme-to-Phoneme Conversion", "answer": ["system presented by deri2016grapheme", "wFST"], "top_k_doc_id": [1784, 4618, 1267, 1268, 1287, 1288, 1785, 4615, 4616, 4617, 4972, 6035, 4973, 3265, 3457], "orig_top_k_doc_id": [4615, 4616, 4617, 4618, 1268, 1267, 1785, 1287, 1784, 6035, 4972, 3265, 1288, 3457, 4973]}, {"qid": 2621, "question": "what evaluation metrics were used? in Massively Multilingual Neural Grapheme-to-Phoneme Conversion", "answer": ["Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)", "PER, WER, WER 100"], "top_k_doc_id": [1784, 4618, 1267, 1268, 1287, 1288, 1785, 4615, 4616, 4617, 4972, 6035, 1336, 6190, 2995], "orig_top_k_doc_id": [4615, 4616, 4617, 4618, 1268, 1287, 1785, 1288, 4972, 1784, 6035, 1267, 1336, 6190, 2995]}, {"qid": 2622, "question": "what datasets did they use? in Massively Multilingual Neural Grapheme-to-Phoneme Conversion", "answer": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "top_k_doc_id": [1784, 4618, 1267, 1268, 1287, 1288, 1785, 4615, 4616, 4617, 4972, 6035, 1336, 6190, 3457], "orig_top_k_doc_id": [4615, 4616, 4618, 4972, 1785, 6035, 4617, 1784, 1287, 1288, 6190, 3457, 1268, 1267, 1336]}, {"qid": 1303, "question": "What are the best within-language data augmentation methods? in Multilingual Graphemic Hybrid ASR with Massive Data Augmentation", "answer": ["Frequency masking, Time masking, Additive noise, Speed and volume perturbation"], "top_k_doc_id": [1784, 2995, 1785, 1786, 1787, 2996, 6310, 6312, 6316, 6315, 653, 6925, 5838, 1597, 485], "orig_top_k_doc_id": [1787, 1784, 1785, 1786, 6310, 6316, 6312, 6315, 653, 6925, 5838, 2996, 1597, 485, 2995]}, {"qid": 1988, "question": "What architectures are explored to improve the seq2seq model? in Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling", "answer": ["VGG-BLSTM, character-level RNNLM"], "top_k_doc_id": [1784, 2995, 436, 4375, 6035, 6036, 1987, 1296, 2997, 4377, 4376, 2996, 2528, 5841, 6066], "orig_top_k_doc_id": [2995, 1987, 6035, 1784, 1296, 2997, 4377, 4375, 4376, 6036, 2996, 436, 2528, 5841, 6066]}, {"qid": 4060, "question": "What does the cache consist of? in Efficient Dynamic WFST Decoding for Personalized Language Models", "answer": ["No", "static public cache stores the most frequent states, lifetime of a private cache actually can last for the entire dialog section for a specific user, subsequent utterances faster as more states are composed and stored"], "top_k_doc_id": [1784, 4618, 380, 381, 495, 6515, 6516, 6517, 6518, 1304, 1303, 1306, 1305, 1307, 7783], "orig_top_k_doc_id": [6518, 6515, 6516, 6517, 381, 1304, 1303, 1306, 1305, 380, 4618, 1307, 495, 7783, 1784]}, {"qid": 4061, "question": "What languages is the model tested on? in Efficient Dynamic WFST Decoding for Personalized Language Models", "answer": ["No", "No", "English"], "top_k_doc_id": [1784, 4618, 380, 381, 495, 6515, 6516, 6517, 6518, 4617, 4615, 2742, 382, 1785, 2494], "orig_top_k_doc_id": [6515, 6518, 381, 4617, 1784, 380, 4618, 6516, 4615, 6517, 495, 2742, 382, 1785, 2494]}]}
{"group_id": 41, "group_size": 13, "items": [{"qid": 2357, "question": "How is dependency parsing empirically verified? in Concurrent Parsing of Constituency and Dependency", "answer": [" At last, our model is evaluated on two benchmark treebanks for both constituent and dependency parsing. The empirical results show that our parser reaches new state-of-the-art for all parsing tasks."], "top_k_doc_id": [4049, 1393, 2811, 6937, 6938, 1389, 2809, 4038, 4039, 4048, 4827, 4062, 4826, 3779, 1586], "orig_top_k_doc_id": [6938, 6937, 4038, 4048, 1389, 4039, 3779, 2809, 1393, 2811, 4062, 1586, 4827, 4826, 4049]}, {"qid": 2360, "question": "What are the models used to perform constituency and dependency parsing? in Concurrent Parsing of Constituency and Dependency", "answer": ["token representation, self-attention encoder,, Constituent Parsing Decoder,  Dependency Parsing Decoder"], "top_k_doc_id": [4049, 1393, 2811, 6937, 6938, 1389, 2809, 4038, 4039, 4048, 4827, 4062, 4826, 4857, 2812], "orig_top_k_doc_id": [6938, 6937, 4038, 4048, 2809, 4857, 2811, 1393, 4826, 2812, 4039, 1389, 4049, 4827, 4062]}, {"qid": 2358, "question": "How are different network components evaluated? in Concurrent Parsing of Constituency and Dependency", "answer": ["For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. "], "top_k_doc_id": [4049, 1393, 2811, 6937, 6938, 1389, 2809, 4038, 4039, 4048, 4827, 2812, 3779, 4050, 3780], "orig_top_k_doc_id": [4038, 6938, 4048, 6937, 2811, 4039, 4049, 1393, 4827, 2812, 3779, 1389, 4050, 2809, 3780]}, {"qid": 2359, "question": "What are the performances obtained for PTB and CTB? in Concurrent Parsing of Constituency and Dependency", "answer": [". On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing., On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing."], "top_k_doc_id": [4049, 1393, 2811, 6937, 6938, 3258, 3780, 3255, 3257, 7054, 863, 4045, 3259, 1392, 4050], "orig_top_k_doc_id": [6938, 3258, 6937, 3780, 3255, 1393, 3257, 7054, 863, 4045, 3259, 4049, 1392, 4050, 2811]}, {"qid": 4653, "question": "Which data sources do they use? in Learning Distributed Representations of Sentences from Unlabelled Data", "answer": ["Toronto Books Corpus, STS 2014 dataset BIBREF37, SICK dataset BIBREF36", "SICK , STS 2014"], "top_k_doc_id": [4049, 7272, 610, 986, 4057, 3661, 6534, 7275, 7276, 2964, 2352, 4066, 4056, 4048, 4059], "orig_top_k_doc_id": [7272, 7275, 6534, 4049, 610, 4057, 2964, 986, 2352, 4066, 7276, 4056, 3661, 4048, 4059]}, {"qid": 4654, "question": "Which tasks do they evaluate supervised systems on? in Learning Distributed Representations of Sentences from Unlabelled Data", "answer": ["paraphrase identification (MSRP), movie review sentiment (MR), product reviews (CR), subjectivity classification (SUBJ), opinion polarity (MPQA), question type classification (TREC)", "paraphrase identification (MSRP), movie review sentiment (MR), product reviews (CR), subjectivity classification (SUBJ), opinion polarity (MPQA),  question type classification (TREC) "], "top_k_doc_id": [4049, 7272, 610, 986, 4057, 3661, 6534, 7275, 7276, 4038, 4046, 1743, 4047, 3663, 3662], "orig_top_k_doc_id": [7272, 7275, 610, 4038, 6534, 986, 7276, 4046, 3661, 4057, 1743, 4047, 4049, 3663, 3662]}, {"qid": 2450, "question": "Which English domains do they evaluate on? in Semi-Supervised Methods for Out-of-Domain Dependency Parsing", "answer": ["Conll, Weblogs, Newsgroups, Reviews, Answers"], "top_k_doc_id": [4049, 4038, 4046, 4050, 4069, 4062, 4041, 4052, 4068, 4058, 4064, 4059, 4053, 4048, 4039], "orig_top_k_doc_id": [4038, 4046, 4050, 4069, 4062, 4041, 4049, 4052, 4068, 4058, 4064, 4059, 4053, 4048, 4039]}, {"qid": 4655, "question": "How do they evaluate domain portability? in Learning Distributed Representations of Sentences from Unlabelled Data", "answer": ["We constrain our comparison to methods that do not require labelled data", "No"], "top_k_doc_id": [4049, 7272, 610, 986, 4057, 4056, 4058, 4053, 4059, 4038, 4062, 4061, 2964, 4066, 4050], "orig_top_k_doc_id": [7272, 4056, 4058, 4053, 4049, 4059, 610, 4038, 4062, 4061, 2964, 4066, 4050, 4057, 986]}, {"qid": 3526, "question": "How does enforcing agreement between parse trees work across different languages? in Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments", "answer": ["we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints"], "top_k_doc_id": [4049, 5819, 5821, 3762, 2806, 2807, 1992, 3984, 5820, 2812, 4062, 4857, 4048, 3255, 4054], "orig_top_k_doc_id": [5819, 5821, 3762, 2806, 2807, 1992, 3984, 5820, 2812, 4062, 4857, 4048, 4049, 3255, 4054]}, {"qid": 4656, "question": "Which unsupervised representation-learning objectives do they introduce? in Learning Distributed Representations of Sentences from Unlabelled Data", "answer": ["Sequential Denoising Autoencoders (SDAEs) and FastSent", "FastSent and Sequential Denoising Autoencoders"], "top_k_doc_id": [4049, 7272, 7275, 7276, 3663, 2146, 7274, 2148, 6534, 7736, 1743, 1148, 3661, 7766, 5313], "orig_top_k_doc_id": [7272, 7275, 7276, 3663, 2146, 7274, 4049, 2148, 6534, 7736, 1743, 1148, 3661, 7766, 5313]}, {"qid": 1441, "question": "How does the model work if no treebank is available? in Many Languages, One Parser", "answer": ["train the parser on six other languages in the Google universal dependency treebanks version 2.0 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags"], "top_k_doc_id": [4049, 1991, 1992, 1996, 4053, 4054, 6467, 1994, 7288, 1995, 3728, 3258, 863, 7327, 736], "orig_top_k_doc_id": [1991, 1996, 6467, 1994, 7288, 1995, 4053, 1992, 3728, 4049, 3258, 4054, 863, 7327, 736]}, {"qid": 1442, "question": "How many languages have this parser been tried on? in Many Languages, One Parser", "answer": ["seven"], "top_k_doc_id": [4049, 1991, 1992, 1996, 4053, 4054, 1993, 3759, 3489, 4038, 7053, 6852, 273, 4025, 3761], "orig_top_k_doc_id": [1991, 1992, 1993, 4049, 3759, 3489, 4038, 7053, 4054, 6852, 273, 4053, 4025, 1996, 3761]}, {"qid": 2342, "question": "Was the system only evaluated over the second shared task? in Neural DrugNet", "answer": ["No"], "top_k_doc_id": [4049, 3742, 2785, 3052, 396, 5202, 4316, 4048, 3605, 7287, 4839, 4612, 5447, 5146, 1410], "orig_top_k_doc_id": [3742, 4049, 2785, 3052, 396, 5202, 4316, 4048, 3605, 7287, 4839, 4612, 5447, 5146, 1410]}]}
{"group_id": 42, "group_size": 13, "items": [{"qid": 2704, "question": "Do they report results only on English data? in Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "answer": ["Yes", "Yes"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 4740, 4743, 4741, 4742, 5979, 1726, 2077, 5976, 1734], "orig_top_k_doc_id": [4739, 4740, 4742, 4743, 4741, 6629, 6633, 5979, 6630, 5976, 1726, 6631, 2077, 6632, 1734]}, {"qid": 2706, "question": "How id Depechemood trained? in Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "answer": ["By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. ", "researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 4740, 4743, 4741, 4742, 5979, 1726, 2077, 1725, 1500], "orig_top_k_doc_id": [4742, 4739, 4743, 4740, 4741, 6629, 6633, 6630, 6631, 5979, 2077, 1725, 1726, 6632, 1500]}, {"qid": 2707, "question": "How are similarities and differences between the texts from violent and non-violent religious groups analyzed? in Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "answer": ["By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum", "A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 4740, 4743, 4741, 4742, 5979, 6634, 5976, 3797, 1725], "orig_top_k_doc_id": [4739, 4743, 4742, 4740, 4741, 6633, 6629, 6631, 6630, 6634, 5976, 3797, 5979, 6632, 1725]}, {"qid": 4169, "question": "What languages feature in the dataset? in Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter", "answer": ["English", "English"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 521, 1725, 2404, 6634, 7534, 1727, 2405, 3582, 3246], "orig_top_k_doc_id": [6629, 6630, 4739, 6633, 6631, 6632, 521, 6634, 3582, 2404, 2405, 7534, 1725, 1727, 3246]}, {"qid": 4172, "question": "Which behavioural features are used? in Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter", "answer": ["frequency of tweets posted, followers/following ratio, degree of influence each user has over their network", "frequency of tweets posted,  followers/following ratio, using hashtags, using mention action", "frequency of tweets posted, followers/following ratio, users' interactions with others through using hashtags, engagement in discussions using mention action"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 521, 1725, 2404, 6634, 7534, 1727, 2405, 6459, 4131], "orig_top_k_doc_id": [6630, 6629, 6633, 6631, 4739, 2404, 521, 7534, 6632, 6634, 6459, 1725, 4131, 2405, 1727]}, {"qid": 2705, "question": "What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar? in Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "answer": ["both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda", "By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 4740, 4743, 1500, 4831, 6634, 4742, 4741, 1495, 5386], "orig_top_k_doc_id": [4739, 4742, 4740, 4743, 4741, 6630, 6629, 6631, 6633, 6634, 6632, 1495, 1500, 5386, 4831]}, {"qid": 2708, "question": "How are prominent topics idenified in Dabiq and Rumiyah? in Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "answer": ["LDA, non-negative matrix factorization (NMF)", "Using NMF based topic modeling and their coherence prominent topics are identified"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 4740, 4743, 4741, 4742, 5979, 2077, 1757, 2075, 2074], "orig_top_k_doc_id": [4739, 4740, 4742, 4743, 4741, 6630, 6629, 2077, 6633, 6631, 1757, 6632, 5979, 2075, 2074]}, {"qid": 4170, "question": "What textual, psychological and behavioural patterns are observed in radical users? in Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter", "answer": ["They use a lot of \"us\" and \"them\" in their vocabulary. They use a lot of mentions, and they tend to be \"central\" in their network. They use a lot of violent words. "], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 521, 1725, 2404, 6634, 7534, 522, 5915, 523, 6459], "orig_top_k_doc_id": [6630, 6629, 6633, 6631, 6632, 4739, 521, 2404, 6634, 522, 7534, 5915, 1725, 523, 6459]}, {"qid": 4171, "question": "Where is the propaganda material sourced from? in Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter", "answer": [" online English magazine called Dabiq", "Dabiq", "English magazine called Dabiq"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 4740, 4743, 1500, 4831, 6634, 521, 1172, 3590, 1494], "orig_top_k_doc_id": [6629, 6630, 4739, 6633, 6631, 6634, 4740, 6632, 4743, 521, 1172, 3590, 4831, 1500, 1494]}, {"qid": 4173, "question": "Which psychological features are used? in Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter", "answer": ["Analytically thinking, Clout, Tone, Authentic, Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism, positive emotions, negative emotions, personal drives, namely power, reward, risk, achievement, and affiliation, number of 1st, 2nd, and 3rd personal pronouns used.", "Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism", "summary variable - analytically thinking, clout, tone, authentic, Big five variable - openness, conscientiousness, extraversion, agreeableness, neuroticism, Emotional variables - positive emotions in the text, negative emotions in the text, personal drives - power, reward, risk, achievement, affiliation, personal pronouns -  counts the number of 1st, 2nd, and 3rd personal pronouns used, Minkowski distance between each profile and average values of these features created from the ISIS magazines"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 521, 1725, 2404, 6634, 2405, 4131, 1701, 523, 3612], "orig_top_k_doc_id": [6629, 6630, 6633, 6631, 4739, 6634, 521, 6632, 1725, 1701, 523, 4131, 2404, 3612, 2405]}, {"qid": 4174, "question": "Which textual features are used? in Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter", "answer": ["N-grams, word2vec", "uni-grams, bi-grams, tri-grams", "ratio of violent words in the tweet, ratio of curse words in the tweet, frequency of words with all capital letters, 200 dimension sized vector for the tweet calculated using word embedding, tf-idf scores for top scoring uni-grams, bi-grams and tri-grams"], "top_k_doc_id": [6631, 4739, 6629, 6630, 6632, 6633, 521, 1725, 2404, 6634, 2405, 4131, 520, 5786, 6455], "orig_top_k_doc_id": [6629, 6630, 6633, 4739, 521, 6631, 6632, 2404, 520, 6634, 5786, 1725, 4131, 2405, 6455]}, {"qid": 2876, "question": "How did they obtain the OSG dataset? in Affective Behaviour Analysis of On-line User Interactions: Are On-line Support Groups more Therapeutic than Twitter?", "answer": ["crawling and pre-processing an OSG web forum", "data has been developed by crawling and pre-processing an OSG web forum"], "top_k_doc_id": [6631, 2157, 3542, 4394, 5056, 5057, 5058, 5059, 5060, 5744, 6804, 7016, 6458, 5039, 6633], "orig_top_k_doc_id": [5056, 5057, 5059, 5058, 5060, 6631, 5744, 4394, 7016, 3542, 6804, 6458, 5039, 2157, 6633]}, {"qid": 2877, "question": "How large is the Twitter dataset? in Affective Behaviour Analysis of On-line User Interactions: Are On-line Support Groups more Therapeutic than Twitter?", "answer": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "top_k_doc_id": [6631, 2157, 3542, 4394, 5056, 5057, 5058, 5059, 5060, 5744, 6804, 7016, 6629, 5191, 3795], "orig_top_k_doc_id": [5057, 5056, 5058, 5059, 5060, 6631, 7016, 4394, 3542, 5744, 6804, 6629, 5191, 3795, 2157]}]}
{"group_id": 43, "group_size": 13, "items": [{"qid": 2722, "question": "Which types of named entities do they recognize? in Multimodal Named Entity Recognition for Short Social Media Posts", "answer": ["PER, LOC, ORG, MISC", "PER, LOC, ORG, MISC"], "top_k_doc_id": [929, 4755, 930, 4756, 931, 4757, 4759, 607, 1262, 7597, 933, 4359, 4858, 2318, 5582], "orig_top_k_doc_id": [929, 4755, 1262, 4756, 930, 4759, 4359, 7597, 607, 4858, 933, 931, 2318, 5582, 4757]}, {"qid": 2723, "question": "Can named entities in SnapCaptions be discontigious? in Multimodal Named Entity Recognition for Short Social Media Posts", "answer": ["No", "No"], "top_k_doc_id": [929, 4755, 930, 4756, 931, 4757, 4759, 607, 1262, 7597, 933, 4359, 4758, 4358, 4750], "orig_top_k_doc_id": [4755, 929, 4757, 4759, 4756, 930, 4758, 607, 933, 1262, 7597, 4359, 931, 4358, 4750]}, {"qid": 745, "question": "Which social media platform is explored? in A multimodal deep learning approach for named entity recognition from social media", "answer": ["twitter "], "top_k_doc_id": [929, 4755, 930, 4756, 933, 1262, 3944, 4750, 521, 607, 5102, 2079, 3736, 3730, 3542], "orig_top_k_doc_id": [929, 4755, 930, 4756, 933, 5102, 1262, 2079, 521, 4750, 3736, 3730, 607, 3944, 3542]}, {"qid": 746, "question": "What datasets did they use? in A multimodal deep learning approach for named entity recognition from social media", "answer": ["BIBREF8 a refined collection of tweets gathered from twitter"], "top_k_doc_id": [929, 4755, 930, 4756, 933, 1262, 3944, 4750, 521, 607, 5102, 931, 5189, 4573, 4757], "orig_top_k_doc_id": [929, 930, 4756, 4755, 933, 931, 5102, 4750, 607, 521, 5189, 4573, 1262, 3944, 4757]}, {"qid": 2720, "question": "Do they inspect their model to see if their model learned to associate image parts with words related to entities? in Multimodal Named Entity Recognition for Short Social Media Posts", "answer": ["Yes", "Yes"], "top_k_doc_id": [929, 4755, 930, 4756, 931, 4757, 4759, 933, 951, 5582, 1262, 4359, 3624, 932, 5377], "orig_top_k_doc_id": [929, 4755, 4756, 931, 1262, 930, 4759, 933, 4359, 951, 3624, 4757, 932, 5582, 5377]}, {"qid": 2721, "question": "Does their NER model learn NER from both text and images? in Multimodal Named Entity Recognition for Short Social Media Posts", "answer": ["Yes", "Yes"], "top_k_doc_id": [929, 4755, 930, 4756, 931, 4757, 4759, 607, 1262, 7597, 6028, 4750, 5879, 4749, 2973], "orig_top_k_doc_id": [929, 4755, 4756, 930, 4759, 607, 4757, 7597, 6028, 4750, 931, 5879, 1262, 4749, 2973]}, {"qid": 2724, "question": "How large is their MNER SnapCaptions dataset? in Multimodal Named Entity Recognition for Short Social Media Posts", "answer": ["10K user-generated image (snap) and textual caption pairs", "10000"], "top_k_doc_id": [929, 4755, 930, 4756, 931, 4757, 4759, 933, 951, 5582, 4758, 4136, 521, 607, 5879], "orig_top_k_doc_id": [4755, 4757, 4756, 4759, 929, 931, 930, 5582, 933, 4758, 4136, 521, 951, 607, 5879]}, {"qid": 747, "question": "What are the baseline state of the art models? in A multimodal deep learning approach for named entity recognition from social media", "answer": ["Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention"], "top_k_doc_id": [929, 4755, 930, 4756, 933, 1262, 3944, 4750, 931, 3736, 2119, 618, 7100, 2120, 854], "orig_top_k_doc_id": [930, 4755, 929, 4756, 933, 931, 4750, 3944, 1262, 3736, 2119, 618, 7100, 2120, 854]}, {"qid": 4553, "question": "Does this model perform better than the state of the art? in Named Entity Recognition in Twitter using Images and Text", "answer": ["No", "No"], "top_k_doc_id": [929, 4755, 931, 933, 7111, 635, 932, 5775, 1262, 1256, 930, 497, 4750, 7553, 6698], "orig_top_k_doc_id": [4755, 7111, 5775, 929, 933, 1262, 1256, 930, 931, 497, 4750, 7553, 635, 6698, 932]}, {"qid": 4555, "question": "What features are extracted from images? in Named Entity Recognition in Twitter using Images and Text", "answer": ["LOC (Building, Suburb, Street, City, Country, Mountain, Highway, Forest, Coast and Map), ORG (Company Logo), PER (Human Face ).", "BoF (Bag of Features) BIBREF13, SIFT (Scale Invariant Feature Transform) features BIBREF12"], "top_k_doc_id": [929, 4755, 931, 933, 7111, 635, 932, 4757, 2345, 522, 5966, 2733, 1486, 7121, 413], "orig_top_k_doc_id": [7111, 4755, 929, 933, 4757, 2345, 931, 932, 522, 5966, 2733, 1486, 7121, 413, 635]}, {"qid": 4554, "question": "What features are extracted from text? in Named Entity Recognition in Twitter using Images and Text", "answer": ["word feature", "extracted features using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-ID"], "top_k_doc_id": [929, 4755, 931, 933, 7111, 1262, 4359, 5189, 7100, 2404, 4757, 7672, 19, 5775, 2080], "orig_top_k_doc_id": [4755, 7111, 929, 1262, 4359, 5189, 931, 7100, 2404, 933, 4757, 7672, 19, 5775, 2080]}, {"qid": 4551, "question": "Do they evaluate only on English datasets? in Named Entity Recognition in Twitter using Images and Text", "answer": ["No", "No"], "top_k_doc_id": [929, 4755, 607, 608, 5775, 2973, 7456, 7111, 2823, 4573, 7355, 5189, 6698, 3743, 4757], "orig_top_k_doc_id": [4755, 2973, 929, 7456, 608, 5775, 7111, 2823, 4573, 7355, 5189, 607, 6698, 3743, 4757]}, {"qid": 4552, "question": "What is the Ritter dataset? in Named Entity Recognition in Twitter using Images and Text", "answer": ["No", " a gold standard for NER in microblogs"], "top_k_doc_id": [929, 4755, 607, 608, 5775, 616, 5256, 2404, 7113, 933, 617, 4300, 5186, 610, 5879], "orig_top_k_doc_id": [608, 616, 5256, 5775, 607, 4755, 929, 2404, 7113, 933, 617, 4300, 5186, 610, 5879]}]}
{"group_id": 44, "group_size": 13, "items": [{"qid": 2882, "question": "What is the best model? in Named Entity Recognition for Nepali Language", "answer": ["BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS "], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 6153, 20, 930, 1094, 4573, 22, 7553, 21, 4750, 7287], "orig_top_k_doc_id": [5081, 5084, 5083, 5082, 22, 1094, 5879, 6153, 7553, 20, 930, 21, 4573, 4750, 7287]}, {"qid": 2885, "question": "What is the baseline? in Named Entity Recognition for Nepali Language", "answer": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 6153, 20, 930, 1094, 4573, 7100, 19, 1090, 618, 1100], "orig_top_k_doc_id": [5084, 5081, 5083, 5082, 930, 4573, 5879, 7100, 6153, 19, 20, 1090, 618, 1094, 1100]}, {"qid": 2887, "question": "What is the size of the dataset? in Named Entity Recognition for Nepali Language", "answer": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 6153, 22, 4574, 6151, 7100, 19, 21, 4575, 4758, 23], "orig_top_k_doc_id": [5081, 5083, 5084, 5082, 22, 6153, 19, 21, 4574, 4575, 5879, 7100, 4758, 23, 6151]}, {"qid": 2888, "question": "What is the source of their dataset? in Named Entity Recognition for Nepali Language", "answer": ["daily newspaper of the year 2015-2016", "daily newspaper of the year 2015-2016"], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 6153, 22, 4574, 6151, 7100, 7672, 929, 7389, 2973, 930], "orig_top_k_doc_id": [5084, 5081, 5083, 5082, 7100, 6151, 6153, 7672, 4574, 929, 5879, 22, 7389, 2973, 930]}, {"qid": 2891, "question": "How big is the new Nepali NER dataset? in Named Entity Recognition for Nepali Language", "answer": ["3606 sentences", "Dataset contains 3606 total sentences and 79087 total entities."], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 6810, 7061, 4790, 5956, 1099, 3845, 65, 610, 4755, 4750], "orig_top_k_doc_id": [5084, 5081, 5083, 5082, 7061, 4790, 5879, 5956, 1099, 3845, 65, 6810, 610, 4755, 4750]}, {"qid": 2893, "question": "Which models are used to solve NER for Nepali? in Named Entity Recognition for Nepali Language", "answer": ["BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21", "BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF"], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 6810, 7061, 7002, 7250, 1422, 3344, 7056, 1100, 5938, 1423], "orig_top_k_doc_id": [5081, 5084, 5083, 5082, 7002, 7250, 1422, 3344, 7056, 1100, 5938, 7061, 1423, 5879, 6810]}, {"qid": 2886, "question": "Which machine learning models do they explore? in Named Entity Recognition for Nepali Language", "answer": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "top_k_doc_id": [5083, 5081, 5082, 5084, 5879, 7687, 7688, 4573, 4755, 4756, 497, 1262, 7553, 6151, 3207], "orig_top_k_doc_id": [5081, 5084, 5083, 7687, 7688, 5879, 4573, 4755, 4756, 497, 1262, 7553, 6151, 3207, 5082]}, {"qid": 2892, "question": "What is the performance improvement of the grapheme-level representation model over the character-level model? in Named Entity Recognition for Nepali Language", "answer": ["On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement", "BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration"], "top_k_doc_id": [5083, 5081, 5082, 5084, 7056, 4615, 3505, 930, 7688, 7687, 3845, 4750, 3504, 4575, 4972], "orig_top_k_doc_id": [5084, 5081, 5083, 5082, 7056, 4615, 3505, 930, 7688, 7687, 3845, 4750, 3504, 4575, 4972]}, {"qid": 2883, "question": "How many sentences does the dataset contain? in Named Entity Recognition for Nepali Language", "answer": ["3606", "6946"], "top_k_doc_id": [5083, 5081, 5082, 5084, 1098, 5956, 4573, 7673, 1090, 3489, 7672, 1591, 4858, 7102, 7061], "orig_top_k_doc_id": [5083, 5081, 5084, 5082, 4573, 7673, 1090, 1098, 3489, 7672, 1591, 5956, 4858, 7102, 7061]}, {"qid": 2890, "question": "How many different types of entities exist in the dataset? in Named Entity Recognition for Nepali Language", "answer": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "top_k_doc_id": [5083, 5081, 5082, 5084, 1098, 5956, 6151, 2973, 1100, 2974, 2318, 1262, 4749, 929, 2477], "orig_top_k_doc_id": [5084, 5083, 5081, 6151, 5956, 2973, 1100, 2974, 2318, 1262, 4749, 929, 5082, 1098, 2477]}, {"qid": 1652, "question": "Do they evaluate ablated versions of their CNN+RNN model? in A Byte-sized Approach to Named Entity Recognition", "answer": ["No"], "top_k_doc_id": [5083, 2348, 4590, 7688, 1544, 6737, 1426, 7287, 930, 4573, 1098, 4755, 7083, 6151, 7771], "orig_top_k_doc_id": [4590, 1544, 6737, 1426, 2348, 7688, 7287, 930, 5083, 4573, 1098, 4755, 7083, 6151, 7771]}, {"qid": 2889, "question": "Do they try to use byte-pair encoding representations? in Named Entity Recognition for Nepali Language", "answer": ["No", "No"], "top_k_doc_id": [5083, 2348, 4590, 7688, 5081, 5084, 1422, 1410, 7056, 7669, 5938, 7042, 7776, 48, 4510], "orig_top_k_doc_id": [5081, 5084, 5083, 4590, 1422, 2348, 1410, 7056, 7669, 5938, 7042, 7688, 7776, 48, 4510]}, {"qid": 2884, "question": "Do the authors train a Naive Bayes classifier on their dataset? in Named Entity Recognition for Nepali Language", "answer": ["No", "No"], "top_k_doc_id": [5083, 5084, 5081, 6480, 4287, 7755, 6479, 2383, 5757, 309, 7111, 4820, 2835, 7534, 6402], "orig_top_k_doc_id": [5084, 5083, 5081, 6480, 4287, 7755, 6479, 2383, 5757, 309, 7111, 4820, 2835, 7534, 6402]}]}
{"group_id": 45, "group_size": 13, "items": [{"qid": 3014, "question": "What are the distinctive characteristics of how Arabic speakers use offensive language? in Arabic Offensive Language on Twitter: Analysis and Experiments", "answer": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "top_k_doc_id": [5976, 5168, 5169, 5170, 5173, 5977, 5171, 5172, 1725, 5978, 1727, 6176, 5979, 876, 6134], "orig_top_k_doc_id": [5168, 5169, 5170, 1725, 5173, 5171, 5976, 5977, 1727, 5172, 5978, 5979, 6176, 876, 6134]}, {"qid": 3017, "question": "How many tweets are in the dataset? in Arabic Offensive Language on Twitter: Analysis and Experiments", "answer": ["10,000 Arabic tweet dataset ", "10,000"], "top_k_doc_id": [5976, 5168, 5169, 5170, 5173, 5977, 5171, 5172, 1725, 5978, 1727, 6176, 6177, 2402, 412], "orig_top_k_doc_id": [5168, 5169, 5976, 5173, 5977, 1727, 6176, 5172, 1725, 5170, 6177, 5171, 5978, 2402, 412]}, {"qid": 3015, "question": "How did they analyze which topics, dialects and gender are most associated with tweets? in Arabic Offensive Language on Twitter: Analysis and Experiments", "answer": ["ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language"], "top_k_doc_id": [5976, 5168, 5169, 5170, 5173, 5977, 5171, 5172, 1725, 5978, 1726, 6131, 5979, 2076, 5144], "orig_top_k_doc_id": [5168, 5169, 5977, 5173, 5170, 1725, 1726, 5976, 5172, 5978, 5171, 6131, 5979, 2076, 5144]}, {"qid": 3643, "question": "What languages does the new dataset contain? in Multilingual and Multi-Aspect Hate Speech Analysis", "answer": ["English, French, Arabic", "English, French, Arabic", "English, French, Arabic", "English, French, and Arabic "], "top_k_doc_id": [5976, 3007, 5977, 1788, 3736, 4140, 3581, 3582, 876, 3011, 5144, 5979, 3010, 5914, 4515], "orig_top_k_doc_id": [5976, 3007, 1788, 5977, 5144, 3011, 4140, 5979, 3736, 3582, 3010, 876, 3581, 5914, 4515]}, {"qid": 3645, "question": "How big is their dataset? in Multilingual and Multi-Aspect Hate Speech Analysis", "answer": ["13 000 tweets", "13014", "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets", "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets"], "top_k_doc_id": [5976, 3007, 5977, 1788, 3736, 4140, 3581, 3582, 876, 3011, 5905, 412, 1725, 2215, 1726], "orig_top_k_doc_id": [5976, 4140, 5977, 1788, 3736, 5905, 3007, 412, 876, 1725, 3011, 2215, 3582, 3581, 1726]}, {"qid": 1993, "question": "How large is the corpus? in Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition", "answer": ["It contains 106,350 documents"], "top_k_doc_id": [5976, 3007, 5977, 3008, 3009, 3010, 3011, 3586, 3988, 3989, 6176, 6285, 1623, 6833, 3045], "orig_top_k_doc_id": [3007, 3010, 3011, 3988, 3008, 5976, 5977, 3009, 3989, 6176, 3586, 6285, 1623, 6833, 3045]}, {"qid": 1995, "question": "How large is the dataset? in Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition", "answer": ["over 104k documents"], "top_k_doc_id": [5976, 3007, 5977, 3008, 3009, 3010, 3011, 3586, 3988, 3989, 6176, 6285, 5907, 6286, 6036], "orig_top_k_doc_id": [3007, 3011, 3010, 3988, 5976, 3008, 3989, 3586, 5977, 6285, 3009, 6176, 5907, 6286, 6036]}, {"qid": 3018, "question": "In what way is the offensive dataset not biased by topic, dialect or target? in Arabic Offensive Language on Twitter: Analysis and Experiments", "answer": ["It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.", "our methodology does not use a seed list of offensive words"], "top_k_doc_id": [5976, 5168, 5169, 5170, 5173, 5977, 5171, 5172, 4948, 6176, 5144, 1727, 3575, 5145, 2797], "orig_top_k_doc_id": [5168, 5169, 5173, 4948, 5170, 6176, 5144, 5976, 5171, 5977, 5172, 1727, 3575, 5145, 2797]}, {"qid": 3644, "question": "What aspects are considered? in Multilingual and Multi-Aspect Hate Speech Analysis", "answer": [" (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments", "whether the text is direct or indirect, if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal, the attribute based on which it discriminates against an individual or a group of people, the name of this group,  how the annotators feel about its content within a range of negative to neutral sentiments", "(a) whether the text is direct or indirect, (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal, (c) the attribute based on which it discriminates against an individual or a group of people, (d) the name of this group, (e) how the annotators feel about its content within a range of negative to neutral sentiments", "Directness, Hostility, Target group, Target, Sentiment of the annotator"], "top_k_doc_id": [5976, 3007, 5977, 1788, 3736, 4140, 3581, 3582, 2215, 5144, 5979, 6472, 5291, 447, 3047], "orig_top_k_doc_id": [5976, 4140, 2215, 5144, 3007, 3581, 3736, 5979, 6472, 5291, 1788, 3582, 447, 5977, 3047]}, {"qid": 1994, "question": "Which document classifiers do they experiment with? in Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition", "answer": ["logistic regression (LR), recurrent neural network (RNN) BIBREF35, convolutional neural network (CNN) BIBREF36 and Google BERT BIBREF37"], "top_k_doc_id": [5976, 3007, 5977, 3008, 3009, 3010, 3011, 3586, 3988, 3989, 6176, 6285, 6894, 6520, 5292], "orig_top_k_doc_id": [3007, 3010, 3011, 3988, 3008, 3009, 3989, 6894, 5976, 6285, 6520, 3586, 5977, 6176, 5292]}, {"qid": 3016, "question": "How many annotators tagged each tweet? in Arabic Offensive Language on Twitter: Analysis and Experiments", "answer": ["One", "One experienced annotator tagged all tweets"], "top_k_doc_id": [5976, 5168, 5169, 5170, 5173, 5977, 5979, 5978, 6752, 86, 6177, 1727, 3046, 6176, 3574], "orig_top_k_doc_id": [5168, 5979, 5976, 5977, 5173, 5978, 5169, 6752, 5170, 86, 6177, 1727, 3046, 6176, 3574]}, {"qid": 3642, "question": "What is their definition of hate speech? in Multilingual and Multi-Aspect Hate Speech Analysis", "answer": ["rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech", "Hate speech is a text that contains one or more of the following aspects: directness, offensiveness, targeting a group or individual based on specific attributes, overall negativity.", " in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis."], "top_k_doc_id": [5976, 3007, 5977, 1788, 3736, 4140, 6131, 3047, 770, 3046, 6133, 6176, 5913, 5912, 412], "orig_top_k_doc_id": [5976, 6131, 3047, 770, 3046, 1788, 3007, 6133, 3736, 4140, 5977, 6176, 5913, 5912, 412]}, {"qid": 2016, "question": "Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator? in Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "answer": ["Personal thought of the annotator."], "top_k_doc_id": [5976, 5168, 5169, 5170, 5173, 3046, 3045, 6177, 6176, 3310, 5978, 5641, 5644, 3309, 5643], "orig_top_k_doc_id": [3046, 3045, 6177, 6176, 5169, 3310, 5978, 5170, 5173, 5641, 5644, 5168, 3309, 5643, 5976]}]}
{"group_id": 46, "group_size": 13, "items": [{"qid": 3324, "question": "What domains are covered in the corpus? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["No specific domain is covered in the corpus."], "top_k_doc_id": [5565, 5566, 5564, 6190, 6208, 4185, 4186, 5716, 5841, 608, 5715, 6658, 4431, 898, 1886], "orig_top_k_doc_id": [5564, 5566, 5565, 6190, 4186, 5716, 4185, 5841, 608, 5715, 6658, 4431, 898, 6208, 1886]}, {"qid": 3325, "question": "What is the architecture of their model? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "top_k_doc_id": [5565, 5566, 5564, 6190, 6208, 4185, 4186, 5716, 5841, 4526, 1623, 6310, 533, 3685, 534], "orig_top_k_doc_id": [5566, 5564, 5565, 4526, 5841, 6190, 5716, 4185, 4186, 6208, 1623, 6310, 533, 3685, 534]}, {"qid": 3326, "question": "How was the dataset collected? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["Contributors record voice clips by reading from a bank of donated sentences.", "crowdsourcing"], "top_k_doc_id": [5565, 5566, 5564, 6190, 6208, 661, 1622, 1623, 3685, 4526, 6443, 6177, 2329, 5702, 5977], "orig_top_k_doc_id": [5564, 5566, 5565, 661, 6443, 1622, 6177, 6208, 2329, 5702, 1623, 4526, 5977, 3685, 6190]}, {"qid": 3327, "question": "Which languages are part of the corpus? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)", "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese"], "top_k_doc_id": [5565, 5566, 5564, 6190, 6208, 661, 1622, 1623, 3685, 4526, 1886, 6031, 1777, 4431, 1593], "orig_top_k_doc_id": [5564, 5566, 5565, 4526, 6190, 661, 1886, 3685, 6031, 1622, 1623, 1777, 4431, 6208, 1593]}, {"qid": 3330, "question": "Is Arabic one of the 11 languages in CoVost? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["No", "No"], "top_k_doc_id": [5565, 5566, 5564, 6190, 5702, 2329, 2330, 5623, 5977, 2798, 1639, 5979, 2284, 2285, 4526], "orig_top_k_doc_id": [5564, 5566, 5565, 5702, 2329, 2330, 5623, 5977, 2798, 6190, 1639, 5979, 2284, 2285, 4526]}, {"qid": 3329, "question": "Is the data in CoVoST annotated for dialect? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["No"], "top_k_doc_id": [5565, 5566, 5564, 2798, 4223, 1286, 3637, 5841, 1899, 6208, 5699, 4002, 6443, 2618, 4752], "orig_top_k_doc_id": [5564, 5566, 5565, 2798, 4223, 1286, 3637, 5841, 1899, 6208, 5699, 4002, 6443, 2618, 4752]}, {"qid": 3908, "question": "What sizes were their datasets? in Analyzing ASR pretraining for low-resource speech-to-text translation", "answer": ["ast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours", "150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data", "20 hours of training data, dev and test sets comprise 4.5 hours of speech"], "top_k_doc_id": [5565, 5566, 4875, 6310, 6311, 6312, 6313, 1111, 3618, 3619, 3126, 2531, 4619, 620, 5715], "orig_top_k_doc_id": [6310, 6313, 6311, 3619, 3618, 2531, 5566, 4875, 4619, 620, 1111, 5565, 5715, 6312, 3126]}, {"qid": 3911, "question": "What languages did they use? in Analyzing ASR pretraining for low-resource speech-to-text translation", "answer": ["Spanish, English , Chinese , Mandarin Chinese , Croatian , Czech , French , Polish , Portuguese , Swedish ", "Spanish, English, Mandarin Chinese, Croatian, Czech, French, Polish, Portuguese, Swedish", "Spanish-English"], "top_k_doc_id": [5565, 5566, 4875, 6310, 6311, 6312, 6313, 1111, 3618, 3619, 3126, 1899, 4615, 3617, 1901], "orig_top_k_doc_id": [6310, 6313, 6311, 4875, 5566, 6312, 1899, 5565, 1111, 4615, 3618, 3617, 1901, 3619, 3126]}, {"qid": 3328, "question": "How is the quality of the data empirically evaluated?  in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "top_k_doc_id": [5565, 5566, 5564, 2971, 987, 1622, 5026, 1235, 6960, 5031, 1887, 5702, 5716, 6443, 533], "orig_top_k_doc_id": [5564, 5566, 5565, 2971, 987, 1622, 5026, 1235, 6960, 5031, 1887, 5702, 5716, 6443, 533]}, {"qid": 3910, "question": "What is their model's architecture? in Analyzing ASR pretraining for low-resource speech-to-text translation", "answer": [" the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2", "encoder-decoder model, end-to-end system architecture", "two CNN layers, three-layer bi-directional long short-term memory network (LSTM),  followed by a three-layer LSTM"], "top_k_doc_id": [5565, 5566, 4875, 6310, 6311, 6312, 6313, 1111, 3618, 3619, 659, 4923, 380, 4651, 4922], "orig_top_k_doc_id": [6310, 6313, 6311, 5566, 5565, 4875, 3618, 3619, 6312, 659, 4923, 380, 4651, 1111, 4922]}, {"qid": 3331, "question": "How big is Augmented LibriSpeech dataset? in CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", "answer": ["No", "No"], "top_k_doc_id": [5565, 5566, 5564, 5054, 381, 6111, 3265, 661, 6312, 5013, 6110, 7269, 6031, 3007, 5053], "orig_top_k_doc_id": [5564, 5566, 5565, 5054, 381, 6111, 3265, 661, 6312, 5013, 6110, 7269, 6031, 3007, 5053]}, {"qid": 3909, "question": "How many layers does their model have? in Analyzing ASR pretraining for low-resource speech-to-text translation", "answer": ["10 ", "two ", "two CNN layers, three-layer bi-directional long short-term memory network (LSTM), followed by a three-layer LSTM"], "top_k_doc_id": [5565, 5566, 4875, 6310, 6311, 6312, 6313, 1593, 380, 1899, 381, 2528, 3617, 4923, 620], "orig_top_k_doc_id": [6310, 6313, 4875, 6311, 5566, 5565, 1593, 380, 6312, 1899, 381, 2528, 3617, 4923, 620]}, {"qid": 1736, "question": "Which model have the smallest Character Error Rate and which have the smallest Word Error Rate? in End-to-End Speech Recognition: A review for the French Language", "answer": ["character unit the RNN-transducer with additional attention module, For subword units, classic RNN-transducer, RNN-transducer with attention and joint CTC-attention show comparable performance"], "top_k_doc_id": [5565, 5566, 1791, 7539, 2490, 3407, 5599, 4863, 5987, 2995, 4772, 3504, 621, 4750, 7290], "orig_top_k_doc_id": [5566, 1791, 7539, 2490, 3407, 5565, 5599, 4863, 5987, 2995, 4772, 3504, 621, 4750, 7290]}]}
{"group_id": 47, "group_size": 13, "items": [{"qid": 3924, "question": "What type of frequency analysis was used? in \"With 1 follower I must be AWESOME :P\". Exploring the role of irony markers in irony recognition", "answer": ["mean of occurrence per utterance and the standard deviation (SD) of each marker", "based on their occurrence", "Mean of occurrence per utterance and the standard deviation is calculated for every marker type; the means between each pair of types is compared via independent t-tests"], "top_k_doc_id": [5175, 5174, 7115, 1329, 2329, 4378, 5177, 6328, 1967, 6329, 2103, 6330, 5176, 5384, 1320], "orig_top_k_doc_id": [6328, 6329, 1329, 6330, 2329, 1967, 4378, 5174, 7115, 5175, 2103, 5384, 1320, 5177, 5176]}, {"qid": 3925, "question": "What type of classifiers were used? in \"With 1 follower I must be AWESOME :P\". Exploring the role of irony markers in irony recognition", "answer": ["Support Vector Machines (SVM) classifier with linear kernel BIBREF16", "Support Vector Machines (SVM) classifier with linear kernel BIBREF16 ", "Support Vector Machines (SVM) classifier with linear kernel"], "top_k_doc_id": [5175, 5174, 7115, 1329, 2329, 4378, 5177, 6328, 1967, 6329, 2103, 6330, 5176, 5384, 5179], "orig_top_k_doc_id": [6328, 1329, 2329, 6329, 4378, 6330, 5175, 5174, 1967, 5176, 5177, 2103, 5179, 7115, 5384]}, {"qid": 3923, "question": "Do they evaluate only on English datasets? in \"With 1 follower I must be AWESOME :P\". Exploring the role of irony markers in irony recognition", "answer": ["The twitter dataset is English-only; no information for the reddit dataset is given", "Yes", "Yes"], "top_k_doc_id": [5175, 5174, 7115, 1329, 2329, 4378, 5177, 6328, 1967, 6329, 2103, 6330, 5176, 3589, 5178], "orig_top_k_doc_id": [6328, 1329, 2329, 5175, 6329, 1967, 6330, 5177, 4378, 5174, 3589, 2103, 5176, 5178, 7115]}, {"qid": 3926, "question": "Who annotated the Twitter and Reddit data for irony? in \"With 1 follower I must be AWESOME :P\". Exploring the role of irony markers in irony recognition", "answer": ["collected using hashtags, such as #irony, #sarcasm, and #sarcastic", "Authors of the tweets and reddit posts", "Twitter and Reddit users of the original data "], "top_k_doc_id": [5175, 5174, 7115, 1329, 2329, 4378, 5177, 6328, 1967, 6329, 2103, 6330, 1330, 1331, 5179], "orig_top_k_doc_id": [6328, 6330, 6329, 2329, 2103, 1329, 1330, 4378, 1967, 5174, 5175, 1331, 5177, 5179, 7115]}, {"qid": 1638, "question": "Do the authors identify any cultural differences in irony use? in Irony Detection in a Multilingual Context", "answer": ["No"], "top_k_doc_id": [5175, 1318, 1319, 1320, 1329, 1967, 2103, 2329, 2330, 2331, 2114, 7115, 2115, 6328, 2111], "orig_top_k_doc_id": [2329, 2330, 2331, 2103, 1318, 1320, 5175, 7115, 2111, 1329, 1967, 1319, 2114, 6328, 2115]}, {"qid": 1640, "question": "What text-based features are used? in Irony Detection in a Multilingual Context", "answer": ["language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities),  language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words)"], "top_k_doc_id": [5175, 1318, 1319, 1320, 1329, 1967, 2103, 2329, 2330, 2331, 2114, 7115, 2115, 6328, 4378], "orig_top_k_doc_id": [2331, 2329, 2330, 1329, 1320, 1318, 2103, 7115, 1967, 2114, 2115, 5175, 6328, 1319, 4378]}, {"qid": 3021, "question": "What are the difficulties in modelling the ironic pattern? in A Neural Approach to Irony Generation", "answer": ["obscure and hard to understand,  lack of previous work and baselines on irony generation", "ironies are often obscure and hard to understand"], "top_k_doc_id": [5175, 5174, 7115, 1329, 2329, 4378, 5177, 6328, 1967, 6329, 2330, 5176, 5179, 5178, 2331], "orig_top_k_doc_id": [5174, 1329, 2330, 7115, 5175, 2329, 5176, 1967, 5179, 5178, 2331, 6328, 5177, 6329, 4378]}, {"qid": 1637, "question": "What multilingual word representations are used? in Irony Detection in a Multilingual Context", "answer": [" a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space"], "top_k_doc_id": [5175, 1318, 1319, 1320, 1329, 1967, 2103, 2329, 2330, 2331, 2789, 6035, 7285, 783, 87], "orig_top_k_doc_id": [1318, 2329, 2331, 2330, 1320, 1319, 1329, 783, 1967, 87, 2789, 6035, 2103, 5175, 7285]}, {"qid": 1639, "question": "What neural architectures are used? in Irony Detection in a Multilingual Context", "answer": ["Convolutional Neural Network (CNN)"], "top_k_doc_id": [5175, 1318, 1319, 1320, 1329, 1967, 2103, 2329, 2330, 2331, 2114, 7115, 1331, 4379, 87], "orig_top_k_doc_id": [2329, 2330, 2331, 1320, 1318, 1329, 1967, 1319, 2103, 7115, 5175, 1331, 2114, 4379, 87]}, {"qid": 1641, "question": "What monolingual word representations are used? in Irony Detection in a Multilingual Context", "answer": ["AraVec for Arabic, FastText for French, and Word2vec Google News for English."], "top_k_doc_id": [5175, 1318, 1319, 1320, 1329, 1967, 2103, 2329, 2330, 2331, 2789, 6035, 7285, 3297, 3296], "orig_top_k_doc_id": [2329, 2331, 2330, 1318, 1329, 1967, 1320, 1319, 2789, 2103, 3297, 5175, 7285, 6035, 3296]}, {"qid": 3019, "question": "What experiments are conducted? in A Neural Approach to Irony Generation", "answer": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "top_k_doc_id": [5175, 5174, 7115, 1318, 1967, 2103, 5176, 5178, 5179, 2329, 1329, 2330, 2112, 6628, 1494], "orig_top_k_doc_id": [2329, 1329, 1967, 5174, 5178, 2330, 2112, 5175, 5176, 5179, 2103, 7115, 6628, 1494, 1318]}, {"qid": 3022, "question": "How did the authors find ironic data on twitter? in A Neural Approach to Irony Generation", "answer": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "top_k_doc_id": [5175, 5174, 7115, 1329, 2329, 4378, 5177, 6328, 2109, 5179, 5176, 1319, 1318, 2105, 2104], "orig_top_k_doc_id": [5175, 5174, 2109, 5177, 6328, 2329, 5179, 5176, 1319, 4378, 1318, 1329, 7115, 2105, 2104]}, {"qid": 3023, "question": "Who judged the irony accuracy, sentiment preservation and content preservation? in A Neural Approach to Irony Generation", "answer": ["Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).", "four annotators who are proficient in English"], "top_k_doc_id": [5175, 5174, 7115, 1318, 1967, 2103, 5176, 5178, 5179, 5177, 1330, 5562, 1005, 481, 5561], "orig_top_k_doc_id": [5178, 5179, 5177, 5174, 5176, 5175, 1330, 5562, 1005, 1967, 2103, 7115, 481, 1318, 5561]}]}
{"group_id": 48, "group_size": 13, "items": [{"qid": 4530, "question": "How many of the attribute-value pairs are found in video? in Multimodal Attribute Extraction", "answer": ["No", "No"], "top_k_doc_id": [7083, 7082, 7084, 4793, 130, 131, 224, 4794, 4797, 7723, 223, 578, 6603, 7141, 7336], "orig_top_k_doc_id": [7083, 7084, 7082, 130, 4793, 578, 131, 7141, 224, 7723, 4797, 223, 4794, 7336, 6603]}, {"qid": 4531, "question": "How many of the attribute-value pairs are found in audio? in Multimodal Attribute Extraction", "answer": ["No", "No"], "top_k_doc_id": [7083, 7082, 7084, 4793, 130, 131, 224, 4794, 4797, 7723, 223, 578, 6603, 339, 853], "orig_top_k_doc_id": [7082, 7083, 7084, 130, 4793, 131, 224, 7723, 4797, 223, 4794, 578, 6603, 339, 853]}, {"qid": 4532, "question": "How many of the attribute-value pairs are found in images? in Multimodal Attribute Extraction", "answer": ["No", "No"], "top_k_doc_id": [7083, 7082, 7084, 4793, 130, 131, 224, 4794, 4797, 7723, 223, 7141, 4594, 3410, 7140], "orig_top_k_doc_id": [7083, 7082, 7084, 130, 131, 4793, 7141, 4594, 3410, 224, 7723, 4797, 223, 4794, 7140]}, {"qid": 4533, "question": "How many of the attribute-value pairs are found in semi-structured text? in Multimodal Attribute Extraction", "answer": ["No", "No"], "top_k_doc_id": [7083, 7082, 7084, 4793, 130, 131, 224, 4794, 4797, 7723, 853, 6397, 5186, 223, 7724], "orig_top_k_doc_id": [7083, 7082, 7084, 4793, 130, 7723, 6397, 5186, 223, 853, 7724, 4797, 224, 131, 4794]}, {"qid": 4534, "question": "How many of the attribute-value pairs are found in unstructured text? in Multimodal Attribute Extraction", "answer": ["No", "No"], "top_k_doc_id": [7083, 7082, 7084, 4793, 130, 131, 224, 4794, 4797, 7723, 853, 6397, 854, 7609, 6396], "orig_top_k_doc_id": [7082, 7083, 7084, 130, 853, 4793, 6397, 4797, 224, 131, 4794, 854, 7609, 7723, 6396]}, {"qid": 99, "question": "What misbehavior is identified? in An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "answer": ["if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations", "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations"], "top_k_doc_id": [7083, 7138, 7141, 84, 112, 115, 116, 7082, 2899, 3414, 3409, 80, 521, 3736, 7686], "orig_top_k_doc_id": [116, 115, 112, 7083, 3409, 7138, 7082, 3414, 84, 80, 2899, 7141, 521, 3736, 7686]}, {"qid": 100, "question": "What is the baseline used? in An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [7083, 7138, 7141, 84, 112, 115, 116, 7082, 2899, 3414, 4169, 7084, 3175, 1237, 3176], "orig_top_k_doc_id": [112, 7083, 7082, 115, 84, 116, 7138, 4169, 7084, 3175, 1237, 3414, 7141, 2899, 3176]}, {"qid": 101, "question": "Which attention mechanisms do they compare? in An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "answer": ["Soft attention, Hard Stochastic attention, Local Attention"], "top_k_doc_id": [7083, 7138, 7141, 84, 112, 115, 116, 7082, 510, 3175, 891, 3736, 4756, 3176, 868], "orig_top_k_doc_id": [112, 116, 7083, 510, 115, 3175, 891, 3736, 84, 7141, 4756, 3176, 7082, 7138, 868]}, {"qid": 4535, "question": "How many different semi-structured templates are represented in the data? in Multimodal Attribute Extraction", "answer": ["7.6 million", "No"], "top_k_doc_id": [7083, 7082, 7084, 4793, 130, 3857, 5186, 7725, 6397, 2377, 223, 1170, 4716, 7724, 7719], "orig_top_k_doc_id": [7084, 7082, 7083, 3857, 5186, 7725, 6397, 2377, 223, 4793, 1170, 130, 4716, 7724, 7719]}, {"qid": 4537, "question": "Do they consider semi-structured webpages? in Multimodal Attribute Extraction", "answer": ["No", "Yes"], "top_k_doc_id": [7083, 7082, 7084, 4793, 4594, 6397, 3410, 4399, 4159, 3412, 5186, 853, 4464, 6765, 6926], "orig_top_k_doc_id": [7084, 7082, 7083, 4594, 4793, 6397, 3410, 4399, 4159, 3412, 5186, 853, 4464, 6765, 6926]}, {"qid": 4576, "question": "What are special architectures this review focuses on that are related to multimodal fusion? in Multimodal Intelligence: Representation Learning, Information Fusion, and Applications", "answer": ["attention mechanism, bilinear pooling", "attention mechanism, bilinear pooling"], "top_k_doc_id": [7083, 7138, 7141, 7148, 3175, 2116, 2119, 7144, 276, 3176, 2805, 1237, 2120, 2804, 413], "orig_top_k_doc_id": [7138, 7148, 7141, 3175, 2116, 2119, 7144, 276, 7083, 3176, 2805, 1237, 2120, 2804, 413]}, {"qid": 2198, "question": "What other multimodal knowledge base embedding methods are there? in Embedding Multimodal Relational Data for Knowledge Base Completion", "answer": ["merging, concatenating, or averaging the entity and its features to compute its embeddings, graph embedding approaches, matrix factorization to jointly embed KB and textual relations"], "top_k_doc_id": [7083, 3409, 3412, 7084, 3411, 3410, 3414, 6002, 3533, 3176, 3175, 7147, 1822, 3736, 7805], "orig_top_k_doc_id": [3409, 3412, 7084, 3411, 3410, 3414, 7083, 6002, 3533, 3176, 3175, 7147, 1822, 3736, 7805]}, {"qid": 4536, "question": "Are all datapoints from the same website? in Multimodal Attribute Extraction", "answer": ["No", "No"], "top_k_doc_id": [7083, 7082, 7084, 131, 1229, 1230, 855, 853, 2819, 6737, 3410, 7140, 7141, 931, 1923], "orig_top_k_doc_id": [7083, 7084, 7082, 131, 1229, 1230, 855, 853, 2819, 6737, 3410, 7140, 7141, 931, 1923]}]}
{"group_id": 49, "group_size": 12, "items": [{"qid": 246, "question": "What was their performance on the dataset? in Exploiting Deep Learning for Persian Sentiment Analysis", "answer": ["accuracy of 82.6%"], "top_k_doc_id": [5395, 987, 309, 310, 311, 5391, 4727, 3542, 986, 7115, 1644, 2103, 1329, 2105, 7472], "orig_top_k_doc_id": [310, 309, 311, 987, 5391, 7115, 3542, 986, 5395, 1644, 2103, 1329, 4727, 2105, 7472]}, {"qid": 247, "question": "How large is the dataset? in Exploiting Deep Learning for Persian Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [5395, 987, 309, 310, 311, 5391, 4727, 3542, 986, 7115, 7548, 5393, 5105, 5106, 462], "orig_top_k_doc_id": [309, 311, 310, 987, 5391, 986, 7548, 7115, 5395, 5393, 3542, 5105, 4727, 5106, 462]}, {"qid": 245, "question": "By how much did the results improve? in Exploiting Deep Learning for Persian Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [5395, 987, 309, 310, 311, 5391, 4727, 3542, 7548, 2105, 1040, 7551, 2626, 7132, 1318], "orig_top_k_doc_id": [309, 311, 310, 5391, 987, 7548, 2105, 4727, 1040, 3542, 7551, 2626, 7132, 5395, 1318]}, {"qid": 3209, "question": "what accents are present in the corpus? in A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: The Deepmine Database", "answer": ["No", "No"], "top_k_doc_id": [5395, 1375, 5391, 5392, 5393, 5394, 5564, 482, 1623, 5565, 987, 3437, 5566, 7658, 3438], "orig_top_k_doc_id": [5395, 5391, 5393, 5392, 5565, 5564, 5394, 5566, 1375, 7658, 3438, 3437, 1623, 987, 482]}, {"qid": 3210, "question": "what evaluation protocols are provided? in A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: The Deepmine Database", "answer": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "top_k_doc_id": [5395, 1375, 5391, 5392, 5393, 5394, 5564, 482, 1623, 5565, 987, 3437, 7794, 2351, 6468], "orig_top_k_doc_id": [5395, 5391, 5393, 5392, 5394, 5564, 5565, 1623, 7794, 2351, 1375, 3437, 6468, 482, 987]}, {"qid": 3211, "question": "what age range is in the data? in A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: The Deepmine Database", "answer": ["No", "No"], "top_k_doc_id": [5395, 1375, 5391, 5392, 5393, 5394, 5564, 482, 1623, 5565, 3438, 6468, 3437, 3007, 7324], "orig_top_k_doc_id": [5391, 5395, 5393, 5392, 5394, 3438, 3437, 5565, 5564, 6468, 482, 1623, 3007, 7324, 1375]}, {"qid": 3212, "question": "what is the source of the data? in A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: The Deepmine Database", "answer": ["Android application"], "top_k_doc_id": [5395, 1375, 5391, 5392, 5393, 5394, 5564, 482, 1623, 5565, 3438, 6468, 6110, 7794, 2528], "orig_top_k_doc_id": [5391, 5395, 5393, 5392, 5394, 482, 5564, 6468, 1623, 1375, 3438, 6110, 5565, 7794, 2528]}, {"qid": 244, "question": "Which deep learning model performed better? in Exploiting Deep Learning for Persian Sentiment Analysis", "answer": ["autoencoders", "CNN"], "top_k_doc_id": [5395, 987, 309, 310, 311, 5391, 4727, 7174, 6403, 6401, 756, 5980, 7548, 2105, 1665], "orig_top_k_doc_id": [309, 311, 310, 987, 7174, 6403, 5395, 4727, 6401, 5391, 756, 5980, 7548, 2105, 1665]}, {"qid": 1799, "question": "What is the WordNet counterpart for Persian? in Improving Information Retrieval Results for Persian Documents using FarsNet", "answer": ["FarsNet"], "top_k_doc_id": [5395, 987, 309, 310, 311, 5391, 2625, 2626, 1644, 5393, 5392, 994, 986, 1091, 1092], "orig_top_k_doc_id": [2625, 2626, 1644, 987, 5393, 5392, 5395, 311, 5391, 310, 309, 994, 986, 1091, 1092]}, {"qid": 3208, "question": "how was the speech collected? in A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: The Deepmine Database", "answer": ["The speech was collected from respondents using an android application.", "Android application"], "top_k_doc_id": [5395, 1375, 5391, 5392, 5393, 5394, 5564, 482, 1623, 3438, 3437, 7794, 2630, 7537, 4875], "orig_top_k_doc_id": [5391, 5395, 5393, 5392, 5394, 482, 1375, 3438, 3437, 5564, 7794, 1623, 2630, 7537, 4875]}, {"qid": 1798, "question": "Which evaluation metric has been measured? in Improving Information Retrieval Results for Persian Documents using FarsNet", "answer": ["Mean Average Precision"], "top_k_doc_id": [5395, 987, 2625, 2626, 5148, 2053, 1817, 5393, 2455, 7739, 1091, 3193, 2785, 1698, 1923], "orig_top_k_doc_id": [2625, 2626, 987, 5148, 2053, 1817, 5395, 5393, 2455, 7739, 1091, 3193, 2785, 1698, 1923]}, {"qid": 3207, "question": "who transcribed the corpus? in A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: The Deepmine Database", "answer": ["No", "No"], "top_k_doc_id": [5395, 1375, 5391, 5392, 5393, 5394, 5564, 6468, 2450, 3401, 7244, 3438, 3437, 2528, 4875], "orig_top_k_doc_id": [5391, 5395, 5392, 5393, 6468, 5394, 2450, 5564, 3401, 7244, 3438, 3437, 2528, 1375, 4875]}]}
{"group_id": 50, "group_size": 12, "items": [{"qid": 619, "question": "What are the supported natural commands? in Conversational Intent Understanding for Passengers in Autonomous Vehicles", "answer": ["Set/Change Destination, Set/Change Route, Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, Other "], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 400, 3773, 7514, 3772, 5016, 7158, 3562, 7156, 7157, 6350], "orig_top_k_doc_id": [1296, 767, 5015, 1297, 7514, 1300, 7158, 5016, 3562, 7156, 3772, 7157, 3773, 400, 6350]}, {"qid": 620, "question": "What is the size of their collected dataset? in Conversational Intent Understanding for Passengers in Autonomous Vehicles", "answer": ["3347 unique utterances "], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 400, 3773, 7514, 3772, 5016, 5917, 1070, 6525, 899, 7515], "orig_top_k_doc_id": [1296, 767, 5015, 1297, 7514, 1300, 3773, 5917, 5016, 1070, 400, 6525, 899, 3772, 7515]}, {"qid": 621, "question": "Did they compare against other systems? in Conversational Intent Understanding for Passengers in Autonomous Vehicles", "answer": ["Yes"], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 400, 3773, 7514, 2969, 3562, 1029, 7515, 1070, 1725, 6776], "orig_top_k_doc_id": [1296, 5015, 767, 7514, 1297, 2969, 1300, 3562, 1029, 7515, 400, 1070, 1725, 6776, 3773]}, {"qid": 987, "question": "What is shared in the joint model? in Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances", "answer": ["jointly trained with slots"], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 400, 401, 1029, 1298, 1299, 2643, 2645, 2646, 5016, 7514], "orig_top_k_doc_id": [5015, 1296, 767, 1300, 5016, 1297, 1299, 2646, 2645, 2643, 400, 1298, 7514, 1029, 401]}, {"qid": 988, "question": "Are the intent labels imbalanced in the dataset? in Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances", "answer": ["Yes"], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 400, 401, 1029, 1298, 1299, 2643, 2645, 2646, 5016, 7156], "orig_top_k_doc_id": [1296, 5015, 767, 1300, 5016, 1297, 2646, 2645, 1299, 2643, 1298, 401, 1029, 7156, 400]}, {"qid": 622, "question": "What intents does the paper explore? in Conversational Intent Understanding for Passengers in Autonomous Vehicles", "answer": ["Set/Change Destination, Set/Change Route, Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, Other "], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 400, 5920, 7514, 6780, 5917, 6781, 6777, 4787, 1632, 2643], "orig_top_k_doc_id": [1296, 767, 5015, 1297, 1300, 5920, 7514, 400, 6780, 5917, 6781, 6777, 4787, 1632, 2643]}, {"qid": 2858, "question": "By how much is performance improved with multimodality? in Towards Multimodal Understanding of Passenger-Vehicle Interactions in Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data", "answer": ["by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling", "F1 score increased from 0.89 to 0.92"], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 226, 230, 3655, 5016, 7138, 7144, 7514, 7515, 7556, 1299], "orig_top_k_doc_id": [5015, 767, 1296, 1297, 1300, 5016, 7514, 7138, 3655, 7515, 7556, 7144, 230, 226, 1299]}, {"qid": 2859, "question": "Is collected multimodal in cabin dataset public? in Towards Multimodal Understanding of Passenger-Vehicle Interactions in Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data", "answer": ["No", "No"], "top_k_doc_id": [1296, 1300, 5015, 767, 1297, 226, 230, 3655, 5016, 7138, 7144, 7514, 7515, 7556, 929], "orig_top_k_doc_id": [5015, 767, 1296, 1297, 1300, 5016, 7138, 7556, 7514, 7515, 226, 929, 3655, 230, 7144]}, {"qid": 4276, "question": "Which dataset do they use? in Towards Better Understanding of Spontaneous Conversations: Overcoming Automatic Speech Recognition Errors With Intent Recognition", "answer": ["500 rescored intent annotations found in the lattices in cancellations and refunds domain", "dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain", "dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain"], "top_k_doc_id": [1296, 898, 1987, 4646, 6776, 6777, 6780, 6781, 1300, 1711, 5015, 5764, 5765, 1161, 5264], "orig_top_k_doc_id": [6776, 6781, 6780, 4646, 1711, 6777, 5764, 5765, 898, 1296, 1161, 5015, 1300, 1987, 5264]}, {"qid": 4277, "question": "How do they use extracted intent to rescore? in Towards Better Understanding of Spontaneous Conversations: Overcoming Automatic Speech Recognition Errors With Intent Recognition", "answer": ["providing a library of intent examples", " the rescoring was judged by two annotators, who labeled 250 examples each"], "top_k_doc_id": [1296, 898, 1987, 4646, 6776, 6777, 6780, 6781, 1300, 1711, 5015, 7759, 1712, 1297, 6350], "orig_top_k_doc_id": [6781, 6780, 6776, 6777, 1711, 1296, 5015, 1300, 4646, 898, 1987, 7759, 1712, 1297, 6350]}, {"qid": 3940, "question": "How was the dataset collected? in Speech Model Pre-training for End-to-End Spoken Language Understanding", "answer": ["data was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice", "crowdsourcing", "using crowdsourcing"], "top_k_doc_id": [1296, 1300, 5015, 6350, 6351, 7138, 4928, 250, 7658, 6480, 2632, 2634, 2197, 3582, 1836], "orig_top_k_doc_id": [6350, 6351, 7138, 4928, 1296, 1300, 5015, 250, 7658, 6480, 2632, 2634, 2197, 3582, 1836]}, {"qid": 4278, "question": "Do they evaluate by how much does ASR improve compared to state-of-the-art just by using their FST? in Towards Better Understanding of Spontaneous Conversations: Overcoming Automatic Speech Recognition Errors With Intent Recognition", "answer": ["No", "No"], "top_k_doc_id": [1296, 898, 1987, 4646, 6776, 6777, 6780, 6781, 5764, 5765, 6778, 1161, 6779, 6517, 2633], "orig_top_k_doc_id": [6776, 6777, 6780, 6781, 5764, 1987, 5765, 4646, 6778, 1161, 1296, 6779, 6517, 2633, 898]}]}
{"group_id": 51, "group_size": 12, "items": [{"qid": 1487, "question": "Which part of the joke is more important in humor? in Humor Detection: A Transformer Gets the Last Laugh", "answer": ["the punchline of the joke "], "top_k_doc_id": [2083, 2084, 2085, 7361, 5272, 7486, 429, 5274, 6082, 430, 6083, 6540, 7, 5275, 7485], "orig_top_k_doc_id": [2084, 2083, 429, 2085, 6082, 5272, 6540, 430, 7, 6083, 5274, 5275, 7361, 7486, 7485]}, {"qid": 1488, "question": "What is improvement in accuracy for short Jokes in relation other types of jokes? in Humor Detection: A Transformer Gets the Last Laugh", "answer": ["It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%"], "top_k_doc_id": [2083, 2084, 2085, 7361, 5272, 7486, 429, 5274, 6082, 430, 6083, 5769, 7362, 5273, 7487], "orig_top_k_doc_id": [2085, 2084, 2083, 429, 6082, 5272, 7361, 6083, 430, 5769, 7486, 5274, 7362, 5273, 7487]}, {"qid": 1490, "question": "How they evaluate if joke is humorous or not? in Humor Detection: A Transformer Gets the Last Laugh", "answer": ["The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes."], "top_k_doc_id": [2083, 2084, 2085, 7361, 5272, 7486, 429, 5274, 6082, 430, 5273, 6540, 7481, 6732, 1865], "orig_top_k_doc_id": [2084, 2085, 2083, 429, 5272, 5273, 6540, 7361, 5274, 7481, 6732, 430, 6082, 7486, 1865]}, {"qid": 360, "question": "What evaluation metrics were used? in What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant", "answer": ["AUC-ROC"], "top_k_doc_id": [2083, 6082, 429, 431, 2084, 2085, 3772, 7222, 7225, 7775, 1074, 7223, 7224, 2438, 5953], "orig_top_k_doc_id": [3772, 429, 2084, 6082, 2083, 2085, 7222, 7225, 431, 7224, 7775, 7223, 2438, 5953, 1074]}, {"qid": 362, "question": "What feedback labels are used? in What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant", "answer": ["five-minute reuse and one-day return"], "top_k_doc_id": [2083, 6082, 429, 431, 2084, 2085, 3772, 7222, 7225, 7775, 1074, 7223, 7224, 6350, 6083], "orig_top_k_doc_id": [429, 3772, 2084, 7222, 6082, 2083, 2085, 7775, 431, 7225, 7224, 7223, 6350, 6083, 1074]}, {"qid": 1489, "question": "What kind of humor they have evaluated? in Humor Detection: A Transformer Gets the Last Laugh", "answer": ["a subset of the population where we can quantitatively measure reactions: the popular Reddit r/Jokes thread, These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or upvotes. "], "top_k_doc_id": [2083, 2084, 2085, 7361, 5272, 7486, 429, 5274, 6082, 7, 6732, 5273, 1501, 7483, 7485], "orig_top_k_doc_id": [5272, 7, 2085, 5274, 6732, 5273, 2083, 2084, 7361, 1501, 7486, 429, 6082, 7483, 7485]}, {"qid": 3739, "question": "What evaluation was performed on the output? in Knowledge Amalgam: Generating Jokes and Quotes Together", "answer": ["similarity of the generated texts with training data objectively, humor content subjectively, syntactic correctness of the generated sentences", "For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria, To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment", "Phrase Overlap match and K-gram-Jaccard similarity"], "top_k_doc_id": [2083, 6082, 6083, 6084, 6085, 429, 430, 2084, 2085, 7361, 5324, 1097, 6056, 2632, 2631], "orig_top_k_doc_id": [6082, 6083, 6085, 6084, 2083, 2084, 430, 7361, 5324, 2085, 429, 1097, 6056, 2632, 2631]}, {"qid": 3740, "question": "Where did the joke data come from? in Knowledge Amalgam: Generating Jokes and Quotes Together", "answer": ["CrowdTruth and Subreddits", "CrowdTruth , Subreddits", "CrowdTruth, Subreddits"], "top_k_doc_id": [2083, 6082, 6083, 6084, 6085, 429, 430, 2084, 2085, 5954, 431, 2630, 5953, 6540, 3665], "orig_top_k_doc_id": [6082, 6083, 6085, 2083, 2084, 429, 430, 2085, 5954, 6084, 431, 2630, 5953, 6540, 3665]}, {"qid": 361, "question": "Where did the real production data come from? in What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant", "answer": [" jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)"], "top_k_doc_id": [2083, 6082, 429, 431, 2084, 2085, 3772, 7222, 7225, 7775, 227, 430, 5953, 3775, 1207], "orig_top_k_doc_id": [3772, 429, 431, 2084, 6082, 2083, 227, 7225, 2085, 430, 7222, 5953, 7775, 3775, 1207]}, {"qid": 3741, "question": "What type of quotes is this system trying to generate? in Knowledge Amalgam: Generating Jokes and Quotes Together", "answer": ["No", "No", "inspirational"], "top_k_doc_id": [2083, 6082, 6083, 6084, 6085, 3665, 738, 523, 6056, 1097, 5324, 3144, 1096, 3551, 3550], "orig_top_k_doc_id": [6085, 6082, 6083, 6084, 3665, 738, 523, 2083, 6056, 1097, 5324, 3144, 1096, 3551, 3550]}, {"qid": 4732, "question": "How are the language models used to make predictions on humorous statements? in Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!", "answer": ["scored tweets by assigning them a probability based on each model, higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data", "We scored tweets by assigning them a probability based on each model"], "top_k_doc_id": [2083, 2084, 2085, 7361, 5272, 7486, 7481, 6477, 5273, 6732, 7059, 2157, 4900, 1863, 2158], "orig_top_k_doc_id": [7361, 7481, 5272, 6477, 5273, 2084, 6732, 2085, 7059, 2157, 4900, 7486, 2083, 1863, 2158]}, {"qid": 4031, "question": "How is the funny score calculated? in Neural Joking Machine : Humorous image captioning", "answer": ["Based on the number of stars users assign funny captions, an LSTM calculates the loss value L as an average of each mini-batch and returns  L when the number of stars is less than 100, otherwise L-1.0", "The funny score is L if the caption has fewer than 100 stars and 1-L if the caption has 100 or more stars, where L is the average loss value calculated with the LSTM on the mini-batch."], "top_k_doc_id": [2083, 2084, 2085, 7361, 6477, 7481, 3736, 3026, 6082, 6478, 4034, 4033, 3034, 4744, 5273], "orig_top_k_doc_id": [6477, 2085, 7361, 2083, 7481, 3736, 3026, 2084, 6082, 6478, 4034, 4033, 3034, 4744, 5273]}]}
{"group_id": 52, "group_size": 12, "items": [{"qid": 1491, "question": "Do they report results only on English data? in Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments", "answer": ["Yes"], "top_k_doc_id": [3069, 2226, 3070, 107, 2086, 2087, 6828, 6829, 3071, 556, 3074, 5716, 5709, 5710, 2124], "orig_top_k_doc_id": [3069, 3070, 6828, 107, 3071, 2086, 2087, 6829, 2226, 5716, 5709, 5710, 3074, 2124, 556]}, {"qid": 1492, "question": "Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model? in Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments", "answer": ["These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions."], "top_k_doc_id": [3069, 2226, 3070, 107, 2086, 2087, 6828, 6829, 3071, 556, 3074, 2088, 2089, 1832, 5700], "orig_top_k_doc_id": [3070, 6828, 3069, 6829, 2226, 2086, 107, 3071, 2087, 2088, 3074, 2089, 1832, 556, 5700]}, {"qid": 1493, "question": "Which models are best for learning long-distance movement? in Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments", "answer": ["the transformer models"], "top_k_doc_id": [3069, 2226, 3070, 107, 2086, 2087, 6828, 6829, 3071, 2124, 5716, 2123, 2088, 3498, 2089], "orig_top_k_doc_id": [3069, 2086, 3070, 107, 2124, 2087, 3071, 6828, 5716, 2123, 2088, 6829, 3498, 2226, 2089]}, {"qid": 1494, "question": "Where does the data in CoLA come from? in Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments", "answer": [" CoLA contains example sentences from linguistics publications labeled by experts"], "top_k_doc_id": [3069, 2226, 3070, 107, 2086, 2087, 6828, 6829, 1560, 1561, 2088, 2089, 6136, 3071, 1779], "orig_top_k_doc_id": [3069, 3070, 2086, 2087, 6829, 2226, 2088, 107, 3071, 1560, 6136, 6828, 1561, 2089, 1779]}, {"qid": 1495, "question": "How is the CoLA grammatically annotated? in Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments", "answer": ["labeled by experts"], "top_k_doc_id": [3069, 2226, 3070, 107, 2086, 2087, 6828, 6829, 1560, 1561, 2088, 2089, 6136, 5716, 6135], "orig_top_k_doc_id": [2086, 3069, 3070, 2226, 6829, 2088, 2087, 2089, 107, 1561, 6136, 5716, 6828, 1560, 6135]}, {"qid": 2029, "question": "Which of the model yields the best performance? in BLiMP: A Benchmark of Linguistic Minimal Pairs for English", "answer": ["GPT-2"], "top_k_doc_id": [3069, 2226, 3070, 3071, 4698, 5699, 709, 5341, 1330, 7509, 5456, 2195, 3717, 707, 500], "orig_top_k_doc_id": [3069, 3070, 5699, 5341, 1330, 7509, 5456, 709, 2226, 4698, 3071, 2195, 3717, 707, 500]}, {"qid": 2030, "question": "What is the performance of the models on the tasks? in BLiMP: A Benchmark of Linguistic Minimal Pairs for English", "answer": ["Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)"], "top_k_doc_id": [3069, 2226, 3070, 3071, 4698, 5699, 709, 5341, 4642, 3749, 3618, 1329, 1560, 5185, 5716], "orig_top_k_doc_id": [3069, 3070, 5699, 2226, 4642, 4698, 3749, 709, 3071, 5341, 3618, 1329, 1560, 5185, 5716]}, {"qid": 4317, "question": "Does the paper list other heuristic biases in the LSTMs? in Modeling German Verb Argument Structures: LSTMs vs. Humans", "answer": ["Yes", "Yes"], "top_k_doc_id": [3069, 3072, 6828, 6829, 6831, 6832, 3979, 3985, 4774, 3058, 3059, 1790, 5963, 6503, 901], "orig_top_k_doc_id": [6828, 6832, 3979, 3072, 6829, 3069, 6831, 3059, 4774, 3058, 1790, 5963, 6503, 3985, 901]}, {"qid": 4318, "question": "What are the performances of LSTMs and humans on the task? in Modeling German Verb Argument Structures: LSTMs vs. Humans", "answer": ["mean AUC of 0.56 for the LTSM and of 0.58 for humans", "LTSM 0.56 AUC, humans 0.58 AUC", "LSTM obtains an overall score of 0.56 while humans' score is 0.58"], "top_k_doc_id": [3069, 3072, 6828, 6829, 6831, 6832, 3979, 3985, 4774, 3058, 3059, 6830, 3071, 5962, 7024], "orig_top_k_doc_id": [6828, 3072, 6832, 6830, 3069, 6831, 6829, 3979, 3071, 4774, 3058, 3059, 5962, 7024, 3985]}, {"qid": 2031, "question": "How is the data automatically generated? in BLiMP: A Benchmark of Linguistic Minimal Pairs for English", "answer": [" The data generation scripts use a basic template to create each paradigm, pulling from a vocabulary of over 3000 words annotated for morphological, syntactic, and semantic features needed to create grammatical and semantically felicitous sentences."], "top_k_doc_id": [3069, 2226, 3070, 3071, 4698, 5699, 7700, 7725, 7701, 1217, 2205, 1263, 861, 7436, 7509], "orig_top_k_doc_id": [3069, 3070, 7700, 7725, 4698, 7701, 1217, 2205, 2226, 5699, 1263, 861, 7436, 3071, 7509]}, {"qid": 4316, "question": "Are the orders of case assignment biases motivated by frequency considerations? in Modeling German Verb Argument Structures: LSTMs vs. Humans", "answer": ["Yes", "Yes"], "top_k_doc_id": [3069, 3072, 6828, 6829, 6831, 6832, 3979, 3985, 4774, 6830, 4344, 7383, 4339, 7329, 7434], "orig_top_k_doc_id": [6831, 6832, 6830, 6829, 6828, 4774, 3072, 3985, 3069, 3979, 4344, 7383, 4339, 7329, 7434]}, {"qid": 4315, "question": "What is the percentage of human judgment agreement on the set? in Modeling German Verb Argument Structures: LSTMs vs. Humans", "answer": ["No", "No"], "top_k_doc_id": [3069, 3072, 6828, 6829, 6831, 6832, 6830, 3074, 3071, 3059, 861, 3070, 3073, 3058, 6808], "orig_top_k_doc_id": [6828, 3069, 3072, 6830, 6831, 6829, 6832, 3074, 3071, 3059, 861, 3070, 3073, 3058, 6808]}]}
{"group_id": 53, "group_size": 12, "items": [{"qid": 1684, "question": "Do the authors suggest any future extensions to this work? in Event detection in Twitter: A keyword volume approach", "answer": ["Yes"], "top_k_doc_id": [2404, 5115, 5189, 579, 2081, 2403, 4359, 5112, 5117, 7413, 7534, 7411, 2396, 883, 4409], "orig_top_k_doc_id": [2403, 2404, 2081, 5115, 7411, 579, 5112, 4359, 7413, 5189, 2396, 5117, 883, 7534, 4409]}, {"qid": 1685, "question": "Which of the classifiers showed the best performance? in Event detection in Twitter: A keyword volume approach", "answer": ["Logistic regression"], "top_k_doc_id": [2404, 5115, 5189, 579, 2081, 2403, 4359, 5112, 5117, 7413, 7534, 4392, 4358, 4322, 5116], "orig_top_k_doc_id": [2403, 579, 5112, 4392, 5115, 2404, 5189, 5117, 4358, 2081, 4322, 7534, 5116, 4359, 7413]}, {"qid": 2939, "question": "Do they report results only on English data? in A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "answer": ["Yes", "No"], "top_k_doc_id": [2404, 5115, 5189, 5112, 5113, 5114, 5116, 5117, 6890, 1643, 4359, 5246, 6433, 3486, 2189], "orig_top_k_doc_id": [5112, 5115, 5113, 5116, 5117, 5114, 2404, 6890, 6433, 3486, 5189, 2189, 1643, 4359, 5246]}, {"qid": 2940, "question": "What type of classifiers are used? in A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "answer": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "top_k_doc_id": [2404, 5115, 5189, 5112, 5113, 5114, 5116, 5117, 6890, 1643, 4359, 5246, 2403, 4353, 34], "orig_top_k_doc_id": [5112, 5115, 5113, 5116, 5117, 5114, 2404, 6890, 1643, 5246, 5189, 4359, 2403, 4353, 34]}, {"qid": 2942, "question": "How are the interpretability merits of the approach demonstrated? in A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "answer": ["By involving humans for post-hoc evaluation of model's interpretability", "directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model"], "top_k_doc_id": [2404, 5115, 5189, 5112, 5113, 5114, 5116, 5117, 6890, 5068, 2189, 6700, 7411, 2181, 3208], "orig_top_k_doc_id": [5112, 5115, 5117, 5116, 5113, 5114, 2189, 2404, 2181, 6890, 5189, 5068, 7411, 3208, 6700]}, {"qid": 2943, "question": "How are the accuracy merits of the approach demonstrated? in A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "answer": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "top_k_doc_id": [2404, 5115, 5189, 5112, 5113, 5114, 5116, 5117, 6890, 5068, 2189, 6700, 7411, 4359, 2403], "orig_top_k_doc_id": [5115, 5112, 5116, 5113, 5117, 5114, 2189, 2404, 6890, 7411, 5189, 5068, 6700, 4359, 2403]}, {"qid": 1687, "question": "How are the keywords associated with events such as protests selected? in Event detection in Twitter: A keyword volume approach", "answer": ["By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events."], "top_k_doc_id": [2404, 5115, 5189, 579, 2081, 2403, 4359, 5112, 5117, 5113, 5186, 2408, 2405, 6140, 4360], "orig_top_k_doc_id": [2403, 2404, 5115, 5112, 5189, 5113, 579, 4359, 2081, 5186, 2408, 5117, 2405, 6140, 4360]}, {"qid": 2941, "question": "Which real-world datasets are used? in A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "answer": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "top_k_doc_id": [2404, 5115, 5189, 5112, 5113, 5114, 5116, 5117, 6890, 5186, 7515, 3486, 2189, 7514, 6433], "orig_top_k_doc_id": [5112, 5117, 5115, 5113, 5116, 5114, 5186, 7515, 5189, 2404, 3486, 2189, 6890, 7514, 6433]}, {"qid": 2944, "question": "How is the keyword specific expectation elicited from the crowd? in A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "answer": ["workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts"], "top_k_doc_id": [2404, 5115, 5189, 5112, 5113, 5114, 5116, 5117, 6890, 5068, 2403, 2814, 5186, 1300, 5188], "orig_top_k_doc_id": [5112, 5115, 5113, 5117, 5116, 5114, 2404, 6890, 5189, 5068, 2403, 2814, 5186, 1300, 5188]}, {"qid": 1686, "question": "Were any other word similar metrics, besides Jaccard metric, tested? in Event detection in Twitter: A keyword volume approach", "answer": ["Yes"], "top_k_doc_id": [2404, 5115, 5189, 579, 2081, 2403, 4359, 2080, 2408, 7411, 7412, 3065, 5116, 5186, 2406], "orig_top_k_doc_id": [2403, 2080, 2408, 2404, 5115, 2081, 7411, 7412, 5189, 579, 3065, 4359, 5116, 5186, 2406]}, {"qid": 2365, "question": "How do people engage in Twitter threads on different types of news? in Event detection in Colombian security Twitter news using fine-grained latent topic analysis", "answer": ["No"], "top_k_doc_id": [2404, 2079, 2405, 3795, 3796, 3797, 4131, 4359, 6817, 2157, 1174, 7534, 3542, 1172, 7500], "orig_top_k_doc_id": [3795, 3797, 4359, 6817, 2404, 4131, 2157, 1174, 7534, 3542, 2079, 2405, 1172, 7500, 3796]}, {"qid": 2366, "question": "How are the clusters related to security, violence and crime identified? in Event detection in Colombian security Twitter news using fine-grained latent topic analysis", "answer": ["Yes"], "top_k_doc_id": [2404, 2079, 2405, 3795, 3796, 3797, 4131, 4359, 1480, 1957, 5112, 5116, 5113, 1958, 448], "orig_top_k_doc_id": [3795, 3797, 3796, 1480, 4131, 1957, 2404, 5112, 5116, 5113, 4359, 2079, 1958, 2405, 448]}]}
{"group_id": 54, "group_size": 12, "items": [{"qid": 1755, "question": "What dimensions of word embeddings do they produce using factorization? in Word Embeddings via Tensor Factorization", "answer": ["300-dimensional vectors"], "top_k_doc_id": [2523, 2524, 2525, 2527, 2881, 2882, 6579, 1848, 7469, 2357, 3207, 422, 4872, 6580, 5305], "orig_top_k_doc_id": [2524, 2523, 2525, 2527, 3207, 2882, 6579, 1848, 2357, 422, 4872, 2881, 6580, 5305, 7469]}, {"qid": 1757, "question": "Do they measure computation time of their factorizations compared to other word embeddings? in Word Embeddings via Tensor Factorization", "answer": ["Yes"], "top_k_doc_id": [2523, 2524, 2525, 2527, 2881, 2882, 6579, 1848, 7469, 2357, 3207, 6581, 3699, 2526, 904], "orig_top_k_doc_id": [2357, 2524, 2882, 2525, 2523, 2527, 6579, 1848, 6581, 3699, 2526, 2881, 7469, 904, 3207]}, {"qid": 1753, "question": "Do they test their word embeddings on downstream tasks? in Word Embeddings via Tensor Factorization", "answer": ["Yes"], "top_k_doc_id": [2523, 2524, 2525, 2527, 2881, 2882, 6579, 3207, 6246, 6580, 2526, 1872, 7680, 6581, 6250], "orig_top_k_doc_id": [2525, 2523, 2527, 2524, 2526, 3207, 2882, 6580, 6579, 1872, 6246, 2881, 7680, 6581, 6250]}, {"qid": 1754, "question": "What are the main disadvantages of their proposed word embeddings? in Word Embeddings via Tensor Factorization", "answer": ["No"], "top_k_doc_id": [2523, 2524, 2525, 2527, 2881, 2882, 6579, 3207, 6246, 6580, 5307, 2874, 178, 4872, 5305], "orig_top_k_doc_id": [2523, 2524, 2882, 2525, 6579, 2527, 2881, 5307, 6580, 2874, 178, 6246, 3207, 4872, 5305]}, {"qid": 1756, "question": "On which dataset(s) do they compute their word embeddings? in Word Embeddings via Tensor Factorization", "answer": ["10 million sentences gathered from Wikipedia"], "top_k_doc_id": [2523, 2524, 2525, 2527, 2881, 2882, 6579, 1848, 7469, 6580, 2526, 6581, 178, 3018, 2243], "orig_top_k_doc_id": [2524, 2525, 2523, 6579, 6580, 2526, 2882, 2527, 6581, 7469, 178, 3018, 2243, 1848, 2881]}, {"qid": 3143, "question": "How do they evaluate their resulting word embeddings? in Incorporating Subword Information into Matrix Factorization Word Embeddings", "answer": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "top_k_doc_id": [2523, 3696, 5302, 4259, 4555, 4872, 7469, 289, 5306, 203, 4873, 5307, 2525, 3846, 3697], "orig_top_k_doc_id": [5302, 289, 5306, 203, 2523, 4555, 4873, 7469, 5307, 4259, 3696, 4872, 2525, 3846, 3697]}, {"qid": 3145, "question": "Which matrix factorization methods do they use? in Incorporating Subword Information into Matrix Factorization Word Embeddings", "answer": ["weighted factorization of a word-context co-occurrence matrix ", "The LexVec BIBREF7"], "top_k_doc_id": [2523, 3696, 5302, 4259, 4555, 4872, 7469, 7681, 5992, 5991, 7680, 2524, 3207, 1033, 422], "orig_top_k_doc_id": [5302, 2523, 3696, 4872, 7681, 7469, 4555, 5992, 5991, 7680, 2524, 4259, 3207, 1033, 422]}, {"qid": 2778, "question": "What novel PMI variants are introduced? in Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics", "answer": ["clipped PMI; NNEGPMI", "clipped $\\mathit {PMI}$, $\\mathit {NNEGPMI}$"], "top_k_doc_id": [2523, 451, 3341, 4872, 4873, 4874, 452, 763, 1399, 3095, 4463, 5000, 6174, 5898, 2775], "orig_top_k_doc_id": [4872, 4873, 4874, 451, 2523, 3341, 452, 5000, 763, 5898, 1399, 3095, 2775, 4463, 6174]}, {"qid": 2779, "question": "What semantic and syntactic tasks are used as probes? in Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics", "answer": ["Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks", "SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging"], "top_k_doc_id": [2523, 451, 3341, 4872, 4873, 4874, 2103, 4332, 2154, 4328, 6828, 5184, 2704, 3369, 5701], "orig_top_k_doc_id": [4872, 4873, 4874, 451, 3341, 4332, 2523, 2154, 4328, 6828, 2103, 5184, 2704, 3369, 5701]}, {"qid": 2780, "question": "What are the disadvantages to clipping negative PMI? in Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics", "answer": ["It may lead to poor rare word representations and word analogies.", "No"], "top_k_doc_id": [2523, 451, 3341, 4872, 4873, 4874, 452, 763, 1399, 3095, 4463, 5000, 6174, 6173, 3018], "orig_top_k_doc_id": [4872, 4873, 451, 4874, 2523, 452, 6174, 6173, 763, 3341, 1399, 5000, 3095, 3018, 4463]}, {"qid": 2781, "question": "Why are statistics from finite corpora unreliable? in Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics", "answer": ["$\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus", "A finite corpora may entirely omit rare word combinations"], "top_k_doc_id": [2523, 451, 3341, 4872, 4873, 4874, 2103, 6898, 1955, 852, 5675, 6133, 1284, 2104, 954], "orig_top_k_doc_id": [4872, 4873, 3341, 451, 4874, 6898, 2103, 2523, 1955, 852, 5675, 6133, 1284, 2104, 954]}, {"qid": 3142, "question": "For which languages do they build word embeddings for? in Incorporating Subword Information into Matrix Factorization Word Embeddings", "answer": ["No", "English"], "top_k_doc_id": [2523, 3696, 5302, 1041, 7680, 2356, 5306, 2776, 2357, 5836, 5710, 289, 1374, 1370, 2348], "orig_top_k_doc_id": [5302, 1041, 2523, 3696, 7680, 2356, 5306, 2776, 2357, 5836, 5710, 289, 1374, 1370, 2348]}]}
{"group_id": 55, "group_size": 12, "items": [{"qid": 1895, "question": "How many parameters does the presented model have? in End-to-End Streaming Keyword Spotting", "answer": ["(infixes 700K, 318K, and 40K) each representing the number of approximate parameters"], "top_k_doc_id": [4487, 1112, 3401, 1228, 2814, 3402, 3976, 4488, 4489, 1113, 2815, 2816, 2817, 2402, 4122], "orig_top_k_doc_id": [2814, 2816, 4487, 4488, 2815, 2817, 3402, 4489, 1228, 3976, 3401, 1112, 1113, 2402, 4122]}, {"qid": 1896, "question": "How do they measure the quality of detection? in End-to-End Streaming Keyword Spotting", "answer": ["We evaluate the false-reject (FR) and false-accept (FA) tradeoff across several end-to-end models of distinct sizes and computational complexities."], "top_k_doc_id": [4487, 1112, 3401, 1228, 2814, 3402, 3976, 4488, 4489, 1113, 2815, 2816, 2817, 3582, 5189], "orig_top_k_doc_id": [2814, 2816, 4487, 2815, 4488, 2817, 1112, 3402, 1113, 4489, 3582, 1228, 3976, 3401, 5189]}, {"qid": 1897, "question": "What previous approaches are considered? in End-to-End Streaming Keyword Spotting", "answer": ["Our baseline system (Baseline_1850K) is taken from BIBREF13 . "], "top_k_doc_id": [4487, 1112, 3401, 1228, 2814, 3402, 3976, 4488, 4489, 1113, 2815, 2816, 2817, 6491, 6351], "orig_top_k_doc_id": [2814, 4487, 2816, 4488, 1112, 1113, 2815, 3402, 3401, 3976, 2817, 1228, 4489, 6491, 6351]}, {"qid": 947, "question": "What are the baselines? in Prototypical Metric Transfer Learning for Continuous Speech Keyword Spotting With Limited Training Data", "answer": ["Honk, DeepSpeech-finetune"], "top_k_doc_id": [4487, 1112, 3401, 1228, 247, 1113, 1229, 1230, 3571, 2814, 3402, 3976, 5822, 3572, 4620], "orig_top_k_doc_id": [1228, 1229, 3571, 1230, 1113, 3401, 1112, 3402, 4487, 5822, 247, 3572, 2814, 3976, 4620]}, {"qid": 948, "question": "What languages are considered? in Prototypical Metric Transfer Learning for Continuous Speech Keyword Spotting With Limited Training Data", "answer": ["English, Hindi"], "top_k_doc_id": [4487, 1112, 3401, 1228, 247, 1113, 1229, 1230, 3571, 2814, 3402, 3976, 5822, 6491, 3403], "orig_top_k_doc_id": [1228, 3401, 1229, 1113, 1112, 247, 3402, 5822, 4487, 3571, 2814, 1230, 6491, 3976, 3403]}, {"qid": 2433, "question": "Do they compare executionttime of their model against other models? in Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions", "answer": ["No"], "top_k_doc_id": [4487, 1112, 3401, 1228, 2814, 3402, 3976, 4488, 4489, 3422, 3423, 3977, 3978, 2816, 2817], "orig_top_k_doc_id": [3976, 4487, 3977, 2814, 3401, 3423, 4489, 1228, 1112, 3422, 4488, 2816, 3402, 3978, 2817]}, {"qid": 2434, "question": "What is the memory footprint decrease of their model in comparison to other models? in Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions", "answer": ["No"], "top_k_doc_id": [4487, 1112, 3401, 1228, 2814, 3402, 3976, 4488, 4489, 3422, 3423, 3977, 3978, 3090, 7603], "orig_top_k_doc_id": [3976, 4487, 3977, 4489, 2814, 4488, 1228, 3423, 3422, 3090, 3978, 3401, 1112, 3402, 7603]}, {"qid": 2561, "question": "What are dilated convolutions? in Efficient keyword spotting using dilated convolutions and gating", "answer": ["Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale."], "top_k_doc_id": [4487, 1112, 3401, 1228, 2814, 3402, 3976, 4488, 4489, 1113, 2815, 1229, 4081, 2049, 1085], "orig_top_k_doc_id": [4487, 4488, 4489, 3976, 2814, 3402, 3401, 1229, 2815, 4081, 2049, 1085, 1113, 1112, 1228]}, {"qid": 946, "question": "What problem do they apply transfer learning to? in Prototypical Metric Transfer Learning for Continuous Speech Keyword Spotting With Limited Training Data", "answer": ["CSKS task"], "top_k_doc_id": [4487, 1112, 3401, 1228, 247, 1113, 1229, 1230, 3571, 3574, 6943, 226, 3978, 3572, 2528], "orig_top_k_doc_id": [1228, 1229, 247, 3571, 4487, 1113, 1112, 3574, 6943, 3401, 1230, 226, 3978, 3572, 2528]}, {"qid": 2454, "question": "Is model compared against state of the art models on these datasets? in Multi-scale Octave Convolutions for Robust Speech Recognition", "answer": ["Yes"], "top_k_doc_id": [4487, 890, 3976, 4079, 4080, 4081, 381, 3423, 6314, 771, 5481, 2607, 194, 96, 436], "orig_top_k_doc_id": [4079, 4081, 4080, 890, 381, 3423, 6314, 771, 3976, 5481, 2607, 4487, 194, 96, 436]}, {"qid": 2455, "question": "How is octave convolution concept extended to multiple resolutions and octaves? in Multi-scale Octave Convolutions for Robust Speech Recognition", "answer": ["The resolution of the low-frequency feature maps is reduced by an octave \u2013 height and width dimensions are divided by 2. In this work, we explore spatial reduction by up to 3 octaves \u2013 dividing by $2^t$, where $t=1,2,3$ \u2013 and for up to 4 groups. We refer to such a layer as a multi-octave convolutional (MultiOctConv) layer,"], "top_k_doc_id": [4487, 890, 3976, 4079, 4080, 4081, 4811, 1401, 4488, 5003, 7108, 888, 6315, 3977, 5590], "orig_top_k_doc_id": [4079, 4081, 4080, 4487, 3976, 890, 4811, 1401, 4488, 5003, 7108, 888, 6315, 3977, 5590]}, {"qid": 2462, "question": "Do the authors do manual evaluation? in Topic Spotting using Hierarchical Networks with Self Attention", "answer": ["No"], "top_k_doc_id": [4487, 1112, 3401, 4120, 4121, 4122, 6065, 4652, 5896, 653, 1683, 1348, 1113, 5898, 1869], "orig_top_k_doc_id": [4120, 4121, 4122, 6065, 4652, 5896, 653, 1683, 1348, 1113, 1112, 5898, 1869, 3401, 4487]}]}
{"group_id": 56, "group_size": 12, "items": [{"qid": 1903, "question": "What was performance of classifiers before/after using distant supervision? in Distant Supervision and Noisy Label Learning for Low Resource Named Entity Recognition: A Study on Hausa and Yor\\`ub\\'a", "answer": ["Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)\nBERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 2825, 2826, 3006, 3005, 3538, 227, 450, 331, 6821, 2622], "orig_top_k_doc_id": [2825, 2824, 2823, 2826, 4858, 3004, 2477, 3006, 3538, 3005, 227, 331, 6821, 450, 2622]}, {"qid": 1904, "question": "What classifiers were used in experiments? in Distant Supervision and Noisy Label Learning for Low Resource Named Entity Recognition: A Study on Hausa and Yor\\`ub\\'a", "answer": ["Bi-LSTM, BERT"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 2825, 2826, 3006, 3005, 3538, 227, 450, 7100, 4573, 1796], "orig_top_k_doc_id": [2824, 2823, 2825, 2826, 4858, 3004, 2477, 3006, 227, 3005, 7100, 450, 3538, 4573, 1796]}, {"qid": 60, "question": "Did they experiment with the dataset? in Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision", "answer": ["Yes"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 65, 66, 3538, 22, 1242, 3006, 4689, 5652, 6989, 6153], "orig_top_k_doc_id": [65, 3538, 2477, 2823, 66, 2824, 4858, 4689, 5652, 3006, 1242, 6989, 3004, 6153, 22]}, {"qid": 61, "question": "What is the size of this dataset? in Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision", "answer": ["29,500 documents", "29,500 documents in the CORD-19 corpus (2020-03-13)"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 65, 66, 3538, 22, 1242, 3006, 4689, 5652, 4690, 7100], "orig_top_k_doc_id": [65, 3538, 2823, 4689, 66, 4858, 2477, 2824, 4690, 5652, 7100, 3006, 22, 3004, 1242]}, {"qid": 1902, "question": "How much labeled data is available for these two languages? in Distant Supervision and Noisy Label Learning for Low Resource Named Entity Recognition: A Study on Hausa and Yor\\`ub\\'a", "answer": ["10k training and 1k test, 1,101 sentences (26k tokens)"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 2825, 2826, 3006, 3005, 3538, 1796, 3617, 7100, 50, 6046], "orig_top_k_doc_id": [2823, 2825, 2824, 2826, 3004, 2477, 4858, 3006, 3005, 1796, 3617, 7100, 3538, 50, 6046]}, {"qid": 62, "question": "Do they list all the named entity types present? in Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision", "answer": ["No"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 65, 66, 3538, 1422, 7672, 497, 2973, 4573, 7100, 1094], "orig_top_k_doc_id": [65, 4858, 3538, 2477, 2823, 2824, 66, 1422, 7672, 497, 2973, 3004, 4573, 7100, 1094]}, {"qid": 1905, "question": "In which countries are Hausa and Yor\\`ub\\'a spoken? in Distant Supervision and Noisy Label Learning for Low Resource Named Entity Recognition: A Study on Hausa and Yor\\`ub\\'a", "answer": ["Nigeria, Benin, Ghana, Cameroon, Togo, C\u00f4te d'Ivoire, Chad, Burkina Faso, and Sudan, Republic of Togo, Ghana, C\u00f4te d'Ivoire, Sierra Leone, Cuba and Brazil"], "top_k_doc_id": [2477, 2824, 3004, 2823, 4858, 2825, 2826, 3006, 1796, 1797, 6944, 6946, 45, 6945, 6943], "orig_top_k_doc_id": [2823, 2825, 2824, 2826, 1796, 3004, 2477, 1797, 4858, 6944, 6946, 3006, 45, 6945, 6943]}, {"qid": 1990, "question": "What is baseline used? in Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels", "answer": ["Base , Base+Noise, Cleaning , Dynamic-CM ,  Global-CM,  Global-ID-CM, Brown-CM ,  K-Means-CM"], "top_k_doc_id": [2477, 2824, 3004, 1781, 1782, 2482, 2825, 3005, 3006, 50, 186, 6504, 4584, 2481, 430], "orig_top_k_doc_id": [3004, 3005, 3006, 2824, 2825, 2477, 1782, 6504, 4584, 1781, 2482, 186, 2481, 430, 50]}, {"qid": 1992, "question": "How they evaluate their approach? in Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels", "answer": ["They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise"], "top_k_doc_id": [2477, 2824, 3004, 1781, 1782, 2482, 2825, 3005, 3006, 50, 186, 6504, 227, 4858, 7113], "orig_top_k_doc_id": [3004, 3005, 3006, 2824, 2825, 2477, 2482, 186, 1781, 1782, 50, 227, 6504, 4858, 7113]}, {"qid": 1991, "question": "Did they evaluate against baseline? in Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels", "answer": ["Yes"], "top_k_doc_id": [2477, 2824, 3004, 1781, 1782, 2482, 2825, 3005, 3006, 99, 5388, 192, 6299, 2481, 430], "orig_top_k_doc_id": [3004, 3005, 3006, 2824, 2477, 2825, 99, 2482, 1782, 5388, 192, 6299, 2481, 430, 1781]}, {"qid": 2362, "question": "What is the new initialization method proposed in this paper? in Named Entity Disambiguation for Noisy Text", "answer": ["They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data."], "top_k_doc_id": [2477, 929, 3784, 7597, 931, 3207, 349, 1262, 4489, 6951, 6810, 5775, 2318, 4158, 215], "orig_top_k_doc_id": [929, 2477, 3784, 931, 3207, 349, 1262, 4489, 6951, 6810, 5775, 2318, 7597, 4158, 215]}, {"qid": 2363, "question": "How was a quality control performed so that the text is noisy but the annotations are accurate? in Named Entity Disambiguation for Noisy Text", "answer": ["The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia."], "top_k_doc_id": [2477, 929, 3784, 7597, 4476, 5912, 2824, 326, 3004, 6736, 47, 209, 1240, 4573, 2015], "orig_top_k_doc_id": [2477, 3784, 4476, 5912, 2824, 929, 326, 3004, 6736, 47, 209, 1240, 4573, 7597, 2015]}]}
{"group_id": 57, "group_size": 12, "items": [{"qid": 1948, "question": "How big is improvement over the old  state-of-the-art performance on CoNLL-2009 dataset? in Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "answer": ["our Open model achieves more than 3 points of f1-score than the state-of-the-art result"], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 3039, 3041, 3150, 2919, 2921, 6331, 3044, 3151, 5359], "orig_top_k_doc_id": [3043, 2917, 3147, 3042, 3041, 2921, 2919, 863, 3150, 3039, 3044, 5359, 6331, 3148, 3151]}, {"qid": 1949, "question": "What is new state-of-the-art performance on CoNLL-2009 dataset? in Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "answer": ["In closed setting 84.22 F1 and in open 87.35 F1."], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 3039, 3041, 3150, 2919, 2921, 6331, 3044, 3151, 6768], "orig_top_k_doc_id": [3043, 2917, 3147, 3042, 3041, 3150, 863, 3039, 2919, 2921, 3151, 3044, 6331, 6768, 3148]}, {"qid": 1950, "question": "How big is CoNLL-2009 dataset? in Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "answer": ["No"], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 3039, 3041, 3150, 2919, 2921, 6331, 5359, 3149, 5183], "orig_top_k_doc_id": [3043, 2917, 3147, 3042, 3039, 3041, 6331, 5359, 2919, 2921, 3150, 3149, 5183, 863, 3148]}, {"qid": 2010, "question": "Are there syntax-agnostic SRL models before? in A Full End-to-End Semantic Role Labeler, Syntax-agnostic Over Syntax-aware?", "answer": ["Yes"], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 3039, 3041, 3150, 864, 3044, 861, 865, 5185, 2921], "orig_top_k_doc_id": [3043, 2917, 3147, 3042, 864, 3044, 863, 3148, 3041, 3039, 861, 3150, 865, 5185, 2921]}, {"qid": 2011, "question": "What is the biaffine scorer? in A Full End-to-End Semantic Role Labeler, Syntax-agnostic Over Syntax-aware?", "answer": ["biaffine attention BIBREF14"], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 3039, 3041, 3150, 864, 3044, 3151, 3149, 3040, 3256], "orig_top_k_doc_id": [3042, 3148, 3043, 3041, 3039, 3150, 3147, 2917, 864, 3151, 3044, 863, 3149, 3040, 3256]}, {"qid": 1176, "question": "What are the datasets used for the task? in Language-Agnostic Syllabification with Neural Sequence Labeling", "answer": ["Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)"], "top_k_doc_id": [3043, 1577, 1578, 1579, 1580, 1581, 3147, 2917, 3530, 3039, 2590, 2258, 1345, 783, 3041], "orig_top_k_doc_id": [1577, 1578, 1581, 1580, 1579, 2917, 3043, 3530, 3147, 3039, 2590, 2258, 1345, 783, 3041]}, {"qid": 1178, "question": "Which models achieve state-of-the-art performances? in Language-Agnostic Syllabification with Neural Sequence Labeling", "answer": ["CELEX (Dutch and English) - SVM-HMM\nFestival, E-Hitz and OpenLexique - Liang hyphenation\nIIT-Guwahat - Entropy CRF"], "top_k_doc_id": [3043, 1577, 1578, 1579, 1580, 1581, 3147, 2917, 863, 4583, 861, 4584, 864, 1883, 439], "orig_top_k_doc_id": [1577, 1578, 1581, 1580, 863, 1579, 3043, 3147, 4583, 861, 4584, 864, 1883, 2917, 439]}, {"qid": 1951, "question": "What different approaches of encoding syntactic information authors present? in Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "answer": ["dependency head and dependency relation label, denoted as Dep and Rel for short, Tree-based Position Feature (TPF) as Dependency Path (DepPath), Shortest Dependency Path (SDP) as Relation Path (RelPath)"], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 1560, 2705, 2921, 5183, 3039, 2920, 2253, 3044, 1561], "orig_top_k_doc_id": [2917, 2921, 3147, 3043, 3148, 5183, 2705, 3042, 863, 3039, 2920, 1560, 2253, 3044, 1561]}, {"qid": 1952, "question": "What are two strong baseline methods authors refer to? in Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "answer": ["Marcheggiani and Titov (2017) and Cai et al. (2018)"], "top_k_doc_id": [3043, 2917, 3147, 3148, 863, 3042, 1560, 2705, 2921, 5183, 3041, 4297, 1181, 527, 529], "orig_top_k_doc_id": [3043, 2917, 2921, 3147, 3041, 3148, 863, 4297, 3042, 1560, 1181, 2705, 5183, 527, 529]}, {"qid": 1179, "question": "Is the LSTM bidirectional? in Language-Agnostic Syllabification with Neural Sequence Labeling", "answer": ["Yes"], "top_k_doc_id": [3043, 1577, 1578, 1579, 1580, 1581, 3147, 1345, 3148, 2238, 2177, 5225, 4944, 6119, 5103], "orig_top_k_doc_id": [1578, 1577, 1581, 1580, 1579, 1345, 3148, 3147, 2238, 3043, 2177, 5225, 4944, 6119, 5103]}, {"qid": 1177, "question": "What is the accuracy of the model for the six languages tested? in Language-Agnostic Syllabification with Neural Sequence Labeling", "answer": ["Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)"], "top_k_doc_id": [3043, 1577, 1578, 1579, 1580, 1581, 4332, 6037, 2590, 2787, 2796, 2917, 2789, 7044, 3245], "orig_top_k_doc_id": [1577, 1578, 1581, 1580, 1579, 4332, 6037, 2590, 2787, 2796, 2917, 2789, 7044, 3245, 3043]}, {"qid": 2072, "question": "what were the baselines? in Dependency or Span, End-to-End Uniform Semantic Role Labeling", "answer": ["2008 Punyakanok et al. \n2009 Zhao et al. + ME \n2008 Toutanova et al. \n2010 Bjorkelund et al.  \n2015 FitzGerald et al. \n2015 Zhou and Xu \n2016 Roth and Lapata \n2017 He et al. \n2017 Marcheggiani et al.\n2017 Marcheggiani and Titov \n2018 Tan et al. \n2018 He et al. \n2018 Strubell et al. \n2018 Cai et al. \n2018 He et al. \n2018 Li et al. \n"], "top_k_doc_id": [3043, 2917, 3147, 3148, 3039, 3151, 3149, 3150, 5816, 19, 7553, 4489, 3121, 5132, 6331], "orig_top_k_doc_id": [3147, 3148, 3039, 3043, 3151, 2917, 3149, 3150, 5816, 19, 7553, 4489, 3121, 5132, 6331]}]}
{"group_id": 58, "group_size": 12, "items": [{"qid": 1953, "question": "How many category tags are considered? in Multi-modal Dense Video Captioning", "answer": ["14 categories"], "top_k_doc_id": [2927, 2938, 2413, 2414, 2922, 2923, 2926, 2939, 3426, 4267, 2998, 7144, 2925, 6995, 1945], "orig_top_k_doc_id": [2927, 2923, 2922, 2938, 2926, 2413, 2939, 2414, 2998, 4267, 6995, 7144, 3426, 1945, 2925]}, {"qid": 1954, "question": "What domain does the dataset fall into? in Multi-modal Dense Video Captioning", "answer": ["YouTube videos"], "top_k_doc_id": [2927, 2938, 2413, 2414, 2922, 2923, 2926, 2939, 3426, 4267, 2998, 7144, 2925, 6995, 575], "orig_top_k_doc_id": [2923, 2927, 2939, 2938, 2413, 2922, 2926, 2414, 4267, 6995, 3426, 2998, 7144, 575, 2925]}, {"qid": 1956, "question": "What is the state of the art? in Multi-modal Dense Video Captioning", "answer": ["No"], "top_k_doc_id": [2927, 2938, 2413, 2414, 2922, 2923, 2926, 2939, 3426, 4267, 2998, 7144, 2418, 3027, 3115], "orig_top_k_doc_id": [2927, 2923, 2922, 2414, 2413, 2939, 2938, 2926, 3426, 4267, 7144, 2418, 3027, 2998, 3115]}, {"qid": 481, "question": "At which interval do they extract video and audio frames? in From FiLM to Video: Multi-turn Question Answering with Multi-modal Context", "answer": ["No"], "top_k_doc_id": [2927, 2938, 2413, 575, 576, 577, 578, 2923, 2939, 2928, 7557, 2804, 2926, 974, 5666], "orig_top_k_doc_id": [575, 2927, 2938, 578, 576, 2923, 577, 2928, 7557, 2939, 2804, 2413, 2926, 974, 5666]}, {"qid": 483, "question": "Do they train a different training method except from scheduled sampling? in From FiLM to Video: Multi-turn Question Answering with Multi-modal Context", "answer": ["Answer with content missing: (list missing) \nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\n\nYes."], "top_k_doc_id": [2927, 2938, 2413, 575, 576, 577, 578, 2923, 2939, 2998, 2214, 2414, 2213, 1608, 5119], "orig_top_k_doc_id": [578, 575, 2938, 2927, 2413, 2939, 2998, 577, 2214, 2414, 576, 2213, 1608, 2923, 5119]}, {"qid": 1955, "question": "What ASR system do they use? in Multi-modal Dense Video Captioning", "answer": ["YouTube ASR system "], "top_k_doc_id": [2927, 2938, 2413, 2414, 2922, 2923, 2926, 2939, 3426, 4267, 2998, 1297, 575, 1784, 4367], "orig_top_k_doc_id": [2923, 2927, 2922, 2926, 2938, 2939, 2413, 2414, 4267, 3426, 2998, 1297, 575, 1784, 4367]}, {"qid": 482, "question": "Do they use pretrained word vectors for dialogue context embedding? in From FiLM to Video: Multi-turn Question Answering with Multi-modal Context", "answer": ["Yes"], "top_k_doc_id": [2927, 2938, 2413, 575, 576, 577, 578, 7541, 2228, 7543, 7542, 7544, 2414, 2227, 2234], "orig_top_k_doc_id": [575, 578, 577, 7541, 2938, 2413, 2228, 576, 7543, 7542, 7544, 2414, 2227, 2234, 2927]}, {"qid": 1962, "question": "How big is the dataset used? in VATEX Captioning Challenge 2019: Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning", "answer": ["over 41,250 videos and 825,000 captions in both English and Chinese., over 206,000 English-Chinese parallel translation pairs"], "top_k_doc_id": [2927, 2938, 2413, 2414, 2922, 2923, 2926, 2939, 3426, 4267, 3031, 7144, 7141, 7138, 3026], "orig_top_k_doc_id": [2938, 2939, 2927, 2923, 3031, 4267, 2413, 2414, 2926, 7144, 2922, 7141, 7138, 3026, 3426]}, {"qid": 4452, "question": "What domain do the audio samples fall under? in Clotho: An Audio Captioning Dataset", "answer": ["\u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d", "from the online platform Freesound BIBREF8"], "top_k_doc_id": [2927, 2938, 2922, 2923, 2939, 6995, 6996, 6997, 6998, 595, 620, 78, 5861, 6314, 6999], "orig_top_k_doc_id": [6995, 6998, 6997, 2922, 6996, 2939, 2923, 595, 2938, 2927, 620, 78, 5861, 6314, 6999]}, {"qid": 4453, "question": "How did they evaluate the quality of annotations? in Clotho: An Audio Captioning Dataset", "answer": ["They manually check the captions and employ extra annotators to further revise the annotations.", "different annotators are instructed to correct any grammatical errors, score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4, top $N_{\\text{cp}}$ captions are selected"], "top_k_doc_id": [2927, 2938, 2922, 2923, 2939, 6995, 6996, 6997, 6998, 1834, 2900, 4875, 7085, 1837, 1836], "orig_top_k_doc_id": [6995, 6998, 6997, 2927, 2938, 2900, 2923, 1834, 4875, 7085, 1837, 1836, 6996, 2922, 2939]}, {"qid": 4454, "question": "How many annotators did they have? in Clotho: An Audio Captioning Dataset", "answer": ["No", "No"], "top_k_doc_id": [2927, 2938, 2922, 2923, 2939, 6995, 6996, 6997, 6998, 1834, 2900, 7356, 7355, 6111, 2899], "orig_top_k_doc_id": [6995, 6998, 6997, 6996, 2938, 2923, 2900, 2922, 7356, 7355, 6111, 2939, 2927, 1834, 2899]}, {"qid": 4455, "question": "What is their baseline method? in Clotho: An Audio Captioning Dataset", "answer": ["previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention", "we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention"], "top_k_doc_id": [2927, 2938, 2922, 2923, 2939, 6995, 6996, 6997, 6998, 2926, 3799, 7580, 3031, 3092, 7087], "orig_top_k_doc_id": [6998, 6995, 6997, 2927, 2922, 2938, 2923, 2939, 2926, 6996, 3799, 7580, 3031, 3092, 7087]}]}
{"group_id": 59, "group_size": 12, "items": [{"qid": 2121, "question": "How is human evaluation performed, what were the criteria? in A Discrete CVAE for Response Generation on Short-Text Conversation", "answer": ["(1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting, (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic, (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query"], "top_k_doc_id": [3236, 1768, 1771, 3235, 3237, 3238, 3239, 3240, 1804, 1806, 2968, 7569, 2969, 2970, 7570], "orig_top_k_doc_id": [3235, 3236, 3240, 3239, 1768, 2969, 3238, 2970, 1771, 3237, 2968, 7569, 1806, 7570, 1804]}, {"qid": 2123, "question": "What other kinds of generation models are used in experiments? in A Discrete CVAE for Response Generation on Short-Text Conversation", "answer": [" Seq2seq, CVAE, Hierarchical Gated Fusion Unit (HGFU), Mechanism-Aware Neural Machine (MANM)"], "top_k_doc_id": [3236, 1768, 1771, 3235, 3237, 3238, 3239, 3240, 1804, 1806, 2968, 7569, 7566, 3359, 1769], "orig_top_k_doc_id": [3235, 3236, 3240, 3239, 1768, 3238, 3237, 1771, 2968, 1804, 7566, 3359, 1806, 1769, 7569]}, {"qid": 2122, "question": "What automatic metrics are used? in A Discrete CVAE for Response Generation on Short-Text Conversation", "answer": ["BLEU, Distinct-1 & distinct-2"], "top_k_doc_id": [3236, 1768, 1771, 3235, 3237, 3238, 3239, 3240, 1804, 1806, 2968, 4444, 5920, 2969, 1669], "orig_top_k_doc_id": [3239, 3235, 3236, 3240, 3238, 1768, 3237, 1771, 1806, 4444, 5920, 2969, 1669, 1804, 2968]}, {"qid": 1288, "question": "What approach performs better in experiments global latent or sequence of fine-grained latent variables? in Variational Transformers for Diverse Response Generation", "answer": ["PPL: SVT\nDiversity: GVT\nEmbeddings Similarity: SVT\nHuman Evaluation: SVT"], "top_k_doc_id": [3236, 1768, 1771, 3235, 1772, 7371, 7372, 1769, 1770, 1232, 3240, 5217, 3239, 5218, 2065], "orig_top_k_doc_id": [1768, 1771, 1772, 1769, 3236, 7372, 1770, 3235, 7371, 1232, 3240, 5217, 3239, 5218, 2065]}, {"qid": 1290, "question": "What three conversational datasets are used for evaluation? in Variational Transformers for Diverse Response Generation", "answer": ["MojiTalk , PersonaChat , Empathetic-Dialogues"], "top_k_doc_id": [3236, 1768, 1771, 3235, 1772, 7371, 7372, 3359, 1807, 1804, 1673, 4444, 495, 2968, 4414], "orig_top_k_doc_id": [1768, 7371, 3359, 3235, 1771, 7372, 1807, 3236, 1804, 1772, 1673, 4444, 495, 2968, 4414]}, {"qid": 1333, "question": "What different properties of the posterior distribution are explored in the paper? in On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation", "answer": ["interdependence between rate and distortion, impact of KL on the sharpness of the approximated posteriors, demonstrate how certain generative behaviours could be imposed on VAEs via a range of maximum channel capacities, some experiments to find if any form of syntactic information is encoded in the latent space"], "top_k_doc_id": [3236, 3970, 1768, 1770, 1771, 1828, 6427, 1769, 3739, 4194, 6959, 6960, 6428, 5691, 4132], "orig_top_k_doc_id": [1828, 6427, 1768, 1769, 3970, 6959, 3236, 1770, 6960, 1771, 6428, 5691, 3739, 4132, 4194]}, {"qid": 1334, "question": "Why does proposed term help to avoid posterior collapse? in On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation", "answer": ["by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$)"], "top_k_doc_id": [3236, 3970, 1768, 1770, 1771, 1828, 6427, 1769, 3739, 4194, 6959, 6960, 1052, 1829, 4195], "orig_top_k_doc_id": [1828, 3739, 6427, 3970, 1768, 4194, 6960, 1770, 1771, 1769, 6959, 1052, 3236, 1829, 4195]}, {"qid": 2124, "question": "How does discrete latent variable has an explicit semantic meaning to improve the CVAE on short-text conversation? in A Discrete CVAE for Response Generation on Short-Text Conversation", "answer": ["we connect each latent variable with a word in the vocabulary, thus each latent variable has an exact semantic meaning."], "top_k_doc_id": [3236, 1768, 1771, 3235, 3237, 3238, 3239, 3240, 1769, 3012, 1772, 2253, 1157, 1156, 1158], "orig_top_k_doc_id": [3235, 3236, 3240, 3239, 1768, 3237, 3238, 1769, 3012, 1771, 1772, 2253, 1157, 1156, 1158]}, {"qid": 1335, "question": "How does explicit constraint on the KL divergence term that authors propose looks like? in On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation", "answer": ["Answer with content missing: (Formula 2) Formula 2 is an answer: \n\\big \\langle\\! \\log p_\\theta({x}|{z}) \\big \\rangle_{q_\\phi({z}|{x})}  -  \\beta |D_{KL}\\big(q_\\phi({z}|{x}) || p({z})\\big)-C|"], "top_k_doc_id": [3236, 3970, 1768, 1770, 1771, 1828, 6427, 7524, 1052, 1829, 1832, 4195, 3969, 5763, 2998], "orig_top_k_doc_id": [1828, 6427, 7524, 1052, 1771, 3236, 3970, 1829, 1832, 1770, 1768, 4195, 3969, 5763, 2998]}, {"qid": 3473, "question": "What are the qualitative experiments performed on benchmark datasets? in Learning Multi-Sense Word Distributions using Approximate Kullback-Leibler Divergence", "answer": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "top_k_doc_id": [3236, 3970, 1052, 1603, 1604, 1769, 3065, 3969, 4132, 5761, 5762, 5763, 6428, 4369, 4240], "orig_top_k_doc_id": [3970, 4132, 5763, 1052, 1603, 5761, 3969, 3236, 1769, 1604, 5762, 6428, 3065, 4369, 4240]}, {"qid": 3474, "question": "How does this approach compare to other WSD approaches employing word embeddings? in Learning Multi-Sense Word Distributions using Approximate Kullback-Leibler Divergence", "answer": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "top_k_doc_id": [3236, 3970, 1052, 1603, 1604, 1769, 3065, 3969, 4132, 5761, 5762, 5763, 7524, 2241, 68], "orig_top_k_doc_id": [1603, 1052, 3970, 4132, 5761, 1604, 3236, 3065, 3969, 7524, 2241, 5762, 5763, 68, 1769]}, {"qid": 1289, "question": "What baselines other than standard transformers are used in experiments? in Variational Transformers for Diverse Response Generation", "answer": ["attention-based sequence-to-sequence model , CVAE"], "top_k_doc_id": [3236, 1768, 1771, 1772, 3359, 2414, 2205, 4412, 4415, 4414, 4413, 1232, 1472, 3358, 5445], "orig_top_k_doc_id": [1768, 1771, 1772, 3359, 2414, 3236, 2205, 4412, 4415, 4414, 4413, 1232, 1472, 3358, 5445]}]}
{"group_id": 60, "group_size": 12, "items": [{"qid": 2915, "question": "What transfer learning tasks are evaluated? in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "answer": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "top_k_doc_id": [5097, 5099, 5100, 5098, 5101, 7472, 3743, 6448, 3273, 2308, 5678, 5679, 2307, 4833, 488], "orig_top_k_doc_id": [5097, 5100, 5098, 6448, 5099, 5101, 3273, 2308, 7472, 5678, 3743, 5679, 2307, 4833, 488]}, {"qid": 2920, "question": "What other sentence embeddings methods are evaluated? in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "answer": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "top_k_doc_id": [5097, 5099, 5100, 5098, 5101, 7472, 3743, 6448, 6449, 5328, 6450, 533, 3499, 5830, 3178], "orig_top_k_doc_id": [5097, 5098, 5101, 5100, 5099, 6448, 6449, 5328, 3743, 7472, 6450, 533, 3499, 5830, 3178]}, {"qid": 2917, "question": "How much time takes its training? in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "answer": ["20 minutes"], "top_k_doc_id": [5097, 5099, 5100, 5098, 5101, 1518, 3273, 3275, 7472, 4833, 2903, 5828, 6839, 7630, 3274], "orig_top_k_doc_id": [5097, 5098, 5101, 5099, 3273, 5100, 7472, 4833, 2903, 5828, 6839, 1518, 7630, 3274, 3275]}, {"qid": 2918, "question": "How many GPUs are used for the training of SBERT? in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "answer": ["No"], "top_k_doc_id": [5097, 5099, 5100, 5098, 5101, 7472, 1798, 1800, 534, 2050, 5679, 7002, 6135, 5543, 6367], "orig_top_k_doc_id": [5101, 5097, 5098, 5099, 5100, 1798, 1800, 534, 2050, 5679, 7002, 6135, 5543, 6367, 7472]}, {"qid": 2919, "question": "How are the siamese networks trained? in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "answer": ["No", "update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function"], "top_k_doc_id": [5097, 5099, 5100, 5098, 5101, 1518, 3273, 3275, 1179, 3648, 1178, 1177, 686, 1519, 3701], "orig_top_k_doc_id": [5097, 5098, 5101, 1179, 3273, 3648, 1518, 5099, 5100, 1178, 3275, 1177, 686, 1519, 3701]}, {"qid": 2865, "question": "Do they train their model starting from a checkpoint? in Symmetric Regularization based BERT for Pair-wise Semantic Reasoning", "answer": ["No", "No"], "top_k_doc_id": [5097, 5019, 5020, 5021, 5023, 5098, 3855, 7830, 7651, 2439, 5073, 2191, 2746, 2711, 4216], "orig_top_k_doc_id": [5019, 5023, 5021, 7651, 7830, 5097, 5020, 2439, 5073, 3855, 5098, 2191, 2746, 2711, 4216]}, {"qid": 2866, "question": "What BERT model do they test? in Symmetric Regularization based BERT for Pair-wise Semantic Reasoning", "answer": ["BERTbase", "BERTbase"], "top_k_doc_id": [5097, 5019, 5020, 5021, 5023, 5098, 3855, 7830, 366, 367, 4277, 2749, 5711, 1145, 1819], "orig_top_k_doc_id": [5019, 5023, 5021, 5020, 5097, 3855, 366, 7830, 367, 4277, 5098, 2749, 5711, 1145, 1819]}, {"qid": 2916, "question": "What metrics are used for the STS tasks? in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "answer": [" Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels", "Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels"], "top_k_doc_id": [5097, 5099, 5100, 5098, 5101, 2899, 2677, 2676, 2903, 2900, 5021, 1563, 3703, 873, 2902], "orig_top_k_doc_id": [5097, 5098, 5101, 5100, 5099, 2899, 2677, 2676, 2903, 2900, 5021, 1563, 3703, 873, 2902]}, {"qid": 3993, "question": "Which NLI data was used to improve the quality of the embeddings? in Universal Text Representation from BERT: An Empirical Study", "answer": ["MNLI BIBREF11, SNLI", "MNLI, SNLI", "Two natural language inference datasets, MNLI BIBREF11 and SNLI"], "top_k_doc_id": [5097, 5099, 5100, 1798, 5019, 6448, 6450, 5098, 1874, 6135, 7005, 2679, 945, 2746, 7137], "orig_top_k_doc_id": [5100, 5098, 6448, 5099, 5019, 5097, 1874, 6135, 7005, 6450, 1798, 2679, 945, 2746, 7137]}, {"qid": 3995, "question": "Which two tasks from SentEval are the sentence embeddings evaluated against? in Universal Text Representation from BERT: An Empirical Study", "answer": ["Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity, probe sentence-level linguistic phenomena", "No"], "top_k_doc_id": [5097, 5099, 5100, 1798, 5019, 6448, 6450, 6449, 290, 3116, 293, 453, 1773, 5101, 2908], "orig_top_k_doc_id": [6448, 5100, 6450, 6449, 5097, 290, 5099, 3116, 293, 5019, 453, 1773, 1798, 5101, 2908]}, {"qid": 2864, "question": "How much is performance improved on NLI? in Symmetric Regularization based BERT for Pair-wise Semantic Reasoning", "answer": [" improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase", "The average score improved by 1.4 points over the previous best result."], "top_k_doc_id": [5097, 5019, 5020, 5021, 5023, 5098, 1145, 2746, 4987, 5586, 5022, 1144, 586, 5099, 587], "orig_top_k_doc_id": [5019, 5021, 5023, 5020, 5097, 5098, 1145, 2746, 4987, 5586, 5022, 1144, 586, 5099, 587]}, {"qid": 2297, "question": "What is the metric that is measures in this paper? in Sampling strategies in Siamese Networks for unsupervised speech representation learning", "answer": ["error rate in a minimal pair ABX discrimination task"], "top_k_doc_id": [5097, 3648, 5098, 3651, 3650, 5217, 5737, 3649, 686, 1061, 6532, 6484, 1178, 2473, 4071], "orig_top_k_doc_id": [3648, 5098, 3651, 3650, 5217, 5737, 3649, 686, 1061, 6532, 5097, 6484, 1178, 2473, 4071]}]}
{"group_id": 61, "group_size": 12, "items": [{"qid": 2935, "question": "How was the spatial aspect of the EEG signal computed? in Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "answer": ["we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.", "They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect."], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 5988, 5989, 5990, 5453, 4878, 5452, 4877, 5900, 5454, 7404], "orig_top_k_doc_id": [5109, 5110, 5111, 5989, 5987, 5988, 5450, 5453, 5452, 5990, 4877, 4878, 5900, 7404, 5454]}, {"qid": 2937, "question": "How many electrodes were used on the subject in EEG sessions? in Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "answer": ["1913 signals", "No"], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 5988, 5989, 5990, 5453, 4878, 5452, 4877, 5900, 5454, 5451], "orig_top_k_doc_id": [5109, 5110, 5111, 5450, 5988, 5452, 5453, 5989, 5987, 5990, 5451, 4877, 4878, 5900, 5454]}, {"qid": 2933, "question": "How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech? in Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "answer": ["we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 ."], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 5988, 5989, 5990, 5453, 4878, 5452, 4877, 5900, 596, 5451], "orig_top_k_doc_id": [5109, 5111, 5110, 5450, 5987, 5900, 5989, 5988, 5990, 4878, 5453, 5452, 596, 4877, 5451]}, {"qid": 2938, "question": "How many subjects does the EEG data come from? in Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "answer": ["14", "14 participants"], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 5988, 5989, 5990, 5453, 4878, 5452, 5451, 5454, 6140, 670], "orig_top_k_doc_id": [5109, 5111, 5110, 5450, 5987, 5988, 5989, 5990, 5453, 5452, 4878, 5451, 5454, 6140, 670]}, {"qid": 2936, "question": "What data was presented to the subjects to elicit event-related responses? in Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "answer": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 5988, 5989, 5990, 5453, 4877, 6140, 6142, 3521, 6755, 1958], "orig_top_k_doc_id": [5109, 5110, 5111, 5450, 4877, 5987, 5453, 6140, 5988, 5989, 6142, 3521, 5990, 6755, 1958]}, {"qid": 3649, "question": "what eeg features were used? in Advancing Speech Recognition With No Speech Or With Noisy Speech", "answer": ["We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0,  So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel, We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. ", "root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy", "root mean square, zero crossing rate, moving window average, kurtosis, power spectral entropy", "root mean square, zero crossing rate, moving window average, kurtosis, power spectral entropy, extracted 31(channels) X 5 or 155 features"], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 5988, 5989, 5990, 7534, 6314, 416, 2292, 381, 1336, 1228], "orig_top_k_doc_id": [5987, 5990, 5989, 5109, 5988, 5111, 7534, 5110, 6314, 416, 2292, 381, 5450, 1336, 1228]}, {"qid": 2785, "question": "Which two pairs of ERPs from the literature benefit from joint training? in Understanding language-elicited EEG data by predicting it from a fine-tuned language model", "answer": ["Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.\nSelect:\n- ELAN, LAN\n- PNP ERP", "No"], "top_k_doc_id": [5450, 5987, 4878, 5989, 5990, 6135, 4877, 2125, 5111, 397, 2233, 2712, 2268, 6484, 3069], "orig_top_k_doc_id": [4878, 4877, 2125, 5450, 5990, 5989, 5111, 5987, 397, 6135, 2233, 2712, 2268, 6484, 3069]}, {"qid": 2786, "question": "What datasets are used? in Understanding language-elicited EEG data by predicting it from a fine-tuned language model", "answer": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "top_k_doc_id": [5450, 5987, 4878, 5989, 5990, 6135, 1560, 5540, 875, 1901, 4414, 4901, 5109, 6137, 4752], "orig_top_k_doc_id": [4878, 5450, 5990, 5989, 5987, 1560, 5540, 875, 1901, 4414, 4901, 5109, 6135, 6137, 4752]}, {"qid": 2934, "question": "What are the five different binary classification tasks? in Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "answer": [" presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels"], "top_k_doc_id": [5450, 5987, 5109, 5110, 5111, 1610, 6140, 1609, 6755, 5453, 5452, 6142, 2140, 1743, 7769], "orig_top_k_doc_id": [5109, 5111, 5110, 5987, 5450, 1610, 6140, 1609, 6755, 5453, 5452, 6142, 2140, 1743, 7769]}, {"qid": 3246, "question": "What is a normal reading paradigm? in ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation", "answer": ["read the sentences normally without any special instructions", "participants were instructed to read the sentences naturally, without any specific task other than comprehension"], "top_k_doc_id": [5450, 2234, 2664, 5451, 5452, 5453, 2232, 4075, 5454, 2442, 490, 3683, 2446, 2445, 7572], "orig_top_k_doc_id": [5450, 5451, 5453, 5452, 2234, 2664, 5454, 2442, 490, 3683, 2446, 4075, 2232, 2445, 7572]}, {"qid": 3247, "question": "Did they experiment with this new dataset? in ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation", "answer": ["No"], "top_k_doc_id": [5450, 2234, 2664, 5451, 5452, 5453, 2232, 4075, 1655, 1217, 2235, 4074, 2231, 5236, 2233], "orig_top_k_doc_id": [5450, 5451, 5453, 5452, 2234, 2664, 2232, 1655, 1217, 2235, 4074, 4075, 2231, 5236, 2233]}, {"qid": 3248, "question": "What kind of sentences were read? in ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation", "answer": ["sentences that were selected from the Wikipedia corpus provided by culotta2006integrating", "seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer"], "top_k_doc_id": [5450, 2234, 2664, 5451, 5452, 5453, 7324, 2398, 4518, 2663, 5900, 6110, 5901, 5019, 4574], "orig_top_k_doc_id": [5450, 5451, 5453, 5452, 7324, 2234, 2398, 4518, 2663, 5900, 6110, 5901, 2664, 5019, 4574]}]}
{"group_id": 62, "group_size": 12, "items": [{"qid": 3196, "question": "what dataset was used? in Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric", "answer": ["48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months", " E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords."], "top_k_doc_id": [5364, 5362, 5363, 5365, 3078, 4968, 6304, 5390, 3208, 4441, 290, 6339, 7441, 781, 6638], "orig_top_k_doc_id": [5362, 5365, 5363, 5364, 6304, 3208, 290, 6339, 7441, 781, 5390, 4968, 3078, 4441, 6638]}, {"qid": 3197, "question": "what algorithms did they use? in Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric", "answer": ["popularity-based, similarity-based, hybrid"], "top_k_doc_id": [5364, 5362, 5363, 5365, 3078, 4968, 6304, 5390, 3208, 4441, 7517, 7097, 1250, 7315, 6590], "orig_top_k_doc_id": [5362, 5365, 5363, 5364, 3208, 7517, 7097, 6304, 4441, 5390, 3078, 1250, 4968, 7315, 6590]}, {"qid": 2921, "question": "What is the average length of the title text? in Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "answer": ["No"], "top_k_doc_id": [5364, 1517, 5102, 5104, 7764, 2160, 5103, 406, 5362, 2040, 5363, 2044, 123, 126, 6555], "orig_top_k_doc_id": [5102, 5104, 1517, 7764, 2160, 5364, 2040, 5103, 5362, 2044, 123, 126, 406, 5363, 6555]}, {"qid": 2923, "question": "What evaluation metrics are used? in Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "answer": ["standard accuracy metric", "accuracy"], "top_k_doc_id": [5364, 1517, 5102, 5104, 7764, 2160, 5103, 406, 5362, 2040, 5363, 491, 7765, 965, 1878], "orig_top_k_doc_id": [5102, 5104, 1517, 7764, 5362, 5364, 491, 5103, 2040, 2160, 7765, 965, 1878, 406, 5363]}, {"qid": 3193, "question": "which algorithm was the highest performer? in Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric", "answer": ["A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach"], "top_k_doc_id": [5364, 5362, 5363, 5365, 3078, 4968, 6304, 5390, 2526, 3872, 7517, 6638, 5067, 5925, 2527], "orig_top_k_doc_id": [5362, 5363, 5365, 5364, 2526, 6304, 4968, 3872, 3078, 7517, 6638, 5390, 5067, 5925, 2527]}, {"qid": 2922, "question": "Which pretrained word vectors did they use? in Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "answer": [" pre-trained GloVe word vectors ", "GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)"], "top_k_doc_id": [5364, 1517, 5102, 5104, 7764, 2160, 5103, 406, 5362, 1743, 7765, 1182, 714, 6555, 3244], "orig_top_k_doc_id": [5102, 5104, 1517, 7764, 5103, 1743, 7765, 5364, 2160, 406, 1182, 714, 6555, 5362, 3244]}, {"qid": 2925, "question": "Where do they obtain the news videos from? in Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "answer": ["NowThisNews Facebook page", "NowThisNews Facebook page"], "top_k_doc_id": [5364, 1517, 5102, 5104, 7764, 2160, 5103, 3287, 6101, 6226, 406, 7762, 2157, 5362, 3826], "orig_top_k_doc_id": [5102, 5104, 1517, 5103, 7764, 2160, 6101, 6226, 406, 7762, 2157, 5364, 5362, 3287, 3826]}, {"qid": 2926, "question": "What is the source of the news articles? in Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "answer": ["main news channels, such as Yahoo News, The Guardian or The Washington Post", "The BreakingNews dataset"], "top_k_doc_id": [5364, 1517, 5102, 5104, 7764, 2160, 5103, 3287, 6101, 6226, 6555, 2334, 7382, 1863, 3860], "orig_top_k_doc_id": [5102, 5104, 1517, 2160, 5103, 7764, 6555, 2334, 6101, 6226, 7382, 1863, 3287, 3860, 5364]}, {"qid": 3195, "question": "how large is the vocabulary? in Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric", "answer": ["33,663", "33,663 distinct review keywords "], "top_k_doc_id": [5364, 5362, 5363, 5365, 3078, 4968, 6304, 1921, 1051, 6638, 4678, 290, 6915, 7510, 6590], "orig_top_k_doc_id": [5362, 5363, 5365, 5364, 3078, 4968, 1921, 1051, 6638, 6304, 4678, 290, 6915, 7510, 6590]}, {"qid": 3194, "question": "how is diversity measured? in Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric", "answer": ["average dissimilarity of all pairs of tags in the list of recommended tags", " the average dissimilarity of all pairs of tags in the list of recommended tags"], "top_k_doc_id": [5364, 5362, 5363, 5365, 3078, 4441, 138, 4744, 2261, 290, 7315, 6638, 2436, 6321, 5703], "orig_top_k_doc_id": [5362, 5365, 5363, 5364, 3078, 4441, 138, 4744, 2261, 290, 7315, 6638, 2436, 6321, 5703]}, {"qid": 2924, "question": "Which shallow approaches did they experiment with? in Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "answer": ["SVM", "SVM with linear kernel using bag-of-words features"], "top_k_doc_id": [5364, 1517, 5102, 5104, 7764, 7765, 7762, 406, 3244, 3826, 5236, 7382, 123, 7763, 5185], "orig_top_k_doc_id": [5102, 5104, 7764, 7765, 7762, 406, 5364, 3244, 3826, 1517, 5236, 7382, 123, 7763, 5185]}, {"qid": 3192, "question": "how many tags do they look at? in Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric", "answer": ["No", "48,705"], "top_k_doc_id": [5364, 5362, 5363, 5365, 4332, 4678, 4968, 4969, 1713, 2482, 6146, 3872, 7441, 6572, 1051], "orig_top_k_doc_id": [5362, 5363, 5365, 5364, 4332, 4678, 4968, 4969, 1713, 2482, 6146, 3872, 7441, 6572, 1051]}]}
{"group_id": 63, "group_size": 12, "items": [{"qid": 3666, "question": "How are the expert and crowd-sourced annotations compared to one another? in Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations", "answer": ["by using them as features in classifying diagrams and\ntheir parts using various graph neural networks.", "Expert annotators incorporate domain knowledge from multimodality theory while non-expert cannot but they are less time-consuming and use less resources.", "results are not entirely comparable due to different node types, more reasonable to compare architectures"], "top_k_doc_id": [5877, 5878, 2376, 6010, 6011, 6012, 6014, 9, 5042, 308, 5041, 5873, 5876, 5874, 2168], "orig_top_k_doc_id": [6010, 6014, 6011, 5877, 5041, 2376, 9, 5873, 5878, 5874, 2168, 5042, 5876, 308, 6012]}, {"qid": 3668, "question": "Who are considered trained experts? in Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations", "answer": ["Annotators trained on multimodality theory", "No", "domain knowledge from multimodality theory", "Those who have domain knowledge on multimodal communication and annotation."], "top_k_doc_id": [5877, 5878, 2376, 6010, 6011, 6012, 6014, 9, 5042, 308, 5041, 5873, 5876, 6013, 780], "orig_top_k_doc_id": [6010, 6014, 6011, 6012, 6013, 5041, 2376, 5877, 9, 5873, 5878, 780, 5876, 5042, 308]}, {"qid": 3560, "question": "How much data is needed to train the task-specific encoder? in Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction", "answer": ["57,505 sentences", "57,505 sentences"], "top_k_doc_id": [5877, 5878, 3743, 5674, 5873, 5874, 5876, 5676, 5875, 6985, 6989, 6823, 6090, 5632, 126], "orig_top_k_doc_id": [5873, 5878, 5877, 5875, 5674, 5676, 5876, 6823, 6985, 3743, 6090, 5632, 6989, 126, 5874]}, {"qid": 3561, "question": "What kind of out-of-domain data? in Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction", "answer": ["No", "No"], "top_k_doc_id": [5877, 5878, 3743, 5674, 5873, 5874, 5876, 5676, 5875, 6985, 6989, 20, 5133, 2694, 1148], "orig_top_k_doc_id": [5873, 5878, 5877, 5674, 20, 3743, 5133, 2694, 5876, 5874, 5676, 1148, 6985, 5875, 6989]}, {"qid": 3667, "question": "What platform do the crowd-sourced workers come from? in Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations", "answer": ["Amazon Mechanical Turk", "Amazon Mechanical Turk", "No", "Amazon Mechanical Turk"], "top_k_doc_id": [5877, 5878, 2376, 6010, 6011, 6012, 6014, 9, 5042, 2169, 6013, 5874, 2122, 780, 1137], "orig_top_k_doc_id": [6014, 6010, 2376, 6011, 2169, 5877, 6013, 5874, 9, 5042, 2122, 5878, 780, 6012, 1137]}, {"qid": 3558, "question": "How much higher quality is the resulting annotated data? in Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction", "answer": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "top_k_doc_id": [5877, 5878, 3743, 5674, 5873, 5874, 5876, 5675, 6090, 2859, 7832, 1149, 7838, 65, 1148], "orig_top_k_doc_id": [5877, 5873, 5878, 5876, 5874, 6090, 2859, 5674, 3743, 7832, 1149, 7838, 65, 5675, 1148]}, {"qid": 3559, "question": "How do they match annotators to instances? in Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction", "answer": ["Annotations from experts are used if they have already been collected."], "top_k_doc_id": [5877, 5878, 3743, 5674, 5873, 5874, 5876, 5676, 5875, 4019, 7838, 6183, 4218, 126, 4012], "orig_top_k_doc_id": [5878, 5873, 5877, 5876, 5674, 4019, 5874, 5875, 3743, 7838, 6183, 4218, 126, 4012, 5676]}, {"qid": 3562, "question": "Is an instance a sentence or an IE tuple? in Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction", "answer": ["sentence"], "top_k_doc_id": [5877, 5878, 3743, 5674, 5873, 5874, 5876, 5675, 5721, 5720, 5676, 346, 6681, 1422, 1853], "orig_top_k_doc_id": [5878, 5877, 5873, 5721, 5720, 5876, 5674, 5676, 346, 6681, 3743, 5874, 1422, 5675, 1853]}, {"qid": 3665, "question": "Are annotators familiar with the science topics annotated? in Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations", "answer": ["The annotation for AI2D was\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts", "No", "No"], "top_k_doc_id": [5877, 5878, 2376, 6010, 6011, 6012, 6014, 6013, 5041, 3363, 780, 5376, 2169, 3608, 5873], "orig_top_k_doc_id": [6010, 6014, 6011, 6013, 2376, 6012, 5041, 5877, 3363, 780, 5376, 2169, 3608, 5873, 5878]}, {"qid": 3664, "question": "What are the parts of the \"multimodal\" resources? in Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations", "answer": ["spatial organisation , discourse structure", "node types that represent different diagram elements, The same features are used for both AI2D and AI2D-RST for nodes with layout information, discourse relations, information about semantic relations", "grouping, connectivity, and discourse structure "], "top_k_doc_id": [5877, 5878, 2376, 6010, 6011, 6012, 6014, 6013, 3736, 112, 5041, 9, 5701, 412, 4585], "orig_top_k_doc_id": [6010, 6014, 6011, 6012, 6013, 2376, 3736, 112, 5041, 5877, 9, 5701, 412, 4585, 5878]}, {"qid": 2019, "question": "What does their system consist of? in Annotating and normalizing biomedical NEs with limited knowledge", "answer": ["rule-based and dictionary-based methods "], "top_k_doc_id": [5877, 5878, 499, 615, 618, 1092, 3052, 3053, 5133, 5149, 5148, 617, 609, 6711, 614], "orig_top_k_doc_id": [615, 5133, 618, 3052, 5148, 3053, 5877, 5149, 617, 609, 1092, 6711, 614, 499, 5878]}, {"qid": 2020, "question": "What are the two PharmaCoNER subtasks? in Annotating and normalizing biomedical NEs with limited knowledge", "answer": ["Entity identification with offset mapping and concept indexing"], "top_k_doc_id": [5877, 5878, 499, 615, 618, 1092, 3052, 3053, 5133, 5149, 3057, 3054, 3056, 4154, 497], "orig_top_k_doc_id": [3052, 3053, 3057, 3054, 618, 615, 3056, 5133, 499, 5877, 4154, 497, 5878, 5149, 1092]}]}
{"group_id": 64, "group_size": 12, "items": [{"qid": 3971, "question": "what is the source of their dataset? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["Google News, follow the related tags on Twitter, scan the list of event archives on the Web, such as earthquakes happened in 2017", "Topics were taken from category names in Google News, tags on Twitter, event archives on the Web. News articles were taken from news websites.", " Google News, Twitter"], "top_k_doc_id": [7281, 7283, 6433, 6436, 1357, 5998, 7280, 6492, 7282, 2334, 6496, 6566, 6495, 5997, 2335], "orig_top_k_doc_id": [6433, 6436, 7283, 7282, 7280, 1357, 7281, 6566, 6492, 6495, 5997, 2335, 2334, 5998, 6496]}, {"qid": 3977, "question": "what does their dataset contain? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["topics, categories, news documents, model summaries,  comments, annotated aspect facets", "45 topics from those 6 predefined categories, Each topic contains 10 news documents and 4 model summaries, On average, each topic contains 215 pieces of comments and 940 comment sentences, Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words., dataset contains 19k annotated aspect facets"], "top_k_doc_id": [7281, 7283, 6433, 6436, 1357, 5998, 7280, 6492, 7282, 2334, 6496, 6861, 7243, 6925, 6860], "orig_top_k_doc_id": [6433, 6436, 7283, 6492, 1357, 2334, 6496, 6861, 7243, 7280, 6925, 7282, 7281, 5998, 6860]}, {"qid": 3973, "question": "how many experts were there? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["Each topic is assigned to 4 experts", "5", "5"], "top_k_doc_id": [7281, 7283, 6433, 6436, 1357, 5998, 7280, 1699, 6496, 6715, 6955, 6226, 6435, 7282, 1698], "orig_top_k_doc_id": [6433, 6436, 7283, 6715, 7280, 6226, 6955, 6496, 6435, 7281, 5998, 1699, 7282, 1357, 1698]}, {"qid": 3974, "question": "what is the size of the data collected? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": [" The dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words", "19000", "45 topics from those 6 predefined categories, On average, each topic contains 215 pieces of comments and 940 comment sentences., 19k annotated aspect facets"], "top_k_doc_id": [7281, 7283, 6433, 6436, 1357, 5998, 7280, 6492, 7282, 6955, 2335, 4619, 5549, 6861, 6715], "orig_top_k_doc_id": [6433, 6436, 7283, 6955, 1357, 2335, 4619, 5549, 7280, 6861, 7281, 6492, 6715, 5998, 7282]}, {"qid": 3976, "question": "how was annotation conducted? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["Experts identified aspect facets and wrote summaries.", "employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing", "Each topic is assigned to 4 experts to conduct the summary writing in two phases: facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets."], "top_k_doc_id": [7281, 7283, 6433, 6436, 1357, 5998, 7280, 1699, 6496, 6715, 6955, 10, 6053, 6228, 5], "orig_top_k_doc_id": [6433, 6436, 1699, 7280, 10, 7283, 7281, 6955, 6053, 6228, 5, 6496, 1357, 5998, 6715]}, {"qid": 4662, "question": "What settings did they experiment with? in Exploring Domain Shift in Extractive Text Summarization", "answer": ["in-domain, out-of-domain and cross-dataset", "in-domain, out-of-domain, cross-dataset"], "top_k_doc_id": [7281, 7283, 1132, 1972, 3718, 7280, 7284, 734, 6496, 3717, 5396, 4380, 5138, 2335, 110], "orig_top_k_doc_id": [7281, 7280, 7284, 1972, 7283, 3718, 3717, 1132, 5396, 6496, 734, 4380, 5138, 2335, 110]}, {"qid": 4664, "question": "what multi-domain dataset is repurposed? in Exploring Domain Shift in Extractive Text Summarization", "answer": ["MULTI-SUM", "dataset Newsroom BIBREF16"], "top_k_doc_id": [7281, 7283, 1132, 1972, 3718, 7280, 7284, 734, 6496, 2334, 7241, 7243, 6226, 735, 6493], "orig_top_k_doc_id": [7280, 7281, 7284, 7283, 1972, 3718, 1132, 2334, 734, 7241, 7243, 6226, 735, 6496, 6493]}, {"qid": 3970, "question": "what evaluation metrics were used? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4", "ROUGE", "ROUGE-1, ROUGE-2 , ROUGE-SU4"], "top_k_doc_id": [7281, 7283, 6433, 6436, 6861, 6496, 6955, 6936, 6932, 6715, 1698, 5549, 7282, 6935, 6116], "orig_top_k_doc_id": [6433, 6436, 6861, 6496, 6955, 6936, 6932, 7283, 6715, 1698, 5549, 7282, 7281, 6935, 6116]}, {"qid": 3972, "question": "by how much did the performance improve? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["The proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.", "No", "They improved by 0.007 on average across R-1, R-2, R-SU4 over the best baseline."], "top_k_doc_id": [7281, 7283, 6433, 6436, 6495, 6496, 6936, 7280, 5997, 5998, 3417, 6327, 1625, 6861, 7137], "orig_top_k_doc_id": [6433, 6436, 6496, 7283, 7280, 6936, 6495, 5997, 7281, 5998, 3417, 6327, 1625, 6861, 7137]}, {"qid": 3975, "question": "did they use a crowdsourcing platform? in Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset", "answer": ["No", "No", "No"], "top_k_doc_id": [7281, 7283, 6433, 6436, 6495, 6496, 6936, 7280, 5, 5718, 6226, 7241, 2012, 10, 6995], "orig_top_k_doc_id": [6433, 5, 6436, 6496, 6495, 5718, 7283, 6226, 7280, 7241, 2012, 6936, 7281, 10, 6995]}, {"qid": 4663, "question": "what domains are explored in this paper? in Exploring Domain Shift in Extractive Text Summarization", "answer": ["No", "NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time, Mashable"], "top_k_doc_id": [7281, 7283, 1132, 1972, 3718, 7280, 7284, 2334, 2335, 4380, 4619, 3194, 4825, 4826, 6226], "orig_top_k_doc_id": [7280, 7284, 7281, 7283, 2334, 1972, 2335, 1132, 3718, 4380, 4619, 3194, 4825, 4826, 6226]}, {"qid": 4665, "question": "what four learning strategies are investigated? in Exploring Domain Shift in Extractive Text Summarization", "answer": ["Model@!START@$^{I}_{Base}$@!END@, $Model^{I}_{Base}$ with BERT BIBREF28, Model@!START@$^{III}_{Tag}$@!END@, Model@!START@$^{IV}_{Meta}$@!END@", "Model@!START@$^{I}_{Base}$@!END@, Model@!START@$^{II}_{BERT}$@!END@, Model@!START@$^{III}_{Tag}$@!END@, Model@!START@$^{IV}_{Meta}$@!END@"], "top_k_doc_id": [7281, 7283, 1132, 1972, 3718, 7280, 7284, 5396, 5138, 733, 5893, 1146, 2334, 7137, 4398], "orig_top_k_doc_id": [7284, 7280, 7283, 7281, 1132, 1972, 5396, 3718, 5138, 733, 5893, 1146, 2334, 7137, 4398]}]}
{"group_id": 65, "group_size": 12, "items": [{"qid": 4287, "question": "How was annotation done? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["No", "annotations for each turn", "No"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 482, 2804, 879, 1907, 2387, 3622, 1135, 5918, 4013, 6833], "orig_top_k_doc_id": [6793, 6795, 2800, 6794, 2074, 2387, 482, 3622, 2804, 879, 1135, 5918, 4013, 1907, 6833]}, {"qid": 4293, "question": "Were annotations done manually? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["No", "No"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 482, 2804, 879, 1907, 2387, 5720, 6006, 3046, 3045, 6140], "orig_top_k_doc_id": [6793, 6795, 6794, 2074, 2800, 482, 5720, 6006, 1907, 2804, 3046, 3045, 879, 6140, 2387]}, {"qid": 879, "question": "Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["Not at the moment, but summaries can be additionaly extended with this annotations."], "top_k_doc_id": [6793, 6794, 6795, 1135, 5917, 1132, 1133, 1134, 1471, 5918, 7156, 1475, 5919, 5921, 4485], "orig_top_k_doc_id": [1135, 1132, 1134, 1471, 6794, 5918, 6793, 7156, 1133, 1475, 5917, 5919, 5921, 4485, 6795]}, {"qid": 880, "question": "How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["Our encoder-decoder framework employs separate encoding for different speakers in the dialog., We integrate semantic slot scaffold by performing delexicalization on original dialogs., We integrate dialog domain scaffold through a multi-task framework."], "top_k_doc_id": [6793, 6794, 6795, 1135, 5917, 1132, 1133, 1134, 1471, 5918, 7156, 1474, 1473, 6395, 3194], "orig_top_k_doc_id": [1135, 1132, 1134, 1133, 5918, 6794, 6793, 7156, 1471, 1474, 5917, 1473, 6795, 6395, 3194]}, {"qid": 4285, "question": "Which baselines did they compare to? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["Fine tuned DIaloGPT and GPT2 on Interview without speaker information.", "finetune (FT) DialoGPT and GPT2 on Interview without speaker information", "two models (GPT2 and DialoGPT) on two datasets (DailyDialog and CALLHOME)"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 482, 2804, 6833, 5915, 3046, 7478, 4656, 4900, 6341, 5916], "orig_top_k_doc_id": [6793, 6795, 6794, 2804, 2074, 2800, 6833, 482, 5915, 3046, 7478, 4656, 4900, 6341, 5916]}, {"qid": 4286, "question": "What dialog tasks was it experimented on? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["role modeling in media dialog , role change detection ", "1) generating appropriate responses for a specific role given a conversation history (speaker role modeling), 2) predicting whether a new speaker will interject on the next sentence of a conversation", "role modeling in media dialog and role change detection on Interview"], "top_k_doc_id": [6793, 6794, 6795, 1135, 5917, 4124, 4548, 4656, 6584, 5377, 2074, 102, 1132, 3772, 5918], "orig_top_k_doc_id": [6793, 6795, 6794, 1135, 4656, 5377, 4124, 2074, 4548, 102, 6584, 1132, 5917, 3772, 5918]}, {"qid": 4288, "question": "Which news outlets did they focus on? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["natural dialog", "No", "NPR"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 762, 1174, 3860, 5322, 6821, 5121, 1135, 7500, 3861, 7481], "orig_top_k_doc_id": [6793, 6795, 5322, 6794, 2074, 6821, 1174, 5121, 1135, 762, 7500, 2800, 3861, 7481, 3860]}, {"qid": 4290, "question": "Which baselines did they compare to? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information", "Fine-tuned DialGPT and GPT2  on Interview without speaker information."], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 482, 2804, 6833, 5915, 3046, 7478, 4656, 4900, 6341, 5916], "orig_top_k_doc_id": [6793, 6795, 6794, 2804, 2074, 2800, 6833, 482, 5915, 3046, 7478, 4656, 4900, 6341, 5916]}, {"qid": 4291, "question": "Which dialog tasks did they experiment on? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": [" role modeling in media dialog and role change detection on Interview", "1) generating appropriate responses for a specific role given a conversation history (speaker role modeling), 2) predicting whether a new speaker will interject on the next sentence of a conversation"], "top_k_doc_id": [6793, 6794, 6795, 1135, 5917, 4124, 4548, 4656, 6584, 852, 5916, 4442, 4128, 7839, 4127], "orig_top_k_doc_id": [6793, 6795, 6794, 4656, 6584, 1135, 852, 4548, 5916, 4124, 5917, 4442, 4128, 7839, 4127]}, {"qid": 4294, "question": "Which news sources do the transcripts come from? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["7 programs on National Public Radio (NPR) over 20 years", " 7 programs on National Public Radio (NPR)"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 762, 1174, 3860, 7671, 6341, 2210, 2387, 2157, 2083, 7029], "orig_top_k_doc_id": [6793, 6795, 7671, 2074, 6341, 6794, 3860, 2210, 2387, 2157, 1174, 2083, 762, 7029, 2800]}, {"qid": 4289, "question": "Do the interviews fall under a specific news category?  in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["No"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 6341, 6343, 2805, 1907, 7671, 6342, 2861, 7388, 955, 7774], "orig_top_k_doc_id": [6793, 2800, 6795, 6341, 6343, 2805, 6794, 1907, 7671, 6342, 2074, 2861, 7388, 955, 7774]}, {"qid": 4292, "question": "Did they use crowdsourcing for annotations? in Interview: A Large-Scale Open-Source Corpus of Media Dialog", "answer": ["No", "No"], "top_k_doc_id": [6793, 6794, 6795, 2074, 2800, 6805, 5720, 2387, 4587, 3622, 6808, 4583, 3626, 5915, 780], "orig_top_k_doc_id": [6793, 2800, 6795, 6794, 2074, 6805, 5720, 2387, 4587, 3622, 6808, 4583, 3626, 5915, 780]}]}
{"group_id": 66, "group_size": 12, "items": [{"qid": 4415, "question": "Did they use any regularization method to constrain the training? in Transfer Learning for Low-Resource Neural Machine Translation", "answer": ["Yes", "Freezing certain portions of the parent model and fine tuning others"], "top_k_doc_id": [5026, 4568, 4590, 6943, 627, 4615, 6944, 626, 628, 6035, 6265, 7339, 7847, 4619, 1583], "orig_top_k_doc_id": [6943, 6944, 4568, 627, 6035, 628, 7339, 4590, 7847, 6265, 5026, 4619, 626, 4615, 1583]}, {"qid": 4416, "question": "How did they constrain training using the parameters? in Transfer Learning for Low-Resource Neural Machine Translation", "answer": ["first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). , by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model., When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. ", "Freezing certain portions of the parent model and fine tuning others"], "top_k_doc_id": [5026, 4568, 4590, 6943, 627, 4615, 6944, 626, 628, 6035, 6265, 659, 1743, 5621, 3617], "orig_top_k_doc_id": [6943, 6035, 6265, 4568, 6944, 626, 627, 659, 4590, 628, 1743, 4615, 5621, 5026, 3617]}, {"qid": 4711, "question": "How much data do they manage to gather online? in Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora", "answer": ["INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia", "INLINEFORM0 bilingual English-Tamil, INLINEFORM1 English-Hindi titles"], "top_k_doc_id": [5026, 67, 80, 84, 1044, 7342, 4590, 4712, 6943, 5714, 2162, 5715, 1040, 661, 3293], "orig_top_k_doc_id": [7342, 1044, 4712, 5026, 80, 84, 1040, 5714, 6943, 67, 5715, 4590, 2162, 661, 3293]}, {"qid": 4714, "question": "What are the BLEU performance improvements they achieve? in Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora", "answer": [" 11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively", "11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively"], "top_k_doc_id": [5026, 67, 80, 84, 1044, 7342, 4590, 4712, 6943, 5714, 2162, 5715, 4864, 1041, 4592], "orig_top_k_doc_id": [7342, 84, 6943, 5026, 1044, 5715, 80, 67, 4712, 4864, 4590, 2162, 5714, 1041, 4592]}, {"qid": 2604, "question": "How do they match words before reordering them? in Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "answer": ["No", "CFILT-preorder system"], "top_k_doc_id": [5026, 4568, 4590, 4591, 4592, 4615, 7342, 1053, 4510, 3920, 6791, 5983, 3125, 4289, 4695], "orig_top_k_doc_id": [4592, 4591, 4590, 5026, 1053, 3920, 6791, 5983, 4568, 4510, 4615, 3125, 7342, 4289, 4695]}, {"qid": 2606, "question": "Which dataset(s) do they experiment with? in Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "answer": ["IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus", "IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus"], "top_k_doc_id": [5026, 4568, 4590, 4591, 4592, 4615, 7342, 1053, 4510, 661, 5841, 1991, 2836, 7339, 1777], "orig_top_k_doc_id": [4590, 4591, 4592, 5026, 4568, 661, 5841, 4615, 1991, 4510, 2836, 1053, 7342, 7339, 1777]}, {"qid": 4414, "question": "What high-resource language pair is the parent model trained on? in Transfer Learning for Low-Resource Neural Machine Translation", "answer": ["French-English parent model", "French-English"], "top_k_doc_id": [5026, 4568, 4590, 6943, 627, 4615, 6944, 6945, 4569, 6946, 4027, 4572, 7342, 6370, 1781], "orig_top_k_doc_id": [6943, 4568, 6944, 4590, 5026, 6945, 4569, 6946, 627, 4615, 4027, 4572, 7342, 6370, 1781]}, {"qid": 4713, "question": "Which models do they use for NMT? in Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora", "answer": [" TensorFlow BIBREF17 implementation of OpenNMT", "OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19"], "top_k_doc_id": [5026, 67, 80, 84, 1044, 7342, 4590, 4712, 6943, 5714, 661, 4568, 6035, 1720, 1040], "orig_top_k_doc_id": [5026, 7342, 6943, 1044, 4590, 84, 661, 4568, 6035, 80, 1720, 67, 5714, 1040, 4712]}, {"qid": 2605, "question": "On how many language pairs do they show that preordering assisting language sentences helps translation quality? in Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "answer": ["5", "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks."], "top_k_doc_id": [5026, 4568, 4590, 4591, 4592, 4615, 7342, 6791, 7339, 4695, 3617, 4526, 5841, 6060, 5564], "orig_top_k_doc_id": [4590, 4591, 4592, 5026, 6791, 7342, 7339, 4695, 3617, 4526, 4568, 5841, 6060, 5564, 4615]}, {"qid": 2869, "question": "what was the baseline? in Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation", "answer": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "top_k_doc_id": [5026, 4568, 4590, 6943, 7342, 4027, 4619, 5030, 1777, 6035, 5031, 3617, 3140, 6031, 2836], "orig_top_k_doc_id": [5026, 4568, 7342, 4027, 4619, 5030, 1777, 6943, 6035, 5031, 3617, 4590, 3140, 6031, 2836]}, {"qid": 4710, "question": "Do they evaluate their parallel sentence generation? in Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora", "answer": ["Yes", "Yes"], "top_k_doc_id": [5026, 67, 80, 84, 1044, 7342, 4590, 4712, 6943, 1040, 1884, 6791, 661, 1041, 4569], "orig_top_k_doc_id": [7342, 80, 5026, 84, 67, 4712, 1040, 1044, 1884, 6791, 6943, 661, 1041, 4569, 4590]}, {"qid": 4712, "question": "Which models do they use for phrase-based SMT? in Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora", "answer": ["Phrase-Based SMT systems were trained using Moses, grow-diag-final-and heuristic were used for extracting phrases,  and lexicalised reordering and Batch MIRA for tuning.", "Moses BIBREF14"], "top_k_doc_id": [5026, 67, 80, 84, 1044, 7342, 6855, 6791, 2894, 4312, 3920, 6856, 6857, 6788, 1720], "orig_top_k_doc_id": [7342, 1044, 5026, 6855, 6791, 84, 2894, 4312, 3920, 6856, 80, 6857, 6788, 1720, 67]}]}
{"group_id": 67, "group_size": 12, "items": [{"qid": 4433, "question": "Which tasks quantify embedding quality? in It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "answer": ["word similarity, sentiment classification", "word similarity, sentiment classification, understanding of gender using non-biased analogies"], "top_k_doc_id": [6735, 1446, 6738, 6739, 3941, 6962, 6963, 6964, 6966, 1444, 5155, 6737, 1443, 6341, 7266], "orig_top_k_doc_id": [6963, 6962, 6964, 1444, 6735, 3941, 6738, 1443, 1446, 6739, 6341, 7266, 6737, 6966, 5155]}, {"qid": 4434, "question": "What empirical comparison methods are used? in It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "answer": ["test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification", "Direct bias, Indirect bias, Word similarity, Sentiment classification, Non-biased gender analogies"], "top_k_doc_id": [6735, 1446, 6738, 6739, 3941, 6962, 6963, 6964, 6966, 1444, 5155, 6737, 1443, 1445, 7775], "orig_top_k_doc_id": [6963, 6962, 6964, 6738, 1444, 6735, 6966, 3941, 6739, 1446, 1443, 6737, 1445, 5155, 7775]}, {"qid": 4430, "question": "How is cluster purity measured? in It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "answer": ["V-measure", "V-measure BIBREF16"], "top_k_doc_id": [6735, 1446, 6738, 6739, 3941, 6962, 6963, 6964, 6966, 1444, 5155, 6737, 6965, 3943, 7266], "orig_top_k_doc_id": [6962, 6964, 6963, 6735, 6966, 6738, 1444, 1446, 5155, 6965, 6739, 3943, 6737, 7266, 3941]}, {"qid": 569, "question": "How do they measure grammaticality? in Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "answer": ["by calculating log ratio of grammatical phrase over ungrammatical phrase"], "top_k_doc_id": [6735, 3943, 7266, 694, 1444, 5155, 6558, 7268, 1443, 3941, 6962, 6963, 697, 5153, 6966], "orig_top_k_doc_id": [697, 694, 1444, 3943, 6963, 6558, 7268, 3941, 1443, 6735, 5155, 5153, 7266, 6962, 6966]}, {"qid": 1093, "question": "How does counterfactual data augmentation aim to tackle bias? in Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation", "answer": ["The training dataset is augmented by swapping all gendered words by their other gender counterparts"], "top_k_doc_id": [6735, 3943, 7266, 694, 1444, 5155, 6558, 7268, 1443, 3941, 6962, 6963, 1446, 1445, 3942], "orig_top_k_doc_id": [1444, 1443, 1446, 5155, 6962, 6735, 6963, 1445, 3941, 7268, 3943, 7266, 3942, 694, 6558]}, {"qid": 4431, "question": "What was the previous state of the art for bias mitigation? in It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "answer": ["WED, CDA", "WED, CDA"], "top_k_doc_id": [6735, 1446, 6738, 6739, 3941, 6962, 6963, 6964, 6966, 1444, 5155, 41, 42, 1445, 1443], "orig_top_k_doc_id": [6962, 6963, 6964, 6738, 6739, 6966, 41, 3941, 1444, 42, 6735, 1445, 5155, 1446, 1443]}, {"qid": 568, "question": "Why does not the approach from English work on other languages? in Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "answer": ["Because, unlike other languages, English does not mark grammatical genders"], "top_k_doc_id": [6735, 3943, 7266, 694, 1444, 5155, 6558, 7268, 697, 6165, 6966, 5872, 377, 655, 6280], "orig_top_k_doc_id": [694, 697, 7268, 7266, 3943, 1444, 6165, 6966, 5872, 377, 5155, 655, 6280, 6558, 6735]}, {"qid": 1092, "question": "What baseline is used to compare the experimental results against? in Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation", "answer": ["Transformer generation model"], "top_k_doc_id": [6735, 1446, 6738, 6739, 41, 1443, 1444, 1445, 5155, 6964, 1768, 52, 6039, 6963, 3193], "orig_top_k_doc_id": [1446, 1444, 1443, 5155, 1445, 6738, 41, 6964, 6735, 1768, 52, 6039, 6963, 3193, 6739]}, {"qid": 1094, "question": "In the targeted data collection approach, what type of data is targetted? in Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation", "answer": ["Gendered characters in the dataset"], "top_k_doc_id": [6735, 1446, 6738, 6739, 41, 1443, 1444, 1445, 5155, 3941, 6705, 3942, 694, 7454, 5191], "orig_top_k_doc_id": [1444, 1443, 1446, 5155, 3941, 6735, 1445, 41, 6738, 6705, 3942, 694, 7454, 6739, 5191]}, {"qid": 4246, "question": "How does counterfactual data augmentation affect gender bias in predictions and performance? in Towards Understanding Gender Bias in Relation Extraction", "answer": ["mitigates these contextual biases", "No"], "top_k_doc_id": [6735, 3943, 7266, 3941, 3942, 5006, 5153, 6737, 6738, 6962, 7267, 7268, 6963, 1444, 1443], "orig_top_k_doc_id": [6735, 6737, 7268, 3942, 3943, 6962, 7266, 6963, 3941, 1444, 5006, 5153, 1443, 6738, 7267]}, {"qid": 4247, "question": "How does hard debiasing affect gender bias in prediction and performance? in Towards Understanding Gender Bias in Relation Extraction", "answer": ["mitigating the difference in F1 scores for all relations, debiased embeddings increases absolute score", "No", "Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations"], "top_k_doc_id": [6735, 3943, 7266, 3941, 3942, 5006, 5153, 6737, 6738, 6962, 7267, 5155, 6736, 6739, 5154], "orig_top_k_doc_id": [6735, 6737, 6738, 3942, 3941, 5153, 6962, 3943, 5155, 7267, 7266, 6736, 6739, 5006, 5154]}, {"qid": 4432, "question": "How are names paired in the Names Intervention? in It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "answer": ["name frequency, the degree of gender-specificity", "By solving the Euclidean-distance bipartite matching problem of names by frequency\nand gender-specificity"], "top_k_doc_id": [6735, 1446, 6738, 6739, 3941, 6962, 6963, 6964, 6966, 6737, 694, 7714, 6701, 7713, 6965], "orig_top_k_doc_id": [6963, 6962, 6964, 6966, 6738, 1446, 6739, 6737, 694, 7714, 6701, 6735, 7713, 6965, 3941]}]}
{"group_id": 68, "group_size": 12, "items": [{"qid": 4480, "question": "How do they preprocess Tweets? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["They removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.", "re-tweets do not bring any additional information to our study, thus we removed them,  removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags, downcased and stripped the punctuation", "removing URLs, emoticons, mentions of other users, hashtags; downcasing and stripping punctuations"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7030, 7034, 7019], "orig_top_k_doc_id": [7029, 7016, 7017, 7035, 7036, 7018, 2534, 7032, 7031, 7028, 7021, 7030, 7019, 7033, 7034]}, {"qid": 4481, "question": "What kind of inference model do they build to estimate socioeconomic status? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["XGBoost", "XGBoost algorithm BIBREF43", "XGBoost, an ensemble of gradient-based decision trees algorithm "], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7030, 7034, 7019], "orig_top_k_doc_id": [7029, 7016, 7035, 7036, 7017, 7031, 7032, 7034, 7028, 2534, 7033, 7018, 7030, 7021, 7019]}, {"qid": 4482, "question": "How much data do they gather in total? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["90,369,215 tweets written in French, posted by 1.3 Million users", "They created 3 datasets with combined size of 37193.", "90,369,215 tweets"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7030, 7034, 5191], "orig_top_k_doc_id": [7016, 7029, 7017, 7035, 7036, 7032, 7031, 7018, 2534, 7028, 7021, 7033, 7034, 7030, 5191]}, {"qid": 4483, "question": "Do they analyze features which help indicate socioeconomic status? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["No", "Yes", "Yes"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7030, 7034, 7019], "orig_top_k_doc_id": [7029, 7016, 7035, 7036, 7017, 7032, 7031, 7028, 7034, 7033, 2534, 7018, 7030, 7021, 7019]}, {"qid": 4484, "question": "What inference models are used? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["XGBoost, AdaBoost, Random Forest"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7030, 7034, 7019], "orig_top_k_doc_id": [7016, 7029, 7017, 7035, 7036, 2534, 7021, 7031, 7018, 7032, 7019, 7033, 7028, 7034, 7030]}, {"qid": 4485, "question": "What baseline model is used? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["AdaBoost, Random Forest"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7030, 7034, 7019], "orig_top_k_doc_id": [7016, 7029, 7017, 7035, 7036, 7031, 7018, 7032, 2534, 7021, 7033, 7034, 7028, 7030, 7019]}, {"qid": 4486, "question": "How is the remotely sensed data annotated? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["The SES score was assigned by architects based on the satellite and Street View images of users' homes."], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7019, 7022, 521], "orig_top_k_doc_id": [7022, 7016, 7029, 7017, 7035, 7036, 7018, 7021, 7031, 7032, 2534, 7028, 7019, 7033, 521]}, {"qid": 4487, "question": "Where are the professional profiles crawled from? in Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter", "answer": ["LinkedIn"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7017, 7021, 7019, 7022, 7034], "orig_top_k_doc_id": [7016, 7017, 7018, 7029, 7021, 7035, 7036, 7022, 7028, 7032, 7031, 7019, 2534, 7033, 7034]}, {"qid": 4494, "question": "How do they combine the socioeconomic maps with Twitter data?  in Socioeconomic Dependencies of Linguistic Patterns in Twitter: A Multivariate Analysis", "answer": ["Match geolocation data for Twitter users with patches from INSEE socioeconomic maps.", "By matching users to locations using geolocated tweets data, then matching locations to socioeconomic status using INSEE sociodemographic data."], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7020, 7034, 7530, 6896, 7017], "orig_top_k_doc_id": [7033, 7028, 7016, 7035, 7029, 7031, 7032, 7034, 7036, 7018, 6896, 7530, 2534, 7017, 7020]}, {"qid": 4496, "question": "How did they define standard language? in Socioeconomic Dependencies of Linguistic Patterns in Twitter: A Multivariate Analysis", "answer": ["Use of both French negative particles and spelling out plural ending on adjectives and nouns", "Standard usage of negation, Standard usage of plural ending of written words, lexical diversity"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7020, 7034, 7530, 6896, 7523], "orig_top_k_doc_id": [7033, 7028, 7035, 7016, 7032, 7029, 7034, 7031, 7036, 7523, 6896, 7020, 7530, 2534, 7018]}, {"qid": 4497, "question": "How do they operationalize socioeconomic status from twitter user data? in Socioeconomic Dependencies of Linguistic Patterns in Twitter: A Multivariate Analysis", "answer": ["we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location", "No"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 2534, 7020, 7034, 7530, 7030, 7017], "orig_top_k_doc_id": [7033, 7029, 7028, 7035, 7016, 7032, 7034, 7031, 7036, 7030, 2534, 7017, 7018, 7530, 7020]}, {"qid": 4495, "question": "Does the fact that people are active during the day time define their SEC? in Socioeconomic Dependencies of Linguistic Patterns in Twitter: A Multivariate Analysis", "answer": ["No, but the authors identified a correlation.", "No"], "top_k_doc_id": [7016, 7018, 7028, 7029, 7031, 7032, 7033, 7035, 7036, 7034, 5783, 7525, 7523, 6804, 6899], "orig_top_k_doc_id": [7034, 7033, 7035, 7028, 7032, 7031, 7029, 7016, 7018, 7036, 5783, 7525, 7523, 6804, 6899]}]}
{"group_id": 69, "group_size": 12, "items": [{"qid": 4523, "question": "what neural network models are used? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["GRU and LSTM models with a combination of the following characteristics: bidirectional vs normal, attention vs no attention, stacked vs flat.", "GRU, Stacked GRU, Bidirectional GRU, Bidirectional stacked GRU, GRU with attention, Stacked GRU with Attention, Bidirectional GRU with attention, Bidirectional Stacked GRU with Attention, LSTM, Stacked LSTM, Bidirectional LSTM, Bidirectional stacked LSTM, LSTM with attention, Stacked LSTM with Attention, Bidirectional LSTM with attention, Bidirectional Stacked LSTM with Attention"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 1960, 2800, 4444, 5992, 4288, 2343, 6101], "orig_top_k_doc_id": [7077, 7078, 2334, 2970, 2335, 1960, 2968, 2337, 2721, 5992, 4444, 2800, 4288, 2343, 6101]}, {"qid": 4526, "question": "What type of neural network models are used? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["Recurrent neural network", "GRU, LSTM"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 1960, 2800, 4444, 5992, 4288, 934, 7855], "orig_top_k_doc_id": [7077, 7078, 2334, 2337, 2970, 2335, 1960, 2968, 2721, 934, 5992, 4444, 2800, 4288, 7855]}, {"qid": 4522, "question": "what dataset is used? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["Essays collected from students from American Institutes for Research tests, Synthetic responses from Reddit and Teen Line", "Student responses to the American Institutes for Research tests."], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 1960, 2800, 4444, 5992, 2336, 934, 7222], "orig_top_k_doc_id": [7078, 7077, 2334, 2337, 2335, 2968, 2970, 1960, 2721, 2336, 2800, 4444, 934, 7222, 5992]}, {"qid": 1646, "question": "Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items? in A Novel ILP Framework for Summarizing Content with High Lexical Variety", "answer": ["They evaluate quantitatively."], "top_k_doc_id": [2334, 2335, 2336, 2339, 2341, 2342, 155, 2340, 5321, 5544, 7205, 7216, 545, 1910, 3559], "orig_top_k_doc_id": [2334, 2335, 2336, 2341, 2342, 5321, 7205, 2340, 7216, 545, 1910, 2339, 5544, 3559, 155]}, {"qid": 1647, "question": "Do they evaluate their framework on content of low lexical variety? in A Novel ILP Framework for Summarizing Content with High Lexical Variety", "answer": ["No"], "top_k_doc_id": [2334, 2335, 2336, 2339, 2341, 2342, 155, 2340, 5321, 5544, 1703, 2156, 5793, 4744, 4005], "orig_top_k_doc_id": [2334, 2335, 2336, 2342, 2341, 2339, 1703, 2156, 5793, 2340, 5544, 4744, 4005, 155, 5321]}, {"qid": 4525, "question": "What baseline model is used? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["Logistic regression with TF-IDF with latent semantic analysis representations", "logistic regression applied to a TF-IDF model"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 1960, 2800, 4444, 2339, 2336, 4619, 2341], "orig_top_k_doc_id": [7078, 7077, 2970, 2334, 2337, 2335, 2968, 1960, 2721, 4444, 2800, 2339, 2336, 4619, 2341]}, {"qid": 4528, "question": "How is severity identified and what metric is used to quantify it? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["Severity is manually identified by a team of reviewers.", "No"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 2343, 6101, 1960, 4869, 6005, 61, 521], "orig_top_k_doc_id": [7078, 7077, 2337, 2334, 2970, 2335, 6005, 61, 521, 2343, 2968, 6101, 2721, 1960, 4869]}, {"qid": 4529, "question": "How is urgency identified and what metric is used to quantify it? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["Urgency is manually identified by a team of reviewers.", "No"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 2343, 6101, 1960, 4869, 2074, 4870, 7232], "orig_top_k_doc_id": [7078, 7077, 2337, 2334, 2970, 2335, 2343, 2968, 6101, 2721, 1960, 4869, 2074, 4870, 7232]}, {"qid": 1644, "question": "What do they constrain using integer linear programming? in A Novel ILP Framework for Summarizing Content with High Lexical Variety", "answer": ["low-rank approximation of the co-occurrence matrix"], "top_k_doc_id": [2334, 2335, 2336, 2339, 2341, 2342, 6435, 7205, 4463, 6434, 7198, 5095, 2961, 7194, 3201], "orig_top_k_doc_id": [2334, 2335, 2342, 2341, 6435, 7205, 2336, 4463, 6434, 7198, 5095, 2339, 2961, 7194, 3201]}, {"qid": 4524, "question": "Do they report results only on English data? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["Yes", "No"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 1960, 2800, 1795, 6101, 1480, 2630, 4284], "orig_top_k_doc_id": [7077, 7078, 2968, 2334, 2721, 2337, 2970, 2335, 1960, 1795, 6101, 1480, 2630, 2800, 4284]}, {"qid": 4527, "question": "How is validity identified and what metric is used to quantify it? in Neural network approach to classifying alarming student responses to online assessment", "answer": ["approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts", "No"], "top_k_doc_id": [2334, 2335, 2337, 2721, 2968, 2970, 7077, 7078, 2343, 6101, 2336, 2800, 4581, 62, 3145], "orig_top_k_doc_id": [7078, 7077, 2337, 2334, 2970, 2335, 2336, 2800, 2343, 4581, 62, 2968, 3145, 6101, 2721]}, {"qid": 1645, "question": "Do they build one model per topic or on all topics? in A Novel ILP Framework for Summarizing Content with High Lexical Variety", "answer": ["One model per topic."], "top_k_doc_id": [2334, 2335, 2336, 2339, 5373, 6435, 7216, 1703, 5066, 5067, 5095, 5544, 117, 1869, 4383], "orig_top_k_doc_id": [2334, 2335, 5373, 2339, 6435, 2336, 7216, 1703, 5066, 5067, 5095, 5544, 117, 1869, 4383]}]}
{"group_id": 70, "group_size": 12, "items": [{"qid": 4564, "question": "By how much does their similarity measure outperform BM25? in Bridging the Gap: Incorporating a Semantic Similarity Measure for Effectively Mapping PubMed Queries to Documents", "answer": ["No", "embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007"], "top_k_doc_id": [302, 7128, 1507, 7126, 7127, 7129, 7130, 303, 4839, 304, 4835, 6449, 6842, 7738, 6571], "orig_top_k_doc_id": [7130, 7127, 7129, 7126, 7128, 302, 303, 6449, 1507, 6842, 4839, 304, 7738, 4835, 6571]}, {"qid": 4565, "question": "How do they represent documents when using their proposed similarity measure? in Bridging the Gap: Incorporating a Semantic Similarity Measure for Effectively Mapping PubMed Queries to Documents", "answer": ["documents are represented by normalized bag-of-words (BOW) vectors", "normalized bag-of-words vectors"], "top_k_doc_id": [302, 7128, 1507, 7126, 7127, 7129, 7130, 303, 4839, 304, 4835, 1315, 1506, 5297, 3202], "orig_top_k_doc_id": [7130, 7127, 7126, 7129, 7128, 1507, 302, 303, 4839, 1315, 1506, 304, 5297, 4835, 3202]}, {"qid": 241, "question": "what is the supervised model they developed? in Learning to Rank Scientific Documents from the Crowd", "answer": ["SVMRank"], "top_k_doc_id": [302, 7128, 303, 308, 2168, 2334, 5148, 307, 1695, 1921, 5719, 6053, 4826, 5401, 6955], "orig_top_k_doc_id": [302, 303, 7128, 308, 1695, 4826, 2168, 2334, 5401, 5148, 5719, 307, 6955, 6053, 1921]}, {"qid": 242, "question": "what is the size of this built corpus? in Learning to Rank Scientific Documents from the Crowd", "answer": ["90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations"], "top_k_doc_id": [302, 7128, 303, 308, 2168, 2334, 5148, 307, 1695, 1921, 5719, 6053, 304, 4008, 7129], "orig_top_k_doc_id": [303, 302, 308, 5719, 2334, 1695, 304, 6053, 7128, 4008, 2168, 5148, 7129, 307, 1921]}, {"qid": 4566, "question": "How do they propose to combine BM25 and word embedding similarity? in Bridging the Gap: Incorporating a Semantic Similarity Measure for Effectively Mapping PubMed Queries to Documents", "answer": ["They merge features of BM25 and semantic measures.", "Yes"], "top_k_doc_id": [302, 7128, 1507, 7126, 7127, 7129, 7130, 303, 4839, 2641, 6449, 4555, 1506, 6842, 5297], "orig_top_k_doc_id": [7130, 7129, 7126, 7127, 7128, 1507, 302, 2641, 6449, 4555, 4839, 1506, 6842, 5297, 303]}, {"qid": 240, "question": "what were the baselines? in Learning to Rank Scientific Documents from the Crowd", "answer": ["Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings", "(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings."], "top_k_doc_id": [302, 7128, 303, 308, 2168, 2334, 5148, 5401, 305, 6091, 2594, 7129, 6014, 6090, 5953], "orig_top_k_doc_id": [303, 302, 5401, 305, 7128, 308, 2334, 6091, 2168, 5148, 2594, 7129, 6014, 6090, 5953]}, {"qid": 997, "question": "What metrics are used to evaluation revision detection? in Semantic Document Distance Measures and Unsupervised Document Revision Detection", "answer": ["precision, recall, F-measure"], "top_k_doc_id": [302, 1313, 1314, 1315, 1316, 1317, 6957, 105, 2623, 5258, 5589, 6205, 6201, 4353, 135], "orig_top_k_doc_id": [1313, 1317, 1314, 1316, 1315, 6957, 2623, 105, 6205, 5258, 6201, 4353, 135, 302, 5589]}, {"qid": 998, "question": "How large is the Wikipedia revision dump dataset? in Semantic Document Distance Measures and Unsupervised Document Revision Detection", "answer": ["eight GB"], "top_k_doc_id": [302, 1313, 1314, 1315, 1316, 1317, 6957, 105, 2623, 5258, 5589, 6205, 2622, 6958, 3716], "orig_top_k_doc_id": [1313, 1317, 1314, 1316, 1315, 6957, 5589, 2623, 105, 2622, 6958, 302, 6205, 5258, 3716]}, {"qid": 999, "question": "What are simulated datasets collected? in Semantic Document Distance Measures and Unsupervised Document Revision Detection", "answer": ["There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents"], "top_k_doc_id": [302, 1313, 1314, 1315, 1316, 1317, 6957, 4353, 5896, 7127, 4740, 191, 7128, 4712, 7126], "orig_top_k_doc_id": [1313, 1317, 1316, 1314, 1315, 5896, 6957, 302, 7127, 4353, 4740, 191, 7128, 4712, 7126]}, {"qid": 1000, "question": "Which are the state-of-the-art models? in Semantic Document Distance Measures and Unsupervised Document Revision Detection", "answer": ["WMD, VSM, PV-DTW, PV-TED"], "top_k_doc_id": [302, 1313, 1314, 1315, 1316, 1317, 6957, 4353, 5896, 7127, 2623, 5589, 2873, 3296, 4885], "orig_top_k_doc_id": [1313, 1317, 1314, 1316, 1315, 2623, 6957, 5896, 7127, 4353, 302, 5589, 2873, 3296, 4885]}, {"qid": 4567, "question": "Do they use pretrained word embeddings to calculate Word Mover's distance? in Bridging the Gap: Incorporating a Semantic Similarity Measure for Effectively Mapping PubMed Queries to Documents", "answer": ["No", "No"], "top_k_doc_id": [302, 7128, 1507, 7126, 7127, 7129, 7130, 1316, 1313, 4200, 1315, 4201, 4891, 6495, 1506], "orig_top_k_doc_id": [7127, 7126, 7130, 7128, 7129, 1316, 1313, 4200, 1507, 1315, 4201, 302, 4891, 6495, 1506]}, {"qid": 2408, "question": "What are the baselines model? in All Fingers are not Equal: Intensity of References in Scientific Articles", "answer": ["(i) Uniform, (ii) SVR+W, (iii) SVR+O, (iv) C4.5SSL, (v) GLM"], "top_k_doc_id": [302, 3931, 3936, 3935, 3937, 303, 6643, 3934, 5128, 1921, 3459, 5521, 6017, 5968, 6228], "orig_top_k_doc_id": [3931, 3936, 3935, 3937, 303, 302, 6643, 3934, 5128, 1921, 3459, 5521, 6017, 5968, 6228]}]}
{"group_id": 71, "group_size": 12, "items": [{"qid": 4852, "question": "What was the baseline? in Transcribing Lyrics From Commercial Song Audio: The First Step Towards Singing Content Processing", "answer": ["Model A, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech.", "a model trained on LibriSpeech data with SAT and a with a LM also trained with LibriSpeech"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 998, 1662, 7539, 7540, 567, 5860, 5861, 5862, 6998], "orig_top_k_doc_id": [7537, 7540, 7538, 7539, 1661, 1129, 5860, 1130, 998, 1662, 1663, 5862, 5861, 6998, 567]}, {"qid": 4853, "question": "How many songs were collected? in Transcribing Lyrics From Commercial Song Audio: The First Step Towards Singing Content Processing", "answer": ["110 pieces of music-removed version of commercial English popular songs", "130 "], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 998, 1662, 7539, 7540, 567, 5860, 5861, 2450, 995], "orig_top_k_doc_id": [7537, 7540, 1661, 7538, 1129, 1662, 1663, 1130, 7539, 5861, 5860, 2450, 998, 995, 567]}, {"qid": 1223, "question": "what is the source of the song lyrics? in Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network", "answer": ["Vagalume website"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 998, 1662, 7539, 7540, 567, 7362, 3622, 5504, 335], "orig_top_k_doc_id": [1663, 1661, 1662, 1129, 7537, 7539, 7540, 7538, 998, 1130, 567, 7362, 3622, 5504, 335]}, {"qid": 1224, "question": "what genre was the most difficult to classify? in Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network", "answer": [" bossa-nova and jovem-guarda genres"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 998, 1662, 7539, 7540, 995, 996, 997, 999, 1845], "orig_top_k_doc_id": [1663, 1661, 1662, 998, 7537, 1129, 7540, 7539, 7538, 1130, 995, 999, 997, 1845, 996]}, {"qid": 1226, "question": "what genres do they songs fall under? in Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network", "answer": ["Gospel, Sertanejo, MPB, Forr\u00f3, Pagode, Rock, Samba, Pop, Ax\u00e9, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 998, 1662, 7539, 7540, 995, 996, 997, 999, 5066], "orig_top_k_doc_id": [1663, 1661, 1662, 7537, 1129, 995, 998, 7538, 1130, 7539, 7540, 999, 997, 996, 5066]}, {"qid": 874, "question": "How long is the model trained? in 'Warriors of the Word' -- Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs", "answer": ["No"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 995, 5066, 5067, 996, 997, 1112, 1113, 4672, 1662], "orig_top_k_doc_id": [1129, 1130, 1661, 7537, 5066, 7538, 5067, 1113, 4672, 995, 1663, 1662, 997, 1112, 996]}, {"qid": 875, "question": "What are lyrical topics present in the metal genre? in 'Warriors of the Word' -- Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs", "answer": ["Table TABREF10 displays the twenty resulting topics"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 995, 5066, 5067, 996, 997, 1112, 1113, 998, 6995], "orig_top_k_doc_id": [1129, 1130, 1661, 998, 997, 996, 7537, 5066, 7538, 995, 6995, 1113, 1663, 5067, 1112]}, {"qid": 796, "question": "Which decades did they look at? in The Wiki Music dataset: A tool for computational analysis of popular music", "answer": ["between 1900s and 2010s"], "top_k_doc_id": [1661, 995, 997, 998, 999, 1114, 5065, 996, 1663, 7537, 1483, 2345, 2344, 1262, 5066], "orig_top_k_doc_id": [995, 999, 998, 996, 997, 1114, 1483, 1663, 1661, 2345, 5065, 2344, 1262, 5066, 7537]}, {"qid": 797, "question": "How many genres did they collect from? in The Wiki Music dataset: A tool for computational analysis of popular music", "answer": ["77 genres"], "top_k_doc_id": [1661, 995, 997, 998, 999, 1114, 5065, 996, 1663, 7537, 1662, 1130, 482, 1847, 1481], "orig_top_k_doc_id": [995, 999, 998, 997, 1661, 1663, 996, 7537, 1662, 1130, 482, 1847, 5065, 1481, 1114]}, {"qid": 873, "question": "Why are the scores for predicting perceived musical hardness and darkness extracted only for subsample of 503 songs? in 'Warriors of the Word' -- Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs", "answer": ["No"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 995, 5066, 5067, 1662, 998, 5861, 1262, 6995, 2450], "orig_top_k_doc_id": [1129, 1130, 1661, 7537, 5066, 995, 1663, 7538, 1662, 998, 5067, 5861, 1262, 6995, 2450]}, {"qid": 1225, "question": "what word embedding techniques did they experiment with? in Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network", "answer": ["Word2Vec, Wang2Vec, and FastText"], "top_k_doc_id": [1661, 1129, 1130, 1663, 7537, 7538, 998, 1662, 7539, 7540, 6232, 7422, 2256, 7420, 7423], "orig_top_k_doc_id": [1663, 1661, 1662, 998, 7539, 1129, 7540, 7537, 7538, 6232, 7422, 2256, 1130, 7420, 7423]}, {"qid": 795, "question": "What trends are found in musical preferences? in The Wiki Music dataset: A tool for computational analysis of popular music", "answer": ["audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious"], "top_k_doc_id": [1661, 995, 997, 998, 999, 1114, 5065, 1262, 1265, 1129, 1130, 1263, 1264, 5066, 2344], "orig_top_k_doc_id": [995, 1262, 1265, 5065, 1129, 999, 1661, 1130, 1263, 998, 997, 1264, 5066, 1114, 2344]}]}
{"group_id": 72, "group_size": 12, "items": [{"qid": 4945, "question": "What is the size of the dataset? in A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates", "answer": ["5,415 sentences", "5,415 sentences"], "top_k_doc_id": [2409, 7671, 7672, 2410, 7673, 2157, 6741, 7674, 7675, 321, 7499, 7416, 330, 3486, 7676], "orig_top_k_doc_id": [7671, 7675, 7673, 7672, 7674, 6741, 321, 2409, 2157, 2410, 7416, 7499, 3486, 330, 7676]}, {"qid": 4946, "question": "What models are trained? in A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates", "answer": ["SVM classifier with an RBF kernel, deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification", "Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) "], "top_k_doc_id": [2409, 7671, 7672, 2410, 7673, 2157, 6741, 7674, 7675, 321, 7499, 7416, 330, 3486, 7114], "orig_top_k_doc_id": [7671, 7675, 7673, 7672, 7674, 6741, 321, 2409, 2157, 2410, 7416, 3486, 7499, 330, 7114]}, {"qid": 4947, "question": "Does the baseline use any contextual information? in A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates", "answer": ["No", "No"], "top_k_doc_id": [2409, 7671, 7672, 2410, 7673, 2157, 6741, 7674, 7675, 321, 7499, 7416, 7676, 7307, 7114], "orig_top_k_doc_id": [7671, 7675, 7673, 7672, 7674, 6741, 2409, 321, 2157, 2410, 7676, 7307, 7114, 7416, 7499]}, {"qid": 4948, "question": "What is the strong rivaling system? in A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates", "answer": ["ClaimBuster ", "ClaimBuster"], "top_k_doc_id": [2409, 7671, 7672, 2410, 7673, 2157, 6741, 7674, 7675, 321, 7499, 7676, 3486, 493, 1645], "orig_top_k_doc_id": [7671, 7675, 7673, 7672, 7676, 7674, 6741, 2409, 321, 2157, 2410, 7499, 3486, 493, 1645]}, {"qid": 1688, "question": "How many speeches are in the dataset? in Analysis of Speeches in Indian Parliamentary Debates", "answer": ["5575 speeches"], "top_k_doc_id": [2409, 2075, 2076, 2410, 2411, 2412, 2667, 2668, 2670, 2671, 4884, 7416, 2074, 6360, 4885], "orig_top_k_doc_id": [2412, 2667, 2409, 2410, 7416, 2668, 2671, 2670, 2411, 2075, 4884, 2074, 6360, 2076, 4885]}, {"qid": 1690, "question": "Do any speeches not fall in these categories? in Analysis of Speeches in Indian Parliamentary Debates", "answer": ["No"], "top_k_doc_id": [2409, 2075, 2076, 2410, 2411, 2412, 2667, 2668, 2670, 2671, 4884, 7416, 2074, 6360, 2669], "orig_top_k_doc_id": [2409, 2412, 2410, 2667, 7416, 2671, 2668, 2075, 2670, 2411, 4884, 2669, 6360, 2076, 2074]}, {"qid": 4949, "question": "Where are the debates from? in A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates", "answer": ["four transcripts of the 2016 US election: one vice-presidential and three presidential debates", "the 2016 US presidential and vice-presidential debates"], "top_k_doc_id": [2409, 7671, 7672, 2410, 7673, 2157, 6741, 7674, 7675, 7416, 330, 7676, 2667, 240, 2412], "orig_top_k_doc_id": [7671, 7675, 7672, 7673, 7674, 6741, 2409, 2410, 7416, 2157, 330, 7676, 2667, 240, 2412]}, {"qid": 1213, "question": "What debate websites did they look at? in Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "answer": ["idebate.com, debatewise.org, procon.org"], "top_k_doc_id": [2409, 7671, 7672, 241, 1645, 1646, 1649, 334, 332, 6743, 333, 3287, 6365, 3288, 6741], "orig_top_k_doc_id": [1645, 1646, 334, 1649, 332, 6743, 333, 3287, 7671, 7672, 241, 2409, 6365, 3288, 6741]}, {"qid": 1217, "question": "What debate topics are included in the dataset? in Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "answer": ["Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law"], "top_k_doc_id": [2409, 7671, 7672, 241, 1645, 1646, 1649, 1647, 7499, 7673, 242, 1482, 4577, 2410, 5377], "orig_top_k_doc_id": [1645, 1647, 1646, 241, 2409, 1649, 7499, 7673, 242, 1482, 4577, 2410, 5377, 7671, 7672]}, {"qid": 1689, "question": "What classification models were used? in Analysis of Speeches in Indian Parliamentary Debates", "answer": ["fastText and SVM BIBREF16"], "top_k_doc_id": [2409, 2075, 2076, 2410, 2411, 2412, 2667, 2668, 2670, 2671, 4884, 7416, 2074, 6358, 330], "orig_top_k_doc_id": [2412, 2409, 2410, 2667, 7416, 2668, 2411, 2671, 2075, 2670, 2074, 4884, 2076, 6358, 330]}, {"qid": 2823, "question": "what debates dataset was used? in Conversational flow in Oxford-style debates", "answer": ["Intelligence Squared Debates", "\u201cIntelligence Squared Debates\u201d (IQ2 for short)"], "top_k_doc_id": [2409, 7671, 7672, 2410, 7673, 4940, 4942, 334, 242, 330, 331, 2667, 332, 2412, 2411], "orig_top_k_doc_id": [4940, 4942, 2410, 334, 2409, 242, 330, 7671, 331, 2667, 7672, 332, 2412, 2411, 7673]}, {"qid": 1834, "question": "what processing was done on the speeches before being parsed? in Database of Parliamentary Speeches in Ireland, 1919-2013", "answer": ["Remove numbers and interjections"], "top_k_doc_id": [2409, 2075, 2076, 2410, 2411, 2412, 2667, 2668, 2670, 2671, 4884, 7416, 2669, 3594, 4888], "orig_top_k_doc_id": [2667, 2668, 2412, 2410, 2409, 2075, 2670, 2671, 4884, 7416, 2411, 2669, 2076, 3594, 4888]}]}
{"group_id": 73, "group_size": 11, "items": [{"qid": 33, "question": "Which dataset do they use a starting point in generating fake reviews? in Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "answer": ["the Yelp Challenge dataset", "Yelp Challenge dataset BIBREF2"], "top_k_doc_id": [6616, 27, 28, 29, 30, 31, 32, 33, 34, 35, 476, 477, 2267, 6410, 3879], "orig_top_k_doc_id": [27, 33, 29, 28, 32, 30, 31, 6616, 476, 2267, 477, 34, 6410, 3879, 35]}, {"qid": 35, "question": "How does using NMT ensure generated reviews stay on topic? in Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "answer": ["No"], "top_k_doc_id": [6616, 27, 28, 29, 30, 31, 32, 33, 34, 35, 476, 477, 7159, 4444, 4445], "orig_top_k_doc_id": [27, 29, 33, 28, 32, 30, 31, 6616, 476, 477, 7159, 34, 4444, 35, 4445]}, {"qid": 34, "question": "Do they use a pretrained NMT model to help generating reviews? in Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "answer": ["No", "No"], "top_k_doc_id": [6616, 27, 28, 29, 30, 31, 32, 33, 34, 35, 476, 477, 2873, 5421, 4780], "orig_top_k_doc_id": [27, 33, 29, 28, 30, 32, 31, 6616, 476, 35, 477, 34, 2873, 5421, 4780]}, {"qid": 36, "question": "What kind of model do they use for detection? in Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "answer": ["AdaBoost-based classifier"], "top_k_doc_id": [6616, 27, 28, 29, 30, 31, 32, 33, 34, 2873, 2874, 5551, 476, 7159, 477], "orig_top_k_doc_id": [27, 33, 31, 28, 32, 29, 30, 6616, 476, 7159, 2874, 477, 34, 2873, 5551]}, {"qid": 37, "question": "Does their detection tool work better than human detection? in Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "answer": ["Yes"], "top_k_doc_id": [6616, 27, 28, 29, 30, 31, 32, 33, 34, 2873, 2874, 5551, 6410, 7499, 7500], "orig_top_k_doc_id": [27, 33, 32, 28, 31, 29, 5551, 34, 6616, 2873, 6410, 30, 7499, 2874, 7500]}, {"qid": 4159, "question": "what dataset was used? in Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness", "answer": ["WMT 2014,  UGC (User Generated Content)", "11.5k French reviews from Foursquare", "WMT 2014, UGC (User Generated Content)"], "top_k_doc_id": [6616, 2308, 5106, 6617, 6618, 6620, 2874, 605, 2310, 2875, 5107, 29, 1165, 28, 6661], "orig_top_k_doc_id": [6616, 6617, 2308, 6620, 5106, 6618, 5107, 2874, 29, 2310, 2875, 1165, 605, 28, 6661]}, {"qid": 4161, "question": "what automatic evaluation is performed? in Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness", "answer": ["BLEU BIBREF28, indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task,  task-related evaluation based on polysemous words", "BLEU, accuracy"], "top_k_doc_id": [6616, 2308, 5106, 6617, 6618, 6620, 2874, 605, 2310, 2875, 6910, 6320, 2215, 5455, 6619], "orig_top_k_doc_id": [6616, 6620, 6617, 2308, 5106, 6618, 2310, 6910, 2875, 6320, 2874, 2215, 5455, 605, 6619]}, {"qid": 4158, "question": "what baseline models are trained? in Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness", "answer": [" Transformer Big BIBREF26", "Transformer Big"], "top_k_doc_id": [6616, 2308, 5106, 6617, 6618, 6620, 2268, 5455, 5107, 1044, 2310, 2501, 2056, 2309, 5108], "orig_top_k_doc_id": [6616, 6617, 2308, 5106, 6620, 6618, 5107, 1044, 2310, 2501, 2268, 2056, 2309, 5108, 5455]}, {"qid": 4160, "question": "what are the human evaluation metrics? in Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness", "answer": [" translation quality.", "The outputs are ranked by human evaluators, the wins, ties and losses are counted, then the Wilcoxon signed-rank test is applied.", "number of wins, ties and losses, and apply the Wilcoxon signed-rank test"], "top_k_doc_id": [6616, 2308, 5106, 6617, 6618, 6620, 2874, 6619, 6674, 6320, 6910, 27, 3193, 1969, 3359], "orig_top_k_doc_id": [6616, 6620, 6617, 2308, 5106, 6619, 6618, 6674, 2874, 6320, 6910, 27, 3193, 1969, 3359]}, {"qid": 4162, "question": "what are the existing online systems? in Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness", "answer": ["Existing online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).", "Google Translate", "Google Translate, DeepL"], "top_k_doc_id": [6616, 2308, 5106, 6617, 6618, 6620, 2268, 5455, 27, 4669, 32, 5026, 2264, 2874, 34], "orig_top_k_doc_id": [6616, 6617, 2308, 6620, 27, 6618, 4669, 5106, 32, 2268, 5026, 5455, 2264, 2874, 34]}, {"qid": 38, "question": "How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk? in Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "answer": ["1,006 fake reviews and 994 real reviews"], "top_k_doc_id": [6616, 27, 28, 29, 30, 31, 32, 33, 35, 194, 476, 6068, 6173, 191, 185], "orig_top_k_doc_id": [27, 31, 32, 29, 33, 28, 35, 194, 30, 476, 6616, 6068, 6173, 191, 185]}]}
{"group_id": 74, "group_size": 11, "items": [{"qid": 281, "question": "What corpus was the source of the OpenIE extractions? in Answering Complex Questions Using Open Information Extraction", "answer": ["domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining", "No"], "top_k_doc_id": [2168, 1853, 5226, 2169, 7072, 7351, 1055, 1057, 2171, 5227, 1854, 4074, 5736, 854, 4646], "orig_top_k_doc_id": [1853, 2168, 5226, 2169, 2171, 7072, 1854, 5227, 854, 1055, 7351, 4074, 1057, 5736, 4646]}, {"qid": 285, "question": "What method was used to generate the OpenIE extractions? in Answering Complex Questions Using Open Information Extraction", "answer": ["for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$"], "top_k_doc_id": [2168, 1853, 5226, 2169, 7072, 7351, 1055, 1057, 2171, 5227, 1854, 4074, 5736, 1360, 1422], "orig_top_k_doc_id": [2168, 1853, 5226, 2171, 1854, 1055, 5227, 2169, 7072, 1360, 1057, 4074, 7351, 1422, 5736]}, {"qid": 288, "question": "What OpenIE method was used to generate the extractions? in Answering Complex Questions Using Open Information Extraction", "answer": ["for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$"], "top_k_doc_id": [2168, 1853, 5226, 2169, 7072, 7351, 1055, 1057, 2171, 5227, 1854, 4074, 5736, 1360, 1422], "orig_top_k_doc_id": [2168, 1853, 5226, 2171, 1854, 1055, 5227, 2169, 7072, 1360, 1057, 4074, 7351, 1422, 5736]}, {"qid": 284, "question": "Are the OpenIE extractions all triples? in Answering Complex Questions Using Open Information Extraction", "answer": ["No"], "top_k_doc_id": [2168, 1853, 5226, 2169, 7072, 7351, 1055, 1057, 2171, 5227, 1422, 1606, 2170, 4154, 4490], "orig_top_k_doc_id": [2171, 5226, 2169, 1853, 2168, 1422, 7072, 1606, 7351, 2170, 4154, 5227, 1055, 4490, 1057]}, {"qid": 287, "question": "What was the textual source to which OpenIE was applied? in Answering Complex Questions Using Open Information Extraction", "answer": ["domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining"], "top_k_doc_id": [2168, 1853, 5226, 2169, 7072, 7351, 5736, 1091, 5735, 3175, 1090, 1533, 854, 4490, 4716], "orig_top_k_doc_id": [1853, 2168, 5226, 7072, 5736, 1091, 5735, 3175, 1090, 1533, 854, 4490, 7351, 4716, 2169]}, {"qid": 835, "question": "What's the input representation of OpenIE tuples into the model? in Improving Open Information Extraction via Iterative Rank-Aware Learning", "answer": ["word embeddings"], "top_k_doc_id": [2168, 1853, 5226, 1055, 1855, 346, 854, 1854, 1533, 4716, 1606, 1422, 154, 295, 153], "orig_top_k_doc_id": [1055, 1853, 5226, 1855, 346, 854, 2168, 1854, 1533, 4716, 1606, 1422, 154, 295, 153]}, {"qid": 1075, "question": "Can this approach model n-ary relations? in QA4IE: A Question Answering based Framework for Information Extraction", "answer": ["No"], "top_k_doc_id": [2168, 1422, 4159, 4216, 1533, 1539, 4158, 5226, 1424, 5227, 1423, 1425, 1428, 2171, 2172], "orig_top_k_doc_id": [1422, 5226, 4159, 1424, 2168, 1533, 5227, 4216, 1423, 1539, 1425, 1428, 2171, 2172, 4158]}, {"qid": 1553, "question": "Were the OpenIE systems more accurate on some scientific disciplines than others? in Open Information Extraction on Scientific Text: An Evaluation", "answer": ["No"], "top_k_doc_id": [2168, 1853, 5226, 1533, 2169, 2170, 2171, 4008, 4105, 4106, 1924, 1921, 5147, 5152, 3857], "orig_top_k_doc_id": [2168, 2169, 2171, 4008, 4105, 1853, 1533, 4106, 1924, 1921, 5147, 5152, 2170, 3857, 5226]}, {"qid": 1555, "question": "Which OpenIE systems were used? in Open Information Extraction on Scientific Text: An Evaluation", "answer": ["OpenIE4 and MiniIE"], "top_k_doc_id": [2168, 1853, 5226, 1533, 2169, 2170, 2171, 4008, 4105, 4106, 854, 1855, 346, 5674, 5400], "orig_top_k_doc_id": [2168, 2171, 2169, 1853, 1533, 5226, 854, 1855, 4008, 2170, 4106, 346, 5674, 5400, 4105]}, {"qid": 1800, "question": "Do they consider relations other than binary relations? in Vietnamese Open Information Extraction", "answer": ["Yes"], "top_k_doc_id": [2168, 1422, 4159, 4216, 1533, 1539, 4158, 2959, 1540, 301, 1605, 1606, 4074, 1603, 4719], "orig_top_k_doc_id": [2959, 2168, 4159, 1540, 1422, 301, 1539, 1605, 4216, 1606, 1533, 4158, 4074, 1603, 4719]}, {"qid": 688, "question": "Do they evaluate on relation extraction? in Information Extraction in Illicit Domains", "answer": ["No"], "top_k_doc_id": [2168, 1422, 4159, 4216, 853, 854, 855, 860, 7610, 1760, 6905, 5675, 1339, 3743, 7055], "orig_top_k_doc_id": [853, 854, 855, 860, 4159, 7610, 1760, 6905, 5675, 1339, 1422, 3743, 2168, 4216, 7055]}]}
{"group_id": 75, "group_size": 11, "items": [{"qid": 520, "question": "What baselines did they compare their model with? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 3906, 5793], "orig_top_k_doc_id": [637, 640, 638, 641, 642, 639, 3905, 1541, 5832, 5834, 318, 317, 6347, 5793, 3906]}, {"qid": 521, "question": "What was the performance of their model? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 3906, 1296], "orig_top_k_doc_id": [637, 640, 638, 641, 642, 639, 5832, 1541, 3905, 318, 5834, 317, 6347, 3906, 1296]}, {"qid": 522, "question": "What evaluation metrics are used? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["exact match, f1 score, edit distance and goal match"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 3906, 1296], "orig_top_k_doc_id": [640, 637, 638, 641, 642, 639, 1541, 5832, 3905, 318, 5834, 6347, 317, 3906, 1296]}, {"qid": 525, "question": "What language is the experiment done in? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["english language"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 3906, 1296], "orig_top_k_doc_id": [637, 640, 638, 641, 642, 639, 3905, 1541, 5832, 318, 317, 5834, 6347, 3906, 1296]}, {"qid": 519, "question": "By how much did their model outperform the baseline? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 3906, 314], "orig_top_k_doc_id": [637, 640, 638, 641, 642, 639, 3905, 317, 1541, 5832, 318, 5834, 6347, 314, 3906]}, {"qid": 524, "question": "How were the navigation instructions collected? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["using Amazon Mechanical Turk using simulated environments with topological maps"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 1296, 7038], "orig_top_k_doc_id": [637, 640, 638, 642, 641, 639, 5832, 1541, 5834, 3905, 318, 317, 6347, 1296, 7038]}, {"qid": 523, "question": "Did the authors use a crowdsourcing platform? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["Yes"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 317, 6347, 1074, 899], "orig_top_k_doc_id": [637, 640, 638, 642, 641, 639, 3905, 1541, 5832, 318, 5834, 1074, 899, 317, 6347]}, {"qid": 518, "question": "Did the collection process use a WoZ method? in Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation", "answer": ["No"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 637, 638, 639, 1541, 3905, 1296, 1073, 899, 1074], "orig_top_k_doc_id": [637, 640, 638, 641, 642, 639, 3905, 5832, 1541, 1296, 1073, 318, 899, 5834, 1074]}, {"qid": 3531, "question": "How well did the baseline perform? in RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation", "answer": ["accuracy of 30.3% on single sentences and 0.3 on complete paragraphs"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 317, 1296, 3882, 3883, 5833, 7037, 2984, 5793, 7356], "orig_top_k_doc_id": [5832, 5834, 5833, 1296, 317, 318, 3882, 7037, 642, 640, 641, 2984, 3883, 5793, 7356]}, {"qid": 3532, "question": "What is the baseline? in RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation", "answer": ["NO-MOVE, RANDOM, JUMP", "NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear"], "top_k_doc_id": [640, 641, 318, 642, 5832, 5834, 317, 1296, 3882, 3883, 5833, 7037, 411, 410, 637], "orig_top_k_doc_id": [5832, 5834, 5833, 1296, 317, 7037, 318, 642, 640, 641, 411, 3882, 3883, 410, 637]}, {"qid": 2458, "question": "How is module that analyzes behavioral state trained? in Behavior Gated Language Models", "answer": ["pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus"], "top_k_doc_id": [640, 641, 4096, 4095, 4582, 4581, 639, 5495, 3507, 6804, 4097, 6272, 7297, 4878, 4552], "orig_top_k_doc_id": [4096, 4095, 4582, 4581, 639, 640, 5495, 3507, 641, 6804, 4097, 6272, 7297, 4878, 4552]}]}
{"group_id": 76, "group_size": 11, "items": [{"qid": 791, "question": "How does proposed word embeddings compare to Sindhi fastText word representations? in A New Corpus for Low-Resourced Sindhi Language with Word Embeddings", "answer": ["Proposed SG model vs SINDHI FASTTEXT:\nAverage cosine similarity score: 0.650 vs 0.388\nAverage semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391"], "top_k_doc_id": [50, 45, 986, 987, 988, 990, 993, 994, 51, 991, 992, 48, 68, 5302, 2448], "orig_top_k_doc_id": [986, 994, 987, 993, 988, 990, 992, 991, 50, 45, 48, 68, 51, 5302, 2448]}, {"qid": 792, "question": "Are trained word embeddings used for any other NLP task? in A New Corpus for Low-Resourced Sindhi Language with Word Embeddings", "answer": ["No"], "top_k_doc_id": [50, 45, 986, 987, 988, 990, 993, 994, 51, 991, 992, 48, 68, 4695, 5048], "orig_top_k_doc_id": [986, 994, 988, 987, 993, 45, 50, 990, 68, 991, 992, 48, 4695, 51, 5048]}, {"qid": 3102, "question": "Which models/frameworks do they compare to? in A Novel Approach for Effective Learning in Low Resourced Scenarios", "answer": ["MLP", "Eusboost, MWMOTE"], "top_k_doc_id": [50, 4694, 4695, 4696, 5835, 45, 68, 986, 994, 2836, 5026, 7232, 7514, 6943, 5263], "orig_top_k_doc_id": [4695, 5835, 994, 986, 50, 4694, 5026, 4696, 68, 45, 2836, 7232, 7514, 6943, 5263]}, {"qid": 3105, "question": "Do they use pretrained models? in A Novel Approach for Effective Learning in Low Resourced Scenarios", "answer": ["No"], "top_k_doc_id": [50, 4694, 4695, 4696, 5835, 45, 68, 986, 994, 2836, 5026, 1593, 5716, 3160, 3273], "orig_top_k_doc_id": [994, 4695, 5835, 1593, 5716, 4694, 50, 986, 3160, 3273, 68, 4696, 5026, 2836, 45]}, {"qid": 793, "question": "How many uniue words are in the dataset? in A New Corpus for Low-Resourced Sindhi Language with Word Embeddings", "answer": ["908456 unique words are available in collected corpus."], "top_k_doc_id": [50, 45, 986, 987, 988, 990, 993, 994, 51, 991, 992, 4695, 4696, 5835, 5838], "orig_top_k_doc_id": [986, 994, 987, 988, 993, 990, 45, 4695, 4696, 991, 50, 51, 992, 5835, 5838]}, {"qid": 3103, "question": "Which classification algorithm do they use for s2sL? in A Novel Approach for Effective Learning in Low Resourced Scenarios", "answer": ["MLP", "MLP"], "top_k_doc_id": [50, 4694, 4695, 4696, 5835, 45, 68, 986, 994, 5263, 5264, 5262, 1041, 2477, 380], "orig_top_k_doc_id": [5263, 5264, 5262, 5835, 994, 4695, 68, 4694, 1041, 4696, 2477, 380, 986, 50, 45]}, {"qid": 45, "question": "What turn out to be more important high volume or high quality data? in Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi", "answer": ["only high-quality data helps", "high-quality"], "top_k_doc_id": [50, 45, 47, 48, 49, 51, 2836, 46, 5716, 986, 395, 994, 612, 613, 3123], "orig_top_k_doc_id": [50, 45, 49, 48, 51, 47, 46, 2836, 986, 5716, 395, 994, 612, 613, 3123]}, {"qid": 47, "question": "What two architectures are used? in Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi", "answer": ["fastText, CWE-LP"], "top_k_doc_id": [50, 45, 47, 48, 49, 51, 2836, 46, 5716, 1043, 2330, 2329, 1796, 4696, 5835], "orig_top_k_doc_id": [50, 48, 45, 49, 51, 47, 2836, 46, 1043, 2330, 2329, 1796, 4696, 5835, 5716]}, {"qid": 794, "question": "How is the data collected, which web resources were used? in A New Corpus for Low-Resourced Sindhi Language with Word Embeddings", "answer": ["daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter"], "top_k_doc_id": [50, 45, 986, 987, 988, 990, 993, 994, 1043, 4695, 7190, 7285, 68, 4437, 3402], "orig_top_k_doc_id": [986, 988, 994, 987, 993, 990, 45, 1043, 4695, 7190, 7285, 50, 68, 4437, 3402]}, {"qid": 3104, "question": "Up to how many samples do they experiment with? in A Novel Approach for Effective Learning in Low Resourced Scenarios", "answer": ["535", "we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier"], "top_k_doc_id": [50, 4694, 4695, 4696, 5835, 5263, 1043, 1593, 3634, 2836, 2418, 4205, 1216, 4692, 1991], "orig_top_k_doc_id": [5835, 4695, 50, 4696, 5263, 1043, 1593, 3634, 4694, 2836, 2418, 4205, 1216, 4692, 1991]}, {"qid": 46, "question": "How much is model improved by massive data and how much by quality? in Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi", "answer": ["No"], "top_k_doc_id": [50, 45, 47, 48, 49, 51, 2836, 7746, 7814, 6656, 613, 1043, 4695, 6657, 2447], "orig_top_k_doc_id": [50, 45, 51, 48, 49, 2836, 7746, 47, 7814, 6656, 613, 1043, 4695, 6657, 2447]}]}
{"group_id": 77, "group_size": 11, "items": [{"qid": 1052, "question": "How many different subjects does the dataset contain? in Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "answer": ["No"], "top_k_doc_id": [1375, 1376, 2765, 2766, 5979, 395, 3538, 1661, 4515, 4516, 1510, 5450, 6725, 594, 876], "orig_top_k_doc_id": [1375, 2765, 1376, 2766, 1510, 5450, 3538, 1661, 395, 5979, 6725, 4515, 4516, 594, 876]}, {"qid": 1054, "question": "How long is the dataset? in Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "answer": ["2000"], "top_k_doc_id": [1375, 1376, 2765, 2766, 5979, 395, 3538, 1661, 4515, 4516, 4517, 6770, 4292, 3926, 3587], "orig_top_k_doc_id": [1375, 2765, 1376, 2766, 1661, 5979, 4517, 4516, 4515, 3538, 395, 6770, 4292, 3926, 3587]}, {"qid": 1882, "question": "Is the corpus annotated? in Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish", "answer": ["No"], "top_k_doc_id": [1375, 1376, 2765, 2766, 2958, 986, 50, 2973, 5957, 1148, 6181, 6640, 2957, 4831, 1149], "orig_top_k_doc_id": [2765, 1375, 1376, 2766, 2958, 986, 6181, 2973, 50, 1148, 6640, 5957, 2957, 4831, 1149]}, {"qid": 1885, "question": "Is the corpus annotated with a phonetic transcription? in Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish", "answer": ["No"], "top_k_doc_id": [1375, 1376, 2765, 2766, 2958, 986, 50, 2973, 5957, 1148, 6181, 6640, 1163, 1286, 5210], "orig_top_k_doc_id": [2765, 1375, 1376, 2766, 2958, 986, 1163, 1286, 6181, 5210, 2973, 50, 1148, 6640, 5957]}, {"qid": 1053, "question": "How many annotators participated? in Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "answer": ["1"], "top_k_doc_id": [1375, 1376, 2765, 2766, 5979, 395, 3538, 7327, 1714, 7182, 414, 3587, 6770, 1713, 3934], "orig_top_k_doc_id": [1375, 2765, 1376, 2766, 5979, 7327, 1714, 7182, 414, 3538, 3587, 6770, 1713, 3934, 395]}, {"qid": 1883, "question": "How is the corpus normalized? in Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish", "answer": ["by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography"], "top_k_doc_id": [1375, 1376, 2765, 2766, 2958, 986, 50, 2973, 5957, 1712, 4696, 51, 7662, 1149, 3608], "orig_top_k_doc_id": [2765, 1375, 1376, 2766, 50, 5957, 1712, 4696, 2958, 51, 7662, 1149, 2973, 986, 3608]}, {"qid": 1049, "question": "What are the results of the experiment? in Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "answer": ["They were able to create a language model from the dataset, but did not test."], "top_k_doc_id": [1375, 1376, 2765, 2766, 5979, 1500, 2126, 3007, 3538, 4864, 5678, 4009, 3502, 6894, 873], "orig_top_k_doc_id": [1375, 2765, 1376, 2766, 3007, 5979, 2126, 5678, 4864, 1500, 3538, 4009, 3502, 6894, 873]}, {"qid": 1050, "question": "How was the dataset collected? in Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "answer": ["extracted text from Sorani Kurdish books of primary school and randomly created sentences"], "top_k_doc_id": [1375, 1376, 2765, 2766, 5979, 395, 1661, 6176, 5391, 4140, 3007, 7182, 5015, 414, 7008], "orig_top_k_doc_id": [1375, 1376, 2765, 2766, 1661, 6176, 5979, 5391, 4140, 3007, 7182, 5015, 414, 395, 7008]}, {"qid": 1051, "question": "What is the size of the dataset? in Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "answer": ["2000 sentences"], "top_k_doc_id": [1375, 1376, 2765, 2766, 5979, 1500, 2126, 3007, 3538, 4864, 1661, 3926, 19, 384, 5377], "orig_top_k_doc_id": [1375, 2765, 1376, 2766, 5979, 4864, 3007, 2126, 1500, 1661, 3926, 19, 384, 5377, 3538]}, {"qid": 1886, "question": "Is the corpus annotated with Part-of-Speech tags? in Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish", "answer": ["No"], "top_k_doc_id": [1375, 1376, 2765, 2766, 2958, 986, 4437, 1712, 4014, 4880, 1148, 6181, 4433, 5275, 2976], "orig_top_k_doc_id": [2765, 2958, 1375, 1376, 986, 4437, 1712, 4014, 4880, 2766, 1148, 6181, 4433, 5275, 2976]}, {"qid": 1884, "question": "What are the 12 categories devised? in Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish", "answer": ["Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study"], "top_k_doc_id": [1375, 1376, 2765, 2766, 2958, 5327, 1712, 4437, 5957, 6731, 5979, 7572, 4802, 2973, 6013], "orig_top_k_doc_id": [2765, 1375, 1376, 5327, 1712, 4437, 5957, 2766, 6731, 2958, 5979, 7572, 4802, 2973, 6013]}]}
{"group_id": 78, "group_size": 11, "items": [{"qid": 1123, "question": "How is gang membership verified? in Finding Street Gang Members on Twitter", "answer": ["Manual verification"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3135, 5156], "orig_top_k_doc_id": [1480, 1481, 1482, 2343, 1483, 1486, 2345, 2344, 1484, 1485, 2347, 2346, 1487, 3135, 5156]}, {"qid": 1124, "question": "Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others? in Finding Street Gang Members on Twitter", "answer": ["No"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3135, 938], "orig_top_k_doc_id": [1480, 2343, 1481, 1482, 1486, 1483, 2344, 2345, 1484, 1485, 2346, 1487, 2347, 3135, 938]}, {"qid": 1119, "question": "What are the differences in the use of emojis between gang member and the rest of the Twitter population? in Finding Street Gang Members on Twitter", "answer": ["32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them, gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3135, 1957], "orig_top_k_doc_id": [1486, 1481, 1482, 2343, 1480, 1483, 1484, 2344, 1485, 2345, 2346, 2347, 1487, 1957, 3135]}, {"qid": 1120, "question": "What are the differences in the use of YouTube links between gang member and the rest of the Twitter population? in Finding Street Gang Members on Twitter", "answer": ["76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3135, 1957], "orig_top_k_doc_id": [1483, 1481, 2343, 1480, 1484, 2345, 2344, 1482, 1485, 2346, 1486, 2347, 1487, 3135, 1957]}, {"qid": 1121, "question": "What are the differences in the use of images between gang member and the rest of the Twitter population? in Finding Street Gang Members on Twitter", "answer": ["user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3135, 1957], "orig_top_k_doc_id": [1482, 2343, 1481, 1480, 1483, 1486, 2345, 2344, 1484, 1485, 2347, 2346, 1487, 3135, 1957]}, {"qid": 1122, "question": "What are the differences in language use between gang member and the rest of the Twitter population? in Finding Street Gang Members on Twitter", "answer": ["Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3135, 1957], "orig_top_k_doc_id": [1482, 1481, 2343, 1483, 1480, 1486, 2344, 1485, 2345, 1484, 2346, 2347, 1487, 3135, 1957]}, {"qid": 1118, "question": "Do they evaluate only on English datasets? in Finding Street Gang Members on Twitter", "answer": ["No"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 6837, 4050], "orig_top_k_doc_id": [2343, 1480, 1481, 1482, 1486, 1483, 1485, 2346, 1484, 2345, 2344, 2347, 1487, 6837, 4050]}, {"qid": 1648, "question": "Do the authors report on English datasets only? in Word Embeddings to Enhance Twitter Gang Member Profile Identification", "answer": ["Yes"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 6522, 7260], "orig_top_k_doc_id": [2343, 2346, 2347, 1482, 2345, 2344, 1486, 1485, 1483, 1487, 1481, 1480, 1484, 6522, 7260]}, {"qid": 1649, "question": "Which supervised learning algorithms are used in the experiments? in Word Embeddings to Enhance Twitter Gang Member Profile Identification", "answer": ["Logistic Regression (LR), Random Forest (RF), Support Vector Machines (SVM)"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 3300, 6817], "orig_top_k_doc_id": [2346, 2347, 1481, 2344, 2343, 2345, 1485, 1484, 1483, 1486, 1482, 1480, 1487, 3300, 6817]}, {"qid": 1650, "question": "How in YouTube content translated into a vector format? in Word Embeddings to Enhance Twitter Gang Member Profile Identification", "answer": ["words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 520, 7125], "orig_top_k_doc_id": [2345, 1484, 2343, 2344, 2346, 1483, 1481, 1480, 1485, 1486, 2347, 1482, 1487, 520, 7125]}, {"qid": 1651, "question": "How is the ground truth of gang membership established in this dataset? in Word Embeddings to Enhance Twitter Gang Member Profile Identification", "answer": [" text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles"], "top_k_doc_id": [1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 2343, 2344, 2345, 2346, 2347, 6076, 522], "orig_top_k_doc_id": [2343, 2345, 1480, 2344, 2346, 1486, 2347, 1483, 1481, 1482, 1484, 1485, 1487, 6076, 522]}]}
{"group_id": 79, "group_size": 11, "items": [{"qid": 1207, "question": "Do they study frequent user responses to help automate modelling of those? in Finding Dominant User Utterances And System Responses in Conversations", "answer": ["Yes"], "top_k_doc_id": [102, 1632, 3190, 3191, 7217, 101, 103, 5916, 5953, 1170, 1636, 1635, 7371, 5441, 4444], "orig_top_k_doc_id": [1632, 1636, 1635, 3190, 5953, 7217, 102, 103, 5916, 7371, 5441, 4444, 3191, 1170, 101]}, {"qid": 1208, "question": "How do they divide text into utterances? in Finding Dominant User Utterances And System Responses in Conversations", "answer": ["No"], "top_k_doc_id": [102, 1632, 3190, 3191, 7217, 101, 103, 5916, 5953, 1170, 1636, 5430, 6859, 5258, 7375], "orig_top_k_doc_id": [3190, 1632, 5953, 102, 103, 7217, 3191, 5430, 1170, 6859, 5258, 101, 1636, 5916, 7375]}, {"qid": 87, "question": "What is the sample size of people used to measure user satisfaction? in Gunrock: A Social Bot for Complex and Engaging Long Conversations", "answer": ["34,432 user conversations", "34,432 "], "top_k_doc_id": [102, 101, 103, 104, 4441, 5427, 7218, 2967, 4128, 7219, 7222, 6155, 7224, 4581, 5425], "orig_top_k_doc_id": [103, 101, 7218, 4441, 102, 7222, 7219, 2967, 4128, 5427, 7224, 4581, 104, 6155, 5425]}, {"qid": 89, "question": "What the system designs introduced? in Gunrock: A Social Bot for Complex and Engaging Long Conversations", "answer": ["Amazon Conversational Bot Toolkit, natural language understanding (NLU) (nlu) module, dialog manager, knowledge bases, natural language generation (NLG) (nlg) module, text to speech (TTS) (tts)"], "top_k_doc_id": [102, 101, 103, 104, 4441, 5427, 7218, 824, 4120, 6653, 6654, 6655, 2967, 7758, 5440], "orig_top_k_doc_id": [103, 101, 102, 4441, 104, 7218, 4120, 5427, 2967, 824, 6655, 6653, 6654, 7758, 5440]}, {"qid": 90, "question": "Do they specify the model they use for Gunrock? in Gunrock: A Social Bot for Complex and Engaging Long Conversations", "answer": ["No"], "top_k_doc_id": [102, 101, 103, 104, 4441, 5427, 7218, 824, 4120, 6653, 6654, 6655, 5917, 1734, 5428], "orig_top_k_doc_id": [103, 101, 102, 104, 4441, 5917, 7218, 6653, 6654, 824, 6655, 1734, 5428, 4120, 5427]}, {"qid": 91, "question": "Do they gather explicit user satisfaction data on Gunrock? in Gunrock: A Social Bot for Complex and Engaging Long Conversations", "answer": ["Yes"], "top_k_doc_id": [102, 101, 103, 104, 4441, 5427, 7218, 2967, 4128, 7219, 7222, 6155, 5440, 7217, 1718], "orig_top_k_doc_id": [103, 101, 102, 104, 7218, 4128, 2967, 4441, 7222, 7219, 5440, 5427, 7217, 6155, 1718]}, {"qid": 1210, "question": "How do they generate the synthetic dataset? in Finding Dominant User Utterances And System Responses in Conversations", "answer": ["using generative process"], "top_k_doc_id": [102, 1632, 3190, 3191, 7217, 101, 103, 5916, 5953, 7375, 4444, 4443, 4719, 3537, 4485], "orig_top_k_doc_id": [1632, 3190, 7217, 5916, 3191, 102, 7375, 5953, 4444, 4443, 103, 4719, 101, 3537, 4485]}, {"qid": 88, "question": "What are all the metrics to measure user engagement? in Gunrock: A Social Bot for Complex and Engaging Long Conversations", "answer": ["overall rating, mean number of turns", "overall rating, mean number of turns"], "top_k_doc_id": [102, 101, 103, 104, 4441, 5427, 7218, 824, 2967, 4581, 5920, 17, 805, 5440, 7758], "orig_top_k_doc_id": [103, 102, 101, 4441, 2967, 4581, 5920, 7218, 17, 805, 104, 824, 5440, 7758, 5427]}, {"qid": 92, "question": "How do they correlate user backstory queries to user satisfaction? in Gunrock: A Social Bot for Complex and Engaging Long Conversations", "answer": ["modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions"], "top_k_doc_id": [102, 101, 103, 104, 4441, 5427, 7218, 2967, 4128, 7219, 7222, 824, 145, 7758, 4317], "orig_top_k_doc_id": [103, 102, 101, 7218, 104, 4441, 824, 2967, 4128, 7219, 145, 7758, 5427, 7222, 4317]}, {"qid": 1209, "question": "Do they use the same distance metric for both the SimCluster and K-means algorithm? in Finding Dominant User Utterances And System Responses in Conversations", "answer": ["Yes"], "top_k_doc_id": [102, 1632, 3190, 3191, 7217, 1635, 1636, 1634, 1633, 7379, 4485, 4443, 3476, 5920, 3192], "orig_top_k_doc_id": [1635, 1636, 1632, 1634, 1633, 7379, 4485, 3190, 7217, 4443, 3191, 102, 3476, 5920, 3192]}, {"qid": 2822, "question": "what aspects of conversation flow do they look at? in Conversational flow in Oxford-style debates", "answer": ["The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.", "\u2014promoting one's own points and attacking the opponents' points"], "top_k_doc_id": [102, 1632, 4940, 4942, 5647, 103, 4941, 2888, 5651, 1711, 5802, 101, 1712, 1817, 4442], "orig_top_k_doc_id": [4940, 4942, 5647, 102, 103, 4941, 1632, 2888, 5651, 1711, 5802, 101, 1712, 1817, 4442]}]}
{"group_id": 80, "group_size": 11, "items": [{"qid": 1232, "question": "What metrics are used to measure performance of models? in Generative Dialog Policy for Task-oriented Dialog Systems", "answer": ["BPRA, APRA, BLEU"], "top_k_doc_id": [4124, 5917, 7839, 1034, 1471, 1674, 1677, 1678, 6588, 7843, 898, 4550, 3193, 7476, 6589], "orig_top_k_doc_id": [4124, 7839, 1034, 7843, 3193, 1677, 5917, 6588, 1471, 4550, 7476, 6589, 1674, 1678, 898]}, {"qid": 1233, "question": "How much is proposed model better than baselines in performed experiments? in Generative Dialog Policy for Task-oriented Dialog Systems", "answer": ["most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)\nGDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)\nGDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)"], "top_k_doc_id": [4124, 5917, 7839, 1034, 1471, 1674, 1677, 1678, 6588, 7843, 1029, 1675, 3193, 6589, 2197], "orig_top_k_doc_id": [1678, 1471, 4124, 1677, 7839, 1034, 1674, 7843, 6588, 6589, 3193, 5917, 1029, 1675, 2197]}, {"qid": 1234, "question": "What are state-of-the-art baselines? in Generative Dialog Policy for Task-oriented Dialog Systems", "answer": ["E2ECM, CDM"], "top_k_doc_id": [4124, 5917, 7839, 1034, 1471, 1674, 1677, 1678, 6588, 7843, 1029, 1675, 3193, 7840, 7504], "orig_top_k_doc_id": [1034, 4124, 1678, 1471, 7839, 1674, 7843, 1677, 1029, 5917, 6588, 3193, 7840, 1675, 7504]}, {"qid": 1235, "question": "What two benchmark datasets are used? in Generative Dialog Policy for Task-oriented Dialog Systems", "answer": ["DSTC2, Maluuba"], "top_k_doc_id": [4124, 5917, 7839, 1034, 1471, 1674, 1677, 1678, 6588, 7843, 898, 4550, 1029, 7504, 1475], "orig_top_k_doc_id": [1471, 7839, 5917, 1674, 4124, 1034, 1678, 7843, 4550, 6588, 1029, 898, 1677, 7504, 1475]}, {"qid": 5034, "question": "Which neural network architecture do they use for the dialog agent and user simulator? in Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models", "answer": ["LSTM", "Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM., State of the dialog agent is maintained in the LSTM BIBREF35"], "top_k_doc_id": [4124, 5917, 7839, 4546, 6587, 6588, 7840, 7843, 7842, 2197, 3773, 3774, 7841, 3530, 4545], "orig_top_k_doc_id": [7839, 7843, 7840, 7842, 7841, 6588, 3773, 4124, 3774, 3530, 6587, 5917, 4546, 2197, 4545]}, {"qid": 5035, "question": "Do they create the basic dialog agent and basic user simulator separately? in Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models", "answer": ["Yes", "Yes"], "top_k_doc_id": [4124, 5917, 7839, 4546, 6587, 6588, 7840, 7843, 7842, 2197, 3773, 3774, 7841, 3772, 7504], "orig_top_k_doc_id": [7839, 7840, 7843, 7842, 7841, 6588, 3773, 5917, 4124, 3774, 3772, 4546, 6587, 2197, 7504]}, {"qid": 5033, "question": "Do they jointly optimize both agents? in Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models", "answer": ["Yes", "Yes"], "top_k_doc_id": [4124, 5917, 7839, 4546, 6587, 6588, 7840, 7843, 7842, 6589, 7504, 6585, 3357, 318, 3775], "orig_top_k_doc_id": [7839, 7843, 7840, 7842, 6588, 4124, 6587, 5917, 6589, 7504, 6585, 3357, 4546, 318, 3775]}, {"qid": 2551, "question": "How large is the Dialog State Tracking Dataset? in Learning End-to-End Goal-Oriented Dialog", "answer": ["1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs"], "top_k_doc_id": [4124, 5917, 7839, 1034, 1471, 7840, 4545, 4546, 3193, 7842, 2197, 1029, 3191, 898, 4129], "orig_top_k_doc_id": [7839, 7840, 4545, 4546, 4124, 3193, 5917, 1034, 1471, 7842, 2197, 1029, 3191, 898, 4129]}, {"qid": 5032, "question": "By how much do they improve upon supervised traning methods? in Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural Dialog Models", "answer": ["A2C and REINFORCE-joint for joint policy optimization achieve the improvement over SL baseline of 29.4% and  25.7 susses rate,  1.21  AND 1.28 AvgRevard and  0.25 and  -1.34 AvgSucccess Turn Size, respectively.", "agent-update-only models using REINFORCE and A2C achieve similar results, outperforming the baseline model by 14.9% and 15.3% respectively,  jointly optimized models improved the performance further"], "top_k_doc_id": [4124, 5917, 7839, 4546, 6587, 6588, 7840, 7843, 3507, 4545, 3357, 4550, 1471, 1678, 2197], "orig_top_k_doc_id": [7839, 7843, 6588, 7840, 3507, 4124, 4546, 4545, 5917, 3357, 6587, 4550, 1471, 1678, 2197]}, {"qid": 2353, "question": "How did they measure effectiveness? in Towards Personalized Dialog Policies for Conversational Skill Discovery", "answer": ["number of dialogs that resulted in launching a skill divided by total number of dialogs"], "top_k_doc_id": [4124, 495, 3772, 4441, 6588, 6589, 6876, 7840, 3775, 3773, 3774, 4656, 5577, 3810, 103], "orig_top_k_doc_id": [3772, 3775, 3773, 3774, 6589, 495, 4441, 6588, 4656, 4124, 7840, 5577, 3810, 6876, 103]}, {"qid": 2463, "question": "What datasets did they use? in Learning Personalized End-to-End Goal-Oriented Dialog", "answer": ["the personalized bAbI dialog dataset"], "top_k_doc_id": [4124, 495, 3772, 4441, 6588, 6589, 6876, 7840, 4129, 4128, 4125, 7839, 5917, 6585, 4545], "orig_top_k_doc_id": [4124, 4129, 4128, 4125, 3772, 4441, 7839, 6588, 7840, 5917, 6589, 495, 6585, 6876, 4545]}]}
{"group_id": 81, "group_size": 11, "items": [{"qid": 1255, "question": "Do they evaluate only on English datasets? in \"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts", "answer": ["Yes"], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1717, 1719, 5059, 3680, 5058, 3190, 824], "orig_top_k_doc_id": [1711, 1712, 1713, 1718, 1715, 1719, 1714, 3537, 5059, 3190, 824, 2967, 1717, 5058, 3680]}, {"qid": 1256, "question": "Which patterns and rules are derived? in \"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts", "answer": ["A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation,  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems , asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers, Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers"], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1717, 1719, 5059, 3190, 5918, 6804, 5058], "orig_top_k_doc_id": [1711, 1718, 1712, 1713, 1719, 1715, 1717, 1714, 3537, 5059, 2967, 5918, 3190, 6804, 5058]}, {"qid": 1258, "question": "Which Twitter customer service industries are investigated? in \"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts", "answer": [" four different companies in the telecommunication, electronics, and insurance industries"], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1717, 1719, 5059, 3190, 5918, 1635, 6159], "orig_top_k_doc_id": [1711, 1713, 1712, 1718, 1719, 2967, 1715, 1714, 3537, 1717, 1635, 3190, 5918, 5059, 6159]}, {"qid": 1259, "question": "Which dialogue acts are more suited to the twitter domain? in \"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts", "answer": ["overlapping dialogue acts"], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1717, 1719, 5059, 3680, 5058, 5057, 4298], "orig_top_k_doc_id": [1711, 1712, 1713, 1718, 1719, 1715, 1714, 1717, 5059, 5058, 3537, 5057, 3680, 2967, 4298]}, {"qid": 496, "question": "What other relations were found in the datasets? in Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.", "answer": ["Quotation (\u2303q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration', Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset, Acknowledgements (b) are mostly with positive or neutral, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP), Questions (qh, qw, qy and qy\u2303d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral, No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny)., Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'"], "top_k_doc_id": [1713, 1715, 589, 590, 591, 592, 2967, 3451, 6590, 1169, 1168, 3453, 7372, 2970, 1443], "orig_top_k_doc_id": [592, 589, 591, 1715, 1169, 1168, 3453, 590, 3451, 6590, 1713, 2967, 7372, 2970, 1443]}, {"qid": 497, "question": "How does the ensemble annotator extract the final label? in Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.", "answer": ["First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category."], "top_k_doc_id": [1713, 1715, 589, 590, 591, 592, 1711, 1714, 1716, 1718, 370, 2969, 8, 2970, 1933], "orig_top_k_doc_id": [592, 589, 591, 590, 1715, 1713, 1714, 370, 2969, 1711, 8, 2970, 1933, 1716, 1718]}, {"qid": 498, "question": "How were dialogue act labels defined? in Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.", "answer": ["Dialogue Act Markup in Several Layers (DAMSL) tag set"], "top_k_doc_id": [1713, 1715, 589, 590, 591, 592, 1711, 1714, 1716, 1718, 1712, 4298, 4296, 4294, 401], "orig_top_k_doc_id": [592, 589, 591, 1715, 590, 1712, 1713, 1714, 1711, 1716, 1718, 4298, 4296, 4294, 401]}, {"qid": 499, "question": "How many models were used? in Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.", "answer": ["five"], "top_k_doc_id": [1713, 1715, 589, 590, 591, 592, 2967, 3451, 6590, 1712, 1711, 2969, 1070, 1807, 1933], "orig_top_k_doc_id": [592, 589, 591, 1715, 6590, 1712, 1713, 590, 3451, 1711, 2969, 1070, 1807, 1933, 2967]}, {"qid": 1257, "question": "How are customer satisfaction, customer frustration and overall problem resolution data collected? in \"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts", "answer": ["By annotators on Amazon Mechanical Turk."], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1717, 1719, 1716, 2264, 4652, 3190, 6159], "orig_top_k_doc_id": [1711, 1718, 1719, 1717, 1712, 1713, 1715, 3537, 2967, 1714, 1716, 2264, 4652, 3190, 6159]}, {"qid": 2253, "question": "What datasets are used to evaluate the introduced method? in Lingke: A Fine-grained Multi-turn Chatbot for Customer Service", "answer": ["They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,\nincluding chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. "], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1817, 5425, 7760, 6548, 5918, 2968, 722], "orig_top_k_doc_id": [3537, 1711, 1713, 2967, 5425, 1712, 1715, 1718, 1817, 6548, 5918, 2968, 722, 1714, 7760]}, {"qid": 2254, "question": "What are the results achieved from the introduced method? in Lingke: A Fine-grained Multi-turn Chatbot for Customer Service", "answer": ["Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates."], "top_k_doc_id": [1713, 1715, 1711, 1712, 1714, 1718, 2967, 3537, 1817, 5425, 7760, 3507, 7449, 5432, 7377], "orig_top_k_doc_id": [3537, 1711, 1713, 2967, 5425, 1715, 1718, 1712, 1817, 1714, 3507, 7449, 5432, 7377, 7760]}]}
{"group_id": 82, "group_size": 11, "items": [{"qid": 1324, "question": "How much training data is used? in A Density Ratio Approach to Language Model Fusion in End-to-End Automatic Speech Recognition", "answer": ["163,110,000 utterances"], "top_k_doc_id": [6110, 1623, 1812, 1813, 1814, 1815, 5564, 7138, 2351, 2354, 6968, 373, 5987, 4771, 2451], "orig_top_k_doc_id": [1812, 1815, 1814, 7138, 1813, 5987, 5564, 373, 6968, 4771, 2451, 2354, 2351, 6110, 1623]}, {"qid": 1325, "question": "How is the training data collected? in A Density Ratio Approach to Language Model Fusion in End-to-End Automatic Speech Recognition", "answer": ["from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering, from a Voice Search service"], "top_k_doc_id": [6110, 1623, 1812, 1813, 1814, 1815, 5564, 7138, 2351, 2354, 6968, 373, 5987, 6350, 4862], "orig_top_k_doc_id": [1812, 1815, 1814, 1813, 7138, 6110, 6968, 5564, 6350, 2354, 2351, 1623, 373, 5987, 4862]}, {"qid": 1323, "question": "What metrics are used for evaluation? in A Density Ratio Approach to Language Model Fusion in End-to-End Automatic Speech Recognition", "answer": ["word error rate"], "top_k_doc_id": [6110, 1623, 1812, 1813, 1814, 1815, 5564, 7138, 2351, 2354, 6968, 2922, 5765, 138, 4960], "orig_top_k_doc_id": [1812, 1815, 1814, 1813, 7138, 5564, 2354, 6110, 2922, 5765, 2351, 138, 6968, 1623, 4960]}, {"qid": 2782, "question": "what is the domain of the corpus? in Towards speech-to-text translation without speech recognition", "answer": ["No", "telephone calls"], "top_k_doc_id": [6110, 4921, 6112, 230, 3007, 4862, 4863, 5841, 3437, 5564, 226, 1623, 3976, 4928, 1593], "orig_top_k_doc_id": [230, 6110, 5841, 4862, 3007, 4921, 3437, 5564, 226, 4863, 6112, 1623, 3976, 4928, 1593]}, {"qid": 2784, "question": "what is the size of the speech corpus? in Towards speech-to-text translation without speech recognition", "answer": ["104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens", "104 telephone calls, which pair 11 hours of audio"], "top_k_doc_id": [6110, 4921, 6112, 230, 3007, 4862, 4863, 5841, 1899, 5765, 7661, 5764, 7687, 2451, 622], "orig_top_k_doc_id": [4862, 3007, 6110, 1899, 4863, 230, 6112, 4921, 5765, 7661, 5764, 7687, 2451, 622, 5841]}, {"qid": 3757, "question": "How is the sentence alignment quality evaluated? in LibriVoxDeEn: A Corpus for German-to-English Speech Translation and German Speech Recognition", "answer": ["Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text", "The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:\n\nWrong alignment\n\nPartial alignment with slightly compositional translational equivalence\n\nPartial alignment with compositional translation and additional or missing information\n\nCorrect alignment with compositional translation and few additional or missing information\n\nCorrect alignment and fully compositional translation\n\nThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.", "5-point scale used in KocabiyikogluETAL:18"], "top_k_doc_id": [6110, 4921, 6112, 3437, 5564, 5565, 6042, 6111, 7434, 7621, 3760, 3759, 248, 661, 4768], "orig_top_k_doc_id": [6111, 6110, 6112, 5564, 7434, 4921, 5565, 3760, 3437, 6042, 3759, 248, 661, 4768, 7621]}, {"qid": 3758, "question": "How is the speech alignment quality evaluated? in LibriVoxDeEn: A Corpus for German-to-English Speech Translation and German Speech Recognition", "answer": ["Through a 3-point scale by annotators.", "Wrong alignment, Partial alignment, some words or sentences may be missing, Correct alignment, allowing non-spoken syllables at start or end.", "The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability."], "top_k_doc_id": [6110, 4921, 6112, 3437, 5564, 5565, 6042, 6111, 7434, 7621, 2631, 2630, 1286, 4863, 3438], "orig_top_k_doc_id": [6111, 6110, 6112, 5564, 7434, 5565, 4921, 3437, 7621, 2631, 2630, 1286, 4863, 6042, 3438]}, {"qid": 1326, "question": "What language(s) is the model trained/tested on? in A Density Ratio Approach to Language Model Fusion in End-to-End Automatic Speech Recognition", "answer": ["No"], "top_k_doc_id": [6110, 1623, 1812, 1813, 1814, 1815, 5564, 7138, 4972, 4918, 1375, 5765, 3835, 2922, 4368], "orig_top_k_doc_id": [1812, 1815, 1814, 1813, 7138, 5564, 4972, 4918, 1375, 5765, 3835, 6110, 2922, 4368, 1623]}, {"qid": 2138, "question": "Is the model evaluated against a baseline? in From Speech-to-Speech Translation to Automatic Dubbing", "answer": ["No"], "top_k_doc_id": [6110, 3264, 3266, 3267, 3800, 5978, 7655, 5564, 1336, 1889, 7658, 7661, 6288, 6723, 1024], "orig_top_k_doc_id": [3266, 3264, 3267, 7655, 3800, 5564, 1336, 1889, 7658, 7661, 6110, 5978, 6288, 6723, 1024]}, {"qid": 2139, "question": "How many people are employed for the subjective evaluation? in From Speech-to-Speech Translation to Automatic Dubbing", "answer": ["14 volunteers"], "top_k_doc_id": [6110, 3264, 3266, 3267, 3800, 5978, 7655, 2922, 6134, 3574, 2930, 7350, 5765, 2531, 6770], "orig_top_k_doc_id": [3264, 3266, 3267, 5978, 2922, 6134, 3574, 2930, 3800, 6110, 7350, 5765, 2531, 6770, 7655]}, {"qid": 2870, "question": "How is segmentation quality evaluated? in Automatic Discourse Segmentation: an evaluation in French", "answer": ["Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.", "we compare the Annodis segmentation with the automatically produced segmentation"], "top_k_doc_id": [6110, 5032, 5033, 6960, 5034, 661, 3253, 6767, 3231, 1889, 6961, 6768, 5564, 3143, 4008], "orig_top_k_doc_id": [5032, 5033, 6960, 5034, 661, 3253, 6767, 3231, 1889, 6110, 6961, 6768, 5564, 3143, 4008]}]}
{"group_id": 83, "group_size": 11, "items": [{"qid": 1667, "question": "Do they compare simultaneous translation performance to regular machine translation? in Simultaneous Neural Machine Translation using Connectionist Temporal Classification", "answer": ["No"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 1750, 2373, 1753, 2375, 2374, 7656, 1751, 4377, 7659, 1752], "orig_top_k_doc_id": [2373, 7661, 7655, 7660, 2374, 7662, 2375, 1749, 1751, 1753, 1750, 7659, 1752, 4377, 7656]}, {"qid": 1668, "question": "Which metrics do they use to evaluate simultaneous translation? in Simultaneous Neural Machine Translation using Connectionist Temporal Classification", "answer": ["BLEU BIBREF8, RIBES BIBREF9, token-level delay"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 1750, 2373, 1753, 2375, 2374, 7656, 1751, 4377, 5450, 1764], "orig_top_k_doc_id": [2373, 2374, 7655, 7661, 7660, 2375, 1749, 7662, 4377, 1753, 1751, 1750, 5450, 7656, 1764]}, {"qid": 1280, "question": "What corpora is used? in How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?", "answer": ["IWSLT16, WMT15, NIST"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 1750, 2373, 1753, 2375, 2374, 7656, 1752, 7043, 2044, 1244], "orig_top_k_doc_id": [1749, 2375, 1750, 1753, 7661, 7655, 2373, 1752, 7660, 7662, 2374, 7043, 2044, 1244, 7656]}, {"qid": 1278, "question": "Has there been previous work on SNMT? in How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?", "answer": ["Yes"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 1750, 2373, 1753, 2375, 1752, 5672, 7659, 1244, 4540, 564], "orig_top_k_doc_id": [1750, 1749, 1753, 7661, 7655, 2375, 7660, 2373, 1752, 7662, 5672, 7659, 1244, 4540, 564]}, {"qid": 4932, "question": "Which datasets do they evaluate on? in DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting", "answer": ["NIST02, NIST03, NIST04, NIST05, NIST08", "2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 1750, 2373, 1247, 1751, 7656, 7658, 7659, 1753, 6041, 4377], "orig_top_k_doc_id": [7661, 7662, 7660, 7655, 7656, 7658, 2373, 1749, 7659, 1753, 1751, 1750, 6041, 4377, 1247]}, {"qid": 4933, "question": "Do they compare against a system that does not use streaming text, but has the entire text at disposal? in DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting", "answer": ["Yes", "Yes"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 1750, 2373, 1247, 1751, 7656, 7658, 7659, 1244, 768, 6158], "orig_top_k_doc_id": [7661, 7662, 7655, 7660, 7658, 1749, 7656, 7659, 1244, 1751, 768, 2373, 1750, 6158, 1247]}, {"qid": 1664, "question": "Which dataset do they use? in Simultaneous Neural Machine Translation using Connectionist Temporal Classification", "answer": ["small_parallel_enja, Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF5"], "top_k_doc_id": [7655, 380, 1739, 2373, 2374, 4377, 4918, 4209, 7660, 7661, 373, 5480, 2375, 5567, 1764], "orig_top_k_doc_id": [2373, 2374, 4377, 7655, 7661, 7660, 4918, 373, 5480, 1739, 380, 2375, 4209, 5567, 1764]}, {"qid": 1666, "question": "Which model architecture do they use to build a model? in Simultaneous Neural Machine Translation using Connectionist Temporal Classification", "answer": ["model is composed of an encoder (\u00a7SECREF5) and a decoder with the attention mechanism (\u00a7SECREF7) that are both implemented using recurrent neural networks (RNNs)"], "top_k_doc_id": [7655, 380, 1739, 2373, 2374, 4377, 4918, 4209, 7660, 7661, 2484, 6119, 1762, 7493, 4919], "orig_top_k_doc_id": [2373, 2374, 4377, 7655, 1739, 380, 7661, 2484, 6119, 1762, 4209, 7493, 7660, 4919, 4918]}, {"qid": 4934, "question": "Does larger granularity lead to better translation quality? in DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting", "answer": ["It depends on the model used.", "No"], "top_k_doc_id": [7655, 1749, 7660, 7661, 7662, 7656, 7659, 1753, 1118, 7658, 1247, 1752, 6215, 1119, 4541], "orig_top_k_doc_id": [7661, 7662, 7655, 7656, 7660, 7659, 1749, 1753, 1118, 7658, 1247, 1752, 6215, 1119, 4541]}, {"qid": 1279, "question": "Which languages do they experiment on? in How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?", "answer": ["German, English, Chinese"], "top_k_doc_id": [7655, 1749, 7660, 7661, 1750, 2373, 1753, 2375, 1752, 7669, 7190, 7043, 6943, 5835, 1053], "orig_top_k_doc_id": [1749, 1750, 2373, 1753, 7661, 2375, 7660, 1752, 7655, 7669, 7190, 7043, 6943, 5835, 1053]}, {"qid": 1665, "question": "Do they trim the search space of possible output sequences? in Simultaneous Neural Machine Translation using Connectionist Temporal Classification", "answer": ["No"], "top_k_doc_id": [7655, 380, 1739, 2373, 2374, 4377, 4918, 6593, 373, 3338, 2484, 6424, 3686, 375, 3279], "orig_top_k_doc_id": [2373, 2374, 1739, 380, 4918, 6593, 4377, 373, 3338, 2484, 6424, 7655, 3686, 375, 3279]}]}
{"group_id": 84, "group_size": 11, "items": [{"qid": 1698, "question": "How are discourse features incorporated into the model? in Leveraging Discourse Information Effectively for Authorship Attribution", "answer": ["They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer."], "top_k_doc_id": [6709, 2421, 709, 2422, 2424, 2425, 6710, 1907, 4780, 7763, 4632, 5377, 6014, 1905, 736], "orig_top_k_doc_id": [2421, 2425, 2424, 2422, 5377, 6014, 4780, 6709, 7763, 1907, 709, 1905, 6710, 4632, 736]}, {"qid": 1699, "question": "What discourse features are used? in Leveraging Discourse Information Effectively for Authorship Attribution", "answer": ["Entity grid with grammatical relations and RST discourse relations."], "top_k_doc_id": [6709, 2421, 709, 2422, 2424, 2425, 6710, 1907, 4780, 7763, 4632, 6708, 4896, 129, 1920], "orig_top_k_doc_id": [2421, 2424, 2422, 2425, 4780, 6710, 7763, 6709, 6708, 4896, 709, 1907, 129, 4632, 1920]}, {"qid": 1696, "question": "How are discourse embeddings analyzed? in Leveraging Discourse Information Effectively for Authorship Attribution", "answer": ["They perform t-SNE clustering to analyze discourse embeddings"], "top_k_doc_id": [6709, 2421, 709, 2422, 2424, 2425, 6710, 1907, 4780, 7763, 736, 4783, 4782, 1172, 3511], "orig_top_k_doc_id": [2421, 2425, 2424, 2422, 4780, 736, 4783, 4782, 709, 1172, 6710, 7763, 3511, 6709, 1907]}, {"qid": 4223, "question": "What IS versification? in Relative contributions of Shakespeare and Fletcher in Henry VIII: An Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "answer": ["the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d)", "the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.", "No"], "top_k_doc_id": [6709, 514, 515, 6050, 6708, 6710, 2040, 2587, 2772, 1191, 2732, 2773, 2774, 3162, 7512], "orig_top_k_doc_id": [6708, 6710, 6709, 514, 2587, 6050, 2040, 2772, 515, 1191, 2773, 3162, 2732, 2774, 7512]}, {"qid": 4224, "question": "How confident is the conclusion about Shakespeare vs Flectcher? in Relative contributions of Shakespeare and Fletcher in Henry VIII: An Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "answer": ["high reliability", "very"], "top_k_doc_id": [6709, 514, 515, 6050, 6708, 6710, 44, 731, 769, 5682, 43, 7484, 6347, 6966, 7042], "orig_top_k_doc_id": [6708, 6710, 6709, 514, 5682, 6050, 44, 515, 769, 43, 731, 7484, 6347, 6966, 7042]}, {"qid": 4227, "question": "What are the modifications by Thomas Merriam? in Relative contributions of Shakespeare and Fletcher in Henry VIII: An Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "answer": ["Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.", "a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.", "measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. "], "top_k_doc_id": [6709, 514, 515, 6050, 6708, 6710, 2040, 2587, 2772, 1191, 2732, 2773, 2774, 3162, 2629], "orig_top_k_doc_id": [6708, 6710, 6709, 514, 515, 2587, 6050, 2040, 2772, 1191, 3162, 2629, 2773, 2732, 2774]}, {"qid": 4228, "question": "What are stop words in Shakespeare? in Relative contributions of Shakespeare and Fletcher in Henry VIII: An Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "answer": ["No", "No"], "top_k_doc_id": [6709, 514, 515, 6050, 6708, 6710, 44, 731, 769, 5682, 43, 991, 1191, 2774, 2406], "orig_top_k_doc_id": [6708, 6710, 6709, 514, 6050, 515, 5682, 731, 769, 991, 1191, 44, 2774, 43, 2406]}, {"qid": 1697, "question": "What was the previous state-of-the-art? in Leveraging Discourse Information Effectively for Authorship Attribution", "answer": ["character bigram CNN classifier"], "top_k_doc_id": [6709, 2421, 709, 2422, 2424, 2425, 6710, 4896, 6708, 3277, 129, 3511, 903, 6457, 1422], "orig_top_k_doc_id": [2421, 6710, 6709, 4896, 6708, 2422, 2425, 3277, 129, 3511, 2424, 903, 6457, 709, 1422]}, {"qid": 4225, "question": "Is Henry VIII reflective of Shakespeare in general? in Relative contributions of Shakespeare and Fletcher in Henry VIII: An Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "answer": ["No", "Yes", "No"], "top_k_doc_id": [6709, 514, 515, 6050, 6708, 6710, 44, 731, 769, 5682, 3933, 6112, 2655, 4528, 658], "orig_top_k_doc_id": [6708, 6710, 6709, 514, 515, 6050, 3933, 5682, 6112, 2655, 4528, 658, 769, 44, 731]}, {"qid": 4226, "question": "Is vocabulary or versification more important for the analysis? in Relative contributions of Shakespeare and Fletcher in Henry VIII: An Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "answer": ["No", "Yes"], "top_k_doc_id": [6709, 514, 515, 6050, 6708, 6710, 2040, 2587, 2772, 7042, 7512, 991, 7511, 7509, 988], "orig_top_k_doc_id": [6708, 6710, 6709, 514, 7042, 2587, 7512, 6050, 991, 7511, 2772, 7509, 988, 515, 2040]}, {"qid": 1322, "question": "Which datasets do they use? in Incorporating Priors with Feature Attribution on Text Classification", "answer": [" Wikipedia toxic comments"], "top_k_doc_id": [6709, 2421, 1809, 1810, 1811, 4896, 1766, 3876, 39, 37, 5206, 7147, 1422, 3877, 6910], "orig_top_k_doc_id": [1809, 1810, 1811, 4896, 1766, 3876, 6709, 39, 2421, 37, 5206, 7147, 1422, 3877, 6910]}]}
{"group_id": 85, "group_size": 11, "items": [{"qid": 2634, "question": "What discourse relations does it work best/worst for? in Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph", "answer": ["explicit discourse relations", "Best: Expansion (Exp). Worst: Comparison (Comp)."], "top_k_doc_id": [1920, 4632, 4634, 4635, 6430, 4633, 6431, 709, 4636, 977, 7023, 4780, 6427, 7024, 5388], "orig_top_k_doc_id": [4636, 4632, 4635, 4633, 4634, 709, 7023, 6430, 6431, 4780, 6427, 977, 7024, 1920, 5388]}, {"qid": 4489, "question": "Do they train discourse relation models with augmented data? in Acquiring Annotated Data with Cross-lingual Explicitation for Implicit Discourse Relation Classification", "answer": ["Yes", "No"], "top_k_doc_id": [1920, 4632, 4634, 4635, 6430, 4633, 6431, 709, 4636, 977, 7023, 4780, 6427, 7024, 1905], "orig_top_k_doc_id": [7023, 4635, 4634, 4632, 709, 6430, 7024, 1920, 4636, 4633, 6431, 1905, 977, 4780, 6427]}, {"qid": 1385, "question": "Where does proposed metric differ from juman judgement? in A Neural Approach to Discourse Relation Signal Detection", "answer": ["model points out plausible signals which were passed over by an annotator, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action"], "top_k_doc_id": [1920, 1905, 1906, 1919, 4780, 1907, 1909, 1910, 1912, 1918, 2, 2225, 3208, 6473, 4634], "orig_top_k_doc_id": [1919, 2, 1918, 1910, 1906, 1907, 1920, 1905, 4780, 6473, 1909, 1912, 3208, 2225, 4634]}, {"qid": 1386, "question": "Where does proposed metric overlap with juman judgement? in A Neural Approach to Discourse Relation Signal Detection", "answer": ["influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments"], "top_k_doc_id": [1920, 1905, 1906, 1919, 4780, 1907, 1909, 1910, 1912, 1918, 2, 2225, 3208, 7353, 3670], "orig_top_k_doc_id": [1919, 1910, 1920, 1918, 1906, 1907, 1912, 2, 1905, 4780, 1909, 7353, 3208, 3670, 2225]}, {"qid": 2635, "question": "How much does this model improve state-of-the-art? in Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph", "answer": ["the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. ", "1 percent"], "top_k_doc_id": [1920, 4632, 4634, 4635, 6430, 4633, 6431, 709, 4636, 977, 7023, 7761, 5388, 713, 4783], "orig_top_k_doc_id": [4632, 4633, 4636, 4635, 4634, 709, 6430, 7023, 7761, 977, 5388, 1920, 713, 4783, 6431]}, {"qid": 577, "question": "Why does their model do better than prior models? in Deep Enhanced Representation for Implicit Discourse Relation Recognition", "answer": ["better sentence pair representations"], "top_k_doc_id": [1920, 4632, 4634, 4635, 6430, 4633, 6431, 709, 4636, 6427, 2424, 6429, 713, 3162, 4783], "orig_top_k_doc_id": [709, 6427, 6431, 6430, 4636, 4635, 4633, 4634, 2424, 6429, 713, 3162, 1920, 4632, 4783]}, {"qid": 1384, "question": "Are some models evaluated using this metric, what are the findings? in A Neural Approach to Discourse Relation Signal Detection", "answer": ["Yes"], "top_k_doc_id": [1920, 1905, 1906, 1919, 4780, 1907, 1909, 1910, 1912, 1918, 4781, 2421, 5132, 6473, 6472], "orig_top_k_doc_id": [1919, 1906, 1918, 4780, 1920, 1905, 1912, 1910, 1907, 4781, 1909, 2421, 5132, 6473, 6472]}, {"qid": 1383, "question": "How is the delta-softmax calculated? in A Neural Approach to Discourse Relation Signal Detection", "answer": ["Answer with content missing: (Formula) Formula is the answer."], "top_k_doc_id": [1920, 1905, 1906, 1919, 4780, 1907, 1909, 1910, 1912, 1918, 1913, 1911, 1914, 4634, 4633], "orig_top_k_doc_id": [1919, 1918, 1912, 1920, 1913, 1910, 1911, 1914, 1907, 4634, 1906, 4633, 4780, 1905, 1909]}, {"qid": 4490, "question": "How many languages do they at most attempt to use to generate discourse relation labelled data? in Acquiring Annotated Data with Cross-lingual Explicitation for Implicit Discourse Relation Classification", "answer": ["4", "four languages"], "top_k_doc_id": [1920, 4632, 4634, 4635, 6430, 4633, 6431, 7023, 736, 7024, 1905, 737, 1906, 6852, 977], "orig_top_k_doc_id": [7023, 4634, 736, 4632, 7024, 4635, 1905, 6430, 737, 1906, 4633, 6431, 1920, 6852, 977]}, {"qid": 3969, "question": "What datasets are used? in Variational Neural Discourse Relation Recognizer", "answer": ["PDTB 2.0", "PDTB 2.0 ", "PDTB 2.0 "], "top_k_doc_id": [1920, 1905, 1906, 1919, 4780, 6431, 6427, 6429, 6430, 6428, 1768, 6472, 1, 4781, 6153], "orig_top_k_doc_id": [6431, 6427, 6429, 6430, 6428, 1768, 1920, 6472, 1906, 4780, 1, 4781, 1919, 6153, 1905]}, {"qid": 4488, "question": "How much additional data do they manage to generate from translations? in Acquiring Annotated Data with Cross-lingual Explicitation for Implicit Discourse Relation Classification", "answer": ["45680", "In case of 2-votes they used 9,298 samples and in case of 3-votes they used 1,298 samples. "], "top_k_doc_id": [1920, 4632, 4634, 4635, 6430, 7023, 7024, 1348, 1350, 1347, 5869, 1349, 632, 2424, 6426], "orig_top_k_doc_id": [7023, 7024, 1348, 1350, 1920, 1347, 4635, 5869, 1349, 632, 2424, 4634, 4632, 6426, 6430]}]}
{"group_id": 86, "group_size": 11, "items": [{"qid": 2762, "question": "What dataset was used? in Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection", "answer": [" news articles in free-text format", "collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators"], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 5499, 5611, 4832, 5503, 3316, 7257, 6640, 3581], "orig_top_k_doc_id": [4834, 4831, 4833, 5498, 5499, 6731, 5612, 3316, 7257, 5611, 6730, 5503, 4832, 6640, 3581]}, {"qid": 2763, "question": "What was the baseline for this task? in Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection", "answer": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 5499, 5611, 4832, 5503, 3316, 7257, 4610, 1320], "orig_top_k_doc_id": [4834, 4831, 4833, 5498, 5499, 6731, 5612, 6730, 4832, 5611, 3316, 5503, 4610, 1320, 7257]}, {"qid": 2761, "question": "What are the 18 propaganda techniques? in Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection", "answer": ["Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating clich\u00e9, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man", "1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating clich\u00e9, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man"], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 5499, 5611, 4832, 5503, 5500, 6729, 6629, 4739], "orig_top_k_doc_id": [4831, 4834, 4833, 5499, 5498, 5611, 6731, 4832, 6730, 5612, 5500, 6729, 5503, 6629, 4739]}, {"qid": 3384, "question": "What extracted features were most influencial on performance? in Neural Architectures for Fine-Grained Propaganda Detection in News", "answer": ["Linguistic", "BERT"], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 5499, 5611, 1495, 3926, 1172, 6013, 3929, 7258], "orig_top_k_doc_id": [5498, 5612, 4831, 4834, 6731, 3926, 5499, 1172, 5611, 4833, 6013, 6730, 3929, 1495, 7258]}, {"qid": 3386, "question": "Which basic neural architecture perform best by itself? in Neural Architectures for Fine-Grained Propaganda Detection in News", "answer": ["BERT"], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 5499, 5611, 1495, 3577, 6729, 7545, 1625, 2330], "orig_top_k_doc_id": [5498, 5612, 6731, 5611, 5499, 3577, 6729, 4831, 6730, 4833, 4834, 7545, 1625, 1495, 2330]}, {"qid": 3383, "question": "What is best performing model among author's submissions, what performance it had? in Neural Architectures for Fine-Grained Propaganda Detection in News", "answer": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 4832, 7120, 7545, 1320, 7121, 5611, 7122, 1172], "orig_top_k_doc_id": [4834, 5612, 4832, 4831, 7120, 4833, 6731, 5498, 1320, 7121, 6730, 5611, 7122, 7545, 1172]}, {"qid": 3387, "question": "What participating systems had better results than ones authors submitted? in Neural Architectures for Fine-Grained Propaganda Detection in News", "answer": ["For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results."], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 4833, 6730, 4832, 7120, 7545, 5502, 1495, 6729, 1499, 7856], "orig_top_k_doc_id": [4834, 5498, 5612, 4831, 4833, 5502, 6731, 1495, 4832, 6729, 1499, 7545, 6730, 7856, 7120]}, {"qid": 3385, "question": "Did ensemble schemes help in boosting peformance, by how much? in Neural Architectures for Fine-Grained Propaganda Detection in News", "answer": ["The best ensemble topped the best single model by 0.029 in F1 score on dev (external).", "They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification"], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 5499, 5611, 6070, 6729, 4833, 3742, 2784, 4046, 3926, 4820], "orig_top_k_doc_id": [5612, 5498, 5611, 4833, 6731, 4831, 3742, 5499, 2784, 4834, 6729, 4046, 3926, 6070, 4820]}, {"qid": 3388, "question": "What is specific to multi-granularity and multi-tasking neural arhiteture design? in Neural Architectures for Fine-Grained Propaganda Detection in News", "answer": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "top_k_doc_id": [5498, 4831, 4834, 6731, 5612, 5499, 5611, 6070, 6729, 6730, 3015, 6013, 7120, 2306, 1320], "orig_top_k_doc_id": [5612, 5611, 6729, 5498, 6730, 5499, 3015, 4834, 6731, 4831, 6013, 7120, 2306, 6070, 1320]}, {"qid": 4237, "question": "What are the propaganda types? in Experiments in Detecting Persuasion Techniques in the News", "answer": ["annotated according to eighteen persuasion techniques BIBREF4", "Although not all of the 18 types are listed, they include using loaded language or appeal to authority and slogans, using logical fallacies such as strawmen, hidden ad-hominen fallacies ad red herrings. "], "top_k_doc_id": [5498, 4831, 4834, 6731, 6729, 5611, 1500, 1495, 5500, 5135, 5499, 6629, 1494, 4742, 4832], "orig_top_k_doc_id": [6729, 5611, 6731, 1500, 4831, 5498, 1495, 4834, 5500, 5135, 5499, 6629, 1494, 4742, 4832]}, {"qid": 3279, "question": "How is \"propaganda\" defined for the purposes of this study? in Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data", "answer": ["an intentional and potentially multicast communication, \u201cthe expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends\"", "First, propaganda is a social phenomenon and takes place as an act of communication BIBREF19, and so it is more than a simple information-theoretic message of zeros and ones\u2014it also incorporates an addresser and addressee(s), each in phatic contact (typically via broadcast media), ideally with a shared denotational code and contextual surround(s) BIBREF20."], "top_k_doc_id": [5498, 5499, 5501, 6633, 5500, 6632, 4833, 4832, 5502, 5611, 6730, 488, 6729, 4116, 5503], "orig_top_k_doc_id": [5499, 5498, 5501, 6633, 5500, 6632, 4833, 4832, 5502, 5611, 6730, 488, 6729, 4116, 5503]}]}
{"group_id": 87, "group_size": 11, "items": [{"qid": 2813, "question": "What was the score of the proposed model? in Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation", "answer": ["Best results authors obtain is EM 51.10 and F1 63.11", "EM Score of 51.10"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 2632, 2633, 4149, 4928, 4930, 6776, 7244, 3855, 4239, 4929], "orig_top_k_doc_id": [4928, 6479, 6480, 6482, 4930, 4149, 2633, 3839, 7244, 4929, 7664, 6776, 2632, 3855, 4239]}, {"qid": 2814, "question": "What was the previous best model? in Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation", "answer": ["(c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 ", "No"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 2632, 2633, 4149, 4928, 4930, 6776, 7244, 3855, 4239, 2723], "orig_top_k_doc_id": [4928, 6479, 6480, 6482, 4930, 3839, 6776, 4149, 7664, 7244, 2632, 2633, 3855, 2723, 4239]}, {"qid": 4034, "question": "Which of their proposed domain adaptation methods proves best overall? in Adapting general-purpose speech recognition engine output for domain-specific natural language question answering", "answer": ["Machine learning approach", "No"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 6483, 2725, 4928, 6776, 7351, 4239, 2056, 4688, 2721, 20], "orig_top_k_doc_id": [6479, 6480, 6482, 6483, 2725, 4239, 4928, 3839, 7664, 7351, 2056, 4688, 2721, 6776, 20]}, {"qid": 4035, "question": "Do they use evolutionary-based optimization algorithms as one of their domain adaptation approaches? in Adapting general-purpose speech recognition engine output for domain-specific natural language question answering", "answer": ["Yes", "No"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 6483, 2725, 4928, 6776, 7351, 6863, 4370, 4841, 3815, 6882], "orig_top_k_doc_id": [6479, 6480, 6482, 6483, 2725, 6863, 7351, 4928, 4370, 6776, 4841, 7664, 3815, 3839, 6882]}, {"qid": 2812, "question": "What evaluation metrics were used? in Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation", "answer": ["Exact Match (EM), Macro-averaged F1 scores (F1)", "Exact Match (EM) and Macro-averaged F1 scores (F1) "], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 2632, 2633, 4149, 4928, 4930, 6776, 7244, 6616, 2630, 2723], "orig_top_k_doc_id": [4928, 6479, 6480, 6482, 2632, 2633, 6776, 4149, 4930, 7664, 3839, 7244, 6616, 2630, 2723]}, {"qid": 4033, "question": "Do they compare their proposed domain adaptation methods to some existing methods? in Adapting general-purpose speech recognition engine output for domain-specific natural language question answering", "answer": ["No", "No", "Yes"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 6483, 2725, 19, 2056, 3815, 4688, 4239, 6810, 7541, 4279], "orig_top_k_doc_id": [6479, 6480, 2725, 6483, 7664, 6482, 19, 2056, 3815, 3839, 4688, 4239, 6810, 7541, 4279]}, {"qid": 129, "question": "Which ASR system(s) is used in this work? in Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "answer": ["Oracle "], "top_k_doc_id": [6479, 150, 152, 1987, 1989, 2197, 2199, 2311, 4928, 4930, 6776, 6780, 2198, 6480, 1988], "orig_top_k_doc_id": [1987, 4928, 2197, 150, 4930, 6776, 6479, 2199, 1989, 2198, 152, 6780, 6480, 2311, 1988]}, {"qid": 131, "question": "Over which datasets/corpora is this work evaluated? in Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "answer": ["$\\sim $ 8.7M annotated anonymised user utterances", "on $\\sim $ 8.7M annotated anonymised user utterances"], "top_k_doc_id": [6479, 150, 152, 1987, 1989, 2197, 2199, 2311, 4928, 4930, 6776, 6780, 2198, 3839, 3838], "orig_top_k_doc_id": [4928, 1987, 2197, 150, 6776, 4930, 2198, 2199, 3839, 6479, 6780, 3838, 1989, 2311, 152]}, {"qid": 2815, "question": "Which datasets did they use for evaluation? in Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation", "answer": ["Spoken-SQuAD testing set", "Spoken-SQuAD"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 2632, 2633, 4149, 4928, 4930, 6776, 7244, 2725, 4929, 6314], "orig_top_k_doc_id": [4928, 6479, 6480, 2632, 6776, 7664, 3839, 2633, 6482, 4930, 2725, 4149, 7244, 4929, 6314]}, {"qid": 4032, "question": "Which dataset do they use? in Adapting general-purpose speech recognition engine output for domain-specific natural language question answering", "answer": ["Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", " survey data and hand crafted a total of 293 textual questions BIBREF13", "U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12"], "top_k_doc_id": [6479, 3839, 6480, 6482, 7664, 6483, 7351, 4841, 19, 3483, 20, 7138, 4817, 4928, 7156], "orig_top_k_doc_id": [6479, 6480, 7351, 7664, 4841, 19, 6483, 3483, 3839, 6482, 20, 7138, 4817, 4928, 7156]}, {"qid": 130, "question": "What are the series of simple models? in Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "answer": ["perform experiments to utilize ASR $n$-best hypotheses during evaluation"], "top_k_doc_id": [6479, 150, 152, 1987, 1989, 2197, 2199, 2311, 4928, 4930, 6776, 6780, 151, 4149, 1296], "orig_top_k_doc_id": [4928, 1987, 2197, 6780, 152, 6776, 151, 4930, 2199, 150, 1989, 2311, 6479, 4149, 1296]}]}
{"group_id": 88, "group_size": 11, "items": [{"qid": 2957, "question": "Do they experiment with other tasks? in Named Entities in Medical Case Reports: Corpus and Experiments", "answer": ["No"], "top_k_doc_id": [4610, 2694, 2695, 4867, 4868, 5131, 5739, 6711, 4609, 5133, 5757, 4611, 5676, 6195, 4967], "orig_top_k_doc_id": [2694, 5131, 6711, 5739, 4868, 4867, 4611, 5133, 5676, 4609, 4610, 6195, 5757, 2695, 4967]}, {"qid": 2959, "question": "How large is the corpus? in Named Entities in Medical Case Reports: Corpus and Experiments", "answer": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "top_k_doc_id": [4610, 2694, 2695, 4867, 4868, 5131, 5739, 6711, 4609, 5133, 5757, 616, 7832, 5676, 5652], "orig_top_k_doc_id": [2694, 5131, 6711, 4867, 5133, 4868, 4609, 4610, 616, 7832, 5757, 5739, 5676, 2695, 5652]}, {"qid": 2961, "question": "How many documents are in the new corpus? in Named Entities in Medical Case Reports: Corpus and Experiments", "answer": ["53 documents", "53 documents"], "top_k_doc_id": [4610, 2694, 2695, 4867, 4868, 5131, 5739, 6711, 4609, 5133, 5757, 616, 6989, 5956, 7314], "orig_top_k_doc_id": [5131, 2694, 4609, 6711, 4867, 4610, 4868, 5739, 5133, 5757, 6989, 5956, 616, 2695, 7314]}, {"qid": 2962, "question": "What baseline systems are proposed? in Named Entities in Medical Case Reports: Corpus and Experiments", "answer": ["Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT", "Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT"], "top_k_doc_id": [4610, 2694, 2695, 4867, 4868, 5131, 5739, 6711, 4609, 5133, 5757, 4611, 610, 5879, 19], "orig_top_k_doc_id": [2694, 4867, 5133, 5131, 6711, 4868, 5739, 2695, 4611, 4609, 4610, 610, 5757, 5879, 19]}, {"qid": 2616, "question": "What are the other algorithms tested? in Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "answer": ["NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up", "As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy "], "top_k_doc_id": [4610, 4609, 4611, 4612, 4613, 4614, 64, 6713, 6714, 5739, 6177, 6401, 6403, 2694, 2284], "orig_top_k_doc_id": [4610, 4609, 4612, 4613, 4614, 4611, 64, 5739, 6177, 6401, 6403, 2694, 6713, 2284, 6714]}, {"qid": 2618, "question": "What are the clinical datasets used in the paper? in Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "answer": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "top_k_doc_id": [4610, 4609, 4611, 4612, 4613, 4614, 64, 6713, 6714, 6711, 3743, 20, 21, 6317, 1339], "orig_top_k_doc_id": [4610, 4609, 4613, 4612, 4611, 4614, 6711, 6714, 3743, 20, 64, 21, 6713, 6317, 1339]}, {"qid": 2958, "question": "What baselines do they introduce? in Named Entities in Medical Case Reports: Corpus and Experiments", "answer": ["Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT\n", "Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT"], "top_k_doc_id": [4610, 2694, 2695, 4867, 4868, 5131, 5739, 6711, 3744, 4611, 5652, 5133, 5740, 21, 5757], "orig_top_k_doc_id": [2694, 5131, 6711, 5133, 4610, 4867, 4868, 5739, 2695, 5652, 4611, 5740, 3744, 21, 5757]}, {"qid": 2960, "question": "How was annotation performed? in Named Entities in Medical Case Reports: Corpus and Experiments", "answer": ["Experienced medical doctors used a linguistic annotation tool to annotate entities.", "WebAnno"], "top_k_doc_id": [4610, 2694, 2695, 4867, 4868, 5131, 5739, 6711, 3744, 4611, 5652, 7832, 616, 5132, 4609], "orig_top_k_doc_id": [2694, 5131, 2695, 7832, 6711, 5739, 4868, 616, 4867, 4611, 5652, 5132, 3744, 4610, 4609]}, {"qid": 2615, "question": "What is the performance of BERT on the task? in Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "answer": ["F1 scores are:\nHUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)\nMedoccan: Detection(0.972), Classification (0.967)", "BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table "], "top_k_doc_id": [4610, 4609, 4611, 4612, 4613, 4614, 20, 21, 1320, 5292, 3141, 138, 3688, 3010, 3744], "orig_top_k_doc_id": [4610, 4609, 4613, 4612, 4614, 4611, 20, 21, 1320, 5292, 3141, 138, 3688, 3010, 3744]}, {"qid": 1852, "question": "Did they experiment on this corpus? in Creation of an Annotated Corpus of Spanish Radiology Reports", "answer": ["No"], "top_k_doc_id": [4610, 2694, 2695, 5490, 4611, 4862, 1001, 1901, 1043, 5210, 6856, 5652, 6195, 6153, 4350], "orig_top_k_doc_id": [2694, 2695, 5490, 4611, 4610, 4862, 1001, 1901, 1043, 5210, 6856, 5652, 6195, 6153, 4350]}, {"qid": 2617, "question": "Does BERT reach the best performance among all the algorithms compared? in Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "answer": ["No", "No"], "top_k_doc_id": [4610, 4609, 4611, 4612, 4613, 4614, 6162, 6402, 3010, 6403, 7250, 2676, 7462, 4649, 6448], "orig_top_k_doc_id": [4610, 4613, 4612, 4609, 4611, 4614, 6162, 6402, 3010, 6403, 7250, 2676, 7462, 4649, 6448]}]}
{"group_id": 89, "group_size": 11, "items": [{"qid": 3096, "question": "What set of approaches to hashtag segmentation are proposed? in Multi-task Pairwise Neural Ranking for Hashtag Segmentation", "answer": ["Adaptive Multi-task Learning\n, Margin Ranking (MR) Loss\n, Pairwise Neural Ranking Model\n"], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 5253, 5256, 7361, 6732, 6734, 2591, 6733, 2105], "orig_top_k_doc_id": [5252, 5254, 5253, 5256, 5255, 2590, 2587, 2588, 2589, 7361, 2591, 6732, 6734, 6733, 2105]}, {"qid": 3097, "question": "How is the dataset of hashtags sourced? in Multi-task Pairwise Neural Ranking for Hashtag Segmentation", "answer": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 5253, 5256, 7361, 6732, 6734, 764, 2329, 5584], "orig_top_k_doc_id": [5252, 5254, 5253, 5255, 5256, 2587, 2590, 2588, 2589, 6732, 7361, 764, 2329, 6734, 5584]}, {"qid": 1777, "question": "Does the paper report the performance on the task of a Neural Machine Translation model? in Char-RNN and Active Learning for Hashtag Segmentation", "answer": ["No"], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 2591, 6082, 6141, 6554, 6551, 6768, 568, 6140], "orig_top_k_doc_id": [2590, 2587, 2591, 2589, 2588, 6554, 6141, 5255, 5252, 5254, 6082, 6551, 6768, 568, 6140]}, {"qid": 1779, "question": "Is the RNN model evaluated against any baseline? in Char-RNN and Active Learning for Hashtag Segmentation", "answer": ["Yes"], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 2591, 6082, 6141, 7069, 5287, 2299, 7068, 6142], "orig_top_k_doc_id": [2590, 2587, 2591, 2589, 2588, 6141, 7069, 5254, 5252, 5287, 5255, 2299, 6082, 7068, 6142]}, {"qid": 3094, "question": "Do the hashtag and SemEval datasets contain only English data? in Multi-task Pairwise Neural Ranking for Hashtag Segmentation", "answer": ["Yes", "Yes"], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 5253, 5256, 7361, 6732, 451, 3543, 1330, 452], "orig_top_k_doc_id": [5252, 5255, 5254, 5256, 5253, 2588, 7361, 2590, 6732, 2587, 451, 3543, 2589, 1330, 452]}, {"qid": 1780, "question": "Which languages are used in the paper? in Char-RNN and Active Learning for Hashtag Segmentation", "answer": ["English, Russian"], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 2591, 6082, 6141, 5253, 6280, 5256, 165, 2432], "orig_top_k_doc_id": [2590, 2591, 2587, 2589, 2588, 5252, 6141, 5253, 6280, 5256, 5255, 5254, 6082, 165, 2432]}, {"qid": 3095, "question": "What current state of the art method was used for comparison? in Multi-task Pairwise Neural Ranking for Hashtag Segmentation", "answer": ["current state-of-the-art approach BIBREF14 , BIBREF15", " BIBREF14, BIBREF15 "], "top_k_doc_id": [2587, 2589, 2590, 2588, 5252, 5254, 5255, 5253, 5256, 7361, 6651, 3200, 6327, 2433, 6925], "orig_top_k_doc_id": [5252, 5254, 5253, 5255, 5256, 7361, 2590, 6651, 2588, 2587, 3200, 6327, 2433, 6925, 2589]}, {"qid": 141, "question": "How does the scoring model work? in Active Learning for Chinese Word Segmentation in Medical Text", "answer": ["First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word", " the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history"], "top_k_doc_id": [2587, 2589, 2590, 165, 166, 169, 483, 7254, 5283, 2948, 168, 167, 564, 861, 7681], "orig_top_k_doc_id": [165, 166, 2590, 169, 5283, 2589, 7254, 2948, 168, 2587, 167, 564, 861, 483, 7681]}, {"qid": 142, "question": "How does the active learning model work? in Active Learning for Chinese Word Segmentation in Medical Text", "answer": ["Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively."], "top_k_doc_id": [2587, 2589, 2590, 165, 166, 169, 483, 7254, 5024, 2591, 6142, 1777, 6143, 5873, 4867], "orig_top_k_doc_id": [165, 166, 2590, 169, 2589, 2587, 5024, 2591, 7254, 6142, 1777, 6143, 5873, 4867, 483]}, {"qid": 143, "question": "Which neural network architectures are employed? in Active Learning for Chinese Word Segmentation in Medical Text", "answer": ["gated neural network "], "top_k_doc_id": [2587, 2589, 2590, 165, 166, 169, 1777, 5835, 3642, 6029, 1373, 3433, 3647, 167, 4749], "orig_top_k_doc_id": [165, 166, 2590, 169, 2587, 1777, 2589, 5835, 3642, 6029, 1373, 3433, 3647, 167, 4749]}, {"qid": 1778, "question": "What are the predefined morpho-syntactic patterns used to filter the training data? in Char-RNN and Active Learning for Hashtag Segmentation", "answer": ["No"], "top_k_doc_id": [2587, 2589, 2590, 2588, 2591, 6328, 6329, 6141, 5253, 4017, 5072, 166, 6082, 2432, 830], "orig_top_k_doc_id": [2590, 2591, 2587, 2588, 2589, 6328, 6329, 6141, 5253, 4017, 5072, 166, 6082, 2432, 830]}]}
{"group_id": 90, "group_size": 11, "items": [{"qid": 3285, "question": "What metrics are used for evaluation? in Generating Personalized Recipes from Historical User Preferences", "answer": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "top_k_doc_id": [7844, 5506, 5507, 3353, 3772, 5508, 5509, 4124, 4126, 6550, 3352, 4125, 6876, 7846, 6880], "orig_top_k_doc_id": [5506, 5507, 5508, 7844, 5509, 3772, 7846, 6876, 4125, 4124, 3352, 6880, 3353, 6550, 4126]}, {"qid": 3288, "question": "What are the baseline models? in Generating Personalized Recipes from Historical User Preferences", "answer": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "top_k_doc_id": [7844, 5506, 5507, 3353, 3772, 5508, 5509, 4124, 4126, 6550, 3352, 4125, 6876, 7846, 4128], "orig_top_k_doc_id": [5506, 5507, 5508, 5509, 7844, 3772, 4124, 4125, 7846, 6876, 3352, 4126, 6550, 4128, 3353]}, {"qid": 3287, "question": "What were their results on the new dataset? in Generating Personalized Recipes from Historical User Preferences", "answer": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "top_k_doc_id": [7844, 5506, 5507, 3353, 3772, 5508, 5509, 4124, 4126, 6550, 3352, 4125, 6876, 7846, 3350], "orig_top_k_doc_id": [5506, 5507, 5508, 7844, 5509, 4124, 7846, 3772, 4125, 3352, 3350, 6550, 4126, 3353, 6876]}, {"qid": 3286, "question": "What natural language(s) are the recipes written in? in Generating Personalized Recipes from Historical User Preferences", "answer": ["English", "English", "No"], "top_k_doc_id": [7844, 5506, 5507, 3353, 3772, 5508, 5509, 1256, 3350, 3351, 3352, 6468, 7846, 7465, 6876], "orig_top_k_doc_id": [5506, 5507, 5508, 7844, 5509, 3352, 7846, 3353, 3350, 3772, 6468, 1256, 7465, 3351, 6876]}, {"qid": 3289, "question": "How did they obtain the interactions? in Generating Personalized Recipes from Historical User Preferences", "answer": ["from Food.com"], "top_k_doc_id": [7844, 5506, 5507, 3353, 3772, 5508, 5509, 4124, 4126, 6550, 7465, 5066, 4128, 7468, 6090], "orig_top_k_doc_id": [5506, 5507, 5508, 5509, 7844, 7465, 3772, 3353, 4124, 5066, 4128, 7468, 6550, 6090, 4126]}, {"qid": 3290, "question": "Where do they get the recipes from? in Generating Personalized Recipes from Historical User Preferences", "answer": ["from Food.com"], "top_k_doc_id": [7844, 5506, 5507, 3353, 3772, 5508, 5509, 1256, 3350, 3351, 3352, 6468, 7846, 4124, 6406], "orig_top_k_doc_id": [5506, 5507, 5508, 7844, 5509, 3353, 3352, 3350, 7846, 6468, 3351, 1256, 3772, 4124, 6406]}, {"qid": 5036, "question": "Is this done in form of unsupervised (clustering) or suppervised learning? in Personalized Taste and Cuisine Preference Modeling via Images", "answer": ["Supervised methods are used to identify the dish and ingredients in the image, and an unsupervised method (KNN) is used to create the food profile.", "Unsupervised"], "top_k_doc_id": [7844, 5506, 5507, 3102, 3350, 4124, 4126, 4128, 7465, 7845, 7846, 2725, 3352, 3103, 6661], "orig_top_k_doc_id": [7844, 7845, 5506, 7465, 3350, 7846, 5507, 3102, 2725, 4126, 3352, 4128, 3103, 6661, 4124]}, {"qid": 5037, "question": "Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines? in Personalized Taste and Cuisine Preference Modeling via Images", "answer": ["Yes", "The study features a radar chart describing inclinations toward particular cuisines, but they do not perform any experiments"], "top_k_doc_id": [7844, 5506, 5507, 3102, 3350, 4124, 4126, 4128, 7465, 7845, 7846, 5508, 3015, 3353, 4127], "orig_top_k_doc_id": [7844, 7845, 5506, 7465, 7846, 5508, 4124, 5507, 4128, 4126, 3350, 3015, 3353, 4127, 3102]}, {"qid": 3960, "question": "What are two baseline methods? in Self-Attention and Ingredient-Attention Based Model for Recipe Retrieval from Image Queries", "answer": ["Joint Neural Embedding (JNE)\nAdaMine", "Answer with content missing: (Table1 merged with Figure 3) Joint Neural\nEmbedding (JNE) and AdaMine", "JNE and AdaMine"], "top_k_doc_id": [7844, 5506, 5507, 6408, 6407, 6406, 1257, 6405, 1259, 3027, 3350, 3026, 1258, 5508, 1256], "orig_top_k_doc_id": [6408, 6407, 6406, 1257, 6405, 5507, 5506, 1259, 3027, 3350, 3026, 7844, 1258, 5508, 1256]}, {"qid": 3961, "question": "How does model compare to the baselines? in Self-Attention and Ingredient-Attention Based Model for Recipe Retrieval from Image Queries", "answer": ["The model outperforms the two baseline models,  since it has higher recall values. ", "Answer with content missing: (Table1 part of Figure 3):\nProposed vs Best baseline result\n- Median Rank: 2.9 vs 3.0 (lower better)\n- Rank 1 recall: 34.6 vs 33.1 (higher better)", "The model improved over the baseline with scores of 34.6, 66.0 and 76.6 for Recall at 1, 5 and 10 respectively"], "top_k_doc_id": [7844, 5506, 5507, 6408, 6407, 6406, 1257, 6405, 1259, 3027, 3350, 3026, 1258, 5508, 1256], "orig_top_k_doc_id": [6408, 6406, 6407, 1257, 5507, 6405, 1259, 3026, 1258, 5506, 1256, 3350, 5508, 7844, 3027]}, {"qid": 2184, "question": "What is barycentric Newton diagram? in A neural network system for transformation of regional cuisine style", "answer": [" The basic idea of the visualization, drawing on Isaac Newton\u2019s visualization of the color spectrum BIBREF8 , is to express a mixture in terms of its constituents as represented in barycentric coordinates."], "top_k_doc_id": [7844, 3350, 3353, 3352, 3351, 6687, 7479, 3247, 4215, 7845, 5966, 6013, 1121, 610, 6686], "orig_top_k_doc_id": [3350, 3353, 3352, 3351, 6687, 7479, 3247, 4215, 7845, 5966, 6013, 1121, 7844, 610, 6686]}]}
{"group_id": 91, "group_size": 11, "items": [{"qid": 3345, "question": "Do they release code? in Structuring an unordered text document", "answer": ["No", "No"], "top_k_doc_id": [6358, 2774, 5575, 5576, 5933, 6472, 19, 1258, 6053, 2772, 2730, 5503, 1136, 5119, 2138], "orig_top_k_doc_id": [5575, 2774, 5576, 19, 1258, 2772, 6358, 2730, 5503, 1136, 5119, 6472, 2138, 5933, 6053]}, {"qid": 3346, "question": "Which languages do they evaluate on? in Structuring an unordered text document", "answer": ["No", "No"], "top_k_doc_id": [6358, 2774, 5575, 5576, 5933, 6472, 19, 1258, 6053, 2772, 1905, 2775, 2769, 6471, 7409], "orig_top_k_doc_id": [5575, 5576, 1258, 2774, 19, 1905, 2772, 6472, 2775, 6358, 2769, 5933, 6053, 6471, 7409]}, {"qid": 3944, "question": "how was quality of sentence transition measured? in Political Speech Generation", "answer": ["Manually, using the criterion score between 0 and 3.", "The quality of sentence transition was measured manually by checking how well do consecutive sentences connect", "Manually evaluated on scale 0 to 3."], "top_k_doc_id": [6358, 3594, 7382, 7383, 3596, 5643, 3595, 5644, 6359, 3333, 2076, 1005, 7761, 2595, 4863], "orig_top_k_doc_id": [3333, 6358, 7382, 2076, 6359, 1005, 7383, 3594, 5644, 7761, 3595, 5643, 2595, 3596, 4863]}, {"qid": 3945, "question": "what is the size of the dataset? in Political Speech Generation", "answer": ["3857 speech segments", "2771 speeches containing 50871 sentences", "3857 speech segments from 53 US Congressional floor debates"], "top_k_doc_id": [6358, 3594, 7382, 7383, 3596, 5643, 3595, 5644, 6359, 5641, 5645, 5574, 5979, 5908, 5879], "orig_top_k_doc_id": [7382, 3594, 6358, 5641, 7383, 5643, 3596, 6359, 5645, 5644, 5574, 5979, 5908, 3595, 5879]}, {"qid": 3344, "question": "Do they release a data set? in Structuring an unordered text document", "answer": ["No", "No"], "top_k_doc_id": [6358, 2774, 5575, 5576, 5933, 6472, 19, 1258, 6053, 2011, 5493, 305, 672, 23, 671], "orig_top_k_doc_id": [5575, 5576, 19, 1258, 6358, 2011, 6472, 2774, 5493, 5933, 305, 672, 6053, 23, 671]}, {"qid": 3946, "question": "what manual evaluation is presented? in Political Speech Generation", "answer": ["Manual evaluation of four evaluation criteria:   grammatical correctness,  sentence transitions, speech structure, and speech content. ", "generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it", "The manual evaluation contains 4 criteria to check grammatical correctness, sentence transitions, speech structure, and speech content of the generated speech and assigning a score between 0 to 3 for each criterion"], "top_k_doc_id": [6358, 3594, 7382, 7383, 3596, 5643, 5574, 6361, 2399, 5037, 7671, 1735, 5573, 825, 3332], "orig_top_k_doc_id": [7382, 5574, 6361, 3596, 7383, 3594, 2399, 5037, 7671, 5643, 1735, 5573, 825, 3332, 6358]}, {"qid": 3343, "question": "What kind of model do they use? in Structuring an unordered text document", "answer": ["Our methodology is described in the Figure 1 "], "top_k_doc_id": [6358, 2774, 5575, 5576, 5933, 6472, 19, 1258, 20, 7743, 1256, 1136, 5493, 6955, 1113], "orig_top_k_doc_id": [5575, 19, 6358, 5576, 20, 6472, 7743, 1258, 1256, 1136, 5493, 2774, 6955, 5933, 1113]}, {"qid": 3609, "question": "What baselines did they compare with? in Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves", "answer": ["LDA, Doc-NADE, HTMM, GMNTM", "LDA, Doc-NADE, HTMM, GMNTM", "LDA BIBREF2, Doc-NADE BIBREF24, HTMM BIBREF9, GMNTM BIBREF12", "LDA BIBREF2 , Doc-NADE BIBREF24, HTMM BIBREF9, GMNTM BIBREF12, LDA BIBREF2 , Doc-NADE BIBREF24, HTMM BIBREF9 , GMNTM BIBREF12"], "top_k_doc_id": [6358, 3594, 5933, 5936, 6725, 7222, 4382, 6016, 5089, 4122, 1114, 5094, 954, 6127, 953], "orig_top_k_doc_id": [5933, 7222, 4382, 6016, 3594, 6358, 6725, 5089, 4122, 1114, 5094, 954, 5936, 6127, 953]}, {"qid": 3610, "question": "Which tasks are explored in this paper? in Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves", "answer": ["generative model evaluation (i.e. test set perplexity) and document classification", "generative model evaluation, document classification", "generative model evaluation (i.e. test set perplexity), document classification", "generative document evaluation task, document classification task, topic2sentence task"], "top_k_doc_id": [6358, 3594, 5933, 5936, 6725, 5937, 3468, 3467, 143, 5934, 6532, 4417, 714, 705, 1679], "orig_top_k_doc_id": [5933, 5937, 3468, 3467, 5936, 6725, 143, 5934, 6358, 6532, 4417, 714, 705, 1679, 3594]}, {"qid": 3943, "question": "how did they measure grammatical correctness? in Political Speech Generation", "answer": ["Identify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.", "They measure grammatical correctness by checking whether a sentence has the same sequence of POS tags.", "identify for each sentence of the speech its POS tags, Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct., points in a certain direction, evaluate those sentences manually"], "top_k_doc_id": [6358, 3594, 7382, 7383, 6361, 6726, 851, 5641, 852, 6169, 6728, 6725, 7721, 6727, 2002], "orig_top_k_doc_id": [6361, 6726, 7382, 851, 6358, 5641, 852, 7383, 6169, 6728, 6725, 7721, 6727, 2002, 3594]}, {"qid": 3342, "question": "What is an unordered text document, do these arise in real-world corpora? in Structuring an unordered text document", "answer": ["A unordered text document is one where sentences in the document are disordered or jumbled. It doesn't appear that unordered text documents appear in corpora, but rather are introduced as part of processing pipeline."], "top_k_doc_id": [6358, 2774, 5575, 5576, 5933, 6472, 953, 671, 672, 1136, 5493, 3753, 3515, 3752, 673], "orig_top_k_doc_id": [5575, 5576, 5933, 953, 671, 672, 1136, 6472, 5493, 2774, 3753, 3515, 3752, 6358, 673]}]}
{"group_id": 92, "group_size": 11, "items": [{"qid": 3534, "question": "what was the baseline? in Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation", "answer": ["traditional phrase-based statistical machine translation (SMT), NMT system", "traditional phrase-based statistical machine translation (SMT), NMT system"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 799, 800, 802, 5316, 7355, 2956, 6979, 2296, 4169], "orig_top_k_doc_id": [5839, 5835, 802, 801, 5838, 5837, 2956, 5316, 2296, 5836, 7355, 799, 800, 4169, 6979]}, {"qid": 3535, "question": "did they collect their own data? in Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 799, 800, 802, 5316, 7355, 2956, 6979, 4712, 6190], "orig_top_k_doc_id": [5839, 5837, 5835, 802, 5838, 801, 5836, 800, 799, 7355, 2956, 5316, 4712, 6979, 6190]}, {"qid": 3536, "question": "what japanese-vietnamese dataset do they use? in Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation", "answer": ["WIT3's corpus"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 799, 800, 802, 5316, 7355, 2956, 2296, 7357, 5319], "orig_top_k_doc_id": [5839, 801, 5837, 802, 5838, 5835, 5836, 800, 7355, 5316, 2956, 799, 2296, 7357, 5319]}, {"qid": 3154, "question": "What are the limitations of existing Vietnamese word segmentation systems? in State-of-the-Art Vietnamese Word Segmentation", "answer": [" ambiguous words, unknown words", "lacks of complete review approaches, datasets and toolkits "], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 360, 876, 5316, 5317, 5318, 5319, 5320, 2956, 2959], "orig_top_k_doc_id": [5316, 5319, 5317, 5318, 5839, 801, 5837, 5836, 5838, 5835, 5320, 360, 2956, 876, 2959]}, {"qid": 3155, "question": "Why challenges does word segmentation in Vietnamese pose? in State-of-the-Art Vietnamese Word Segmentation", "answer": ["Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.", "to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 360, 876, 5316, 5317, 5318, 5319, 5320, 359, 799], "orig_top_k_doc_id": [5319, 5316, 5317, 5318, 5837, 5839, 5836, 801, 5838, 5835, 360, 876, 359, 799, 5320]}, {"qid": 3533, "question": "what methods were used to reduce data sparsity effects? in Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation", "answer": ["Back Translation, Mix-Source Approach", "data augmentation"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 799, 800, 802, 5316, 7355, 5027, 1583, 1664, 3920], "orig_top_k_doc_id": [5835, 5839, 802, 5838, 5837, 799, 801, 5027, 5836, 800, 5316, 1583, 1664, 3920, 7355]}, {"qid": 653, "question": "Are synonymous relation taken into account in the Japanese-Vietnamese task? in Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation", "answer": ["Yes"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 799, 800, 802, 5026, 3920, 5704, 633, 5702, 2956], "orig_top_k_doc_id": [802, 799, 800, 801, 5835, 5837, 5838, 5026, 3920, 5836, 5839, 5704, 633, 5702, 2956]}, {"qid": 3156, "question": "How successful are the approaches used to solve word segmentation in Vietnamese? in State-of-the-Art Vietnamese Word Segmentation", "answer": ["Their accuracy in word segmentation is about 94%-97%."], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 360, 876, 5316, 5317, 5318, 5319, 802, 800, 799], "orig_top_k_doc_id": [5316, 5318, 5319, 5317, 801, 5837, 5839, 5836, 5838, 5835, 360, 876, 802, 800, 799]}, {"qid": 3157, "question": "Which approaches have been applied to solve word segmentation in Vietnamese? in State-of-the-Art Vietnamese Word Segmentation", "answer": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "top_k_doc_id": [801, 5838, 5839, 5835, 5836, 5837, 360, 876, 5316, 5317, 5318, 5319, 802, 800, 799], "orig_top_k_doc_id": [5316, 5318, 5319, 5317, 801, 5839, 5837, 5836, 5835, 5838, 802, 800, 360, 799, 876]}, {"qid": 4721, "question": "What are the other two Vietnamese datasets? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": ["MS-COCO dataset translated to Vietnamese using Google Translate and through human annotation", "datasets generated by two methods (translated by Google Translation service and annotated by human)"], "top_k_doc_id": [801, 5838, 5839, 7358, 7357, 7355, 7356, 7580, 2956, 5319, 802, 3249, 5316, 2957, 2960], "orig_top_k_doc_id": [7358, 7357, 7355, 7356, 7580, 2956, 5839, 5838, 801, 5319, 802, 3249, 5316, 2957, 2960]}, {"qid": 654, "question": "Is the supervised morphological learner tested on Japanese? in Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [801, 802, 800, 799, 7509, 5835, 2223, 5026, 648, 4387, 627, 7045, 3920, 633, 395], "orig_top_k_doc_id": [802, 800, 799, 7509, 5835, 2223, 5026, 648, 4387, 627, 7045, 3920, 801, 633, 395]}]}
{"group_id": 93, "group_size": 11, "items": [{"qid": 3725, "question": "What baselines do they compare against? in Hierarchical Neural Story Generation", "answer": ["gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism, LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention, an ensemble of two Conv seq2seq with self-attention models, KNN model", "Language Models, seq2seq, Ensemble, KNN", "Language Models, seq2seq: using LSTMs and convolutional seq2seq architectures, Conv seq2seq with decoder self-attention, an ensemble of two Conv seq2seq with self-attention models, KNN model"], "top_k_doc_id": [6065, 6066, 6067, 6068, 2007, 558, 559, 560, 1081, 2004, 5495, 2594, 3800, 1058, 5916], "orig_top_k_doc_id": [6067, 2004, 6068, 6066, 559, 6065, 5495, 2594, 2007, 3800, 1081, 558, 560, 1058, 5916]}, {"qid": 3727, "question": "What model is used to generate the premise? in Hierarchical Neural Story Generation", "answer": ["convolutional language model from BIBREF4", " convolutional language model from BIBREF4", "convolutional language model"], "top_k_doc_id": [6065, 6066, 6067, 6068, 2007, 558, 559, 560, 1081, 2004, 2065, 2064, 2005, 1822, 1156], "orig_top_k_doc_id": [6066, 6065, 6068, 6067, 2004, 2065, 2064, 560, 559, 2007, 2005, 558, 1822, 1156, 1081]}, {"qid": 3728, "question": "Are the stories in the dataset fictional stories? in Hierarchical Neural Story Generation", "answer": ["No", "No", "Yes"], "top_k_doc_id": [6065, 6066, 6067, 6068, 2007, 558, 1058, 1082, 1822, 1825, 4976, 4977, 370, 1630, 934], "orig_top_k_doc_id": [6065, 6068, 6067, 6066, 1058, 1822, 2007, 4976, 558, 1825, 370, 1630, 1082, 4977, 934]}, {"qid": 3729, "question": "Where are the stories collected from? in Hierarchical Neural Story Generation", "answer": ["online forum", "Reddit's WritingPrompts forum", "Reddit's WritingPrompts forum"], "top_k_doc_id": [6065, 6066, 6067, 6068, 2007, 558, 1058, 1082, 1822, 1825, 4976, 4977, 1081, 171, 5138], "orig_top_k_doc_id": [6065, 6068, 1058, 6066, 6067, 2007, 4976, 1822, 558, 1825, 1081, 171, 4977, 1082, 5138]}, {"qid": 851, "question": "Do they evaluate in other language appart from English? in Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation", "answer": ["No"], "top_k_doc_id": [6065, 558, 559, 1081, 1082, 2435, 2438, 1170, 6066, 3800, 6040, 1171, 7700, 2436, 4414], "orig_top_k_doc_id": [1082, 1081, 558, 6065, 2435, 2438, 559, 1171, 6066, 1170, 3800, 7700, 2436, 4414, 6040]}, {"qid": 852, "question": "What are the baselines? in Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation", "answer": ["Title-to-Story system"], "top_k_doc_id": [6065, 558, 559, 1081, 1082, 2435, 2438, 1170, 6066, 3800, 6040, 6701, 34, 560, 3799], "orig_top_k_doc_id": [1082, 1081, 558, 559, 3800, 6065, 2438, 2435, 6040, 6066, 6701, 34, 560, 1170, 3799]}, {"qid": 3723, "question": "What human evaluation metrics do they look at? in Hierarchical Neural Story Generation", "answer": ["human preference", "triple pairing task, hierarchical generation", "Accuracy at pairing stories with the prompts used to generate them; accuracy of prompt ranking"], "top_k_doc_id": [6065, 6066, 6067, 6068, 2007, 2435, 2438, 4976, 2594, 2595, 1082, 5138, 5495, 560, 3800], "orig_top_k_doc_id": [6068, 6065, 6067, 2594, 2438, 2595, 2435, 1082, 2007, 5138, 5495, 4976, 6066, 560, 3800]}, {"qid": 3724, "question": "Which automated evaluation metrics are used? in Hierarchical Neural Story Generation", "answer": ["perplexity, prompt ranking accuracy", "model perplexity on the test set , prompt ranking accuracy", "perplexity , prompt ranking accuracy"], "top_k_doc_id": [6065, 6066, 6067, 6068, 2007, 2435, 2438, 4976, 2436, 285, 2008, 2004, 2005, 1058, 2065], "orig_top_k_doc_id": [6065, 2435, 2438, 6067, 2007, 2436, 285, 6068, 2008, 2004, 6066, 4976, 2005, 1058, 2065]}, {"qid": 850, "question": "How do they evaluate generated stories? in Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation", "answer": ["separate set of Turkers to rate the stories for overall quality and the three improvement areas"], "top_k_doc_id": [6065, 558, 559, 1081, 1082, 2435, 2438, 1170, 6066, 6067, 6068, 4977, 3669, 2594, 4976], "orig_top_k_doc_id": [1082, 1081, 558, 6065, 2438, 6067, 6066, 2435, 1170, 6068, 4977, 559, 3669, 2594, 4976]}, {"qid": 849, "question": "How is human interaction consumed by the model? in Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation", "answer": ["displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages"], "top_k_doc_id": [6065, 558, 559, 1081, 1082, 2435, 2438, 3800, 1083, 4656, 2673, 3798, 4720, 2436, 4719], "orig_top_k_doc_id": [1082, 1081, 558, 2435, 6065, 559, 3800, 2438, 1083, 4656, 2673, 3798, 4720, 2436, 4719]}, {"qid": 3726, "question": "Do they use pre-trained embeddings like BERT? in Hierarchical Neural Story Generation", "answer": ["No", "No", "No"], "top_k_doc_id": [6065, 6066, 6067, 6068, 1364, 1138, 1363, 1058, 6409, 1822, 2004, 123, 734, 5804, 560], "orig_top_k_doc_id": [6065, 1364, 6067, 6066, 1138, 1363, 1058, 6409, 1822, 2004, 6068, 123, 734, 5804, 560]}]}
{"group_id": 94, "group_size": 11, "items": [{"qid": 3777, "question": "What is the performance improvement of their method over state-of-the-art models on the used datasets?  in Mitigating Annotation Artifacts in Natural Language Inference Datasets to Improve Cross-dataset Generalization Ability", "answer": ["Average improvement in accuracy is 2.26 points", "No", "No"], "top_k_doc_id": [5868, 5872, 6137, 3953, 872, 6138, 6139, 2663, 1141, 5871, 3416, 1143, 5249, 2746, 2725], "orig_top_k_doc_id": [6139, 6137, 5872, 872, 5868, 6138, 3416, 3953, 5871, 1143, 1141, 2663, 5249, 2746, 2725]}, {"qid": 3779, "question": "How does the proposed training framework mitigate the bias pattern? in Mitigating Annotation Artifacts in Natural Language Inference Datasets to Improve Cross-dataset Generalization Ability", "answer": ["Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading", "Artifacts in biased datasets are balanced by assigning specific weights for every sample", "by balancing or, smoothing the artifacts across different classes by assigning specific weights for every sample"], "top_k_doc_id": [5868, 5872, 6137, 3953, 872, 6138, 6139, 2663, 1141, 5871, 1443, 6558, 1473, 3941, 5155], "orig_top_k_doc_id": [6137, 6139, 6138, 3953, 5868, 872, 5872, 1443, 6558, 5871, 1473, 3941, 5155, 1141, 2663]}, {"qid": 2419, "question": "Is such bias caused by bad annotation? in Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference", "answer": ["No"], "top_k_doc_id": [5868, 5872, 6137, 3953, 872, 6138, 6139, 2663, 1270, 3954, 584, 2746, 585, 1443, 5384], "orig_top_k_doc_id": [6137, 3953, 6139, 1270, 5868, 3954, 5872, 872, 584, 2746, 585, 1443, 2663, 5384, 6138]}, {"qid": 3778, "question": "Could the proposed training framework be applied to other NLP problems? in Mitigating Annotation Artifacts in Natural Language Inference Datasets to Improve Cross-dataset Generalization Ability", "answer": ["No", "No", "No"], "top_k_doc_id": [5868, 5872, 6137, 3953, 872, 6138, 6139, 325, 5655, 6036, 7632, 1473, 7847, 4915, 453], "orig_top_k_doc_id": [6137, 6139, 6138, 5872, 5868, 872, 3953, 6036, 7632, 1473, 5655, 7847, 4915, 453, 325]}, {"qid": 3780, "question": "Which datasets do they use in the cross-dataset evaluation? in Mitigating Annotation Artifacts in Natural Language Inference Datasets to Improve Cross-dataset Generalization Ability", "answer": ["SNLI, MultiNLI, JOCI, SICK", "SNLI, MultiNLI, JOCI, SICK", "SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13, SICK BIBREF14"], "top_k_doc_id": [5868, 5872, 6137, 3953, 872, 6138, 6139, 325, 5655, 6036, 2663, 6870, 6558, 4752, 5711], "orig_top_k_doc_id": [6137, 6139, 5872, 5868, 6138, 3953, 872, 2663, 6870, 325, 6558, 4752, 6036, 5655, 5711]}, {"qid": 695, "question": "Which training dataset allowed for the best generalization to benchmark sets? in Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "answer": ["MultiNLI"], "top_k_doc_id": [5868, 5872, 6137, 872, 875, 1874, 3080, 5685, 5871, 6139, 5246, 6138, 2118, 5338, 1217], "orig_top_k_doc_id": [872, 6137, 3080, 5871, 5685, 5868, 875, 5872, 2118, 5338, 5246, 6139, 1217, 6138, 1874]}, {"qid": 696, "question": "Which model generalized the best? in Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "answer": ["BERT"], "top_k_doc_id": [5868, 5872, 6137, 872, 875, 1874, 3080, 5685, 5871, 6139, 5246, 6138, 3985, 3984, 5335], "orig_top_k_doc_id": [872, 6137, 875, 6139, 6138, 3080, 5871, 5872, 1874, 5685, 3985, 3984, 5246, 5335, 5868]}, {"qid": 697, "question": "Which models were compared? in Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "answer": ["BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT"], "top_k_doc_id": [5868, 5872, 6137, 872, 875, 1874, 3080, 5685, 5871, 6139, 874, 1747, 5338, 2301, 2065], "orig_top_k_doc_id": [872, 875, 6137, 3080, 5871, 5872, 874, 1874, 6139, 5685, 5868, 1747, 5338, 2301, 2065]}, {"qid": 698, "question": "Which datasets were used? in Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "answer": ["SNLI, MultiNLI and SICK"], "top_k_doc_id": [5868, 5872, 6137, 872, 875, 1874, 3080, 5685, 5871, 6139, 874, 1747, 2118, 6138, 3081], "orig_top_k_doc_id": [872, 875, 6137, 3080, 5872, 5871, 1874, 874, 5868, 6139, 2118, 5685, 1747, 6138, 3081]}, {"qid": 3554, "question": "Do they recommend translating the premise and hypothesis together? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["No", "No"], "top_k_doc_id": [5868, 5872, 6137, 3953, 3954, 4752, 5871, 6035, 6424, 5869, 4027, 1874, 6031, 5699, 4184], "orig_top_k_doc_id": [5872, 5868, 5871, 6424, 5869, 3953, 6137, 3954, 4027, 1874, 6031, 5699, 6035, 4184, 4752]}, {"qid": 3556, "question": "What are examples of these artifacts? in Translation Artifacts in Cross-lingual Transfer Learning", "answer": ["hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap"], "top_k_doc_id": [5868, 5872, 6137, 3953, 3954, 4752, 5871, 6035, 6138, 6139, 6034, 2663, 4571, 2664, 7409], "orig_top_k_doc_id": [5872, 5868, 5871, 6138, 6137, 6139, 3953, 6035, 6034, 4752, 3954, 2663, 4571, 2664, 7409]}]}
{"group_id": 95, "group_size": 11, "items": [{"qid": 3834, "question": "What are the steps in the MagiCoder algorithm? in From narrative descriptions to MedDRA: automagically encoding adverse drug reactions", "answer": ["Preprocessing of the original text, Word-by-word linear scan of the description and \u201cvoting task\u201d, Weights calculation, Sorting of voted terms and winning terms release", "Procedure INLINEFORM0 takes the narrative description, performs tokenization and stop-word removal and puts it into an array of words., Procedures INLINEFORM0 and INLINEFORM1 get LLTs and create a dictionary of words and of their stemmed versions, respectively, By the functional notation INLINEFORM0 (resp., INLINEFORM1 ), we refer to the set of LLTs containing the word INLINEFORM2 (resp., the stem of INLINEFORM3 )., Function INLINEFORM0 returns the stemmed version of word INLINEFORM1 .\n\n, Function INLINEFORM0 returns the position of word INLINEFORM1 in term INLINEFORM2 .\n\n, INLINEFORM0 is a flag, initially set to 0, which holds 1 if at least a stemmed matching with the MedDRA term INLINEFORM1 is found., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are arrays and INLINEFORM3 appends INLINEFORM4 to array INLINEFORM5 , where INLINEFORM6 may be an element or a sequence of elements., INLINEFORM0 ( INLINEFORM1 ) are the weights related to the criteria defined in Section UID23 .\n\n, Procedure INLINEFORM0 performs the multi-value sorting of the array INLINEFORM1 based on the values of the properties INLINEFORM2 of its elements., Procedure INLINEFORM0 , where INLINEFORM1 is a set of terms and INLINEFORM2 is a term, tests whether INLINEFORM3 (considered as a string) is prefix of a term in INLINEFORM4 ., Dually, procedure INLINEFORM5 tests if in INLINEFORM6 there are one or more prefixes of INLINEFORM7 , and eventually remove them from INLINEFORM8 ., Function INLINEFORM0 specifies whether a word INLINEFORM1 has been already covered (i.e., a term voted by INLINEFORM2 has been selected) in the (partial) solution during the term release: INLINEFORM3 holds 1 if INLINEFORM4 has been covered (with or without stemming) and it holds 0 otherwise., Procedures INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 is a set of terms, implement ordered-phrases and maximal-set-of-voters criteria (defined in Section UID28 ), respectively., Function INLINEFORM0 , returns the first INLINEFORM1 elements of an ordered set INLINEFORM2 . If INLINEFORM3 , the function returns the complete list of ordered terms and INLINEFORM4 nil values.", "Definition of ad hoc data structures, Preprocessing of the original text, Word-by-word linear scan of the description and \u201cvoting task\u201d, Weights calculation, Sorting of voted terms and winning terms release"], "top_k_doc_id": [6193, 6194, 6195, 6916, 6196, 6198, 6201, 6202, 6203, 6204, 6205, 2127, 2332, 6200, 6199], "orig_top_k_doc_id": [6194, 6195, 6201, 6202, 6193, 6205, 6196, 6203, 6204, 6916, 6200, 6199, 6198, 2127, 2332]}, {"qid": 3835, "question": "How is the system constructed to be linear in the size of the narrative input and the terminology? in From narrative descriptions to MedDRA: automagically encoding adverse drug reactions", "answer": ["The system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.", "main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms", "The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms., INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description."], "top_k_doc_id": [6193, 6194, 6195, 6916, 6196, 6198, 6201, 6202, 6203, 6204, 6205, 2127, 2332, 6200, 2592], "orig_top_k_doc_id": [6194, 6195, 6193, 6202, 6201, 6205, 6196, 6916, 6204, 6198, 2127, 2592, 6200, 6203, 2332]}, {"qid": 4395, "question": "what were the evaluation metrics? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)", "F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)"], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 5827, 6919, 7836], "orig_top_k_doc_id": [6916, 3742, 6920, 2333, 6917, 6918, 6195, 2127, 2855, 2332, 6193, 6194, 7836, 5827, 6919]}, {"qid": 4396, "question": "what were their results on both tasks? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["No", "0.435 on Task1 and 0.673 on Task2."], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 5827, 6919, 451], "orig_top_k_doc_id": [6916, 3742, 6920, 2333, 6917, 6918, 6195, 2332, 2127, 2855, 6193, 6194, 6919, 5827, 451]}, {"qid": 4399, "question": "what surface-form features were used? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["INLINEFORM0 -grams, General-domain word embeddings, General-domain word clusters, Negation: presence of simple negators, the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words, presence of exclamation and question marks, whether the last token contains an exclamation or question mark", "INLINEFORM0 -grams, General-domain word embeddings, General-domain word clusters, Negation, Twitter-specific features, Punctuation"], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 5827, 6919, 451], "orig_top_k_doc_id": [6916, 3742, 6920, 6917, 2333, 6918, 6195, 2127, 2332, 6194, 6193, 2855, 6919, 5827, 451]}, {"qid": 3833, "question": "Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones? in From narrative descriptions to MedDRA: automagically encoding adverse drug reactions", "answer": ["No", "No", "No"], "top_k_doc_id": [6193, 6194, 6195, 6916, 6196, 6198, 6201, 6202, 6203, 6204, 6205, 2127, 6199, 176, 5784], "orig_top_k_doc_id": [6195, 6194, 6193, 6202, 6201, 6205, 6916, 6196, 6204, 6199, 2127, 6198, 6203, 176, 5784]}, {"qid": 4393, "question": "why do they think sentiment features do not result in improvement? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["Because sentiment features extracted the same information as other features.", "did not observe any improvement in the cross-validation experiments"], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 451, 6919, 3300], "orig_top_k_doc_id": [6916, 6920, 3742, 2333, 6918, 6917, 6195, 2332, 2127, 6193, 2855, 3300, 451, 6194, 6919]}, {"qid": 4394, "question": "what was the size of the datasets? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["10822, 4845", "training set containing 10,822 tweets and a development set containing 4,845 tweets, test set of 9,961 tweets was provided without labels, training set containing 8,000 tweets and a development set containing 2,260 tweets, test set of 7,513 tweets"], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 5827, 451, 3347], "orig_top_k_doc_id": [6916, 3742, 6920, 2333, 6918, 6917, 2332, 6195, 2127, 6193, 2855, 6194, 451, 5827, 3347]}, {"qid": 4397, "question": "what domain-specific features did they train on? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["INLINEFORM0 -grams generalized over domain terms, Pronoun Lexicon features, domain word embeddings, domain word clusters", "INLINEFORM0 -grams generalized over domain terms, Pronoun Lexicon features, domain word embeddings, domain word clusters"], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 451, 6919, 3348], "orig_top_k_doc_id": [6916, 6920, 3742, 6918, 2333, 6917, 2127, 6195, 2332, 6193, 6919, 6194, 2855, 451, 3348]}, {"qid": 4398, "question": "what are the sentiment features used? in NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake", "answer": ["the number of tokens with INLINEFORM0, the total score = INLINEFORM0, the maximal score = INLINEFORM0, the score of the last token in the tweet", "The following set of features were calculated separately for each tweet and each lexicon:\n\nthe number of tokens with INLINEFORM0 ;\n\nthe total score = INLINEFORM0 ;\n\nthe maximal score = INLINEFORM0 ;\n\nthe score of the last token in the tweet."], "top_k_doc_id": [6193, 6194, 6195, 6916, 2127, 2332, 2333, 2855, 3742, 6917, 6918, 6920, 451, 6919, 3300], "orig_top_k_doc_id": [6916, 6920, 3742, 2333, 6918, 6917, 6195, 2332, 2127, 2855, 6193, 451, 6194, 3300, 6919]}, {"qid": 3832, "question": "Did they test the idea that the system reduces the time needed to encode ADR reports on real pharmacologists?  in From narrative descriptions to MedDRA: automagically encoding adverse drug reactions", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6193, 6194, 6195, 6916, 6196, 6198, 6201, 6202, 6203, 6204, 6205, 6197, 6199, 6920, 6918], "orig_top_k_doc_id": [6193, 6195, 6194, 6201, 6202, 6916, 6196, 6205, 6204, 6203, 6198, 6197, 6199, 6920, 6918]}]}
{"group_id": 96, "group_size": 11, "items": [{"qid": 4040, "question": "Do they use pretrained embeddings in their model? in Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "answer": ["No", "No", "No"], "top_k_doc_id": [4826, 5998, 5997, 1132, 4619, 6492, 6493, 6496, 3157, 5554, 730, 7243, 5540, 5541, 3158], "orig_top_k_doc_id": [6492, 4826, 6496, 5540, 5541, 3157, 3158, 6493, 5998, 5997, 730, 7243, 4619, 5554, 1132]}, {"qid": 4041, "question": "What results are obtained by their model? in Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "answer": ["Our model outperforms PG-MMR when trained and tested on the Multi-News dataset, Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU", "Their model ranked 2nd on R-1 metric and ranked 1st on R-2 and R-SU metrics"], "top_k_doc_id": [4826, 5998, 5997, 1132, 4619, 6492, 6493, 6496, 3157, 5554, 730, 7243, 7241, 6716, 4760], "orig_top_k_doc_id": [6492, 4826, 5998, 6496, 7243, 6493, 7241, 5997, 1132, 5554, 730, 6716, 4619, 4760, 3157]}, {"qid": 1246, "question": "What dataset is used for this task? in Features in Extractive Supervised Single-document Summarization: Case of Persian News", "answer": ["the Pasokh dataset BIBREF42 "], "top_k_doc_id": [4826, 5998, 5997, 1694, 3715, 4825, 5554, 6839, 6955, 3198, 6492, 6925, 6716, 1132, 3200], "orig_top_k_doc_id": [6955, 3715, 1694, 4826, 4825, 6492, 5998, 6716, 5997, 6839, 5554, 3198, 6925, 1132, 3200]}, {"qid": 1247, "question": "What features of the document are integrated into vectors of every sentence? in Features in Extractive Supervised Single-document Summarization: Case of Persian News", "answer": ["Ordinal position, Length of sentence, The Ratio of Nouns, The Ratio of Numerical entities, Cue Words, Cosine position, Relative Length, TF-ISF, POS features, Document sentences, Document words, Topical category, Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs"], "top_k_doc_id": [4826, 5998, 5997, 1694, 3715, 4825, 5554, 6839, 6955, 3198, 6492, 6925, 1695, 5542, 5540], "orig_top_k_doc_id": [1694, 4826, 3715, 1695, 5998, 4825, 6839, 5997, 6955, 5542, 6492, 5540, 5554, 3198, 6925]}, {"qid": 1248, "question": "By how much is precission increased? in Features in Extractive Supervised Single-document Summarization: Case of Persian News", "answer": ["ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09"], "top_k_doc_id": [4826, 5998, 5997, 1694, 3715, 4825, 5554, 6839, 6955, 1132, 6496, 6716, 6492, 4380, 1698], "orig_top_k_doc_id": [4826, 1694, 3715, 6955, 5998, 5997, 5554, 4825, 6492, 4380, 1698, 6496, 6839, 6716, 1132]}, {"qid": 1249, "question": "Is new approach tested against state of the art? in Features in Extractive Supervised Single-document Summarization: Case of Persian News", "answer": ["No"], "top_k_doc_id": [4826, 5998, 5997, 1694, 3715, 4825, 5554, 6839, 6955, 1132, 6496, 6716, 5540, 2334, 7281], "orig_top_k_doc_id": [5540, 3715, 4826, 1694, 4825, 6716, 6955, 1132, 5997, 6496, 6839, 2334, 5998, 5554, 7281]}, {"qid": 4042, "question": "What sources do the news come from? in Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "answer": ["1500 news sites", "From a diverse set of news sources on site newser.com", "newser.com"], "top_k_doc_id": [4826, 5998, 5997, 1132, 4619, 6492, 6493, 6496, 3157, 5554, 4825, 7280, 6955, 3158, 6716], "orig_top_k_doc_id": [6492, 6493, 4826, 5998, 6496, 3157, 4825, 7280, 6955, 5997, 4619, 5554, 1132, 3158, 6716]}, {"qid": 2759, "question": "How much does their model outperform existing models? in Extractive Summarization of Long Documents by Combining Global and Local Context", "answer": ["Best proposed model result vs best previous result:\nArxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)\nPubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)", "On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.\n"], "top_k_doc_id": [4826, 5998, 4825, 4828, 4829, 1697, 4827, 4380, 1132, 543, 2419, 1694, 544, 2334, 730], "orig_top_k_doc_id": [4829, 4825, 4826, 4828, 4380, 4827, 1132, 543, 5998, 2419, 1694, 544, 2334, 1697, 730]}, {"qid": 2760, "question": "What do they mean by global and local context? in Extractive Summarization of Long Documents by Combining Global and Local Context", "answer": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "top_k_doc_id": [4826, 5998, 4825, 4828, 4829, 1697, 4827, 3137, 891, 2983, 210, 2048, 1683, 2985, 2984], "orig_top_k_doc_id": [4825, 4829, 4826, 4828, 4827, 3137, 891, 2983, 210, 2048, 1683, 1697, 5998, 2985, 2984]}, {"qid": 4043, "question": "What is the size of Multi-news dataset? in Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "answer": ["56216", "56,216", "56216 "], "top_k_doc_id": [4826, 5998, 5997, 1132, 4619, 6492, 6493, 6496, 7243, 6716, 1135, 3158, 3090, 6926, 7280], "orig_top_k_doc_id": [6492, 4826, 6493, 6496, 1132, 7243, 6716, 1135, 4619, 5998, 5997, 3158, 3090, 6926, 7280]}, {"qid": 2064, "question": "How do they define local variance? in Attention Optimization for Abstractive Document Summarization", "answer": ["The reciprocal of the variance of the attention distribution"], "top_k_doc_id": [4826, 5998, 4825, 4828, 4829, 3138, 3137, 4380, 6716, 5554, 4478, 544, 1255, 7761, 5997], "orig_top_k_doc_id": [3138, 3137, 4380, 6716, 4826, 5554, 4478, 4825, 4829, 544, 4828, 5998, 1255, 7761, 5997]}]}
{"group_id": 97, "group_size": 11, "items": [{"qid": 4203, "question": "How do they obtain structured data? in Toward Unsupervised Text Content Manipulation", "answer": ["The structured data is obtained from the box-score tables.", "split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record, we found some simple rules are sufficient to obtain high-quality results"], "top_k_doc_id": [7447, 7443, 7444, 7446, 6679, 6680, 6681, 6683, 6817, 7448, 447, 3451, 4399, 7445, 1827], "orig_top_k_doc_id": [7443, 7447, 6680, 7444, 6679, 7448, 6683, 6817, 7446, 6681, 3451, 4399, 7445, 1827, 447]}, {"qid": 4206, "question": "Which content coverage constraints do they design? in Toward Unsupervised Text Content Manipulation", "answer": ["Answer with content missing: (Content Coverage Constraint section) We thus devise an additional learning constraint based on the nature of content description\u2014each data tuple in the content record should usually be mentioned exactly once in the generated sentence.\nThe copy mechanism over content record x enables a simple yet effective way to encourage the behavior. Intuitively, we want each tuple to be copied once and only once on average.", "No", "No"], "top_k_doc_id": [7447, 7443, 7444, 7446, 6679, 6680, 6681, 6683, 6817, 7448, 447, 3452, 730, 1253, 1864], "orig_top_k_doc_id": [7443, 6683, 6681, 7444, 7447, 6679, 6680, 6817, 3452, 7448, 730, 1253, 7446, 447, 1864]}, {"qid": 4204, "question": "Which prior approaches for style transfer do they test with? in Toward Unsupervised Text Content Manipulation", "answer": ["Multi-Attribute Style Transfer, Adversarial Style Transfer ", "AttnCopy-S2S, Rule-based Method, Multi-Attribute Style Transfer (MAST) BIBREF11, Adversarial Style Transfer (AdvST) BIBREF12", "Multi-Attribute Style Transfer, Adversarial Style Transfer"], "top_k_doc_id": [7447, 7443, 7444, 7446, 6679, 6680, 6681, 6683, 6817, 7448, 6957, 5845, 6703, 5174, 6955], "orig_top_k_doc_id": [7447, 6679, 6681, 7443, 7448, 6680, 6683, 7444, 7446, 6817, 6957, 5845, 6703, 5174, 6955]}, {"qid": 4776, "question": "How better are results of new model compared to competitive methods? in Learning to Select Bi-Aspect Information for Document-Scale Text Content Manipulation", "answer": ["For Document- level comparison, the model achieves highest CS precision and  F1 score and it achieves higher BLEU score that TMTE, Coatt, CCDT, and HEDT. \nIn terms of Human Evaluation, the model had the highest average score, the highest Fluency score, and the second highest Content Fidelity. \nIn terms of Sentence-level comparison the model had the highest Recall and F1 scores for Content Fidelity.", "No"], "top_k_doc_id": [7447, 7443, 7444, 7446, 4826, 5690, 6686, 6689, 7762, 6683, 5808, 7448, 5695, 2306, 6401], "orig_top_k_doc_id": [7447, 7443, 7444, 7446, 4826, 6689, 6686, 7762, 6683, 5808, 7448, 5690, 5695, 2306, 6401]}, {"qid": 4777, "question": "What is the metrics used for benchmarking methods? in Learning to Select Bi-Aspect Information for Document-Scale Text Content Manipulation", "answer": ["Content Fidelity (CF) , Content selection, (CS), BLEU ", "Content Fidelity (CF), Style Preservation, BLEU score, Content selection"], "top_k_doc_id": [7447, 7443, 7444, 7446, 4826, 447, 3945, 6435, 6472, 6681, 6651, 1396, 6689, 2969, 6683], "orig_top_k_doc_id": [7447, 7443, 7446, 7444, 4826, 6681, 6651, 1396, 6472, 6689, 6435, 3945, 2969, 447, 6683]}, {"qid": 4778, "question": "What are other competitive methods? in Learning to Select Bi-Aspect Information for Document-Scale Text Content Manipulation", "answer": ["Rule-based Slot Filling Method (Rule-SF), Copy-based Slot Filling Method (Copy-SF), Conditional Copy based Data-To-Text (CCDT), Hierarchical Encoder for Data-To-Text (HEDT), Text Manipulation with Table Encoder (TMTE), Co-attention-based Method (Coatt), attention-based Seq2Seq method with copy mechanism, rule-based method, MAST, AdvST, S-SOTA", " Rule-based Slot Filling Method (Rule-SF), Copy-based Slot Filling Method (Copy-SF) , Conditional Copy based Data-To-Text (CCDT), Data-To-Text (HEDT) , Table Encoder (TMTE),  Co-attention-based Method (Coatt)"], "top_k_doc_id": [7447, 7443, 7444, 7446, 4826, 5690, 6686, 6689, 7762, 6435, 447, 6687, 433, 2969, 460], "orig_top_k_doc_id": [7447, 7443, 7444, 7446, 4826, 6435, 6686, 7762, 447, 6689, 6687, 433, 2969, 5690, 460]}, {"qid": 4779, "question": "What is the size of built dataset? in Learning to Select Bi-Aspect Information for Document-Scale Text Content Manipulation", "answer": ["Document-level dataset has total of 4821 instances. \nSentence-level dataset has total of 45583 instances. ", "Total number of documents is 4821. Total number of sentences is 47583."], "top_k_doc_id": [7447, 7443, 7444, 7446, 4826, 447, 3945, 6435, 6472, 6687, 5690, 4587, 1822, 7293, 6680], "orig_top_k_doc_id": [7447, 7443, 7444, 7446, 6472, 6687, 4826, 5690, 6435, 4587, 1822, 447, 7293, 3945, 6680]}, {"qid": 4205, "question": "Which competing objectives for their unsupevised method do they use? in Toward Unsupervised Text Content Manipulation", "answer": ["A combination of Content Objective and Style Objective", "Reconstructing the auxiliary sentence and reconstructing the reference sentence."], "top_k_doc_id": [7447, 7443, 7444, 7446, 6679, 6680, 6681, 6683, 6817, 7448, 6558, 7275, 3486, 6690, 436], "orig_top_k_doc_id": [6683, 7444, 6679, 6817, 6681, 7447, 7443, 6680, 7446, 6558, 7448, 7275, 3486, 6690, 436]}, {"qid": 4209, "question": "Which dataset do they use for text altering attributes matching to image parts? in ManiGAN: Text-Guided Image Manipulation", "answer": ["No", "CUB bird, COCO"], "top_k_doc_id": [7447, 413, 3033, 6438, 6686, 6687, 6688, 6689, 6690, 6691, 3945, 276, 3414, 6407, 1436], "orig_top_k_doc_id": [6686, 6690, 6689, 6691, 6688, 6687, 3033, 6438, 3945, 276, 413, 3414, 7447, 6407, 1436]}, {"qid": 4210, "question": "Is it possible for the DCM module to correct text-relevant content? in ManiGAN: Text-Guided Image Manipulation", "answer": ["No", "No", "Yes"], "top_k_doc_id": [7447, 413, 3033, 6438, 6686, 6687, 6688, 6689, 6690, 6691, 914, 5690, 4033, 7443, 5694], "orig_top_k_doc_id": [6686, 6690, 6689, 6688, 6691, 6687, 3033, 6438, 7447, 914, 5690, 4033, 7443, 413, 5694]}, {"qid": 3278, "question": "What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)? in A Hierarchical Model for Data-to-Text Generation", "answer": ["Hierarchical-k"], "top_k_doc_id": [7447, 5495, 3467, 1943, 5496, 5079, 7446, 3468, 6043, 3092, 1253, 7374, 5587, 5896, 1544], "orig_top_k_doc_id": [5495, 3467, 1943, 5496, 5079, 7446, 7447, 3468, 6043, 3092, 1253, 7374, 5587, 5896, 1544]}]}
{"group_id": 98, "group_size": 11, "items": [{"qid": 4722, "question": "Which English dataset do they evaluate on? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": ["the original MS-COCO English dataset", "MS-COCO"], "top_k_doc_id": [4744, 7085, 7355, 7356, 7357, 7358, 2729, 4745, 4746, 2899, 3034, 7580, 83, 7088, 3031], "orig_top_k_doc_id": [7357, 7355, 7358, 7356, 7580, 7085, 4745, 83, 7088, 4744, 4746, 3034, 2899, 3031, 2729]}, {"qid": 4723, "question": "What neural network models do they use in their evaluation? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": ["CNN , RNN - LSTM", "Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey"], "top_k_doc_id": [4744, 7085, 7355, 7356, 7357, 7358, 3799, 7088, 7580, 2733, 3800, 7581, 4033, 4036, 6995], "orig_top_k_doc_id": [7357, 7355, 7358, 7356, 7085, 4744, 7580, 3799, 2733, 7088, 3800, 7581, 4033, 4036, 6995]}, {"qid": 4726, "question": "What deep neural network models are used in evaluation? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": ["encoder-decoder architecture of CNN for encoding and LSTM for decoding", "CNN, RNN - LSTM"], "top_k_doc_id": [4744, 7085, 7355, 7356, 7357, 7358, 3799, 7088, 7580, 2733, 3800, 7581, 3026, 491, 4745], "orig_top_k_doc_id": [7355, 7358, 7357, 7356, 2733, 4744, 7580, 3026, 7085, 491, 3799, 7088, 3800, 7581, 4745]}, {"qid": 4727, "question": "How authors evaluate datasets using models trained on different datasets? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": [" The two models are trained with three mentioned datasets, then validated on  subset for each dataset and evaluated using BLEU, ROUGE and CIDEr measures.", "They evaluate on three metrics BLUE, ROUGE and CIDEr trained on the mentioned datasets."], "top_k_doc_id": [4744, 7085, 7355, 7356, 7357, 7358, 2729, 4745, 4746, 2899, 3034, 7580, 2414, 2900, 4747], "orig_top_k_doc_id": [7357, 7358, 7355, 7356, 7580, 4744, 7085, 4746, 4745, 2899, 2414, 3034, 2729, 2900, 4747]}, {"qid": 2710, "question": "Which datasets are used? in Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity", "answer": ["Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE", "ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio"], "top_k_doc_id": [4744, 3800, 4745, 4746, 4747, 6320, 7355, 7357, 2258, 5850, 7085, 6995, 490, 7037, 7690], "orig_top_k_doc_id": [4744, 4747, 4745, 4746, 6320, 3800, 7357, 2258, 6995, 490, 5850, 7037, 7690, 7355, 7085]}, {"qid": 2711, "question": "Which existing models are evaluated? in Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity", "answer": ["Show&Tell and LRCN1u", "Show&Tell model, LRCN1u"], "top_k_doc_id": [4744, 3800, 4745, 4746, 4747, 6320, 7355, 7357, 2258, 5850, 7085, 4748, 2733, 2900, 697], "orig_top_k_doc_id": [4744, 4747, 4745, 4746, 6320, 4748, 3800, 7357, 7355, 2258, 2733, 7085, 5850, 2900, 697]}, {"qid": 4724, "question": "Do they use crowdsourcing for the captions? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": ["Yes", "Yes"], "top_k_doc_id": [4744, 7085, 7355, 7356, 7357, 7358, 2729, 4745, 4746, 6995, 7088, 7087, 4036, 4747, 7086], "orig_top_k_doc_id": [7357, 7358, 7355, 7356, 6995, 7085, 4744, 7088, 7087, 4745, 2729, 4036, 4746, 4747, 7086]}, {"qid": 4725, "question": "What methods are used to build two other Viatnamese datsets? in UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning", "answer": ["Translation and annotation.", "human translation and Google Translation service"], "top_k_doc_id": [4744, 7085, 7355, 7356, 7357, 7358, 3799, 7088, 7580, 2922, 3031, 6995, 2414, 2956, 2729], "orig_top_k_doc_id": [7355, 7357, 7358, 7356, 7580, 2922, 7085, 3031, 3799, 6995, 2414, 4744, 2956, 7088, 2729]}, {"qid": 2709, "question": "Are the images from a specific domain? in Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity", "answer": ["Yes", "Yes"], "top_k_doc_id": [4744, 3800, 4745, 4746, 4747, 6320, 7355, 7357, 7690, 6995, 7145, 3804, 2729, 2733, 490], "orig_top_k_doc_id": [4744, 4745, 4747, 4746, 3800, 6320, 7357, 7355, 7690, 6995, 7145, 3804, 2729, 2733, 490]}, {"qid": 2449, "question": "What are the common captioning metrics? in Image Captioning: Transforming Objects into Words", "answer": ["the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics"], "top_k_doc_id": [4744, 7085, 7355, 7356, 4036, 3026, 4033, 7144, 4035, 4746, 4745, 3799, 6995, 2413, 2922], "orig_top_k_doc_id": [4036, 4744, 7355, 3026, 4033, 7144, 4035, 7356, 4746, 7085, 4745, 3799, 6995, 2413, 2922]}, {"qid": 2712, "question": "How is diversity measured? in Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity", "answer": ["diversity score as the ratio of observed number versus optimal number", " we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number"], "top_k_doc_id": [4744, 3800, 4745, 4746, 4747, 6320, 2258, 2261, 613, 612, 5730, 7690, 6995, 3236, 5727], "orig_top_k_doc_id": [4744, 4747, 4745, 4746, 3800, 2258, 6320, 2261, 613, 612, 5730, 7690, 6995, 3236, 5727]}]}
{"group_id": 99, "group_size": 11, "items": [{"qid": 4878, "question": "Which dataset do they evaluate on? in Response Generation by Context-aware Prototype Editing", "answer": [" a large scale Chinese conversation corpus", "Chinese conversation corpus comprised of 20 million context-response pairs", "Chinese dataset containing human-human context response pairs collected from Douban Group "], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 848, 2442, 3190, 6649, 6646, 6648, 6705], "orig_top_k_doc_id": [7570, 7566, 7567, 7571, 7568, 6703, 7569, 6704, 2442, 6649, 6646, 6705, 6648, 848, 3190]}, {"qid": 4879, "question": "What model architecture do they use for the decoder? in Response Generation by Context-aware Prototype Editing", "answer": ["a GRU language model", "a GRU language model", "GRU"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 848, 2442, 3190, 6649, 6646, 6648, 2970], "orig_top_k_doc_id": [7566, 7567, 7568, 7571, 7570, 6703, 7569, 6704, 6646, 6649, 2442, 3190, 6648, 848, 2970]}, {"qid": 4220, "question": "What are existing baseline models on these benchmark datasets? in Prototype-to-Style: Dialogue Generation with Style-Aware Editing on Retrieval Memory", "answer": ["Seq2seq, GPT2-FT, Speaker, ECM, Skeleton-to-Response (SR), Retrieval + Style Transfer (RST), Retrieval + Reranking (RRe)", "Generative Approaches ::: Seq2seq, Generative Approaches ::: GPT2-FT:, Generative Approaches ::: Speaker:, Generative Approaches ::: ECM:, Retrieval-Based Approaches ::: Skeleton-to-Response (SR), Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST), Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST), Retrieval-Based Approaches ::: Retrieval + Reranking (RRe)"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 5798, 6705, 2210, 5813, 2442, 6706, 7443], "orig_top_k_doc_id": [6703, 6705, 7566, 6704, 7567, 7570, 7571, 7568, 7569, 2210, 2442, 6706, 5798, 7443, 5813]}, {"qid": 4222, "question": "What three benchmark datasets are used? in Prototype-to-Style: Dialogue Generation with Style-Aware Editing on Retrieval Memory", "answer": ["gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, sentiment-specific (English) dataset", "Gender-Specific Dialogue Dataset, Emotion-Specific Dialogue Dataset, Sentiment-Specific Dialogue Dataset"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 5798, 6705, 2210, 5813, 2442, 6706, 5174], "orig_top_k_doc_id": [6703, 6705, 6704, 7567, 7566, 7570, 7571, 7568, 7569, 6706, 2210, 5174, 5813, 5798, 2442]}, {"qid": 4877, "question": "Which aspects of response generation do they evaluate on? in Response Generation by Context-aware Prototype Editing", "answer": ["fluency, relevance, diversity , originality"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 848, 2442, 3190, 6649, 2970, 6705, 2969], "orig_top_k_doc_id": [7567, 7566, 7570, 7571, 7568, 6703, 7569, 6704, 848, 2970, 6705, 2969, 2442, 6649, 3190]}, {"qid": 4221, "question": "On what two languages is experimented on? in Prototype-to-Style: Dialogue Generation with Style-Aware Editing on Retrieval Memory", "answer": ["Chinese and English", "Chinese, English", "English and Chinese"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 5798, 6705, 2210, 5813, 2156, 3194, 7443], "orig_top_k_doc_id": [6703, 7567, 6704, 6705, 7566, 7570, 7571, 7568, 5813, 7569, 2156, 2210, 5798, 3194, 7443]}, {"qid": 4881, "question": "What do they use as the pre-defined index of prototype responses? in Response Generation by Context-aware Prototype Editing", "answer": ["similar context INLINEFORM1 and its associated response INLINEFORM2", "to compute the context similarity."], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 848, 2442, 3190, 6705, 5257, 6594, 6595], "orig_top_k_doc_id": [7566, 7567, 7568, 7571, 7570, 6703, 6704, 7569, 6705, 2442, 5257, 6594, 848, 3190, 6595]}, {"qid": 4219, "question": "Is there a metric that also rewards good stylistic response? in Prototype-to-Style: Dialogue Generation with Style-Aware Editing on Retrieval Memory", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 5798, 6705, 6706, 5175, 6679, 5174, 6589], "orig_top_k_doc_id": [6703, 6704, 6705, 7570, 7567, 6706, 7566, 7568, 7571, 7569, 5175, 6679, 5174, 5798, 6589]}, {"qid": 4880, "question": "Do they ensure the edited response is grammatical? in Response Generation by Context-aware Prototype Editing", "answer": ["Yes"], "top_k_doc_id": [7568, 7571, 6703, 6704, 7566, 7567, 7569, 7570, 4962, 4963, 6705, 6649, 2969, 3238, 6648], "orig_top_k_doc_id": [7566, 7570, 7567, 7571, 7568, 6703, 6704, 7569, 4962, 4963, 6705, 6649, 2969, 3238, 6648]}, {"qid": 4923, "question": "Is this model trained in unsuperized manner? in Insertion-Deletion Transformer", "answer": ["No", "No"], "top_k_doc_id": [7568, 7571, 2488, 2489, 2490, 4202, 6425, 6426, 7635, 7636, 7637, 4148, 5242, 1140, 1886], "orig_top_k_doc_id": [7635, 7636, 7637, 2488, 6425, 4148, 7571, 4202, 7568, 2489, 5242, 1140, 1886, 2490, 6426]}, {"qid": 4924, "question": "How much is BELU score difference between proposed approach and insertion-only method? in Insertion-Deletion Transformer", "answer": [" deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points", "Learning shifted alphabetic sequences: 21.34\nCaesar's Cipher: 2.02"], "top_k_doc_id": [7568, 7571, 2488, 2489, 2490, 4202, 6425, 6426, 7635, 7636, 7637, 4203, 7566, 7567, 7570], "orig_top_k_doc_id": [7635, 7636, 7637, 4202, 2488, 6426, 2489, 7568, 2490, 4203, 7571, 7566, 7567, 6425, 7570]}]}
{"group_id": 100, "group_size": 10, "items": [{"qid": 108, "question": "By how much do they outperform standard BERT? in Enriching BERT with Knowledge Graph Embeddings for Document Classification", "answer": ["up to four percentage points in accuracy"], "top_k_doc_id": [123, 126, 124, 7545, 2306, 7472, 7546, 7678, 4276, 1560, 1704, 3777, 125, 1363, 7288], "orig_top_k_doc_id": [126, 123, 124, 7545, 4276, 2306, 7472, 1560, 1704, 3777, 7546, 125, 1363, 7678, 7288]}, {"qid": 109, "question": "What dataset do they use? in Enriching BERT with Knowledge Graph Embeddings for Document Classification", "answer": ["2019 GermEval shared task on hierarchical text classification", "GermEval 2019 shared task"], "top_k_doc_id": [123, 126, 124, 7545, 2306, 7472, 7546, 7678, 2234, 5956, 5898, 6051, 6922, 4652, 417], "orig_top_k_doc_id": [126, 124, 123, 7545, 2234, 2306, 5956, 7678, 5898, 7546, 6051, 6922, 4652, 7472, 417]}, {"qid": 4747, "question": "Does the paper report F1-scores with and without post-processing for the second task? in TwistBytes -- Hierarchical Classification at GermEval 2019: walking the fine line (of recall and precision)", "answer": ["Yes", "With post-processing"], "top_k_doc_id": [123, 126, 125, 7399, 7402, 4516, 4613, 4614, 6018, 4439, 6179, 5815, 2458, 764, 1151], "orig_top_k_doc_id": [7402, 126, 7399, 123, 4613, 4516, 4614, 125, 6018, 4439, 6179, 5815, 2458, 764, 1151]}, {"qid": 4748, "question": "What does post-processing do to the output? in TwistBytes -- Hierarchical Classification at GermEval 2019: walking the fine line (of recall and precision)", "answer": ["Set treshold  for prediction.", "Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample"], "top_k_doc_id": [123, 126, 125, 7399, 7402, 4516, 153, 5913, 6093, 4116, 5547, 5144, 4338, 5235, 5615], "orig_top_k_doc_id": [7402, 7399, 123, 126, 153, 5913, 6093, 125, 4516, 4116, 5547, 5144, 4338, 5235, 5615]}, {"qid": 4749, "question": "Do they test any neural architecture? in TwistBytes -- Hierarchical Classification at GermEval 2019: walking the fine line (of recall and precision)", "answer": ["No", "Yes"], "top_k_doc_id": [123, 126, 125, 7399, 7402, 3577, 6017, 124, 6266, 6018, 3769, 153, 1709, 3768, 4440], "orig_top_k_doc_id": [123, 7399, 7402, 124, 3577, 125, 6266, 6017, 6018, 126, 3769, 153, 1709, 3768, 4440]}, {"qid": 4750, "question": "Is the performance of a Naive Bayes approach evaluated? in TwistBytes -- Hierarchical Classification at GermEval 2019: walking the fine line (of recall and precision)", "answer": ["No", "No"], "top_k_doc_id": [123, 126, 125, 7399, 7402, 3577, 6017, 4116, 4950, 7187, 5202, 4819, 1485, 6179, 5769], "orig_top_k_doc_id": [7402, 123, 4116, 7399, 4950, 126, 7187, 5202, 3577, 4819, 1485, 6179, 125, 6017, 5769]}, {"qid": 110, "question": "How do they combine text representations with the knowledge graph embeddings? in Enriching BERT with Knowledge Graph Embeddings for Document Classification", "answer": ["all three representations are concatenated and passed into a MLP"], "top_k_doc_id": [123, 126, 124, 7545, 5212, 460, 5215, 3777, 68, 3628, 2101, 2234, 5898, 4980, 6818], "orig_top_k_doc_id": [124, 126, 123, 5212, 460, 5215, 7545, 3777, 68, 3628, 2101, 2234, 5898, 4980, 6818]}, {"qid": 1236, "question": "What languages are the model evaluated on? in Bidirectional Context-Aware Hierarchical Attention Network for Document Understanding", "answer": ["No"], "top_k_doc_id": [123, 1679, 1683, 1684, 5160, 5369, 1512, 2362, 2804, 5896, 6715, 7359, 6044, 6043, 1251], "orig_top_k_doc_id": [1679, 1683, 5369, 1684, 5160, 5896, 6715, 6044, 6043, 7359, 2804, 123, 2362, 1512, 1251]}, {"qid": 1238, "question": "What are the datasets used in Bidirectional Context-Aware Hierarchical Attention Network for Document Understanding", "answer": ["large-scale document classification datasets introduced by BIBREF14"], "top_k_doc_id": [123, 1679, 1683, 1684, 5160, 5369, 1512, 2362, 2804, 5896, 6715, 7359, 6381, 1357, 5370], "orig_top_k_doc_id": [1683, 1679, 1684, 5896, 5369, 7359, 1512, 5160, 6381, 123, 2362, 6715, 1357, 5370, 2804]}, {"qid": 1237, "question": "Do they compare to other models appart from HAN? in Bidirectional Context-Aware Hierarchical Attention Network for Document Understanding", "answer": ["No"], "top_k_doc_id": [123, 1679, 1683, 1684, 5160, 5369, 6381, 1682, 6018, 1681, 2684, 1680, 4652, 7549, 6044], "orig_top_k_doc_id": [1683, 1679, 1684, 6381, 1682, 6018, 1681, 2684, 1680, 123, 4652, 5369, 7549, 5160, 6044]}]}
{"group_id": 101, "group_size": 10, "items": [{"qid": 366, "question": "What is CamemBERT trained on? in CamemBERT: a Tasty French Language Model", "answer": ["unshuffled version of the French OSCAR corpus"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 1009, 7226, 4727, 5349, 6944, 6945, 703, 1008], "orig_top_k_doc_id": [439, 436, 440, 438, 437, 534, 533, 7226, 1009, 6945, 4727, 703, 6944, 5349, 1008]}, {"qid": 372, "question": "What data is used for training CamemBERT? in CamemBERT: a Tasty French Language Model", "answer": ["unshuffled version of the French OSCAR corpus"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 1009, 7226, 4727, 5349, 6944, 6945, 1639, 7024], "orig_top_k_doc_id": [439, 436, 440, 437, 438, 534, 533, 7226, 6945, 1009, 4727, 5349, 6944, 1639, 7024]}, {"qid": 368, "question": "What is the state of the art? in CamemBERT: a Tasty French Language Model", "answer": ["POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 1009, 7226, 1008, 1581, 2330, 2329, 7245, 5773], "orig_top_k_doc_id": [439, 436, 440, 438, 437, 534, 533, 1008, 1581, 2330, 7226, 2329, 1009, 7245, 5773]}, {"qid": 369, "question": "How much better was results of CamemBERT than previous results on these tasks? in CamemBERT: a Tasty French Language Model", "answer": ["2.36 point increase in the F1 score with respect to the best SEM architecture, on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM), lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa, For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT, For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 4213, 4727, 4682, 6945, 2811, 5348, 7408, 4030], "orig_top_k_doc_id": [439, 436, 440, 438, 437, 534, 533, 4682, 4213, 4727, 6945, 2811, 5348, 7408, 4030]}, {"qid": 371, "question": "How long was CamemBERT trained? in CamemBERT: a Tasty French Language Model", "answer": ["No"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 4213, 4727, 7226, 785, 1289, 1049, 699, 6943], "orig_top_k_doc_id": [439, 436, 440, 438, 437, 534, 533, 4727, 7226, 4213, 785, 1289, 1049, 699, 6943]}, {"qid": 367, "question": "Which tasks does CamemBERT not improve on? in CamemBERT: a Tasty French Language Model", "answer": ["its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 6944, 6943, 1885, 4727, 786, 4031, 1477, 6310], "orig_top_k_doc_id": [439, 436, 437, 440, 438, 534, 533, 6944, 6943, 1885, 4727, 786, 4031, 1477, 6310]}, {"qid": 458, "question": "What is the state of the art? in RobBERT: a Dutch RoBERTa-based Language Model", "answer": ["BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT"], "top_k_doc_id": [438, 439, 533, 534, 535, 536, 2875, 3620, 436, 6135, 7119, 6853, 2226, 3617, 5248], "orig_top_k_doc_id": [536, 534, 535, 533, 439, 438, 7119, 3620, 6135, 2875, 436, 6853, 2226, 3617, 5248]}, {"qid": 459, "question": "What language tasks did they experiment on? in RobBERT: a Dutch RoBERTa-based Language Model", "answer": ["sentiment analysis, the disambiguation of demonstrative pronouns,"], "top_k_doc_id": [438, 439, 533, 534, 535, 536, 2875, 3620, 436, 6135, 3548, 4993, 3893, 4994, 2906], "orig_top_k_doc_id": [536, 534, 535, 533, 438, 439, 3548, 4993, 3893, 3620, 2875, 436, 6135, 4994, 2906]}, {"qid": 370, "question": "Was CamemBERT compared against multilingual BERT on these tasks? in CamemBERT: a Tasty French Language Model", "answer": ["Yes"], "top_k_doc_id": [438, 439, 533, 534, 436, 437, 440, 2908, 5570, 6039, 5572, 7285, 3621, 4030, 7290], "orig_top_k_doc_id": [439, 436, 440, 437, 438, 534, 533, 2908, 5570, 6039, 5572, 7285, 3621, 4030, 7290]}, {"qid": 457, "question": "What data did they use? in RobBERT: a Dutch RoBERTa-based Language Model", "answer": ["the Dutch section of the OSCAR corpus"], "top_k_doc_id": [438, 439, 533, 534, 535, 536, 2875, 3620, 3893, 1219, 4994, 1218, 4277, 6853, 5248], "orig_top_k_doc_id": [536, 534, 535, 533, 438, 3893, 2875, 439, 1219, 4994, 1218, 3620, 4277, 6853, 5248]}]}
{"group_id": 102, "group_size": 10, "items": [{"qid": 770, "question": "How is the data annotated? in The First Evaluation of Chinese Human-Computer Dialogue Technology", "answer": ["No"], "top_k_doc_id": [966, 965, 2970, 6705, 196, 6211, 7379, 7149, 5257, 5260, 736, 6036, 1817, 5427, 2013], "orig_top_k_doc_id": [965, 966, 7149, 5257, 5260, 6705, 736, 6211, 6036, 1817, 5427, 2970, 2013, 196, 7379]}, {"qid": 771, "question": "What collection steps do they mention? in The First Evaluation of Chinese Human-Computer Dialogue Technology", "answer": ["No"], "top_k_doc_id": [966, 965, 2970, 6705, 196, 6211, 7379, 7351, 6592, 7222, 5744, 482, 197, 5911, 6039], "orig_top_k_doc_id": [965, 966, 7351, 6705, 6592, 7222, 196, 6211, 5744, 2970, 482, 197, 5911, 7379, 6039]}, {"qid": 769, "question": "What problems are found with the evaluation scheme? in The First Evaluation of Chinese Human-Computer Dialogue Technology", "answer": ["no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue"], "top_k_doc_id": [966, 965, 2970, 3192, 6651, 736, 4317, 404, 4105, 2630, 3193, 6036, 4666, 1091, 787], "orig_top_k_doc_id": [965, 966, 2970, 6651, 736, 4317, 404, 3192, 4105, 2630, 3193, 6036, 4666, 1091, 787]}, {"qid": 773, "question": "What was the result of the highest performing system? in The First Evaluation of Chinese Human-Computer Dialogue Technology", "answer": ["For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2"], "top_k_doc_id": [966, 965, 2970, 6705, 7478, 2013, 3193, 5426, 865, 7372, 5798, 7149, 5749, 5744, 7479], "orig_top_k_doc_id": [965, 966, 6705, 7478, 2013, 3193, 2970, 5426, 865, 7372, 5798, 7149, 5749, 5744, 7479]}, {"qid": 774, "question": "What metrics are used in the evaluation? in The First Evaluation of Chinese Human-Computer Dialogue Technology", "answer": ["For task 1, we use F1-score, Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, Guidance ability for out of scope input"], "top_k_doc_id": [966, 965, 2970, 3192, 6651, 6705, 1817, 6861, 286, 2436, 2435, 4744, 566, 6862, 5260], "orig_top_k_doc_id": [965, 966, 3192, 6705, 1817, 6651, 6861, 286, 2436, 2970, 2435, 4744, 566, 6862, 5260]}, {"qid": 2739, "question": "What is the size of this dataset? in An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction", "answer": ["23,700 ", " 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries."], "top_k_doc_id": [966, 200, 3571, 4785, 4786, 4787, 5920, 3607, 3609, 138, 201, 965, 3606, 3610, 98], "orig_top_k_doc_id": [4785, 4787, 4786, 138, 3609, 3607, 3571, 5920, 3606, 200, 966, 965, 3610, 98, 201]}, {"qid": 2740, "question": "Where does the data come from? in An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction", "answer": ["crowsourcing platform", "For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. \nFor out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere."], "top_k_doc_id": [966, 200, 3571, 4785, 4786, 4787, 5920, 3607, 3609, 138, 201, 965, 3606, 5919, 3309], "orig_top_k_doc_id": [4787, 4786, 4785, 3571, 3607, 966, 138, 5919, 200, 5920, 201, 965, 3609, 3309, 3606]}, {"qid": 2737, "question": "How was the dataset annotated? in An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction", "answer": ["intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task", "manually "], "top_k_doc_id": [966, 200, 3571, 4785, 4786, 4787, 5920, 3607, 3609, 5918, 3562, 1299, 5919, 3610, 4140], "orig_top_k_doc_id": [4787, 4786, 4785, 3607, 5918, 3562, 5920, 1299, 5919, 3610, 3609, 3571, 200, 4140, 966]}, {"qid": 772, "question": "How many intents were classified? in The First Evaluation of Chinese Human-Computer Dialogue Technology", "answer": ["two"], "top_k_doc_id": [966, 965, 2277, 6211, 2276, 1817, 1296, 400, 5427, 286, 401, 3680, 1712, 3681, 6039], "orig_top_k_doc_id": [965, 966, 2277, 6211, 2276, 1817, 1296, 400, 5427, 286, 401, 3680, 1712, 3681, 6039]}, {"qid": 2738, "question": "Which classifiers are evaluated? in An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction", "answer": ["SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU", "SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU"], "top_k_doc_id": [966, 200, 3571, 4785, 4786, 4787, 5920, 5919, 5918, 201, 98, 3334, 7756, 150, 100], "orig_top_k_doc_id": [4785, 4786, 4787, 5919, 5918, 3571, 200, 201, 98, 3334, 5920, 966, 7756, 150, 100]}]}
{"group_id": 103, "group_size": 10, "items": [{"qid": 884, "question": "What measures were used for human evaluation? in CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning", "answer": ["To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself)."], "top_k_doc_id": [945, 1136, 1137, 1138, 1139, 1140, 5473, 5474, 495, 1159, 4993, 1822, 4744, 6050, 3973], "orig_top_k_doc_id": [1136, 1140, 1139, 1137, 1138, 5473, 495, 5474, 6050, 945, 1822, 4744, 1159, 3973, 4993]}, {"qid": 885, "question": "What automatic metrics are used for this task? in CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning", "answer": ["BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, BERTScore"], "top_k_doc_id": [945, 1136, 1137, 1138, 1139, 1140, 5473, 5474, 495, 1159, 4993, 1822, 4744, 1156, 4994], "orig_top_k_doc_id": [1136, 1140, 1139, 1138, 1137, 5473, 5474, 495, 4744, 4993, 1822, 1159, 1156, 4994, 945]}, {"qid": 886, "question": "Are the models required to also generate rationales? in CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning", "answer": ["No"], "top_k_doc_id": [945, 1136, 1137, 1138, 1139, 1140, 5473, 5474, 495, 1159, 4993, 1156, 1925, 4994, 1290], "orig_top_k_doc_id": [1136, 1139, 1140, 1137, 1138, 1156, 5473, 4994, 1290, 4993, 5474, 495, 945, 1925, 1159]}, {"qid": 887, "question": "Are the rationales generated after the sentences were written? in CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning", "answer": ["Yes"], "top_k_doc_id": [945, 1136, 1137, 1138, 1139, 1140, 5473, 5474, 495, 1159, 4993, 1156, 1925, 4994, 946], "orig_top_k_doc_id": [1136, 1139, 1137, 1140, 1138, 5473, 5474, 4994, 945, 1925, 1156, 495, 1159, 946, 4993]}, {"qid": 566, "question": "Which of their training domains improves performance the most? in A Simple Method for Commonsense Reasoning", "answer": ["documents from the CommonCrawl dataset that has the most overlapping n-grams with the question"], "top_k_doc_id": [945, 1136, 1820, 1821, 3972, 4273, 4277, 693, 4278, 4994, 5473, 692, 1159, 6002, 946], "orig_top_k_doc_id": [4278, 945, 4277, 692, 693, 1821, 1159, 1820, 1136, 4273, 6002, 3972, 5473, 946, 4994]}, {"qid": 567, "question": "Do they fine-tune their model on the end task? in A Simple Method for Commonsense Reasoning", "answer": ["Yes"], "top_k_doc_id": [945, 1136, 1820, 1821, 3972, 4273, 4277, 693, 4278, 4994, 5473, 4993, 7830, 1819, 1139], "orig_top_k_doc_id": [945, 1820, 4278, 4993, 4273, 5473, 1136, 4277, 7830, 1821, 693, 1819, 3972, 1139, 4994]}, {"qid": 888, "question": "Are the sentences in the dataset written by humans who were shown the concept-sets? in CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning", "answer": ["Yes"], "top_k_doc_id": [945, 1136, 1137, 1138, 1139, 1140, 5473, 5474, 946, 1819, 4273, 4994, 495, 28, 5609], "orig_top_k_doc_id": [1136, 1139, 1137, 1140, 1138, 5474, 945, 5473, 495, 4273, 1819, 946, 28, 4994, 5609]}, {"qid": 889, "question": "Where do the concept sets come from? in CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning", "answer": ["These concept-sets are sampled from several large corpora of image/video captions"], "top_k_doc_id": [945, 1136, 1137, 1138, 1139, 1140, 5473, 5474, 946, 1819, 4273, 4994, 6002, 6050, 1821], "orig_top_k_doc_id": [1136, 1140, 1139, 1137, 1138, 945, 5473, 6002, 4273, 946, 6050, 1819, 5474, 4994, 1821]}, {"qid": 1330, "question": "Which datasets do they evaluate on? in Attention Is (not) All You Need for Commonsense Reasoning", "answer": ["PDP-60, WSC-273"], "top_k_doc_id": [945, 1136, 1820, 1821, 3972, 4273, 4277, 1137, 1139, 1819, 3973, 4994, 4278, 1159, 7572], "orig_top_k_doc_id": [3972, 3973, 1137, 1136, 1819, 945, 1821, 4273, 4994, 4277, 4278, 1159, 7572, 1139, 1820]}, {"qid": 1331, "question": "How does their model differ from BERT? in Attention Is (not) All You Need for Commonsense Reasoning", "answer": ["Their model does not differ from BERT."], "top_k_doc_id": [945, 1136, 1820, 1821, 3972, 4273, 4277, 1137, 1139, 1819, 3973, 3776, 949, 5473, 7520], "orig_top_k_doc_id": [3972, 1819, 3776, 4277, 3973, 1821, 949, 945, 1136, 5473, 1820, 4273, 1137, 1139, 7520]}]}
{"group_id": 104, "group_size": 10, "items": [{"qid": 1043, "question": "Which multitask annotated corpus is used? in Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks", "answer": ["IEMOCAP"], "top_k_doc_id": [95, 281, 7420, 276, 1365, 6589, 94, 1366, 1367, 1368, 5980, 2528, 285, 1665, 5197], "orig_top_k_doc_id": [1365, 276, 6589, 94, 95, 5980, 2528, 281, 7420, 1368, 1366, 285, 1665, 1367, 5197]}, {"qid": 1044, "question": "What are the tasks in the multitask learning setup? in Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks", "answer": ["set of related tasks are learned (e.g., emotional activation), primary task (e.g., emotional valence)"], "top_k_doc_id": [95, 281, 7420, 276, 1365, 6589, 94, 1366, 1367, 1368, 5980, 7472, 6723, 226, 5488], "orig_top_k_doc_id": [1365, 276, 1366, 1368, 7472, 7420, 95, 6723, 6589, 226, 1367, 281, 5980, 94, 5488]}, {"qid": 78, "question": "Which GAN do they use? in Generative Adversarial Nets for Multiple Text Corpora", "answer": ["We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . ", "weGAN, deGAN"], "top_k_doc_id": [95, 94, 276, 1005, 5186, 89, 282, 4253, 4257, 285, 280, 281, 3328, 5187, 6314], "orig_top_k_doc_id": [94, 89, 5186, 95, 276, 282, 1005, 4253, 280, 281, 4257, 3328, 5187, 285, 6314]}, {"qid": 79, "question": "Do they evaluate grammaticality of generated text? in Generative Adversarial Nets for Multiple Text Corpora", "answer": ["No"], "top_k_doc_id": [95, 94, 276, 1005, 5186, 89, 282, 4253, 4257, 285, 1897, 1894, 3181, 4204, 1895], "orig_top_k_doc_id": [94, 5186, 89, 95, 4257, 1005, 1897, 1894, 282, 276, 285, 3181, 4204, 1895, 4253]}, {"qid": 1042, "question": "What model achieves state of the art performance on this task? in Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks", "answer": ["BIBREF16"], "top_k_doc_id": [95, 281, 7420, 276, 1365, 6589, 7472, 5481, 2709, 1665, 1005, 6723, 226, 485, 7138], "orig_top_k_doc_id": [1365, 276, 7472, 5481, 7420, 2709, 1665, 1005, 6723, 281, 6589, 226, 485, 95, 7138]}, {"qid": 80, "question": "Which corpora do they use? in Generative Adversarial Nets for Multiple Text Corpora", "answer": ["CNN, TIME, 20 Newsgroups, and Reuters-21578"], "top_k_doc_id": [95, 94, 276, 1005, 5186, 89, 282, 4253, 4257, 4679, 1006, 7140, 2513, 4486, 7472], "orig_top_k_doc_id": [94, 89, 5186, 1005, 95, 4679, 4257, 276, 1006, 7140, 4253, 2513, 4486, 282, 7472]}, {"qid": 1045, "question": "What are the subtle changes in voice which have been previously overshadowed? in Learning Representations of Emotional Speech with Deep Convolutional Generative Adversarial Networks", "answer": ["No"], "top_k_doc_id": [95, 281, 7420, 276, 1365, 6589, 227, 3562, 4487, 7472, 5264, 5186, 94, 5481, 5351], "orig_top_k_doc_id": [1365, 276, 227, 3562, 4487, 7472, 5264, 6589, 95, 5186, 281, 94, 7420, 5481, 5351]}, {"qid": 2169, "question": "How much better in terms of JSD measure did their model perform? in TextKD-GAN: Text Generation using KnowledgeDistillation and Generative Adversarial Networks", "answer": ["No"], "top_k_doc_id": [95, 94, 276, 1005, 5186, 280, 281, 287, 3328, 3329, 3330, 3331, 282, 277, 285], "orig_top_k_doc_id": [3331, 3330, 3328, 3329, 280, 94, 276, 5186, 282, 287, 281, 1005, 277, 95, 285]}, {"qid": 2170, "question": "What does the Jensen-Shannon distance measure? in TextKD-GAN: Text Generation using KnowledgeDistillation and Generative Adversarial Networks", "answer": ["No"], "top_k_doc_id": [95, 94, 276, 1005, 5186, 280, 281, 287, 3328, 3329, 3330, 3331, 3065, 3066, 6314], "orig_top_k_doc_id": [3331, 3330, 3328, 5186, 3329, 3065, 280, 276, 94, 281, 95, 3066, 6314, 287, 1005]}, {"qid": 402, "question": "How does the model differ from Generative Adversarial Networks? in Learning with Noisy Labels for Sentence-level Sentiment Classification", "answer": ["No"], "top_k_doc_id": [95, 281, 7420, 94, 472, 3562, 1007, 474, 1005, 5558, 475, 2592, 3564, 7472, 5351], "orig_top_k_doc_id": [94, 472, 3562, 1007, 474, 7420, 1005, 95, 5558, 475, 2592, 3564, 7472, 281, 5351]}]}
{"group_id": 105, "group_size": 10, "items": [{"qid": 1084, "question": "How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling? in N-GrAM: New Groningen Author-profiling Model", "answer": ["They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline"], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 6521, 6522, 6523, 1434, 1435, 6242, 3547, 1433, 2788], "orig_top_k_doc_id": [1432, 6512, 6243, 6522, 6240, 1435, 6242, 6523, 3547, 1319, 1433, 2788, 1434, 6521, 6519]}, {"qid": 1085, "question": "On which task does do model do worst? in N-GrAM: New Groningen Author-profiling Model", "answer": ["Gender prediction task"], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 6521, 6522, 6523, 1434, 1435, 6242, 6524, 7261, 6513], "orig_top_k_doc_id": [1435, 6240, 6512, 1432, 6522, 6523, 6243, 6242, 1434, 1319, 6524, 7261, 6521, 6519, 6513]}, {"qid": 1086, "question": "On which task does do model do best? in N-GrAM: New Groningen Author-profiling Model", "answer": ["Variety prediction task"], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 6521, 6522, 6523, 1434, 1435, 6242, 6524, 7261, 6513], "orig_top_k_doc_id": [1435, 6240, 6512, 1432, 6522, 6243, 6523, 1319, 6242, 1434, 6524, 6519, 6513, 6521, 7261]}, {"qid": 1479, "question": "Does the paper report F1-scores for the age and language variety tasks? in BERT-Based Arabic Social Media Author Profiling", "answer": ["No"], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 88, 1320, 2072, 2073, 1318, 2789, 6242, 85, 3010], "orig_top_k_doc_id": [1432, 1319, 2072, 6243, 2073, 6519, 6512, 6240, 85, 1320, 88, 1318, 3010, 6242, 2789]}, {"qid": 1480, "question": "Are the models compared to some baseline models? in BERT-Based Arabic Social Media Author Profiling", "answer": ["Yes"], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 88, 1320, 2072, 2073, 85, 6523, 7172, 6513, 6242], "orig_top_k_doc_id": [1319, 6512, 1320, 1432, 2072, 2073, 6240, 6243, 6519, 85, 88, 6513, 7172, 6523, 6242]}, {"qid": 1481, "question": "What are the in-house data employed? in BERT-Based Arabic Social Media Author Profiling", "answer": ["we manually label an in-house dataset of 1,100 users with gender tags, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task"], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 88, 1320, 2072, 2073, 1318, 2789, 6242, 7260, 6459], "orig_top_k_doc_id": [2072, 2073, 1320, 6243, 1319, 1432, 6512, 6240, 6519, 1318, 7260, 2789, 6242, 6459, 88]}, {"qid": 1482, "question": "What are the three datasets used in the paper? in BERT-Based Arabic Social Media Author Profiling", "answer": ["Data released for APDA shared task contains 3 datasets."], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 88, 1320, 2072, 2073, 85, 6523, 7172, 2788, 2789], "orig_top_k_doc_id": [2072, 1319, 1432, 6512, 1320, 6519, 6240, 2073, 6243, 85, 7172, 6523, 2788, 2789, 88]}, {"qid": 4065, "question": "What community-based profiling features are used? in Author Profiling for Hate Speech Detection", "answer": ["The features are the outputs from node2vec when run on a community graph where nodes are users and edges are connections if one user follows the other on Twitter.", "The features are the output of running node2vec on a community graph where the nodes are users, and they are connected if one of them follows the other on Twitter.", "The features are the output of running node2vec on a community graph where the nodes are users, and they are connected if one of them follows the other on Twitter."], "top_k_doc_id": [6519, 1319, 1432, 6240, 6243, 6512, 6521, 6522, 6523, 6524, 7261, 7260, 6520, 2396, 3547], "orig_top_k_doc_id": [6523, 6519, 6524, 6522, 6521, 7261, 1432, 7260, 6520, 1319, 6512, 6243, 2396, 3547, 6240]}, {"qid": 4063, "question": "Is the dataset used in other work? in Author Profiling for Hate Speech Detection", "answer": ["Yes, in Waseem and Hovy (2016)", "Yes", "Yes"], "top_k_doc_id": [6519, 412, 3007, 6520, 6521, 6522, 6523, 6524, 7258, 5816, 3011, 6131, 1319, 7125, 2396], "orig_top_k_doc_id": [6519, 6523, 6524, 6522, 6521, 6520, 3007, 5816, 3011, 6131, 1319, 7125, 7258, 2396, 412]}, {"qid": 4064, "question": "What is the drawback to methods that rely on textual cues? in Author Profiling for Hate Speech Detection", "answer": ["tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues", "They don't provide wider discourse information"], "top_k_doc_id": [6519, 412, 3007, 6520, 6521, 6522, 6523, 6524, 7258, 6770, 3547, 3045, 3581, 4137, 5173], "orig_top_k_doc_id": [6523, 6519, 6524, 6522, 6520, 6770, 6521, 3007, 3547, 3045, 3581, 7258, 412, 4137, 5173]}]}
{"group_id": 106, "group_size": 10, "items": [{"qid": 1285, "question": "Is this the first paper to propose a joint model for event and temporal relation extraction? in Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction", "answer": ["Yes"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 1363, 1364, 1762, 5186, 2623, 6604, 4355, 6603, 4347], "orig_top_k_doc_id": [1760, 1765, 1762, 1764, 1763, 1761, 2621, 5186, 1363, 2623, 4355, 1364, 6603, 6604, 4347]}, {"qid": 1286, "question": "What datasets were used for this work? in Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction", "answer": ["TB-Dense,  MATRES"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 1363, 1364, 1762, 5186, 2623, 6604, 7610, 3344, 7609], "orig_top_k_doc_id": [1760, 1765, 1764, 1762, 1763, 2621, 1761, 5186, 2623, 1364, 1363, 6604, 7610, 3344, 7609]}, {"qid": 1284, "question": "What kind of events do they extract? in Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction", "answer": ["No"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 1363, 1364, 1762, 5186, 4355, 4347, 6250, 4359, 5676], "orig_top_k_doc_id": [1760, 1762, 1765, 1764, 5186, 1763, 1761, 2621, 4355, 4347, 6250, 1363, 1364, 4359, 5676]}, {"qid": 1037, "question": "Do they report results only on English data? in Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding", "answer": ["No"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 1363, 1364, 6250, 6606, 6888, 1762, 4355, 4350, 6604], "orig_top_k_doc_id": [1363, 1364, 1760, 6888, 1764, 1763, 6606, 1762, 4355, 1761, 4350, 6250, 1765, 2621, 6604]}, {"qid": 1039, "question": "Do the BERT-based embeddings improve results? in Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding", "answer": ["Yes"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 1363, 1364, 6250, 6606, 6888, 4330, 7056, 4331, 6051], "orig_top_k_doc_id": [1363, 1364, 1760, 1764, 1763, 6888, 4330, 1761, 7056, 4331, 6606, 2621, 1765, 6250, 6051]}, {"qid": 1041, "question": "What type of baseline are established for the two datasets? in Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding", "answer": ["CAEVO"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 1363, 1364, 1762, 6888, 559, 5676, 6606, 2623, 4347], "orig_top_k_doc_id": [1363, 1364, 1760, 6888, 1764, 1761, 1765, 1763, 2621, 559, 1762, 5676, 6606, 2623, 4347]}, {"qid": 1038, "question": "What conclusions do the authors draw from their detailed analyses? in Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding", "answer": ["neural network-based models can outperform feature-based models with wide margins, contextualized representation learning can boost performance of NN models"], "top_k_doc_id": [1760, 1761, 1763, 1764, 709, 1363, 1364, 1958, 6606, 6888, 2125, 1765, 559, 4331, 1273], "orig_top_k_doc_id": [1363, 1364, 1760, 6888, 2125, 1764, 1765, 6606, 559, 1763, 4331, 1958, 1273, 1761, 709]}, {"qid": 1040, "question": "What were the traditional linguistic feature-based models? in Contextualized Word Embeddings Enhanced Event Temporal Relation Extraction for Story Understanding", "answer": ["CAEVO"], "top_k_doc_id": [1760, 1761, 1763, 1764, 709, 1363, 1364, 1958, 6606, 6888, 6607, 5676, 6604, 2623, 6603], "orig_top_k_doc_id": [1363, 1364, 1760, 1761, 6606, 6888, 1764, 6607, 709, 1763, 1958, 5676, 6604, 2623, 6603]}, {"qid": 1797, "question": "Which structured prediction approach do they adopt for temporal entity extraction? in Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction", "answer": ["DeepDive BIBREF1"], "top_k_doc_id": [1760, 1761, 1763, 1764, 1765, 2621, 2623, 1762, 7610, 6604, 4293, 1043, 6603, 6605, 5186], "orig_top_k_doc_id": [2621, 2623, 1760, 1764, 1763, 1762, 1765, 1761, 7610, 6604, 4293, 1043, 6603, 6605, 5186]}, {"qid": 1796, "question": "How do they obtain distant supervision rules for predicting relations? in Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction", "answer": ["dominant temporal associations can be learned from training data"], "top_k_doc_id": [1760, 2621, 2623, 2622, 3538, 895, 3539, 1043, 2874, 4075, 3162, 1762, 6604, 4074, 451], "orig_top_k_doc_id": [2621, 2623, 2622, 3538, 1760, 895, 3539, 1043, 2874, 4075, 3162, 1762, 6604, 4074, 451]}]}
{"group_id": 107, "group_size": 10, "items": [{"qid": 1316, "question": "How does framework automatically chooses different curricula at the evolving learning process according to the learning status of the neural dialogue generation model? in Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation", "answer": ["The multi-curricula learning scheme is scheduled according to the model's performance on the validation set, where the scheduling mechanism acts as the policy $\\pi $ interacting with the dialogue model to acquire the learning status $s$. The reward of the multi-curricula learning mechanism $m_t$ indicates how well the current dialogue model performs."], "top_k_doc_id": [1805, 1806, 1804, 1807, 1808, 3358, 3359, 3507, 6587, 7351, 705, 575, 1768, 5744, 573], "orig_top_k_doc_id": [1808, 1807, 1805, 1806, 1804, 3507, 3359, 705, 575, 1768, 3358, 6587, 5744, 7351, 573]}, {"qid": 1319, "question": "What state of the art models were used in experiments? in Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation", "answer": ["SEQ2SEQ, CVAE, Transformer, HRED, DialogWAE"], "top_k_doc_id": [1805, 1806, 1804, 1807, 1808, 3358, 3359, 3507, 6587, 7351, 3190, 200, 1674, 400, 3193], "orig_top_k_doc_id": [1808, 1805, 1807, 1804, 1806, 3190, 3358, 3359, 3507, 200, 1674, 400, 7351, 3193, 6587]}, {"qid": 1320, "question": "What five dialogue attributes were analyzed? in Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation", "answer": ["Model Confidence, Continuity, Query-relatedness, Repetitiveness, Specificity"], "top_k_doc_id": [1805, 1806, 1804, 1807, 1808, 3358, 3359, 3190, 3451, 5798, 6587, 7146, 3013, 5257, 5793], "orig_top_k_doc_id": [1805, 1808, 1807, 1804, 1806, 5798, 6587, 3190, 3359, 7146, 3358, 3013, 3451, 5257, 5793]}, {"qid": 1321, "question": "What three publicly available coropora are used? in Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation", "answer": ["PersonaChat BIBREF12, DailyDialog BIBREF13, OpenSubtitles BIBREF7"], "top_k_doc_id": [1805, 1806, 1804, 1807, 1808, 3358, 3359, 3190, 3451, 1674, 400, 196, 197, 3681, 575], "orig_top_k_doc_id": [1804, 1808, 1807, 1805, 1806, 1674, 400, 196, 3358, 3359, 3190, 197, 3681, 575, 3451]}, {"qid": 2314, "question": "What datasets are used for experiments? in Adaptive Scheduling for Multi-Task Learning", "answer": ["the WMT'14 English-French (En-Fr) and English-German (En-De) datasets."], "top_k_doc_id": [1805, 1806, 3691, 3692, 3693, 1807, 7070, 1284, 1808, 4731, 4485, 3359, 402, 490, 3532], "orig_top_k_doc_id": [3692, 1806, 1805, 3693, 7070, 1284, 1808, 4731, 3691, 4485, 3359, 1807, 402, 490, 3532]}, {"qid": 2316, "question": "What baselines non-adaptive baselines are used? in Adaptive Scheduling for Multi-Task Learning", "answer": ["Transformer models in their base configuration BIBREF11, using 6 encoder and decoder layers, with model and hidden dimensions of 512 and 2048 respectively, and 8 heads for all attention layers"], "top_k_doc_id": [1805, 1806, 3691, 3692, 3693, 1807, 7070, 1284, 1808, 4731, 3572, 7069, 7167, 5411, 4758], "orig_top_k_doc_id": [3692, 3693, 1805, 1806, 7070, 4731, 3691, 3572, 1284, 1808, 7069, 7167, 5411, 1807, 4758]}, {"qid": 1317, "question": "What human judgement metrics are used? in Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation", "answer": ["coherence, logical consistency, fluency and diversity"], "top_k_doc_id": [1805, 1806, 1804, 1807, 1808, 3358, 3359, 491, 2435, 2970, 1771, 3190, 2225, 3477, 7353], "orig_top_k_doc_id": [1807, 1808, 1805, 1804, 1806, 2970, 3358, 1771, 3190, 2435, 3359, 491, 2225, 3477, 7353]}, {"qid": 1318, "question": "What automatic evaluation metrics are used? in Learning from Easy to Complex: Adaptive Multi-curricula Learning for Neural Dialogue Generation", "answer": ["BLEU, embedding-based metrics (Average, Extrema, Greedy and Coherence), , entropy-based metrics (Ent-{1,2}), distinct metrics (Dist-{1,2,3} and Intra-{1,2,3})"], "top_k_doc_id": [1805, 1806, 1804, 1807, 1808, 3358, 3359, 491, 2435, 6651, 1673, 6590, 6932, 111, 7352], "orig_top_k_doc_id": [1807, 1808, 1805, 1806, 1804, 6651, 3359, 491, 3358, 1673, 6590, 6932, 2435, 111, 7352]}, {"qid": 2313, "question": "How big are negative effects of proposed techniques on high-resource tasks? in Adaptive Scheduling for Multi-Task Learning", "answer": ["The negative effects were insignificant."], "top_k_doc_id": [1805, 1806, 3691, 3692, 3693, 1807, 7070, 3730, 2818, 3004, 2823, 5991, 7069, 5716, 6005], "orig_top_k_doc_id": [3693, 3692, 1805, 1806, 3691, 3730, 2818, 1807, 3004, 2823, 7070, 5991, 7069, 5716, 6005]}, {"qid": 2315, "question": "Are this techniques used in training multilingual models, on what languages? in Adaptive Scheduling for Multi-Task Learning", "answer": ["English to French and English to German"], "top_k_doc_id": [1805, 1806, 3691, 3692, 3693, 2995, 5716, 6035, 1786, 4708, 5711, 6395, 4707, 2998, 5712], "orig_top_k_doc_id": [3692, 3693, 1805, 1806, 2995, 5716, 6035, 1786, 4708, 3691, 5711, 6395, 4707, 2998, 5712]}]}
{"group_id": 108, "group_size": 10, "items": [{"qid": 1327, "question": "was bert used? in Measuring Conversational Fluidity in Automated Dialogue Agents", "answer": ["Yes"], "top_k_doc_id": [6583, 6590, 7299, 899, 1711, 1712, 575, 1718, 1817, 1818, 7758, 1719, 6036, 3479, 6586], "orig_top_k_doc_id": [1817, 1818, 1711, 7299, 1712, 1718, 6583, 6590, 575, 7758, 1719, 6036, 3479, 6586, 899]}, {"qid": 1328, "question": "what datasets did they use? in Measuring Conversational Fluidity in Automated Dialogue Agents", "answer": ["Single-Turn, Multi-Turn"], "top_k_doc_id": [6583, 6590, 7299, 899, 1711, 1712, 575, 1718, 1817, 1818, 7758, 1719, 6584, 1717, 6589], "orig_top_k_doc_id": [1817, 1818, 6590, 1711, 6583, 1712, 7299, 6584, 1718, 575, 1717, 7758, 899, 1719, 6589]}, {"qid": 4122, "question": "Is there a benchmark to compare the different approaches? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["No", "No", "No"], "top_k_doc_id": [6583, 6590, 7299, 6589, 1716, 3357, 4445, 899, 1071, 6586, 1070, 6793, 575, 6036, 4124], "orig_top_k_doc_id": [6590, 6583, 6589, 3357, 1070, 6586, 6793, 1716, 7299, 575, 899, 6036, 1071, 4124, 4445]}, {"qid": 4124, "question": "What type of neural models are used? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["Sequence to Sequence approaches for dialogue modelling, Language Model based approaches for dialogue modelling", "Sequence to Sequence approaches, Language Model based approaches", "Sequence to Sequence approaches, Language Model "], "top_k_doc_id": [6583, 6590, 7299, 6589, 1716, 3357, 4445, 899, 1071, 6586, 3451, 6585, 7839, 6584, 7759], "orig_top_k_doc_id": [6590, 6583, 6589, 7299, 3451, 6586, 1071, 4445, 3357, 899, 6585, 7839, 1716, 6584, 7759]}, {"qid": 1329, "question": "which existing metrics do they compare with? in Measuring Conversational Fluidity in Automated Dialogue Agents", "answer": ["F1-score, BLEU score"], "top_k_doc_id": [6583, 6590, 7299, 899, 1711, 1712, 575, 1718, 1817, 1818, 7758, 1171, 2435, 576, 3479], "orig_top_k_doc_id": [1817, 1818, 7299, 1711, 6590, 575, 1712, 1171, 2435, 576, 7758, 6583, 1718, 3479, 899]}, {"qid": 4121, "question": "What metrics are typically used to compare models? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["BLeU, perplexity", " perplexity and BLEU score", "BLeU , perplexity "], "top_k_doc_id": [6583, 6590, 7299, 6589, 1716, 3357, 4445, 7759, 1817, 6651, 3775, 5425, 3479, 3451, 1632], "orig_top_k_doc_id": [6590, 6583, 7299, 7759, 1716, 6589, 4445, 1817, 6651, 3357, 3775, 5425, 3479, 3451, 1632]}, {"qid": 4126, "question": "What was the proposed use of conversational agents in pioneering work? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.", "allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries, conversational assistant called Persona which was one of the first true personal assistant similar to what we have in recent times (like Siri, etc), Let\u2019s Go project (Raux et al, 2003 BIBREF8 ) that was designed to provide Pittsburgh area bus information"], "top_k_doc_id": [6583, 6590, 7299, 899, 1711, 1712, 3479, 4445, 5425, 5428, 6584, 6586, 6589, 6880, 2967], "orig_top_k_doc_id": [6590, 6583, 6589, 6584, 7299, 5425, 899, 6586, 3479, 5428, 6880, 1712, 4445, 2967, 1711]}, {"qid": 4127, "question": "What work pioneered the field of conversational agents? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 )", " ESPRIT SUNDIAL project"], "top_k_doc_id": [6583, 6590, 7299, 899, 1711, 1712, 3479, 4445, 5425, 5428, 6584, 6586, 6589, 6880, 6587], "orig_top_k_doc_id": [6590, 6583, 4445, 7299, 6586, 5425, 6584, 6589, 3479, 899, 1711, 5428, 1712, 6587, 6880]}, {"qid": 4125, "question": "What type of statistical models were used initially? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["semi-continuous HMMs", "The speech recognition was done using n-gram statistical model, The grammar rules used to identify bus stops were generated automatically from the schedule database, they trained a statistical language model on the artificial corpus"], "top_k_doc_id": [6583, 6590, 7299, 6589, 6584, 3095, 1071, 899, 900, 3451, 1029, 3094, 1817, 4669, 239], "orig_top_k_doc_id": [6590, 6584, 6583, 3095, 6589, 1071, 899, 900, 3451, 1029, 7299, 3094, 1817, 4669, 239]}, {"qid": 4120, "question": "What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["perplexity and BLEU score are not good enough and correlate very weakly with human judgments, word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses, metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality", "The metrics correlate very weakly with human judgements, word-overlap metrics require too many ground-truth reposnses and embedding-based metrics are insufficiently complex for modeling sentence-level compositionality in dialogue", "As there can be multiple correct output utterances for a given input utterance there is no quantitative way to evaluate how well a model is performing., The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments. , According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses."], "top_k_doc_id": [6583, 6590, 6585, 242, 5906, 5246, 6586, 1170, 6584, 5428, 404, 491, 6589, 6651, 5429], "orig_top_k_doc_id": [6590, 6583, 6585, 242, 5906, 5246, 6586, 1170, 6584, 5428, 404, 491, 6589, 6651, 5429]}]}
{"group_id": 109, "group_size": 10, "items": [{"qid": 1352, "question": "What are the baseline models? in MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "answer": ["MC-CNN\nMVCNN\nCNN"], "top_k_doc_id": [1860, 1861, 1862, 6070, 6071, 4826, 884, 2979, 885, 3799, 6115, 7551, 5579, 4481, 5896], "orig_top_k_doc_id": [1862, 1860, 1861, 6071, 6070, 4826, 3799, 7551, 885, 884, 2979, 6115, 5579, 4481, 5896]}, {"qid": 1354, "question": "What dataset/corpus is this evaluated over? in MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "answer": [" SST-1, SST-2, Subj , TREC , Irony "], "top_k_doc_id": [1860, 1861, 1862, 6070, 6071, 4826, 884, 2979, 885, 3799, 6115, 5369, 2982, 7409, 5542], "orig_top_k_doc_id": [1862, 1861, 1860, 6070, 6071, 6115, 885, 2979, 3799, 5369, 4826, 2982, 7409, 5542, 884]}, {"qid": 1474, "question": "What are the effects of extracting features of multigranular phrases? in Multichannel Variable-Size Convolution for Sentence Classification", "answer": ["The system benefits from filters of each size., features of multigranular phrases are extracted with variable-size convolution filters."], "top_k_doc_id": [1860, 2058, 2059, 2060, 2061, 2062, 2063, 6070, 6071, 4285, 462, 1861, 6119, 603, 1841], "orig_top_k_doc_id": [2063, 2058, 2060, 2059, 2061, 462, 2062, 1861, 1860, 6071, 6119, 6070, 4285, 603, 1841]}, {"qid": 1476, "question": "How is MVCNN compared to CNN? in Multichannel Variable-Size Convolution for Sentence Classification", "answer": ["MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization \u2013 diverse versions of pretrained word embeddings are used \u2013 and variable-size filters \u2013 features of multigranular phrases are extracted with variable-size convolution filters. "], "top_k_doc_id": [1860, 2058, 2059, 2060, 2061, 2062, 2063, 6070, 6071, 4285, 462, 1861, 3307, 1327, 2650], "orig_top_k_doc_id": [2058, 2063, 2059, 2060, 1860, 6070, 2062, 6071, 2061, 4285, 1861, 3307, 1327, 462, 2650]}, {"qid": 1355, "question": "What are the comparable alternative architectures? in MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "answer": ["standard CNN, C-CNN, MVCNN "], "top_k_doc_id": [1860, 1861, 1862, 6070, 6071, 4826, 884, 2979, 945, 4816, 3604, 5590, 2330, 2978, 3647], "orig_top_k_doc_id": [1862, 1860, 1861, 6071, 6070, 945, 4826, 884, 4816, 2979, 3604, 5590, 2330, 2978, 3647]}, {"qid": 1472, "question": "Where is MVCNN pertained? in Multichannel Variable-Size Convolution for Sentence Classification", "answer": ["on the unlabeled data of each task"], "top_k_doc_id": [1860, 2058, 2059, 2060, 2061, 2062, 2063, 6070, 6071, 4285, 2650, 4324, 7108, 1326, 1841], "orig_top_k_doc_id": [2058, 2063, 2059, 2060, 2062, 1860, 6071, 6070, 2061, 4285, 2650, 4324, 7108, 1326, 1841]}, {"qid": 1353, "question": "By how much of MGNC-CNN out perform the baselines? in MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "answer": ["In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. \nIn case of Irony the difference is about 2.0. \n"], "top_k_doc_id": [1860, 1861, 1862, 6070, 6071, 4826, 829, 6115, 3571, 3090, 422, 5314, 6894, 7551, 1989], "orig_top_k_doc_id": [1862, 1861, 1860, 6071, 6070, 829, 6115, 3571, 4826, 3090, 422, 5314, 6894, 7551, 1989]}, {"qid": 1473, "question": "How much gain does the model achieve with pretraining MVCNN? in Multichannel Variable-Size Convolution for Sentence Classification", "answer": ["0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj"], "top_k_doc_id": [1860, 2058, 2059, 2060, 2061, 2062, 2063, 6070, 6071, 6312, 4562, 2433, 3087, 2650, 462], "orig_top_k_doc_id": [2058, 2063, 2059, 2062, 2060, 1860, 2061, 6070, 6071, 6312, 4562, 2433, 3087, 2650, 462]}, {"qid": 1351, "question": "How much faster is training time for MGNC-CNN over the baselines? in MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "answer": ["It is an order of magnitude more efficient in terms of training time., his model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour"], "top_k_doc_id": [1860, 1861, 1862, 6070, 6071, 3647, 7770, 7769, 7139, 2414, 3646, 4481, 6115, 4296, 7766], "orig_top_k_doc_id": [1862, 1861, 1860, 6071, 6070, 3647, 7770, 7769, 7139, 2414, 3646, 4481, 6115, 4296, 7766]}, {"qid": 1475, "question": "What are the effects of diverse versions of pertained word embeddings?  in Multichannel Variable-Size Convolution for Sentence Classification", "answer": ["each embedding version is crucial for good performance"], "top_k_doc_id": [1860, 2058, 2059, 2060, 2061, 2062, 2063, 462, 5590, 4409, 2650, 4324, 6190, 1771, 4285], "orig_top_k_doc_id": [2063, 2058, 2059, 2060, 2061, 2062, 1860, 462, 5590, 4409, 2650, 4324, 6190, 1771, 4285]}]}
{"group_id": 110, "group_size": 10, "items": [{"qid": 1356, "question": "Which state-of-the-art model is surpassed by 9.68% attraction score? in Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "answer": ["pure summarization model NHG"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 1868, 6716, 7483, 5892, 3241, 5891, 3242, 200], "orig_top_k_doc_id": [1867, 1866, 1868, 1865, 1863, 5889, 5893, 5890, 5892, 5891, 6716, 3242, 200, 3241, 7483]}, {"qid": 1358, "question": "How is attraction score measured? in Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "answer": ["annotators are asked how attractive the headlines are, Likert scale from 1 to 10 (integer values)"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 1868, 6716, 7483, 5892, 3241, 5891, 7481, 6555], "orig_top_k_doc_id": [1867, 1866, 1868, 1865, 1863, 5889, 5893, 5890, 5892, 7483, 5891, 3241, 7481, 6555, 6716]}, {"qid": 3578, "question": "What is future work planed? in Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning", "answer": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 5891, 5892, 6555, 6557, 2187, 1888, 6556, 1517], "orig_top_k_doc_id": [5889, 1863, 5893, 5891, 1867, 6557, 5890, 6555, 5892, 1865, 1866, 1517, 2187, 1888, 6556]}, {"qid": 3582, "question": "How is sensationalism scorer trained? in Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning", "answer": ["by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\\text{sen}}$", "classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 5891, 5892, 6555, 6557, 2187, 1888, 6556, 805], "orig_top_k_doc_id": [5889, 5893, 5891, 5890, 5892, 1863, 1867, 1865, 6557, 6555, 1866, 2187, 805, 6556, 1888]}, {"qid": 1360, "question": "How is fluency automatically evaluated? in Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "answer": ["fine-tuned the GPT-2 medium model BIBREF51 on our collected headlines and then used it to measure the perplexity (PPL) on the generated outputs"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 1868, 6716, 7483, 5892, 7481, 481, 3624, 6555], "orig_top_k_doc_id": [1865, 1867, 1863, 1866, 5893, 5892, 5889, 1868, 6716, 7481, 481, 5890, 7483, 3624, 6555]}, {"qid": 3579, "question": "What is this method improvement over the best performing state-of-the-art? in Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning", "answer": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 5891, 5892, 6555, 6557, 1498, 4760, 7619, 6556], "orig_top_k_doc_id": [5889, 1863, 5891, 5893, 1867, 6555, 5892, 6557, 5890, 1865, 1866, 4760, 1498, 7619, 6556]}, {"qid": 3580, "question": "Which baselines are used for evaluation? in Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning", "answer": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 5891, 5892, 6555, 6557, 1498, 2187, 5174, 6716], "orig_top_k_doc_id": [5889, 1863, 5891, 5893, 1867, 5892, 1866, 1865, 5890, 6557, 6555, 1498, 2187, 5174, 6716]}, {"qid": 3581, "question": "Did they used dataset from another domain for evaluation? in Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning", "answer": ["No", "No"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 5891, 5892, 6555, 6557, 2187, 805, 2157, 1517], "orig_top_k_doc_id": [5889, 1863, 5891, 5893, 1867, 1865, 5890, 1866, 5892, 6555, 6557, 2187, 805, 2157, 1517]}, {"qid": 1359, "question": "How is presence of three target styles detected? in Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "answer": ["human evaluation task about the style strength"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 5890, 5893, 1868, 6716, 7483, 6555, 1864, 972, 3624, 3241], "orig_top_k_doc_id": [1863, 1867, 1866, 1865, 1868, 5893, 6555, 1864, 5889, 972, 7483, 3624, 3241, 5890, 6716]}, {"qid": 1357, "question": "What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)? in Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "answer": ["Humor in headlines (TitleStylist vs Multitask baseline):\nRelevance: +6.53% (5.87 vs 5.51)\nAttraction: +3.72% (8.93 vs 8.61)\nFluency: 1,98% (9.29 vs 9.11)"], "top_k_doc_id": [1863, 1865, 1866, 1867, 5889, 1868, 7483, 7481, 7482, 7486, 7484, 7487, 5892, 7488, 7485], "orig_top_k_doc_id": [1867, 1863, 1866, 1865, 1868, 7483, 7481, 7482, 7486, 7484, 7487, 5892, 7488, 7485, 5889]}]}
{"group_id": 111, "group_size": 10, "items": [{"qid": 1447, "question": "What two types the Chinese reading comprehension dataset consists of? in Dataset for the First Evaluation on Chinese Machine Reading Comprehension", "answer": ["cloze-style reading comprehension and user query reading comprehension questions"], "top_k_doc_id": [2836, 2011, 7589, 4790, 4791, 4792, 7728, 1370, 2840, 7590, 2013, 2012, 1373, 2839, 1822], "orig_top_k_doc_id": [2011, 2836, 4790, 4791, 1370, 7589, 2840, 7590, 2013, 7728, 2012, 1373, 2839, 4792, 1822]}, {"qid": 1448, "question": "For which languages most of the existing MRC datasets are created? in Dataset for the First Evaluation on Chinese Machine Reading Comprehension", "answer": ["English"], "top_k_doc_id": [2836, 2011, 7589, 4790, 4791, 4792, 7728, 4637, 3972, 3416, 1357, 7572, 1512, 4915, 2442], "orig_top_k_doc_id": [2836, 7728, 2011, 4790, 4791, 4637, 7589, 3972, 3416, 1357, 7572, 1512, 4792, 4915, 2442]}, {"qid": 2432, "question": "Which models do they try out? in ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension", "answer": ["DocQA, SAN, QANet, ASReader, LM, Random Guess"], "top_k_doc_id": [2836, 2011, 7589, 4278, 4415, 5266, 7590, 3972, 3973, 1822, 1512, 5610, 2445, 7616, 1139], "orig_top_k_doc_id": [3972, 3973, 7589, 1822, 1512, 5266, 2011, 5610, 4278, 4415, 7590, 2836, 2445, 7616, 1139]}, {"qid": 3106, "question": "Do they report results only on English datasets? in Explicit Utilization of General Knowledge in Machine Reading Comprehension", "answer": ["No", "Yes"], "top_k_doc_id": [2836, 2011, 7589, 4278, 4415, 5266, 7590, 7573, 4791, 6932, 4790, 2584, 7615, 2840, 352], "orig_top_k_doc_id": [2836, 4278, 7573, 5266, 2011, 4791, 6932, 7589, 4415, 4790, 2584, 7615, 2840, 352, 7590]}, {"qid": 4882, "question": "what are all the datasets they experiment with? in Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets", "answer": ["CoQA , DuoRC , HotpotQA , SQuAD v1.1 , SQuAD v2.0, ARC (Challenge), MCTest , MultiRC , RACE , SWAG", "CoQA BIBREF17, DuoRC BIBREF18, HotpotQA (distractor) BIBREF1, SQuAD v1.1 BIBREF0, SQuAD v2.0 BIBREF20, ARC (Challenge) BIBREF21, MCTest BIBREF22,  MultiRC BIBREF23, RACE BIBREF24,  SWAG BIBREF25"], "top_k_doc_id": [2836, 2205, 2011, 2755, 2885, 3972, 5605, 7572, 7577, 3416, 4791, 2661, 3805, 4189, 7589], "orig_top_k_doc_id": [7572, 3972, 2885, 2011, 3416, 7577, 4791, 2836, 2661, 3805, 2205, 4189, 7589, 2755, 5605]}, {"qid": 4883, "question": "what was the baseline model? in Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets", "answer": ["BERT-large BIBREF3", " BERT-large"], "top_k_doc_id": [2836, 2205, 2011, 2755, 2885, 3972, 5605, 7572, 7577, 4915, 2752, 2442, 7573, 2839, 2012], "orig_top_k_doc_id": [7572, 2836, 2885, 2011, 4915, 3972, 7577, 2205, 2752, 2442, 5605, 7573, 2839, 2755, 2012]}, {"qid": 2570, "question": "How much performance improvements they achieve on SQuAD? in Stochastic Answer Networks for Machine Reading Comprehension", "answer": ["Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. "], "top_k_doc_id": [2836, 2205, 4522, 4518, 2840, 2839, 1961, 4791, 2759, 3416, 2442, 5371, 1651, 7589, 5022], "orig_top_k_doc_id": [4522, 4518, 2836, 2840, 2839, 1961, 4791, 2759, 3416, 2442, 2205, 5371, 1651, 7589, 5022]}, {"qid": 3107, "question": "How do the authors examine whether a model is robust to noise or not? in Explicit Utilization of General Knowledge in Machine Reading Comprehension", "answer": ["By evaluating their model on adversarial sets containing misleading sentences", "we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise"], "top_k_doc_id": [2836, 4278, 5266, 7573, 3805, 4204, 5664, 2578, 4915, 4075, 7589, 1512, 7685, 2755, 2840], "orig_top_k_doc_id": [5266, 4204, 4278, 2578, 7573, 4915, 4075, 2836, 7589, 1512, 7685, 5664, 3805, 2755, 2840]}, {"qid": 3109, "question": "Do the authors hypothesize that humans' robustness to noise is due to their general knowledge? in Explicit Utilization of General Knowledge in Machine Reading Comprehension", "answer": ["Yes", "Yes"], "top_k_doc_id": [2836, 4278, 5266, 7573, 3805, 4204, 5664, 3985, 5270, 3972, 352, 7459, 5269, 2013, 3054], "orig_top_k_doc_id": [5266, 4278, 4204, 5664, 3985, 5270, 3972, 352, 2836, 3805, 7459, 7573, 5269, 2013, 3054]}, {"qid": 3108, "question": "What type of model is KAR? in Explicit Utilization of General Knowledge in Machine Reading Comprehension", "answer": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "top_k_doc_id": [2836, 4278, 5266, 7573, 5270, 5269, 4074, 4075, 2755, 690, 3972, 2011, 1512, 5267, 5257], "orig_top_k_doc_id": [5266, 5270, 5269, 4074, 4075, 2755, 4278, 7573, 690, 3972, 2836, 2011, 1512, 5267, 5257]}]}
{"group_id": 112, "group_size": 10, "items": [{"qid": 1503, "question": "Do they evaluate only on English datasets? in Effectiveness of Data-Driven Induction of Semantic Spaces and Traditional Classifiers for Sarcasm Detection", "answer": ["No"], "top_k_doc_id": [1329, 5902, 5903, 2103, 2104, 2106, 2110, 2105, 2109, 5406, 5900, 7115, 2330, 1967, 2111], "orig_top_k_doc_id": [2106, 2104, 2103, 2105, 2110, 5903, 5406, 2109, 7115, 2330, 1329, 5902, 5900, 1967, 2111]}, {"qid": 1504, "question": "What baseline models are used? in Effectiveness of Data-Driven Induction of Semantic Spaces and Traditional Classifiers for Sarcasm Detection", "answer": ["No"], "top_k_doc_id": [1329, 5902, 5903, 2103, 2104, 2106, 2110, 2105, 2109, 5406, 2111, 3581, 5410, 5288, 2014], "orig_top_k_doc_id": [2106, 2103, 2104, 2110, 5903, 2105, 5410, 5902, 5406, 2111, 1329, 3581, 2109, 5288, 2014]}, {"qid": 1506, "question": "What are the different methods used for different corpora? in Effectiveness of Data-Driven Induction of Semantic Spaces and Traditional Classifiers for Sarcasm Detection", "answer": ["Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)"], "top_k_doc_id": [1329, 5902, 5903, 2103, 2104, 2106, 2110, 2105, 2109, 5406, 2111, 3581, 5410, 2114, 2154], "orig_top_k_doc_id": [2104, 2103, 2106, 2110, 2105, 2111, 5903, 2109, 5406, 3581, 5410, 1329, 5902, 2114, 2154]}, {"qid": 1507, "question": "In which domains is sarcasm conveyed in different ways? in Effectiveness of Data-Driven Induction of Semantic Spaces and Traditional Classifiers for Sarcasm Detection", "answer": ["Amazon reviews"], "top_k_doc_id": [1329, 5902, 5903, 2103, 2104, 2106, 2110, 2105, 2109, 5406, 5900, 2114, 6172, 6173, 7114], "orig_top_k_doc_id": [2106, 2104, 2103, 2110, 5903, 2105, 2114, 5902, 6172, 5406, 6173, 7114, 2109, 5900, 1329]}, {"qid": 1505, "question": "What classical machine learning algorithms are used? in Effectiveness of Data-Driven Induction of Semantic Spaces and Traditional Classifiers for Sarcasm Detection", "answer": ["Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), gradient boosting (XGB)"], "top_k_doc_id": [1329, 5902, 5903, 2103, 2104, 2106, 2110, 2105, 2109, 2330, 5430, 3581, 7514, 309, 2162], "orig_top_k_doc_id": [2104, 2106, 2103, 2105, 1329, 2110, 5902, 2330, 5903, 2109, 5430, 3581, 7514, 309, 2162]}, {"qid": 3588, "question": "What other evaluation metrics are looked at? in Harnessing Cognitive Features for Sarcasm Detection", "answer": ["F-score, Kappa", "No"], "top_k_doc_id": [1329, 5902, 5903, 5406, 5900, 5904, 491, 2104, 5901, 5932, 7262, 2168, 1331, 5929, 5927], "orig_top_k_doc_id": [5900, 5903, 5904, 5406, 5902, 2104, 7262, 5932, 1329, 2168, 1331, 491, 5929, 5927, 5901]}, {"qid": 3590, "question": "What kind of stylistic features are obtained? in Harnessing Cognitive Features for Sarcasm Detection", "answer": ["No"], "top_k_doc_id": [1329, 5902, 5903, 5406, 5900, 5904, 2105, 2106, 5410, 5927, 3944, 1331, 5932, 2103, 5409], "orig_top_k_doc_id": [5900, 5903, 5904, 5406, 2106, 5902, 5410, 3944, 1329, 2105, 1331, 5932, 2103, 5409, 5927]}, {"qid": 3591, "question": "What traditional linguistics features did they use? in Harnessing Cognitive Features for Sarcasm Detection", "answer": ["No"], "top_k_doc_id": [1329, 5902, 5903, 5406, 5900, 5904, 2105, 2106, 5410, 5927, 2409, 597, 2110, 2104, 6173], "orig_top_k_doc_id": [5900, 5903, 5406, 5904, 2409, 2106, 597, 5927, 5902, 2110, 2104, 2105, 5410, 1329, 6173]}, {"qid": 3592, "question": "What cognitive features are used? in Harnessing Cognitive Features for Sarcasm Detection", "answer": ["Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half\nto first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze\ngraph (ED),  Fixation Duration at Left/Source\n(F1H, F1S),  Fixation Duration at Right/Target\n(F2H, F2S),  Forward Saccade Word Count of\nSource (PSH, PSS),  Forward SaccadeWord Count of Destination\n(PSDH, PSDS), Regressive Saccade Word Count of\nSource (RSH, RSS),  Regressive Saccade Word Count of\nDestination (RSDH, RSDS)"], "top_k_doc_id": [1329, 5902, 5903, 5406, 5900, 5904, 491, 2104, 5901, 5932, 2106, 5450, 3550, 6755, 4877], "orig_top_k_doc_id": [5900, 5903, 5904, 5902, 5406, 2104, 2106, 5450, 3550, 5901, 491, 6755, 4877, 1329, 5932]}, {"qid": 3589, "question": "What is the best reported system? in Harnessing Cognitive Features for Sarcasm Detection", "answer": ["Gaze Sarcasm using Multi Instance Logistic Regression.", "the MILR classifier"], "top_k_doc_id": [1329, 5902, 5903, 2103, 2104, 2106, 2110, 5900, 5904, 5406, 6175, 756, 5901, 2111, 7115], "orig_top_k_doc_id": [5903, 5900, 5902, 5904, 5406, 6175, 1329, 2110, 2103, 2104, 756, 5901, 2106, 2111, 7115]}]}
{"group_id": 113, "group_size": 10, "items": [{"qid": 1764, "question": "Where did they get training data? in Open Information Extraction from Question-Answer Pairs", "answer": ["AmazonQA and ConciergeQA datasets"], "top_k_doc_id": [7351, 3851, 1853, 4074, 2548, 5736, 5739, 2664, 4640, 5735, 2051, 5472, 4558, 7606, 144], "orig_top_k_doc_id": [4074, 1853, 2051, 5739, 2548, 4640, 5472, 4558, 7606, 7351, 144, 5736, 5735, 2664, 3851]}, {"qid": 1766, "question": "Which datasets did they experiment on? in Open Information Extraction from Question-Answer Pairs", "answer": ["ConciergeQA and AmazonQA"], "top_k_doc_id": [7351, 3851, 1853, 4074, 2548, 5736, 5739, 2664, 4640, 5735, 2547, 510, 7352, 2210, 7164], "orig_top_k_doc_id": [4074, 1853, 4640, 5739, 2548, 3851, 5736, 2664, 2547, 510, 5735, 7352, 7351, 2210, 7164]}, {"qid": 1763, "question": "How did they evaluate the system? in Open Information Extraction from Question-Answer Pairs", "answer": ["No"], "top_k_doc_id": [7351, 3851, 1853, 4074, 2548, 5736, 5739, 4658, 7804, 494, 7679, 5809, 1091, 3175, 4656], "orig_top_k_doc_id": [4074, 7351, 4658, 1853, 3851, 7804, 2548, 5739, 5736, 494, 7679, 5809, 1091, 3175, 4656]}, {"qid": 286, "question": "Can the method answer multi-hop questions? in Answering Complex Questions Using Open Information Extraction", "answer": ["Yes"], "top_k_doc_id": [7351, 3851, 7164, 7610, 2366, 4216, 4496, 4640, 6932, 7800, 7801, 2371, 7679, 2370, 5736], "orig_top_k_doc_id": [7610, 7351, 6932, 7164, 3851, 7801, 4640, 2366, 2371, 7679, 4496, 7800, 4216, 2370, 5736]}, {"qid": 289, "question": "Is their method capable of multi-hop reasoning? in Answering Complex Questions Using Open Information Extraction", "answer": ["Yes"], "top_k_doc_id": [7351, 3851, 7164, 7610, 2366, 4216, 4496, 4640, 6932, 7800, 7801, 1240, 4560, 6936, 4219], "orig_top_k_doc_id": [4216, 7801, 7610, 6932, 7800, 3851, 1240, 7351, 4496, 4560, 7164, 4640, 6936, 4219, 2366]}, {"qid": 1765, "question": "What extraction model did they use? in Open Information Extraction from Question-Answer Pairs", "answer": ["Multi-Encoder, Constrained-Decoder model"], "top_k_doc_id": [7351, 3851, 1853, 4074, 2548, 4075, 2051, 7606, 559, 7605, 2547, 4077, 5186, 560, 4216], "orig_top_k_doc_id": [4074, 1853, 4075, 2548, 2051, 7606, 3851, 559, 7605, 2547, 4077, 5186, 560, 7351, 4216]}, {"qid": 282, "question": "What is the accuracy of the proposed technique? in Answering Complex Questions Using Open Information Extraction", "answer": ["51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge"], "top_k_doc_id": [7351, 3851, 7164, 7610, 5736, 1091, 4560, 559, 7805, 1090, 5735, 4559, 5327, 3175, 7163], "orig_top_k_doc_id": [5736, 1091, 3851, 4560, 559, 7805, 7164, 1090, 7351, 5735, 4559, 5327, 3175, 7163, 7610]}, {"qid": 1767, "question": "What types of facts can be extracted from QA pairs that can't be extracted from general text? in Open Information Extraction from Question-Answer Pairs", "answer": ["No"], "top_k_doc_id": [7351, 3851, 1853, 4074, 3855, 5740, 560, 1422, 7804, 3854, 4640, 144, 7352, 5809, 4819], "orig_top_k_doc_id": [3855, 5740, 7351, 4074, 560, 3851, 1422, 7804, 3854, 1853, 4640, 144, 7352, 5809, 4819]}, {"qid": 4950, "question": "What is the state-of-the-art model in this task? in Multi-Module System for Open Domain Chinese Question Answering over Knowledge Base", "answer": ["No", "No"], "top_k_doc_id": [7351, 345, 1091, 2464, 2465, 7677, 7804, 7805, 2234, 7679, 7800, 7245, 680, 3507, 1334], "orig_top_k_doc_id": [7351, 2464, 2234, 7679, 7805, 1091, 7677, 7800, 7245, 7804, 2465, 680, 345, 3507, 1334]}, {"qid": 4951, "question": "How does this result compare to other methods KB QA in CCKS2019? in Multi-Module System for Open Domain Chinese Question Answering over Knowledge Base", "answer": ["No", "No"], "top_k_doc_id": [7351, 345, 1091, 2464, 2465, 7677, 7804, 7805, 4640, 826, 4637, 7610, 1109, 4555, 144], "orig_top_k_doc_id": [7677, 7351, 345, 4640, 1091, 826, 7805, 4637, 7610, 1109, 7804, 4555, 2464, 2465, 144]}]}
{"group_id": 114, "group_size": 10, "items": [{"qid": 1813, "question": "How do they identify abbreviations? in Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "answer": ["identify all abbreviations using regular expressions"], "top_k_doc_id": [2640, 5757, 5759, 2641, 2642, 3052, 3053, 6051, 2970, 5740, 6050, 7598, 1169, 6605, 7456], "orig_top_k_doc_id": [2640, 2641, 2642, 5757, 3053, 6051, 5759, 3052, 5740, 1169, 6605, 7598, 7456, 2970, 6050]}, {"qid": 1814, "question": "What kind of model do they build to expand abbreviations? in Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "answer": ["word2vec BIBREF0"], "top_k_doc_id": [2640, 5757, 5759, 2641, 2642, 3052, 3053, 6051, 758, 2234, 2625, 4300, 5740, 2970, 5175], "orig_top_k_doc_id": [2640, 2642, 2641, 5757, 3053, 6051, 5740, 2970, 5759, 2234, 5175, 758, 3052, 4300, 2625]}, {"qid": 1815, "question": "Do they use any knowledge base to expand abbreviations? in Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "answer": ["Yes"], "top_k_doc_id": [2640, 5757, 5759, 2641, 2642, 3052, 3053, 6051, 758, 2234, 2625, 4300, 3269, 123, 6050], "orig_top_k_doc_id": [2640, 2642, 2641, 5757, 3053, 6051, 5759, 3269, 123, 2625, 3052, 6050, 4300, 2234, 758]}, {"qid": 1816, "question": "In their used dataset, do they study how many abbreviations are ambiguous? in Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "answer": ["No"], "top_k_doc_id": [2640, 5757, 5759, 2641, 2642, 3052, 3053, 6051, 2970, 5740, 6050, 7598, 123, 4300, 2154], "orig_top_k_doc_id": [2640, 2641, 2642, 5757, 3053, 6051, 5759, 6050, 5740, 7598, 2970, 3052, 123, 4300, 2154]}, {"qid": 3471, "question": "How big is dataset for testing? in A Neural Topic-Attention Model for Medical Term Abbreviation Disambiguation", "answer": ["30 terms, each term-sanse pair has around 15 samples for testing"], "top_k_doc_id": [2640, 5757, 5759, 5736, 5758, 5760, 2641, 6229, 7597, 3089, 1058, 7741, 1889, 7598, 4945], "orig_top_k_doc_id": [5757, 5759, 5758, 2640, 5760, 3089, 2641, 5736, 6229, 7597, 1058, 7741, 1889, 7598, 4945]}, {"qid": 3472, "question": "What existing dataset is re-examined and corrected for training? in A Neural Topic-Attention Model for Medical Term Abbreviation Disambiguation", "answer": [" UM Inventory "], "top_k_doc_id": [2640, 5757, 5759, 5736, 5758, 5760, 2641, 6229, 7597, 5739, 2694, 912, 910, 3079, 3784], "orig_top_k_doc_id": [5757, 5759, 5758, 5736, 2640, 5739, 2641, 2694, 6229, 912, 7597, 5760, 910, 3079, 3784]}, {"qid": 3470, "question": "To what baseline models is proposed model compared? in A Neural Topic-Attention Model for Medical Term Abbreviation Disambiguation", "answer": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "top_k_doc_id": [2640, 5757, 5759, 5736, 5758, 5760, 2641, 5739, 4946, 7250, 6723, 7251, 4967, 4837, 5079], "orig_top_k_doc_id": [5757, 5759, 5758, 2640, 2641, 5739, 5736, 4946, 7250, 6723, 7251, 5760, 4967, 4837, 5079]}, {"qid": 1817, "question": "Which dataset do they use to build their model? in Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion", "answer": ["1,160 physician logs of Medical ICU admission requests, 42,506 Wikipedia articles, 6 research papers and 2 critical care medicine textbooks"], "top_k_doc_id": [2640, 5757, 5759, 2641, 2642, 3052, 3053, 2234, 2970, 123, 5740, 3294, 3269, 5105, 1471], "orig_top_k_doc_id": [2640, 2641, 5757, 2642, 3053, 5759, 3052, 2234, 2970, 123, 5740, 3294, 3269, 5105, 1471]}, {"qid": 3469, "question": "How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information? in A Neural Topic-Attention Model for Medical Term Abbreviation Disambiguation", "answer": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "top_k_doc_id": [2640, 5757, 5759, 5736, 5758, 5760, 3089, 1889, 5206, 5079, 1934, 1936, 3088, 1476, 3825], "orig_top_k_doc_id": [5759, 5757, 5758, 5760, 3089, 1889, 5206, 5079, 1934, 2640, 1936, 3088, 1476, 3825, 5736]}, {"qid": 2777, "question": "Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon? in A matter of words: NLP for quality evaluation of Wikipedia medical articles", "answer": ["No", "No"], "top_k_doc_id": [2640, 4865, 4869, 4870, 4867, 4866, 4868, 5874, 5873, 2641, 1982, 5128, 5589, 5739, 1547], "orig_top_k_doc_id": [4865, 4869, 4870, 4867, 2640, 4866, 4868, 5874, 5873, 2641, 1982, 5128, 5589, 5739, 1547]}]}
{"group_id": 115, "group_size": 10, "items": [{"qid": 1889, "question": "How is \"hirability\" defined? in HireNet: a Hierarchical Attention Model for the Automatic Analysis of Asynchronous Video Job Interviews", "answer": ["candidates who have been liked or shortlisted are considered part of the hirable class"], "top_k_doc_id": [2803, 2800, 2801, 2118, 2802, 2804, 2805, 6274, 6208, 6341, 7297, 5056, 5666, 3266, 1296], "orig_top_k_doc_id": [2800, 2805, 2801, 2804, 2802, 2803, 6341, 7297, 2118, 6274, 5056, 6208, 5666, 3266, 1296]}, {"qid": 1891, "question": "Do they analyze if their system has any bias? in HireNet: a Hierarchical Attention Model for the Automatic Analysis of Asynchronous Video Job Interviews", "answer": ["No"], "top_k_doc_id": [2803, 2800, 2801, 2118, 2802, 2804, 2805, 6274, 6208, 6341, 7297, 2253, 6343, 6804, 5258], "orig_top_k_doc_id": [2800, 2805, 2801, 2802, 2804, 2803, 6341, 6208, 2118, 6274, 7297, 2253, 6343, 6804, 5258]}, {"qid": 1892, "question": "Is there any ethical consideration in the research? in HireNet: a Hierarchical Attention Model for the Automatic Analysis of Asynchronous Video Job Interviews", "answer": ["No"], "top_k_doc_id": [2803, 2800, 2801, 2118, 2802, 2804, 2805, 6274, 6208, 6341, 6209, 5056, 2922, 3521, 976], "orig_top_k_doc_id": [2800, 2805, 2801, 2802, 2804, 6208, 2803, 6209, 6341, 2118, 6274, 5056, 2922, 3521, 976]}, {"qid": 4448, "question": "How long is their dataset? in Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study", "answer": ["670", "670 publications"], "top_k_doc_id": [2803, 1929, 2184, 6985, 6986, 6989, 7551, 668, 1930, 6532, 2802, 4671, 404, 7597, 7860], "orig_top_k_doc_id": [6985, 6986, 6989, 1929, 2184, 404, 7551, 2802, 668, 4671, 7597, 6532, 7860, 1930, 2803]}, {"qid": 4449, "question": "What is a study descriptor? in Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study", "answer": ["Study descriptor is a set of structured data elements extracted from a publication text that contains specific expert knowledge pertaining to domain topics.", "No"], "top_k_doc_id": [2803, 1929, 2184, 6985, 6986, 6989, 7551, 668, 1930, 6532, 2802, 4671, 3437, 5015, 5728], "orig_top_k_doc_id": [6985, 1929, 6986, 2802, 6989, 1930, 3437, 4671, 2184, 7551, 6532, 5015, 668, 5728, 2803]}, {"qid": 1890, "question": "Have the candidates given their consent to have their videos used for the research? in HireNet: a Hierarchical Attention Model for the Automatic Analysis of Asynchronous Video Job Interviews", "answer": ["Yes"], "top_k_doc_id": [2803, 2800, 2801, 2118, 2802, 2804, 2805, 6274, 2213, 6207, 2922, 6209, 5666, 5258, 7772], "orig_top_k_doc_id": [2800, 2805, 2801, 2802, 2804, 2803, 2213, 2118, 6207, 6274, 2922, 6209, 5666, 5258, 7772]}, {"qid": 4445, "question": "Do they compare to previous work? in Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study", "answer": ["Yes", "Yes"], "top_k_doc_id": [2803, 1929, 2184, 6985, 6986, 6989, 7551, 2802, 3332, 1040, 4671, 2801, 6817, 7860, 3651], "orig_top_k_doc_id": [6985, 6986, 6989, 2803, 1040, 4671, 2184, 1929, 2801, 6817, 7860, 2802, 3651, 3332, 7551]}, {"qid": 4446, "question": "What is the source of their data? in Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study", "answer": ["a curated database of high-quality in vivo rodent uterotrophic bioassay data", "GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays"], "top_k_doc_id": [2803, 1929, 2184, 6985, 6986, 6989, 7551, 668, 1930, 6532, 5716, 7548, 4734, 923, 5459], "orig_top_k_doc_id": [6985, 6989, 6986, 1929, 7551, 2184, 5716, 1930, 6532, 2803, 7548, 4734, 668, 923, 5459]}, {"qid": 4447, "question": "What is their binary classifier? in Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study", "answer": ["Bernoulli Na\u00efve Bayes classifier", "Bernoulli Na\u00efve Bayes classifier"], "top_k_doc_id": [2803, 1929, 2184, 6985, 6986, 6989, 7551, 2802, 3332, 6532, 5421, 247, 5909, 4734, 6622], "orig_top_k_doc_id": [6985, 6989, 6986, 2803, 1929, 2184, 6532, 2802, 5421, 3332, 7551, 247, 5909, 4734, 6622]}, {"qid": 3676, "question": "what features were derived from the videos? in Neural Language Modeling with Visual Features", "answer": ["1500-dimensional vectors similar to those used for large scale image classification tasks.", "features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks", "1500-dimensional vectors, extracted from the video frames at 1-second intervals"], "top_k_doc_id": [2803, 2800, 2801, 6020, 2414, 2922, 285, 2119, 5104, 6021, 4033, 1237, 576, 7557, 5665], "orig_top_k_doc_id": [6020, 2801, 2414, 2922, 2800, 2803, 285, 2119, 5104, 6021, 4033, 1237, 576, 7557, 5665]}]}
{"group_id": 116, "group_size": 10, "items": [{"qid": 2107, "question": "What are the weaknesses of their proposed interpretability quantification method? in Semantic Structure and Interpretability of Word Embeddings", "answer": ["can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 5311, 3213, 3796, 2193, 2225], "orig_top_k_doc_id": [3208, 5305, 5310, 3207, 5311, 3214, 3215, 5306, 5307, 2191, 3212, 3796, 2193, 2225, 3213]}, {"qid": 3147, "question": "What experiments do they use to quantify the extent of interpretability? in Imparting Interpretability to Word Embeddings while Preserving Semantic Structure", "answer": ["Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).", "semantic category-based approach"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 5311, 3213, 3796, 5913, 2189], "orig_top_k_doc_id": [3208, 3207, 5306, 5307, 3214, 5310, 3212, 5311, 5305, 3215, 5913, 3213, 2191, 2189, 3796]}, {"qid": 3148, "question": "Along which dimension do the semantically related words take larger values? in Imparting Interpretability to Word Embeddings while Preserving Semantic Structure", "answer": ["dimension corresponding to the concept that the particular word belongs to"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 5311, 3213, 5309, 2655, 3210], "orig_top_k_doc_id": [5307, 3208, 5310, 3214, 5311, 5305, 3212, 3207, 5309, 5306, 3215, 2655, 3210, 3213, 2191]}, {"qid": 2108, "question": "What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to? in Semantic Structure and Interpretability of Word Embeddings", "answer": ["it is less expensive and quantifies interpretability using continuous values rather than binary evaluations"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 5311, 5116, 5117, 5537, 3796], "orig_top_k_doc_id": [3214, 5310, 3208, 3207, 3215, 5307, 5311, 3212, 2191, 5116, 5305, 5306, 5117, 5537, 3796]}, {"qid": 2052, "question": "How do they evaluate interpretability in this paper? in Rotations and Interpretability of Word Embeddings: the Case of the Russian Language", "answer": ["we suggest to estimate the interpretability of $k$ th component as $ \\operatorname{interp}_k W = \\sum _{i,j=1}^N W_{i,k} W_{j,k} \\left(W_i \\cdot W_j \\right). $"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 5311, 3112, 3113, 6127, 2193], "orig_top_k_doc_id": [3112, 3113, 3208, 5310, 3212, 3207, 3214, 5305, 3215, 5306, 5311, 5307, 2191, 6127, 2193]}, {"qid": 3146, "question": "Do they report results only on English data? in Imparting Interpretability to Word Embeddings while Preserving Semantic Structure", "answer": ["Yes", "No"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 5311, 2655, 4676, 6853, 3986], "orig_top_k_doc_id": [3208, 5306, 5307, 3207, 5305, 3215, 5310, 3214, 5311, 2655, 4676, 6853, 2191, 3212, 3986]}, {"qid": 1570, "question": "How do they evaluate interpretability? in Inducing Interpretability in Knowledge Graph Embeddings", "answer": ["For evaluating the interpretability, we use $Coherence@k$ (Equation 6 ) , automated and manual word intrusion tests."], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 3212, 3215, 2193, 182, 4475, 181, 4321], "orig_top_k_doc_id": [2191, 2193, 182, 4475, 3208, 5310, 181, 3212, 3207, 5305, 3214, 5306, 5307, 4321, 3215]}, {"qid": 3149, "question": "What is the additive modification to the objective function? in Imparting Interpretability to Word Embeddings while Preserving Semantic Structure", "answer": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 3214, 5311, 182, 6025, 4202, 4676, 6246, 3696], "orig_top_k_doc_id": [5306, 5305, 5307, 5311, 3208, 182, 5310, 3207, 6025, 4202, 4676, 6246, 3696, 2191, 3214]}, {"qid": 1568, "question": "Do the authors analyze what kinds of cases their new embeddings fail in where the original, less-interpretable embeddings didn't? in Inducing Interpretability in Knowledge Graph Embeddings", "answer": ["No"], "top_k_doc_id": [2191, 5310, 3207, 3208, 5305, 5306, 5307, 1934, 182, 5913, 5311, 178, 69, 4278, 3215], "orig_top_k_doc_id": [2191, 5306, 5305, 3208, 1934, 5307, 3207, 182, 5913, 5311, 178, 69, 4278, 3215, 5310]}, {"qid": 1569, "question": "When they say \"comparable performance\", how much of a performance drop do these new embeddings result in? in Inducing Interpretability in Knowledge Graph Embeddings", "answer": ["Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method."], "top_k_doc_id": [2191, 5310, 2193, 5311, 182, 2022, 181, 4981, 3269, 3271, 4982, 4601, 2258, 3634, 4493], "orig_top_k_doc_id": [2191, 2193, 5311, 182, 2022, 181, 4981, 3269, 5310, 3271, 4982, 4601, 2258, 3634, 4493]}]}
{"group_id": 117, "group_size": 10, "items": [{"qid": 2927, "question": "which non-english language had the best performance? in Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "answer": ["Russian", "Russsian"], "top_k_doc_id": [5106, 2995, 5105, 5107, 2874, 2875, 2284, 2282, 5108, 7172, 245, 7343, 5980, 2998, 2285], "orig_top_k_doc_id": [5105, 5107, 5106, 5108, 2282, 7172, 2284, 2995, 2874, 2875, 245, 7343, 5980, 2998, 2285]}, {"qid": 2928, "question": "which non-english language was the had the worst results? in Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "answer": ["Turkish"], "top_k_doc_id": [5106, 2995, 5105, 5107, 2874, 2875, 2284, 2282, 5108, 7172, 1048, 6332, 3011, 309, 2329], "orig_top_k_doc_id": [5105, 5107, 5108, 5106, 7172, 1048, 6332, 2874, 2282, 3011, 309, 2875, 2284, 2995, 2329]}, {"qid": 2929, "question": "what datasets were used in evaluation? in Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "answer": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "top_k_doc_id": [5106, 2995, 5105, 5107, 2874, 2875, 2284, 2285, 2590, 6190, 6472, 7746, 6159, 5570, 6473], "orig_top_k_doc_id": [5105, 5106, 2995, 2874, 7746, 2875, 6472, 6190, 6159, 2284, 5570, 2285, 5107, 2590, 6473]}, {"qid": 2932, "question": "what dataset was used for training? in Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "answer": ["Amazon reviews, Yelp restaurant reviews, restaurant reviews", "Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26"], "top_k_doc_id": [5106, 2995, 5105, 5107, 2874, 2875, 2284, 2285, 2590, 6190, 6472, 534, 5108, 2282, 2329], "orig_top_k_doc_id": [5105, 5106, 2995, 2284, 5107, 2590, 2874, 2285, 534, 5108, 6190, 2875, 2282, 6472, 2329]}, {"qid": 1617, "question": "How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets? in A Simple Approach to Multilingual Polarity Classification in Twitter", "answer": ["Total number of annotated data:\nSemeval'15: 10712\nSemeval'16: 28632\nTass'15: 69000\nSentipol'14: 6428"], "top_k_doc_id": [5106, 2282, 2284, 2285, 2219, 447, 448, 6752, 7755, 6856, 2286, 451, 450, 2215, 2982], "orig_top_k_doc_id": [2284, 2285, 2282, 448, 6856, 2286, 5106, 451, 447, 2219, 7755, 450, 2215, 2982, 6752]}, {"qid": 1619, "question": "What eight language are reported on? in A Simple Approach to Multilingual Polarity Classification in Twitter", "answer": ["Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish"], "top_k_doc_id": [5106, 2282, 2284, 2285, 2219, 447, 448, 6752, 7755, 5105, 2537, 7410, 502, 6207, 2220], "orig_top_k_doc_id": [2282, 5105, 7755, 2537, 2219, 448, 5106, 2285, 2284, 7410, 502, 447, 6207, 6752, 2220]}, {"qid": 1620, "question": "What are the components of the multilingual framework? in A Simple Approach to Multilingual Polarity Classification in Twitter", "answer": ["text-transformations to the messages, vector space model, Support Vector Machine"], "top_k_doc_id": [5106, 2282, 2284, 2285, 2219, 5105, 2789, 783, 7409, 2220, 4693, 2215, 2995, 3007, 5419], "orig_top_k_doc_id": [2282, 5105, 5106, 2789, 2219, 783, 2285, 7409, 2220, 4693, 2215, 2284, 2995, 3007, 5419]}, {"qid": 2931, "question": "how did the authors translate the reviews to other languages? in Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "answer": ["Using Google translation API.", "Google translation API"], "top_k_doc_id": [5106, 2995, 5105, 5107, 2874, 2875, 309, 5108, 784, 6616, 534, 5841, 1044, 6971, 6181], "orig_top_k_doc_id": [5105, 5106, 309, 2995, 5108, 784, 5107, 2874, 6616, 534, 5841, 1044, 6971, 2875, 6181]}, {"qid": 1618, "question": "In which languages did the approach outperform the reported results? in A Simple Approach to Multilingual Polarity Classification in Twitter", "answer": ["Arabic, German, Portuguese, Russian, Swedish"], "top_k_doc_id": [5106, 2282, 2284, 2285, 5105, 5421, 5980, 7259, 6174, 5420, 3296, 1050, 3604, 1041, 2330], "orig_top_k_doc_id": [2282, 5105, 5421, 5980, 2284, 7259, 5106, 2285, 6174, 5420, 3296, 1050, 3604, 1041, 2330]}, {"qid": 2930, "question": "what are the baselines? in Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "answer": ["majority baseline, lexicon-based approach", "majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach"], "top_k_doc_id": [5106, 2995, 5105, 5107, 3011, 3764, 3817, 6005, 5108, 2284, 5979, 2590, 6387, 6190, 1327], "orig_top_k_doc_id": [5105, 5106, 5107, 2995, 3011, 3764, 3817, 6005, 5108, 2284, 5979, 2590, 6387, 6190, 1327]}]}
{"group_id": 118, "group_size": 10, "items": [{"qid": 3569, "question": "what datasets were used? in EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "answer": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "top_k_doc_id": [5881, 5883, 5884, 5882, 7299, 7300, 1931, 2968, 1933, 2970, 6590, 1332, 1932, 371, 87], "orig_top_k_doc_id": [5883, 5881, 5884, 1931, 5882, 1933, 2970, 2968, 1932, 7300, 7299, 6590, 371, 1332, 87]}, {"qid": 3571, "question": "What are the sources of the datasets? in EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "answer": ["Friends TV sitcom, Facebook messenger chats"], "top_k_doc_id": [5881, 5883, 5884, 5882, 7299, 7300, 1931, 2968, 1933, 2970, 6590, 1332, 1500, 3622, 3623], "orig_top_k_doc_id": [5883, 5881, 5884, 1931, 5882, 2970, 1500, 1933, 7300, 2968, 7299, 3622, 1332, 3623, 6590]}, {"qid": 1395, "question": "What was the baseline? in SocialNLP EmotionX 2019 Challenge Overview: Predicting Emotions in Spoken Dialogues and Chats", "answer": ["No"], "top_k_doc_id": [5881, 5883, 5884, 1712, 1931, 1932, 5430, 6858, 1446, 5745, 5428, 5429, 227, 6655, 1074], "orig_top_k_doc_id": [5881, 1931, 5883, 5884, 5430, 6858, 1446, 1712, 5428, 1932, 227, 5745, 5429, 6655, 1074]}, {"qid": 1396, "question": "What is the size of the second dataset? in SocialNLP EmotionX 2019 Challenge Overview: Predicting Emotions in Spoken Dialogues and Chats", "answer": ["1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation"], "top_k_doc_id": [5881, 5883, 5884, 1712, 1931, 1932, 5430, 6858, 1446, 5745, 5428, 5429, 5882, 370, 5260], "orig_top_k_doc_id": [5881, 1931, 5883, 5884, 5430, 6858, 1712, 5882, 1932, 370, 5260, 5745, 5428, 1446, 5429]}, {"qid": 3568, "question": "what were the baselines? in EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "answer": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency\u2013inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "top_k_doc_id": [5881, 5883, 5884, 5882, 7299, 7300, 1931, 2968, 1933, 2970, 6590, 6355, 1715, 848, 1981], "orig_top_k_doc_id": [5883, 5881, 5884, 5882, 1931, 6355, 1933, 6590, 7299, 1715, 848, 2968, 7300, 2970, 1981]}, {"qid": 1394, "question": "What model was used by the top team? in SocialNLP EmotionX 2019 Challenge Overview: Predicting Emotions in Spoken Dialogues and Chats", "answer": ["Two different BERT models were developed"], "top_k_doc_id": [5881, 5883, 5884, 1712, 1931, 1932, 5430, 6858, 227, 674, 2280, 4833, 5428, 4834, 5258], "orig_top_k_doc_id": [5881, 1931, 5883, 5884, 1932, 4833, 5430, 6858, 4834, 1712, 2280, 674, 227, 5258, 5428]}, {"qid": 1397, "question": "How large is the first dataset? in SocialNLP EmotionX 2019 Challenge Overview: Predicting Emotions in Spoken Dialogues and Chats", "answer": ["1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation"], "top_k_doc_id": [5881, 5883, 5884, 1712, 1931, 1932, 5430, 6858, 1446, 5745, 6655, 250, 5882, 123, 227], "orig_top_k_doc_id": [5881, 1931, 5883, 5884, 6858, 5430, 1712, 1446, 6655, 250, 5745, 5882, 123, 1932, 227]}, {"qid": 1398, "question": "Who was the top-scoring team? in SocialNLP EmotionX 2019 Challenge Overview: Predicting Emotions in Spoken Dialogues and Chats", "answer": ["IDEA"], "top_k_doc_id": [5881, 5883, 5884, 1712, 1931, 1932, 5430, 6858, 227, 674, 2280, 4833, 5428, 2630, 6652], "orig_top_k_doc_id": [5881, 1931, 5883, 5884, 1932, 5430, 4833, 2630, 6858, 2280, 1712, 674, 6652, 227, 5428]}, {"qid": 3572, "question": "What labels does the dataset have? in EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "answer": ["Ekman\u2019s six basic emotions,  neutral"], "top_k_doc_id": [5881, 5883, 5884, 5882, 7299, 7300, 1931, 2968, 1981, 9, 1980, 10, 1715, 1932, 3129], "orig_top_k_doc_id": [5881, 5883, 5884, 5882, 1931, 7299, 7300, 1981, 9, 1980, 10, 2968, 1715, 1932, 3129]}, {"qid": 3570, "question": "What BERT models are used? in EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation", "answer": ["BERT-base, BERT-large, BERT-uncased, BERT-cased"], "top_k_doc_id": [5881, 5883, 5884, 5882, 7299, 7300, 1933, 1332, 1932, 6356, 1319, 372, 6355, 9, 371], "orig_top_k_doc_id": [5881, 5883, 5884, 5882, 1933, 1332, 7299, 1932, 7300, 6356, 1319, 372, 6355, 9, 371]}]}
{"group_id": 119, "group_size": 10, "items": [{"qid": 3867, "question": "What is the machine learning method used to make the predictions? in Event Representation Learning Enhanced with External Commonsense Knowledge", "answer": ["SGNN", "SGNN, Word, BIBREF23, Event, BIBREF24, NTN, BIBREF4, KGEB, BIBREF18 ", "Compositional Neural Network, Element-wise Multiplicative Composition, Neural Tensor Network"], "top_k_doc_id": [1822, 2004, 5605, 6250, 1159, 4278, 6246, 6249, 3973, 4273, 7514, 7520, 6002, 1826, 690], "orig_top_k_doc_id": [6250, 1822, 7520, 6246, 6249, 7514, 4278, 4273, 3973, 5605, 2004, 6002, 1826, 690, 1159]}, {"qid": 3869, "question": "What are the datasets used in the paper? in Event Representation Learning Enhanced with External Commonsense Knowledge", "answer": ["ATOMIC, hard similarity small and big dataset, the transitive sentence similarity dataset, the standard multiple choice narrative cloze (MCNC) dataset", "ATOMIC , MCNC", "ATOMIC, New York Times Gigaword, an unreleased extension of the dataset by BIBREF5, MCNC"], "top_k_doc_id": [1822, 2004, 5605, 6250, 1159, 4278, 6246, 6249, 3973, 4273, 7514, 7520, 1363, 6546, 6543], "orig_top_k_doc_id": [6250, 1822, 6249, 6246, 2004, 7520, 4278, 1159, 4273, 1363, 5605, 3973, 7514, 6546, 6543]}, {"qid": 3868, "question": "How is the event prediction task evaluated? in Event Representation Learning Enhanced with External Commonsense Knowledge", "answer": ["accuracy", "replacing the event embeddings on SGNN and running it on the MCNC dataset", "we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings"], "top_k_doc_id": [1822, 2004, 5605, 6250, 1159, 4278, 6246, 6249, 1363, 1765, 1823, 6248, 5608, 1827, 2009], "orig_top_k_doc_id": [6250, 6249, 6246, 1822, 1159, 1363, 4278, 1765, 1823, 5605, 6248, 5608, 1827, 2004, 2009]}, {"qid": 1445, "question": "Did they compare to Transformer based large language models? in Story Ending Generation with Incremental Encoding and Commonsense Knowledge", "answer": ["No"], "top_k_doc_id": [1822, 558, 1159, 1823, 1826, 1827, 2004, 2007, 2008, 1824, 1825, 2005, 2006, 2009, 1138], "orig_top_k_doc_id": [2004, 2009, 2008, 2005, 2007, 1822, 1826, 2006, 1823, 1825, 1827, 1824, 1159, 1138, 558]}, {"qid": 1446, "question": "Which baselines are they using? in Story Ending Generation with Incremental Encoding and Commonsense Knowledge", "answer": ["Seq2Seq, HLSTM, HLSTM+Copy, HLSTM+Graph Attention, HLSTM+Contextual Attention"], "top_k_doc_id": [1822, 558, 1159, 1823, 1826, 1827, 2004, 2007, 2008, 1824, 1825, 2005, 2006, 2009, 2594], "orig_top_k_doc_id": [2004, 2008, 2009, 2007, 2005, 2006, 1826, 1822, 1825, 1827, 1824, 1159, 1823, 2594, 558]}, {"qid": 3658, "question": "What types of commonsense knowledge are they talking about? in Improved Representation Learning for Predicting Commonsense Ontologies", "answer": ["hypernym relations", "the collection of information that an ordinary person would have", "Hypernymy or is-a relations between words or phrases", "Knowledge than an ordinary person would have such as transitive entailment relation, complex ordering, compositionality, multi-word entities"], "top_k_doc_id": [1822, 2004, 5605, 6250, 6002, 1826, 6050, 945, 7514, 1827, 946, 7520, 2009, 4273, 5607], "orig_top_k_doc_id": [2004, 6002, 1822, 1826, 6050, 945, 5605, 7514, 1827, 946, 7520, 6250, 2009, 4273, 5607]}, {"qid": 4084, "question": "How you incorporate commonsense into an LSTM? in Augmenting End-to-End Dialog Systems with Commonsense Knowledge", "answer": ["by employing an external memory module containing commonsense knowledge", "using another LSTM for encoding all assertions $a$ in $A_x$ , as illustrated in Figure 3 . Each $a$ , originally in the form of $$ , is transformed into a sequence of tokens by chunking $c_1$ , $c_2$ , concepts which are potentially multi-word phrases, into $[c_{11},c_{12},c_{13}...]$ and $[c_{21},c_{22},c_{23}...]$ . Thus, $a=[c_{11},c_{12},c_{13}...,r,c_{21},c_{22},c_{23}...]$ .", "using another LSTM for encoding commonsense assertions"], "top_k_doc_id": [1822, 693, 949, 6543, 6546, 6587, 1136, 945, 4550, 4273, 3972, 1137, 950, 4278, 2005], "orig_top_k_doc_id": [6543, 6587, 1822, 693, 1136, 945, 4550, 4273, 3972, 1137, 950, 6546, 4278, 2005, 949]}, {"qid": 4086, "question": "Which commonsense knowledge base are they using? in Augmenting End-to-End Dialog Systems with Commonsense Knowledge", "answer": ["ConceptNet", "ConceptNet", "ConceptNet"], "top_k_doc_id": [1822, 693, 949, 6543, 6546, 6587, 5605, 2004, 946, 7518, 1826, 1159, 6050, 6545, 7520], "orig_top_k_doc_id": [6543, 6587, 1822, 693, 5605, 2004, 946, 949, 7518, 1826, 1159, 6050, 6546, 6545, 7520]}, {"qid": 1332, "question": "Which metrics are they evaluating with? in Incorporating Structured Commonsense Knowledge in Story Completion", "answer": ["accuracy"], "top_k_doc_id": [1822, 558, 1159, 1823, 1826, 1827, 2004, 2007, 2008, 7517, 562, 6002, 945, 7518, 950], "orig_top_k_doc_id": [1822, 1827, 1823, 7517, 562, 6002, 558, 1826, 2008, 2007, 945, 2004, 7518, 950, 1159]}, {"qid": 1571, "question": "What types of word representations are they evaluating? in CA-EHN: Commonsense Word Analogy from E-HowNet", "answer": ["GloVE; SGNS"], "top_k_doc_id": [1822, 2195, 2194, 2196, 5302, 2007, 3994, 690, 2008, 531, 5603, 2526, 3049, 945, 946], "orig_top_k_doc_id": [2195, 2194, 2196, 5302, 2007, 3994, 690, 2008, 531, 1822, 5603, 2526, 3049, 945, 946]}]}
{"group_id": 120, "group_size": 10, "items": [{"qid": 4462, "question": "what was the baseline? in Application of Pre-training Models in Named Entity Recognition", "answer": ["BiGRU+CRF", "BiGRU+CRF"], "top_k_doc_id": [7100, 6810, 1256, 3743, 6153, 20, 22, 23, 5879, 930, 19, 5051, 5368, 4573, 7553], "orig_top_k_doc_id": [7100, 6810, 1256, 22, 23, 20, 3743, 930, 19, 5051, 5368, 5879, 4573, 6153, 7553]}, {"qid": 4463, "question": "what were roberta's results? in Application of Pre-training Models in Named Entity Recognition", "answer": [" the RoBERTa model achieves the highest F1 value of 94.17", "F1 value of 94.17"], "top_k_doc_id": [7100, 6810, 1256, 3743, 6153, 20, 22, 23, 5879, 534, 7055, 21, 7759, 4575, 4750], "orig_top_k_doc_id": [7100, 22, 6810, 534, 6153, 1256, 3743, 20, 7055, 21, 7759, 4575, 4750, 5879, 23]}, {"qid": 4461, "question": "what evaluation metrics did they use? in Application of Pre-training Models in Named Entity Recognition", "answer": ["Precision, recall and F1 score.", "Precision \nRecall\nF1"], "top_k_doc_id": [7100, 6810, 1256, 3743, 6153, 7055, 7759, 5368, 1591, 2318, 820, 6050, 4573, 5051, 7688], "orig_top_k_doc_id": [6153, 7055, 7759, 5368, 6810, 7100, 1591, 2318, 820, 6050, 3743, 4573, 5051, 7688, 1256]}, {"qid": 4544, "question": "Does the SESAME dataset include discontiguous entities? in Building a Massive Corpus for Named Entity Recognition Using Free Open Data Sources", "answer": ["No", "No"], "top_k_doc_id": [7100, 1090, 65, 4158, 4755, 6093, 6153, 7102, 7103, 6810, 7672, 4573, 6151, 4574, 1262], "orig_top_k_doc_id": [7100, 7102, 7103, 1090, 4158, 4755, 65, 6810, 6153, 7672, 4573, 6093, 6151, 4574, 1262]}, {"qid": 4545, "question": "How big is the resulting SESAME dataset? in Building a Massive Corpus for Named Entity Recognition Using Free Open Data Sources", "answer": ["3,650,909 sentences,  87,769,158 tokens", "3,650,909 sentences"], "top_k_doc_id": [7100, 1090, 65, 4158, 4755, 6093, 6153, 7102, 7103, 2447, 1934, 6467, 22, 6297, 3744], "orig_top_k_doc_id": [7100, 7102, 7103, 1090, 2447, 65, 1934, 6467, 4755, 4158, 22, 6093, 6153, 6297, 3744]}, {"qid": 2824, "question": "what is the state of the art? in Robust Named Entity Recognition in Idiosyncratic Domains", "answer": ["Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2"], "top_k_doc_id": [7100, 1256, 2318, 2326, 4944, 4945, 4946, 4947, 854, 1262, 4750, 2973, 2328, 3784, 501], "orig_top_k_doc_id": [4947, 4946, 4944, 4945, 1256, 854, 2318, 7100, 2326, 1262, 4750, 2973, 2328, 3784, 501]}, {"qid": 2825, "question": "what standard dataset were used? in Robust Named Entity Recognition in Idiosyncratic Domains", "answer": ["The GENIA Corpus , CoNLL2003", "GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC", "CoNLL2003-testA, GENIA"], "top_k_doc_id": [7100, 1256, 2318, 2326, 4944, 4945, 4946, 4947, 6153, 5104, 3743, 4573, 6151, 4141, 4575], "orig_top_k_doc_id": [4947, 4944, 4946, 4945, 7100, 2318, 6153, 2326, 5104, 1256, 3743, 4573, 6151, 4141, 4575]}, {"qid": 4464, "question": "which was the worst performing model? in Application of Pre-training Models in Named Entity Recognition", "answer": ["ERNIE-tiny", "ERNIE-tiny"], "top_k_doc_id": [7100, 6810, 20, 4589, 7287, 6050, 19, 6698, 1591, 4667, 7553, 4750, 7055, 7288, 5502], "orig_top_k_doc_id": [20, 4589, 7287, 6810, 6050, 19, 6698, 1591, 4667, 7553, 4750, 7055, 7100, 7288, 5502]}, {"qid": 4543, "question": "Is the dataset completely automatically generated? in Building a Massive Corpus for Named Entity Recognition Using Free Open Data Sources", "answer": ["Yes", "Yes"], "top_k_doc_id": [7100, 1090, 65, 4158, 4755, 6093, 4141, 7806, 1262, 4573, 20, 6810, 1422, 883, 6701], "orig_top_k_doc_id": [7100, 4141, 65, 7806, 1090, 4158, 1262, 6093, 4573, 4755, 20, 6810, 1422, 883, 6701]}, {"qid": 505, "question": "What web and user-generated NER datasets are used for the analysis? in Generalisation in Named Entity Recognition: A Quantitative Analysis", "answer": ["MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC"], "top_k_doc_id": [7100, 1090, 618, 2973, 1094, 4858, 607, 1262, 5879, 6810, 5775, 616, 1099, 7697, 5956], "orig_top_k_doc_id": [1090, 618, 2973, 7100, 1094, 4858, 607, 1262, 5879, 6810, 5775, 616, 1099, 7697, 5956]}]}
{"group_id": 121, "group_size": 10, "items": [{"qid": 4920, "question": "How many GPUs do they train their models on? in Patient Knowledge Distillation for BERT Model Compression", "answer": ["No", "No"], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 4627, 6368, 5995, 7634, 2682, 4626, 6367, 4817], "orig_top_k_doc_id": [7630, 5992, 7634, 7631, 5994, 4624, 6368, 6367, 5991, 2682, 5995, 5993, 4627, 4626, 4817]}, {"qid": 4922, "question": "What downstream tasks are tested? in Patient Knowledge Distillation for BERT Model Compression", "answer": ["Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, Machine Reading Comprehension", "Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, Machine Reading Comprehension"], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 4627, 6368, 5995, 7634, 2682, 4626, 6367, 4628], "orig_top_k_doc_id": [7630, 5992, 7634, 5994, 4624, 7631, 5991, 2682, 4628, 6368, 4627, 4626, 6367, 5993, 5995]}, {"qid": 4921, "question": "What of the two strategies works best? in Patient Knowledge Distillation for BERT Model Compression", "answer": ["PKD-Skip", "PKD-Skip"], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 4627, 6368, 5995, 7634, 2682, 7632, 7603, 4817], "orig_top_k_doc_id": [7630, 5992, 7631, 5994, 7634, 5991, 4624, 5993, 5995, 6368, 7632, 7603, 4817, 2682, 4627]}, {"qid": 3652, "question": "Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems? in LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression", "answer": ["No", "No", "No"], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 4627, 6368, 5995, 7634, 4239, 5996, 4628, 2679], "orig_top_k_doc_id": [5991, 5992, 5995, 5994, 5993, 7630, 4239, 4624, 4627, 5996, 7631, 4628, 6368, 7634, 2679]}, {"qid": 1841, "question": "How much is pre-training loss increased in Low/Medium/Hard level of pruning? in Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "answer": ["The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0"], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 2682, 5995, 7600, 7603, 2679, 2681, 2680, 4277], "orig_top_k_doc_id": [2679, 2681, 2682, 5992, 5994, 5991, 5995, 7603, 2680, 5993, 4624, 7630, 7600, 4277, 7631]}, {"qid": 2628, "question": "Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?   in Extreme Language Model Compression with Optimal Subwords and Shared Projections", "answer": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 4627, 6368, 4625, 7603, 4628, 4626, 7600, 6367], "orig_top_k_doc_id": [4624, 4627, 5992, 4625, 7630, 7603, 5994, 4628, 7631, 5993, 5991, 4626, 7600, 6368, 6367]}, {"qid": 4899, "question": "What type of weight pruning do they use? in Sequence-Level Knowledge Distillation", "answer": ["pruning parameters by removing the weights with the lowest absolute values", "Prune %x of the parameters by removing the weights with the lowest absolute values."], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 4624, 7631, 2682, 5995, 7600, 7603, 7604, 6368, 5477, 7601], "orig_top_k_doc_id": [7603, 5993, 5992, 7600, 5995, 5994, 4624, 2682, 5991, 7630, 7604, 6368, 7631, 5477, 7601]}, {"qid": 3653, "question": "Do they evaluate which compression method yields the most gains? in LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression", "answer": ["Yes", "Yes", "No", "Yes"], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 5995, 4239, 111, 6368, 2679, 7634, 3559, 4627, 6295, 6294], "orig_top_k_doc_id": [5991, 5992, 5993, 5995, 5994, 7630, 4239, 111, 6368, 2679, 7634, 3559, 4627, 6295, 6294]}, {"qid": 3654, "question": "On which datasets does LadaBERT achieve state-of-the-art? in LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression", "answer": ["MNLI-m, MNLI-mm, SST-2, QQP, QNLI", "LadaBERT -1, -2 achieves state of art on all datasets namely, MNLI-m MNLI-mm, SST-2, QQP, and QNLI. \nLadaBERT-3 achieves SOTA on the first four dataset. \nLadaBERT-4 achieves SOTA on MNLI-m, MNLI-mm, and QNLI ", "SST-2, MNLI-m, MNLI-mm, QNLI, QQP", "LadaBERT-1 and LadaBERT-2  on MNLI-m, MNLI-mm, SST-2, QQP and QNLI .\nLadaBERT-3  on MNLI-m, MNLI-mm, SST-2, and QQP . LadaBERT-4  on MNLI-m, MNLI-mm and QNLI ."], "top_k_doc_id": [5991, 5992, 7630, 5993, 5994, 5995, 5996, 2310, 4239, 2679, 2306, 3167, 368, 1560, 6135], "orig_top_k_doc_id": [5991, 5995, 5994, 5992, 5993, 5996, 7630, 2310, 4239, 2679, 2306, 3167, 368, 1560, 6135]}, {"qid": 1840, "question": "How they observe that fine-tuning BERT on a specific task does not improve its prunability? in Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "answer": ["we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. "], "top_k_doc_id": [5991, 5992, 7630, 2682, 2679, 2681, 2680, 7472, 4754, 4277, 6870, 5995, 6368, 485, 2665], "orig_top_k_doc_id": [2682, 2679, 2681, 2680, 7630, 5992, 5991, 7472, 4754, 4277, 6870, 5995, 6368, 485, 2665]}]}
{"group_id": 122, "group_size": 10, "items": [{"qid": 4984, "question": "How well does the system perform? in Real-World Conversational AI for Hotel Bookings", "answer": ["For NER,  combined entity model achieves the best performance (F1 0.96). For IR, BERT+fine-tuning model achieves TOP-1 Recall 0.895 and Top-3 Recall 0.961.", "F1 score of 0.96 on recognizing both hotel and location entities and Top-1 recall of 0.895 with the IR BERT model"], "top_k_doc_id": [6636, 7759, 6543, 7758, 7760, 6635, 4663, 6674, 3451, 6638, 400, 3100, 717, 821, 1817], "orig_top_k_doc_id": [7760, 7758, 7759, 6636, 6674, 6543, 6638, 6635, 4663, 3100, 717, 3451, 821, 400, 1817]}, {"qid": 4985, "question": "Where does their information come from? in Real-World Conversational AI for Hotel Bookings", "answer": ["From conversions between users and customer support agents through their partners, and professional annotators creating data.", "Information  from users and information  from database of approximately 100,000 cities and 300,000 hotels, populated using data from their partners."], "top_k_doc_id": [6636, 7759, 6543, 7758, 7760, 6635, 4663, 6674, 3451, 6638, 400, 1706, 6583, 6876, 6525], "orig_top_k_doc_id": [7760, 7758, 6636, 7759, 6543, 6635, 3451, 6674, 1706, 6583, 4663, 6876, 6525, 6638, 400]}, {"qid": 4982, "question": "How is their NER model trained? in Real-World Conversational AI for Hotel Bookings", "answer": ["Using SpaCy", "Trained using SpaCy and fine-tuned with their data of hotel and location entities"], "top_k_doc_id": [6636, 7759, 6543, 7758, 7760, 6635, 4663, 6674, 3451, 6638, 7514, 5260, 1817, 4445, 4577], "orig_top_k_doc_id": [7760, 7758, 7759, 6543, 6636, 6635, 7514, 6674, 3451, 6638, 5260, 1817, 4445, 4577, 4663]}, {"qid": 4177, "question": "what other applications did they experiment in? in Hotel2vec: Learning Attribute-Aware Hotel Embeddings with Self-Supervision", "answer": ["No", "None"], "top_k_doc_id": [6636, 7759, 531, 854, 1683, 6635, 6637, 6638, 6639, 6821, 2917, 3129, 7760, 3634, 855], "orig_top_k_doc_id": [6636, 6638, 6637, 6635, 2917, 6821, 1683, 6639, 7759, 7760, 3634, 855, 3129, 531, 854]}, {"qid": 4178, "question": "what dataset was used for training? in Hotel2vec: Learning Attribute-Aware Hotel Embeddings with Self-Supervision", "answer": ["Our dataset contains more than 40M user click sessions", " dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels", "A dataset containing 40M user click sessions with more than 1.1M unique hotels."], "top_k_doc_id": [6636, 7759, 531, 854, 1683, 6635, 6637, 6638, 6639, 6821, 2917, 3129, 527, 6113, 2853], "orig_top_k_doc_id": [6636, 6638, 6637, 6635, 6639, 531, 7759, 854, 3129, 527, 6113, 2853, 6821, 1683, 2917]}, {"qid": 4981, "question": "What is the baseline? in Real-World Conversational AI for Hotel Bookings", "answer": ["rule-based unigram matching baseline", "a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match"], "top_k_doc_id": [6636, 7759, 6543, 7758, 7760, 6635, 4663, 6674, 3100, 1168, 3175, 400, 6583, 320, 7514], "orig_top_k_doc_id": [7760, 7758, 7759, 6636, 6543, 3100, 4663, 6635, 1168, 6674, 3175, 400, 6583, 320, 7514]}, {"qid": 4176, "question": "how was the experiment evaluated? in Hotel2vec: Learning Attribute-Aware Hotel Embeddings with Self-Supervision", "answer": ["the average number of times the correct selection appears in the top k predictions", "Hits@k for hotel context prediction, Comparison using cosine similarity, Average intra/inter market embedding similarities, Visualization of embeddings, Most similar hotels, Algebraic operations on hotel embeddings"], "top_k_doc_id": [6636, 7759, 531, 854, 1683, 6635, 6637, 6638, 6639, 6821, 7760, 3193, 1697, 6045, 5417], "orig_top_k_doc_id": [6636, 6638, 6637, 6635, 6639, 7760, 7759, 531, 3193, 1697, 6045, 1683, 5417, 854, 6821]}, {"qid": 4983, "question": "Do they use pretrained word embeddings such as BERT? in Real-World Conversational AI for Hotel Bookings", "answer": ["Yes", "Yes"], "top_k_doc_id": [6636, 7759, 6543, 7758, 7760, 6635, 3129, 4414, 3836, 3837, 1817, 5710, 680, 3835, 135], "orig_top_k_doc_id": [7760, 7758, 7759, 3129, 6636, 6635, 4414, 3836, 3837, 1817, 6543, 5710, 680, 3835, 135]}, {"qid": 4175, "question": "what is the cold-start problem? in Hotel2vec: Learning Attribute-Aware Hotel Embeddings with Self-Supervision", "answer": ["Dealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\"", "hotels/items appear infrequently or never in historical data, Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data", "No"], "top_k_doc_id": [6636, 7759, 531, 854, 1683, 6635, 6637, 6638, 6639, 7816, 7336, 5069, 7333, 6066, 4247], "orig_top_k_doc_id": [6637, 6635, 6638, 6639, 6636, 7816, 7336, 5069, 1683, 854, 7333, 531, 6066, 4247, 7759]}, {"qid": 4986, "question": "What intents do they have? in Real-World Conversational AI for Hotel Bookings", "answer": ["thanks, cancel, stop, search, unknown ", "The most common intents are thanks, cancel, stop, search, and unknown"], "top_k_doc_id": [6636, 7759, 6543, 7758, 7760, 400, 2277, 7156, 1632, 3100, 198, 3680, 1168, 3679, 3562], "orig_top_k_doc_id": [7760, 7759, 7758, 400, 2277, 7156, 1632, 6636, 3100, 198, 3680, 1168, 3679, 3562, 6543]}]}
{"group_id": 123, "group_size": 9, "items": [{"qid": 1, "question": "What are the results? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO."], "top_k_doc_id": [0, 1, 2, 4155, 5546, 6804, 4355, 2423, 4351, 1920, 5537, 1273, 1906, 1905, 6431], "orig_top_k_doc_id": [0, 1, 2, 6804, 1920, 2423, 4155, 4355, 1273, 5546, 4351, 5537, 1906, 1905, 6431]}, {"qid": 3, "question": "How big is the Japanese data? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus", "The ACP corpus has around 700k events split into positive and negative polarity "], "top_k_doc_id": [0, 1, 2, 4155, 5546, 6804, 4355, 2423, 4351, 1920, 5537, 3880, 6053, 6473, 1170], "orig_top_k_doc_id": [0, 1, 2, 6804, 4351, 4155, 5546, 3880, 6053, 1920, 4355, 2423, 5537, 6473, 1170]}, {"qid": 8, "question": "How large is raw corpus used for training? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["100 million sentences"], "top_k_doc_id": [0, 1, 2, 4155, 5546, 6804, 4355, 2423, 4351, 4188, 4359, 6431, 1910, 4347, 1906], "orig_top_k_doc_id": [0, 1, 2, 6804, 4188, 4359, 5546, 4355, 6431, 4351, 1910, 4347, 4155, 2423, 1906]}, {"qid": 6, "question": "How does their model learn using mostly raw data? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"], "top_k_doc_id": [0, 1, 2, 4155, 5546, 6804, 4355, 4380, 4359, 1906, 4347, 5537, 1905, 1540, 3701], "orig_top_k_doc_id": [0, 1, 2, 4380, 4359, 6804, 1906, 4347, 5537, 4155, 1905, 1540, 4355, 5546, 3701]}, {"qid": 4, "question": "What are labels available in dataset for supervision? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["negative, positive"], "top_k_doc_id": [0, 1, 2, 4155, 5546, 6804, 7628, 6821, 6816, 5537, 1911, 1920, 331, 2423, 4351], "orig_top_k_doc_id": [0, 2, 1, 5546, 7628, 6804, 6821, 6816, 5537, 1911, 4155, 1920, 331, 2423, 4351]}, {"qid": 0, "question": "What is the seed lexicon? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["a vocabulary of positive and negative predicates that helps determine the polarity score of an event", "seed lexicon consists of positive and negative predicates"], "top_k_doc_id": [0, 1, 2, 451, 2162, 2971, 2972, 4351, 5525, 5537, 5546, 6804, 4155, 1920, 1273], "orig_top_k_doc_id": [0, 1, 2, 4351, 5537, 2972, 5546, 5525, 6804, 2971, 4155, 1920, 1273, 451, 2162]}, {"qid": 2, "question": "How are relations used to propagate polarity? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ", "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity"], "top_k_doc_id": [0, 1, 2, 4155, 1920, 4347, 6431, 1760, 4355, 2423, 7761, 5525, 1906, 4782, 1905], "orig_top_k_doc_id": [0, 1, 2, 1920, 4347, 6431, 1760, 4155, 4355, 2423, 7761, 5525, 1906, 4782, 1905]}, {"qid": 7, "question": "How big is seed lexicon used for training? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["30 words"], "top_k_doc_id": [0, 1, 2, 451, 2162, 2971, 2972, 4351, 5525, 5537, 5546, 6804, 3880, 2164, 2019], "orig_top_k_doc_id": [0, 1, 2, 4351, 2972, 5546, 5525, 3880, 5537, 6804, 2971, 2164, 2019, 2162, 451]}, {"qid": 5, "question": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach? in Minimally Supervised Learning of Affective Events Using Discourse Relations", "answer": ["3%"], "top_k_doc_id": [0, 1, 2, 4351, 6116, 6249, 1540, 6805, 6248, 2424, 7023, 6473, 3880, 6804, 1920], "orig_top_k_doc_id": [0, 4351, 1, 2, 6116, 6249, 1540, 6805, 6248, 2424, 7023, 6473, 3880, 6804, 1920]}]}
{"group_id": 124, "group_size": 9, "items": [{"qid": 9, "question": "Does the paper report macro F1? in PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry", "answer": ["Yes", "Yes"], "top_k_doc_id": [5, 4, 6, 10, 7, 8, 9, 3437, 5884, 1931, 6853, 6457, 5980, 564, 6458], "orig_top_k_doc_id": [5, 10, 6, 4, 8, 7, 1931, 5884, 6853, 3437, 6457, 9, 5980, 564, 6458]}, {"qid": 10, "question": "How is the annotation experiment evaluated? in PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry", "answer": ["confusion matrices of labels between annotators"], "top_k_doc_id": [5, 4, 6, 10, 7, 8, 9, 3437, 5884, 1931, 6853, 3622, 5211, 5360, 11], "orig_top_k_doc_id": [5, 10, 6, 4, 8, 7, 9, 5884, 3437, 1931, 3622, 5211, 5360, 11, 6853]}, {"qid": 111, "question": "What is the algorithm used for the classification tasks? in Diachronic Topics in New High German Poetry", "answer": ["Random Forest Ensemble classifiers"], "top_k_doc_id": [5, 4, 6, 10, 128, 129, 5447, 5527, 8, 556, 557, 2074, 6211, 4533, 9], "orig_top_k_doc_id": [128, 10, 129, 5, 6, 4, 5527, 556, 557, 2074, 5447, 6211, 4533, 8, 9]}, {"qid": 113, "question": "What is the corpus used in the study? in Diachronic Topics in New High German Poetry", "answer": ["TextGrid Repository", "The Digital Library in the TextGrid Repository"], "top_k_doc_id": [5, 4, 6, 10, 128, 129, 5447, 5527, 8, 556, 557, 5378, 3795, 5209, 5210], "orig_top_k_doc_id": [128, 10, 129, 5, 5527, 6, 4, 556, 5447, 8, 5378, 557, 3795, 5209, 5210]}, {"qid": 11, "question": "What are the aesthetic emotions formalized? in PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry", "answer": ["feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking), Emotions that exhibit this dual capacity have been defined as \u201caesthetic emotions\u201d"], "top_k_doc_id": [5, 4, 6, 10, 7, 8, 9, 3437, 5884, 523, 520, 3622, 564, 521, 3623], "orig_top_k_doc_id": [5, 4, 10, 6, 8, 7, 523, 520, 3622, 9, 564, 521, 3623, 3437, 5884]}, {"qid": 112, "question": "Is the outcome of the LDA analysis evaluated in any way? in Diachronic Topics in New High German Poetry", "answer": ["Yes"], "top_k_doc_id": [5, 4, 6, 10, 128, 129, 5447, 5527, 4422, 2074, 1129, 3795, 6128, 5069, 1130], "orig_top_k_doc_id": [128, 129, 10, 5447, 5, 5527, 4422, 2074, 6, 1129, 3795, 4, 6128, 5069, 1130]}, {"qid": 3244, "question": "What is the corpus used for the task? in Shared Task: Lexical Semantic Change Detection in German", "answer": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "top_k_doc_id": [5, 1000, 3575, 5447, 5845, 6016, 6332, 6852, 2789, 5570, 6853, 3622, 231, 7439, 6745], "orig_top_k_doc_id": [5447, 6016, 1000, 3575, 2789, 6853, 6852, 3622, 5845, 6332, 231, 7439, 5, 5570, 6745]}, {"qid": 3245, "question": "How is evaluation performed? in Shared Task: Lexical Semantic Change Detection in German", "answer": ["As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ was used", "Spearman's rank-order correlation"], "top_k_doc_id": [5, 1000, 3575, 5447, 5845, 6016, 6332, 6852, 2789, 5570, 1889, 153, 7260, 5965, 4668], "orig_top_k_doc_id": [5447, 1889, 5570, 6016, 5845, 1000, 153, 3575, 2789, 7260, 6332, 5, 5965, 4668, 6852]}, {"qid": 3243, "question": "What is the algorithm used to create word embeddings? in Shared Task: Lexical Semantic Change Detection in German", "answer": ["No"], "top_k_doc_id": [5, 1000, 3575, 5447, 5845, 6016, 6332, 6852, 6853, 5701, 2287, 82, 124, 4948, 3615], "orig_top_k_doc_id": [5447, 6016, 6853, 3575, 5, 6852, 5701, 6332, 2287, 82, 1000, 124, 4948, 3615, 5845]}]}
{"group_id": 125, "group_size": 9, "items": [{"qid": 64, "question": "how many languages exactly is the sentiment lexica for? in UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "answer": ["No"], "top_k_doc_id": [3612, 5546, 67, 6005, 6006, 6007, 6857, 3613, 5137, 6856, 2971, 7509, 3616, 4433, 7391], "orig_top_k_doc_id": [67, 6005, 6007, 6857, 3612, 3613, 5546, 2971, 7509, 6006, 6856, 3616, 4433, 5137, 7391]}, {"qid": 65, "question": "what sentiment sources do they compare with? in UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "answer": ["manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15"], "top_k_doc_id": [3612, 5546, 67, 6005, 6006, 6007, 6857, 3613, 5137, 6856, 1874, 534, 5100, 1051, 2875], "orig_top_k_doc_id": [67, 6005, 6007, 1874, 3612, 5546, 534, 6857, 6006, 3613, 5100, 1051, 5137, 2875, 6856]}, {"qid": 3312, "question": "What was their performance on emotion detection? in Distant supervision for emotion detection using Facebook reactions", "answer": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "top_k_doc_id": [3612, 5546, 227, 3622, 5547, 5549, 87, 1979, 3623, 5008, 5893, 2968, 1931, 5011, 5881], "orig_top_k_doc_id": [5546, 5549, 5547, 3622, 3623, 227, 87, 5893, 2968, 3612, 1931, 5008, 5011, 1979, 5881]}, {"qid": 3313, "question": "Which existing benchmarks did they compare to? in Distant supervision for emotion detection using Facebook reactions", "answer": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "top_k_doc_id": [3612, 5546, 227, 3622, 5547, 5549, 87, 1979, 3623, 5008, 3894, 3445, 1242, 6951, 5010], "orig_top_k_doc_id": [5546, 5549, 5547, 227, 3894, 5008, 3622, 87, 3612, 3445, 3623, 1242, 1979, 6951, 5010]}, {"qid": 63, "question": "how is quality measured? in UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "answer": ["Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.", "No"], "top_k_doc_id": [3612, 5546, 67, 6005, 6006, 6007, 6857, 7752, 5100, 4863, 4695, 534, 6467, 1051, 3528], "orig_top_k_doc_id": [67, 6006, 6005, 6007, 7752, 6857, 3612, 5546, 5100, 4863, 4695, 534, 6467, 1051, 3528]}, {"qid": 3314, "question": "Which Facebook pages did they look at? in Distant supervision for emotion detection using Facebook reactions", "answer": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "top_k_doc_id": [3612, 5546, 227, 3622, 5547, 5549, 5548, 3894, 1951, 3311, 6455, 3445, 447, 7522, 1489], "orig_top_k_doc_id": [5546, 5549, 5547, 5548, 3894, 227, 1951, 3311, 3612, 6455, 3445, 3622, 447, 7522, 1489]}, {"qid": 4233, "question": "Is the performance improvement (with and without affect attributes) statistically significant? in Affect-LM: A Neural Language Model for Customizable Affective Text Generation", "answer": ["Yes", "Yes"], "top_k_doc_id": [3612, 3543, 3616, 6723, 6724, 6725, 6728, 849, 1494, 2967, 6726, 6727, 2178, 3615, 3881], "orig_top_k_doc_id": [6723, 6726, 6728, 6724, 6727, 6725, 2967, 1494, 3612, 3616, 849, 2178, 3543, 3615, 3881]}, {"qid": 4234, "question": "How to extract affect attributes from the sentence? in Affect-LM: A Neural Language Model for Customizable Affective Text Generation", "answer": ["Using a dictionary of emotional words, LIWC, they perform keyword spotting.", "A sentence is represented by five features that each mark presence or absence of an emotion: positive emotion, angry, sad, anxious, and negative emotion.", "either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$"], "top_k_doc_id": [3612, 3543, 3616, 6723, 6724, 6725, 6728, 849, 1494, 2967, 6726, 6727, 7300, 3880, 3613], "orig_top_k_doc_id": [6723, 6724, 6726, 6728, 6725, 6727, 3612, 849, 3616, 2967, 7300, 3880, 3543, 3613, 1494]}, {"qid": 2283, "question": "Is the affect of a word affected by context? in Aff2Vec: Affect--Enriched Distributional Word Representations", "answer": ["No"], "top_k_doc_id": [3612, 3543, 3616, 6723, 6724, 6725, 6728, 3615, 3613, 3614, 4783, 538, 1935, 6014, 4946], "orig_top_k_doc_id": [3612, 3616, 3615, 3613, 3614, 6724, 6723, 4783, 3543, 538, 6725, 6728, 1935, 6014, 4946]}]}
{"group_id": 126, "group_size": 9, "items": [{"qid": 84, "question": "Do they test their approach on a dataset without incomplete data? in Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "answer": ["No", "No"], "top_k_doc_id": [99, 96, 97, 100, 98, 1597, 3161, 4098, 5506, 7113, 7644, 1539, 7057, 5197, 4762], "orig_top_k_doc_id": [96, 100, 99, 97, 98, 1597, 3161, 5506, 7644, 7113, 1539, 7057, 5197, 4098, 4762]}, {"qid": 85, "question": "Should their approach be applied only when dealing with incomplete data? in Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "answer": ["No", "No"], "top_k_doc_id": [99, 96, 97, 100, 98, 1597, 3161, 4098, 5506, 7113, 7644, 4721, 1523, 466, 1152], "orig_top_k_doc_id": [96, 100, 99, 97, 98, 1597, 3161, 4721, 1523, 466, 7644, 5506, 1152, 7113, 4098]}, {"qid": 81, "question": "Do they report results only on English datasets? in Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "answer": ["Yes"], "top_k_doc_id": [99, 96, 97, 100, 98, 2607, 2610, 4682, 785, 3639, 2874, 5197, 2120, 2321, 3640], "orig_top_k_doc_id": [96, 99, 100, 97, 785, 3639, 2874, 2610, 4682, 98, 5197, 2120, 2321, 2607, 3640]}, {"qid": 82, "question": "How do the authors define or exemplify 'incorrect words'? in Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "answer": ["typos in spellings or ungrammatical words"], "top_k_doc_id": [99, 96, 97, 100, 98, 1597, 3161, 1347, 3004, 6101, 5197, 6158, 785, 4204, 6610], "orig_top_k_doc_id": [96, 99, 100, 97, 98, 1347, 3161, 3004, 6101, 5197, 6158, 785, 4204, 6610, 1597]}, {"qid": 86, "question": "By how much do they outperform other models in the sentiment in intent classification tasks? in Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "answer": ["In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average"], "top_k_doc_id": [99, 96, 97, 100, 98, 2607, 2610, 4682, 3571, 3572, 4833, 1048, 7160, 4812, 2058], "orig_top_k_doc_id": [96, 99, 100, 97, 98, 3571, 2607, 3572, 4833, 1048, 7160, 4682, 2610, 4812, 2058]}, {"qid": 403, "question": "What is the dataset used to train the model? in Learning with Noisy Labels for Sentence-level Sentiment Classification", "answer": [" movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source"], "top_k_doc_id": [99, 462, 472, 473, 474, 475, 5314, 390, 450, 502, 1053, 3129, 391, 2951, 86], "orig_top_k_doc_id": [472, 474, 390, 502, 473, 475, 5314, 462, 391, 99, 3129, 1053, 2951, 86, 450]}, {"qid": 404, "question": "What is the performance of the model? in Learning with Noisy Labels for Sentence-level Sentiment Classification", "answer": ["Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates\nExperiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)"], "top_k_doc_id": [99, 462, 472, 473, 474, 475, 5314, 390, 450, 502, 1053, 3129, 3130, 3152, 1052], "orig_top_k_doc_id": [474, 472, 475, 1053, 473, 390, 462, 5314, 3130, 502, 3129, 99, 3152, 1052, 450]}, {"qid": 83, "question": "How many vanilla transformers do they use after applying an embedding layer? in Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "answer": ["No"], "top_k_doc_id": [99, 96, 97, 100, 1138, 6839, 6135, 3273, 7579, 7472, 785, 651, 5881, 7553, 4682], "orig_top_k_doc_id": [96, 97, 100, 99, 1138, 6839, 6135, 3273, 7579, 7472, 785, 651, 5881, 7553, 4682]}, {"qid": 405, "question": "Is the model evaluated against a CNN baseline? in Learning with Noisy Labels for Sentence-level Sentiment Classification", "answer": ["Yes"], "top_k_doc_id": [99, 462, 472, 473, 474, 475, 5314, 100, 2951, 3307, 2950, 6505, 2143, 5388, 5333], "orig_top_k_doc_id": [474, 472, 475, 100, 473, 2951, 462, 3307, 99, 5314, 2950, 6505, 2143, 5388, 5333]}]}
{"group_id": 127, "group_size": 9, "items": [{"qid": 102, "question": "Which paired corpora did they use in the other experiment? in Unsupervised Machine Commenting with Neural Variational Topic Model", "answer": ["dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1", "Chinese dataset BIBREF0"], "top_k_doc_id": [117, 118, 119, 121, 122, 120, 5934, 6959, 188, 189, 5935, 4712, 2664, 190, 191], "orig_top_k_doc_id": [117, 122, 118, 121, 119, 120, 4712, 2664, 5934, 189, 190, 5935, 6959, 191, 188]}, {"qid": 104, "question": "Which lexicon-based models did they compare with? in Unsupervised Machine Commenting with Neural Variational Topic Model", "answer": ["TF-IDF, NVDM"], "top_k_doc_id": [117, 118, 119, 121, 122, 120, 5934, 6959, 188, 189, 5935, 3916, 1768, 1044, 6433], "orig_top_k_doc_id": [122, 121, 118, 117, 3916, 119, 120, 1768, 5934, 189, 5935, 6959, 1044, 188, 6433]}, {"qid": 106, "question": "How many articles did they have? in Unsupervised Machine Commenting with Neural Variational Topic Model", "answer": ["198,112"], "top_k_doc_id": [117, 118, 119, 121, 122, 120, 5934, 6959, 188, 189, 5282, 2074, 1768, 2075, 6436], "orig_top_k_doc_id": [118, 117, 122, 121, 119, 120, 189, 6959, 5282, 2074, 188, 5934, 1768, 2075, 6436]}, {"qid": 105, "question": "How many comments were used? in Unsupervised Machine Commenting with Neural Variational Topic Model", "answer": ["from 50K to 4.8M"], "top_k_doc_id": [117, 118, 119, 121, 122, 120, 5934, 6959, 6433, 6436, 5282, 6434, 5913, 5057, 5935], "orig_top_k_doc_id": [118, 117, 122, 121, 119, 6433, 6436, 120, 6959, 5282, 6434, 5913, 5934, 5057, 5935]}, {"qid": 107, "question": "What news comment dataset was used? in Unsupervised Machine Commenting with Neural Variational Topic Model", "answer": ["Chinese dataset BIBREF0"], "top_k_doc_id": [117, 118, 119, 121, 122, 120, 6433, 6434, 6436, 235, 3542, 5648, 3521, 6435, 7856], "orig_top_k_doc_id": [117, 118, 121, 119, 122, 6433, 6434, 6436, 120, 235, 3542, 5648, 3521, 6435, 7856]}, {"qid": 4109, "question": "What is the model trained? in Informative and Controllable Opinion Summarization", "answer": ["Condense-Abstract Framework, consisting of BiLSTM autoencoder and LSTM decoder with attention.", "BiLSTM autoencoder as the Condense model, simple LSTM decoder enhanced with attention BIBREF14 and copy mechanisms BIBREF32"], "top_k_doc_id": [117, 118, 3102, 6566, 6570, 7617, 4760, 5149, 7162, 2258, 6860, 972, 3202, 5557, 235], "orig_top_k_doc_id": [6566, 117, 3102, 6570, 2258, 7617, 118, 6860, 7162, 972, 3202, 5557, 235, 4760, 5149]}, {"qid": 4110, "question": "How large is the dataset used? in Informative and Controllable Opinion Summarization", "answer": ["3731 movies containing around 372353 reviews", "3731", "3,731 movies; for each movie we are given a large set of reviews (99.8 on average)"], "top_k_doc_id": [117, 118, 3102, 6566, 6570, 7617, 4760, 5149, 7162, 6840, 6493, 369, 6922, 6716, 3805], "orig_top_k_doc_id": [6566, 117, 3102, 118, 6840, 6570, 6493, 7617, 7162, 369, 6922, 4760, 5149, 6716, 3805]}, {"qid": 103, "question": "By how much does their system outperform the lexicon-based models? in Unsupervised Machine Commenting with Neural Variational Topic Model", "answer": ["Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029", "Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc."], "top_k_doc_id": [117, 118, 119, 121, 122, 3916, 5417, 185, 5822, 5934, 2162, 6959, 2187, 7372, 4380], "orig_top_k_doc_id": [121, 122, 117, 118, 3916, 5417, 185, 5822, 5934, 2162, 6959, 2187, 119, 7372, 4380]}, {"qid": 4108, "question": "Do they compare to previous work? in Informative and Controllable Opinion Summarization", "answer": ["Yes", "No", "Yes"], "top_k_doc_id": [117, 118, 3102, 6566, 6570, 7617, 5554, 808, 6493, 4825, 4478, 6716, 6268, 1255, 5804], "orig_top_k_doc_id": [6566, 117, 3102, 6570, 7617, 5554, 808, 118, 6493, 4825, 4478, 6716, 6268, 1255, 5804]}]}
{"group_id": 128, "group_size": 9, "items": [{"qid": 269, "question": "What further analysis is done? in Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "answer": ["we use t-SNE tool BIBREF27 to visualize the learned embedding"], "top_k_doc_id": [340, 131, 337, 338, 339, 341, 342, 3017, 130, 4274, 2917, 3634, 3628, 2868, 3629], "orig_top_k_doc_id": [338, 337, 341, 339, 342, 340, 131, 4274, 3017, 3634, 3628, 2868, 3629, 2917, 130]}, {"qid": 271, "question": "What three datasets are used to measure performance? in Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "answer": ["FB24K, DBP24K, Game30K", "Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph"], "top_k_doc_id": [340, 131, 337, 338, 339, 341, 342, 3017, 130, 4274, 2917, 4160, 5898, 3632, 4275], "orig_top_k_doc_id": [338, 337, 341, 342, 340, 339, 4274, 131, 3017, 4160, 2917, 5898, 3632, 130, 4275]}, {"qid": 273, "question": "What are recent works on knowedge graph embeddings authors mention? in Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "answer": ["entity types or concepts BIBREF13, relations paths BIBREF17,  textual descriptions BIBREF11, BIBREF12, logical rules BIBREF23, deep neural network models BIBREF24"], "top_k_doc_id": [340, 131, 337, 338, 339, 341, 342, 3017, 130, 4274, 466, 268, 3628, 215, 4979], "orig_top_k_doc_id": [338, 337, 342, 341, 339, 340, 4274, 131, 466, 268, 3628, 215, 130, 3017, 4979]}, {"qid": 268, "question": "How much better is performance of proposed method than state-of-the-art methods in experiments? in Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "answer": ["Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively."], "top_k_doc_id": [340, 131, 337, 338, 339, 341, 342, 123, 3632, 3633, 5215, 7625, 2173, 4160, 4273], "orig_top_k_doc_id": [338, 337, 341, 339, 342, 340, 131, 5215, 7625, 3633, 123, 2173, 4160, 3632, 4273]}, {"qid": 270, "question": "What seven state-of-the-art methods are used for comparison? in Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "answer": ["TransE, TransR and TransH, PTransE, and ALL-PATHS, R-GCN BIBREF24 and KR-EAR BIBREF26"], "top_k_doc_id": [340, 131, 337, 338, 339, 341, 342, 123, 3632, 3633, 5215, 7625, 3017, 4274, 6521], "orig_top_k_doc_id": [337, 338, 342, 341, 339, 340, 131, 5215, 3017, 3632, 4274, 123, 6521, 3633, 7625]}, {"qid": 272, "question": "How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner? in Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "answer": ["To capture both high-order structural information of KGs, we used an attention-based embedding propagation method."], "top_k_doc_id": [340, 131, 337, 338, 339, 341, 342, 3017, 466, 6636, 3633, 471, 3629, 3634, 6086], "orig_top_k_doc_id": [338, 337, 339, 341, 342, 340, 466, 131, 6636, 3017, 3633, 471, 3629, 3634, 6086]}, {"qid": 400, "question": "Apart from using desired properties, do they evaluate their LAN approach in some other way? in Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding", "answer": ["No"], "top_k_doc_id": [340, 466, 467, 468, 469, 470, 471, 3630, 3633, 2426, 3018, 5898, 3629, 4320, 2173], "orig_top_k_doc_id": [466, 467, 468, 470, 471, 469, 3630, 5898, 340, 3633, 2426, 3018, 3629, 4320, 2173]}, {"qid": 401, "question": "Do they evaluate existing methods in terms of desired properties? in Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding", "answer": ["Yes"], "top_k_doc_id": [340, 466, 467, 468, 469, 470, 471, 3630, 3633, 2426, 3018, 5898, 337, 339, 3628], "orig_top_k_doc_id": [466, 467, 468, 469, 3630, 471, 470, 340, 3633, 5898, 3018, 2426, 337, 339, 3628]}, {"qid": 399, "question": "Which knowledge graph completion tasks do they experiment with? in Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding", "answer": ["link prediction , triplet classification"], "top_k_doc_id": [340, 466, 467, 468, 469, 470, 471, 3630, 3633, 337, 1822, 4490, 3535, 339, 341], "orig_top_k_doc_id": [466, 467, 471, 340, 469, 470, 468, 3630, 3633, 337, 1822, 4490, 3535, 339, 341]}]}
{"group_id": 129, "group_size": 9, "items": [{"qid": 336, "question": "What are the models evaluated on? in Interactive Machine Comprehension with Information Seeking Agents", "answer": ["They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)"], "top_k_doc_id": [5119, 6876, 406, 410, 2455, 5257, 312, 558, 7037, 411, 1541, 6583, 6879, 2264, 1632], "orig_top_k_doc_id": [6876, 5257, 5119, 410, 6583, 558, 406, 312, 7037, 2455, 6879, 411, 1541, 2264, 1632]}, {"qid": 337, "question": "How do they train models in this setup? in Interactive Machine Comprehension with Information Seeking Agents", "answer": ["Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."], "top_k_doc_id": [5119, 6876, 406, 410, 2455, 5257, 312, 558, 7037, 411, 1541, 3805, 575, 6589, 7156], "orig_top_k_doc_id": [5119, 410, 6876, 406, 312, 5257, 558, 3805, 575, 411, 6589, 2455, 1541, 7156, 7037]}, {"qid": 2946, "question": "What functionality does Macaw provide? in Macaw: An Extensible Conversational Information Seeking Platform", "answer": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data"], "top_k_doc_id": [5119, 6876, 3451, 5118, 5120, 1273, 6851, 7809, 1169, 5425, 3772, 1948, 1281, 2888, 7037], "orig_top_k_doc_id": [5118, 5119, 5120, 6851, 3772, 6876, 1948, 3451, 1273, 1281, 2888, 7809, 5425, 7037, 1169]}, {"qid": 2950, "question": "What are the different modules in Macaw? in Macaw: An Extensible Conversational Information Seeking Platform", "answer": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "top_k_doc_id": [5119, 6876, 3451, 5118, 5120, 1273, 6851, 7809, 1169, 5425, 101, 6850, 7793, 4669, 98], "orig_top_k_doc_id": [5118, 5119, 5120, 6851, 7809, 3451, 6876, 101, 6850, 1273, 7793, 4669, 5425, 98, 1169]}, {"qid": 335, "question": "Do they provide decision sequences as supervision while training models? in Interactive Machine Comprehension with Information Seeking Agents", "answer": ["No"], "top_k_doc_id": [5119, 6876, 406, 410, 2455, 5257, 312, 558, 7037, 2264, 2910, 6589, 6879, 7156, 3752], "orig_top_k_doc_id": [6876, 406, 2264, 5257, 5119, 410, 2910, 6589, 7037, 6879, 312, 558, 7156, 3752, 2455]}, {"qid": 2945, "question": "Does the paper provide any case studies to illustrate how one can use Macaw for CIS research? in Macaw: An Extensible Conversational Information Seeking Platform", "answer": ["No", "No"], "top_k_doc_id": [5119, 6876, 3451, 5118, 5120, 5425, 5577, 2848, 18, 2264, 7793, 3552, 3553, 2888, 5373], "orig_top_k_doc_id": [5118, 5119, 5120, 2848, 18, 2264, 7793, 5577, 3552, 3451, 5425, 3553, 2888, 5373, 6876]}, {"qid": 2948, "question": "What interface does Macaw currently have? in Macaw: An Extensible Conversational Information Seeking Platform", "answer": ["File IO, Standard IO, Telegram", "The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps."], "top_k_doc_id": [5119, 6876, 3451, 5118, 5120, 5425, 5577, 4663, 6851, 3126, 5578, 6583, 7222, 1622, 4414], "orig_top_k_doc_id": [5119, 5118, 5120, 4663, 6851, 5425, 3126, 6876, 5577, 3451, 5578, 6583, 7222, 1622, 4414]}, {"qid": 2949, "question": "What modalities are supported by Macaw? in Macaw: An Extensible Conversational Information Seeking Platform", "answer": ["text, speech, image, click, etc"], "top_k_doc_id": [5119, 6876, 3451, 5118, 5120, 1273, 6851, 7809, 7037, 7244, 6728, 18, 6850, 2849, 5581], "orig_top_k_doc_id": [5118, 5119, 5120, 7037, 6851, 7244, 7809, 6728, 6876, 18, 3451, 6850, 1273, 2849, 5581]}, {"qid": 338, "question": "What commands does their setup provide to models seeking information? in Interactive Machine Comprehension with Information Seeking Agents", "answer": ["previous, next, Ctrl+F $<$query$>$, stop"], "top_k_doc_id": [5119, 6876, 406, 410, 2455, 5257, 411, 7156, 5118, 7157, 409, 2264, 7758, 5577, 5258], "orig_top_k_doc_id": [5119, 406, 6876, 410, 5257, 411, 7156, 5118, 7157, 2455, 409, 2264, 7758, 5577, 5258]}]}
{"group_id": 130, "group_size": 9, "items": [{"qid": 381, "question": "How many sentence transformations on average are available per unique sentence in dataset? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["27.41 transformation on average of single seed sentence is available in dataset."], "top_k_doc_id": [453, 454, 455, 1877, 2748, 2749, 3053, 3054, 885, 4431, 5659, 818, 109, 3799, 3291], "orig_top_k_doc_id": [455, 454, 453, 3054, 885, 3053, 4431, 2749, 2748, 5659, 1877, 818, 109, 3799, 3291]}, {"qid": 383, "question": "How are possible sentence transformations represented in dataset, as new sentences? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["Yes, as new sentences."], "top_k_doc_id": [453, 454, 455, 1877, 2748, 2749, 3053, 2282, 5863, 3271, 1878, 4906, 1879, 816, 485], "orig_top_k_doc_id": [455, 454, 453, 2748, 2749, 2282, 5863, 3271, 1878, 4906, 1879, 1877, 816, 485, 3053]}, {"qid": 386, "question": "Are some baseline models trained on this dataset? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["Yes"], "top_k_doc_id": [453, 454, 455, 2749, 3307, 6295, 2748, 2282, 4449, 884, 818, 4493, 4492, 291, 1234], "orig_top_k_doc_id": [455, 454, 453, 3307, 2282, 4449, 2749, 884, 818, 6295, 4493, 4492, 2748, 291, 1234]}, {"qid": 388, "question": "How do they introduce language variation? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": [" we were looking for original and uncommon sentence change suggestions"], "top_k_doc_id": [453, 454, 455, 2749, 3307, 6295, 2748, 1231, 4606, 1194, 6413, 1213, 4747, 4431, 7589], "orig_top_k_doc_id": [455, 454, 453, 2749, 2748, 1231, 4606, 1194, 6413, 3307, 1213, 4747, 4431, 6295, 7589]}, {"qid": 384, "question": "What are all 15 types of modifications ilustrated in the dataset? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["- paraphrase 1\n- paraphrase 2\n- different meaning\n- opposite meaning\n- nonsense\n- minimal change\n- generalization\n- gossip\n- formal sentence\n- non-standard sentence\n- simple sentence\n- possibility\n- ban\n- future\n- past"], "top_k_doc_id": [453, 454, 455, 1877, 2282, 6260, 2285, 3054, 2628, 7159, 3061, 2376, 7356, 7146, 1879], "orig_top_k_doc_id": [454, 455, 453, 2282, 1877, 6260, 2285, 3054, 2628, 7159, 3061, 2376, 7356, 7146, 1879]}, {"qid": 387, "question": "Do they do any analysis of of how the modifications changed the starting set of sentences? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["Yes"], "top_k_doc_id": [453, 454, 455, 2749, 3307, 6295, 6325, 5843, 517, 7356, 6326, 6260, 6864, 3195, 37], "orig_top_k_doc_id": [455, 454, 453, 6325, 3307, 5843, 517, 7356, 6326, 6295, 6260, 6864, 3195, 37, 2749]}, {"qid": 382, "question": "What annotations are available in the dataset? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)"], "top_k_doc_id": [453, 454, 455, 58, 870, 3271, 4431, 4449, 5450, 3054, 5453, 3799, 5451, 291, 3053], "orig_top_k_doc_id": [455, 454, 453, 3054, 870, 5450, 5453, 3799, 4431, 5451, 291, 3271, 58, 4449, 3053]}, {"qid": 385, "question": "Is this dataset publicly available? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["Yes"], "top_k_doc_id": [453, 454, 455, 58, 870, 3271, 4431, 4449, 5450, 56, 57, 6405, 2183, 5841, 2326], "orig_top_k_doc_id": [455, 454, 453, 58, 5450, 56, 57, 870, 6405, 4431, 3271, 2183, 4449, 5841, 2326]}, {"qid": 389, "question": "Do they use external resources to make modifications to sentences? in COSTRA 1.0: A Dataset of Complex Sentence Transformations", "answer": ["No"], "top_k_doc_id": [453, 454, 455, 3536, 3269, 1684, 4257, 2919, 3271, 5459, 5307, 224, 3195, 7619, 3659], "orig_top_k_doc_id": [454, 455, 453, 3536, 3269, 1684, 4257, 2919, 3271, 5459, 5307, 224, 3195, 7619, 3659]}]}
{"group_id": 131, "group_size": 9, "items": [{"qid": 440, "question": "What metrics are used for automatic evaluation? in Low-Level Linguistic Controls for Style Transfer and Content Preservation", "answer": ["classification accuracy, BLEU scores, model perplexities of the reconstruction"], "top_k_doc_id": [518, 1005, 1006, 514, 515, 516, 519, 6682, 7446, 7447, 5178, 6681, 6679, 5179, 6683], "orig_top_k_doc_id": [6682, 519, 518, 5178, 6681, 7447, 1006, 7446, 515, 6683, 514, 516, 1005, 6679, 5179]}, {"qid": 441, "question": "How they know what are content words? in Low-Level Linguistic Controls for Style Transfer and Content Preservation", "answer": [" words found in the control word lists are then removed, The remaining words, which represent the content"], "top_k_doc_id": [518, 1005, 1006, 514, 515, 516, 519, 6682, 7446, 7447, 5178, 6681, 6679, 5179, 5174], "orig_top_k_doc_id": [518, 519, 515, 7447, 6682, 1005, 5174, 5178, 1006, 514, 516, 7446, 6681, 6679, 5179]}, {"qid": 438, "question": "Is this style generator compared to some baseline? in Low-Level Linguistic Controls for Style Transfer and Content Preservation", "answer": ["Yes"], "top_k_doc_id": [518, 1005, 1006, 514, 515, 516, 519, 6682, 7446, 7447, 5178, 6681, 6679, 5174, 7443], "orig_top_k_doc_id": [1006, 1005, 519, 518, 6682, 516, 515, 7447, 7446, 5178, 6681, 514, 5174, 6679, 7443]}, {"qid": 803, "question": "By how much do proposed architectures autperform state-of-the-art? in Style Transfer for Texts: to Err is Human, but Error Margins Matter", "answer": ["No"], "top_k_doc_id": [518, 1005, 1006, 1007, 1008, 1473, 5844, 5845, 6679, 1474, 1555, 5854, 5853, 6674, 4561], "orig_top_k_doc_id": [1008, 1005, 1007, 1555, 1006, 5854, 1473, 1474, 5845, 6679, 5853, 518, 4561, 6674, 5844]}, {"qid": 804, "question": "What are three new proposed architectures? in Style Transfer for Texts: to Err is Human, but Error Margins Matter", "answer": ["special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information, shifted autoencoder or SAE, combination of both approaches"], "top_k_doc_id": [518, 1005, 1006, 1007, 1008, 1473, 5844, 5845, 6679, 1474, 1555, 5854, 5853, 6674, 5841], "orig_top_k_doc_id": [1008, 1005, 1007, 1473, 6679, 5854, 5844, 1006, 5845, 1555, 518, 6674, 1474, 5841, 5853]}, {"qid": 439, "question": "How they perform manual evaluation, what is criteria? in Low-Level Linguistic Controls for Style Transfer and Content Preservation", "answer": ["accuracy"], "top_k_doc_id": [518, 1005, 1006, 514, 515, 516, 519, 6682, 7446, 7447, 5178, 6681, 5562, 5844, 264], "orig_top_k_doc_id": [518, 6682, 519, 7447, 5562, 515, 1005, 5178, 7446, 514, 1006, 516, 5844, 6681, 264]}, {"qid": 802, "question": "What is state of the art method? in Style Transfer for Texts: to Err is Human, but Error Margins Matter", "answer": ["No"], "top_k_doc_id": [518, 1005, 1006, 1007, 1008, 1473, 5844, 5845, 6679, 1474, 1555, 5854, 5841, 4561, 7447], "orig_top_k_doc_id": [1008, 1005, 1007, 1555, 1006, 5844, 1474, 5854, 5845, 1473, 5841, 518, 4561, 7447, 6679]}, {"qid": 442, "question": "How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions? in Low-Level Linguistic Controls for Style Transfer and Content Preservation", "answer": ["style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style"], "top_k_doc_id": [518, 1005, 1006, 514, 515, 516, 519, 6682, 7446, 7447, 517, 1703, 1863, 5846, 5845], "orig_top_k_doc_id": [518, 519, 515, 514, 516, 517, 1005, 1006, 1703, 7447, 6682, 1863, 5846, 5845, 7446]}, {"qid": 805, "question": "How much does the standard metrics for style accuracy vary on different re-runs? in Style Transfer for Texts: to Err is Human, but Error Margins Matter", "answer": ["accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points"], "top_k_doc_id": [518, 1005, 1006, 1007, 1008, 1473, 5844, 5845, 6679, 5178, 6705, 6682, 5841, 6674, 5853], "orig_top_k_doc_id": [1005, 1008, 1007, 1006, 5845, 518, 1473, 5844, 6679, 5178, 6705, 6682, 5841, 6674, 5853]}]}
{"group_id": 132, "group_size": 9, "items": [{"qid": 474, "question": "What are some guidelines in writing input vernacular so model can generate  in Generating Classical Chinese Poems from Vernacular Chinese", "answer": [" if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score, poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs"], "top_k_doc_id": [564, 5504, 565, 6211, 6212, 6213, 6214, 566, 567, 568, 1413, 2355, 3459, 527, 2220], "orig_top_k_doc_id": [564, 565, 567, 566, 568, 6211, 6212, 6214, 6213, 5504, 1413, 2355, 3459, 527, 2220]}, {"qid": 476, "question": "What dataset is used for training? in Generating Classical Chinese Poems from Vernacular Chinese", "answer": ["We collected a corpus of poems and a corpus of vernacular literature from online resources"], "top_k_doc_id": [564, 5504, 565, 6211, 6212, 6213, 6214, 566, 567, 568, 5, 7294, 6386, 6387, 5681], "orig_top_k_doc_id": [564, 565, 567, 566, 568, 6211, 6212, 6214, 6213, 5504, 5, 7294, 6386, 6387, 5681]}, {"qid": 475, "question": "How much is proposed model better in perplexity and BLEU score than typical UMT models? in Generating Classical Chinese Poems from Vernacular Chinese", "answer": ["Perplexity of the best model is 65.58 compared to best baseline 105.79.\nBleu of the best model is 6.57 compared to best baseline 5.50."], "top_k_doc_id": [564, 5504, 565, 6211, 6212, 6213, 6214, 566, 567, 568, 650, 2221, 3024, 3193, 4389], "orig_top_k_doc_id": [564, 568, 565, 567, 566, 6211, 6212, 6214, 5504, 650, 2221, 3024, 6213, 3193, 4389]}, {"qid": 3841, "question": "What is the source of the training/testing data? in Generating Major Types of Chinese Classical Poetry in a Uniformed Framework", "answer": ["CCPC1.0", "Two major forms(Jueju and Lvshi) of SHI and 121 major forms of CI from Chinese Classical Poerty Corpus (CCPC1.0)", "Chinese poem corpus with 250,000 Jueju and Lvshi, 20,000 CIs, 700,000 pairs of couplets"], "top_k_doc_id": [564, 5504, 565, 6211, 6212, 6213, 6214, 4, 1534, 6411, 1372, 1263, 788, 129, 3209], "orig_top_k_doc_id": [6211, 6212, 564, 6214, 565, 5504, 4, 6213, 1534, 6411, 1372, 1263, 788, 129, 3209]}, {"qid": 3842, "question": "What are the types of chinese poetry that are generated? in Generating Major Types of Chinese Classical Poetry in a Uniformed Framework", "answer": ["SHI , CI ", "two major forms of SHI, Jueju, and Lvshi,, 121 major forms (Cipai) of CI ", "two primary categories, SHI and CI, SHI and CI can be further divided into many different types"], "top_k_doc_id": [564, 5504, 565, 6211, 6212, 6213, 6214, 4, 1534, 5, 738, 567, 6410, 2916, 7693], "orig_top_k_doc_id": [6211, 6212, 564, 6214, 6213, 5504, 4, 1534, 5, 738, 565, 567, 6410, 2916, 7693]}, {"qid": 3282, "question": "Do they report results only on English data? in Creative GANs for generating poems, lyrics, and metaphors", "answer": ["Yes", "Yes"], "top_k_doc_id": [564, 5504, 566, 567, 568, 1663, 3667, 5681, 6417, 5, 6, 7537, 1661, 8, 2125], "orig_top_k_doc_id": [5504, 5681, 567, 6417, 568, 5, 564, 6, 566, 3667, 7537, 1661, 8, 1663, 2125]}, {"qid": 3284, "question": "Which datasets are used? in Creative GANs for generating poems, lyrics, and metaphors", "answer": ["A corpus of 740 classical and contemporary English poems,  a corpus of 14950 metaphor sentences retrieved from a metaphor database website , a corpus of 1500 song lyrics ranging across genres, Gutenberg dataset ", "(1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website, (3) a corpus of 1500 song lyrics ranging across genres, Gutenberg dataset BIBREF24"], "top_k_doc_id": [564, 5504, 566, 567, 568, 1663, 3667, 5681, 6417, 287, 281, 280, 6213, 279, 286], "orig_top_k_doc_id": [5504, 5681, 567, 6417, 568, 564, 287, 3667, 1663, 281, 566, 280, 6213, 279, 286]}, {"qid": 3503, "question": "what NMT models did they compare with? in Ancient-Modern Chinese Translation with a Large Training Dataset", "answer": ["RNN-based NMT model, Transformer-NMT"], "top_k_doc_id": [564, 566, 567, 5788, 5789, 5790, 5791, 5792, 7658, 6943, 4390, 4391, 5241, 4389, 7661], "orig_top_k_doc_id": [5792, 5788, 5789, 5791, 5790, 567, 566, 564, 6943, 7658, 4390, 4391, 5241, 4389, 7661]}, {"qid": 3504, "question": "Where does the ancient Chinese dataset come from? in Ancient-Modern Chinese Translation with a Large Training Dataset", "answer": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "top_k_doc_id": [564, 566, 567, 5788, 5789, 5790, 5791, 5792, 7658, 2195, 6881, 3125, 2839, 4752, 1777], "orig_top_k_doc_id": [5789, 5788, 5792, 5790, 5791, 567, 2195, 564, 6881, 566, 3125, 2839, 4752, 1777, 7658]}]}
{"group_id": 133, "group_size": 9, "items": [{"qid": 512, "question": "How big are the datasets? in Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "answer": ["In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents"], "top_k_doc_id": [633, 1040, 2162, 5714, 80, 629, 630, 632, 6424, 2971, 3748, 1926, 5621, 3746, 500], "orig_top_k_doc_id": [629, 632, 633, 630, 5714, 2162, 1040, 3746, 6424, 3748, 1926, 80, 5621, 500, 2971]}, {"qid": 513, "question": "What languages do they experiment on? in Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "answer": ["English, German, Spanish, Italian, Japanese and Portuguese,  English, Arabic and Chinese"], "top_k_doc_id": [633, 1040, 2162, 5714, 80, 629, 630, 632, 6424, 2971, 3748, 1926, 5621, 2330, 1053], "orig_top_k_doc_id": [629, 632, 630, 633, 5714, 2162, 1040, 6424, 3748, 2330, 80, 1053, 1926, 5621, 2971]}, {"qid": 514, "question": "What datasets are used? in Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "answer": ["in-house dataset, ACE05 dataset "], "top_k_doc_id": [633, 1040, 2162, 5714, 80, 629, 630, 632, 6424, 2971, 3748, 3746, 500, 1039, 5715], "orig_top_k_doc_id": [629, 632, 633, 630, 5714, 3748, 2162, 6424, 3746, 1040, 2971, 500, 1039, 80, 5715]}, {"qid": 1540, "question": "How many words are translated between the cross-lingual translation pairs? in Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "answer": ["No"], "top_k_doc_id": [633, 1040, 2162, 5714, 2154, 2155, 2156, 629, 5713, 6060, 4568, 1039, 5715, 6424, 5699], "orig_top_k_doc_id": [2154, 2155, 2156, 1040, 6060, 2162, 5714, 633, 4568, 1039, 5715, 6424, 5713, 5699, 629]}, {"qid": 1541, "question": "What are the six target languages? in Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "answer": ["Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI)."], "top_k_doc_id": [633, 1040, 2162, 5714, 2154, 2155, 2156, 629, 5713, 5622, 5621, 81, 6871, 7410, 80], "orig_top_k_doc_id": [2154, 2156, 2155, 633, 2162, 5622, 5621, 5714, 1040, 81, 6871, 629, 7410, 5713, 80]}, {"qid": 511, "question": "Do they train their own RE model? in Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping", "answer": ["Yes"], "top_k_doc_id": [633, 1040, 2162, 5714, 80, 629, 630, 632, 6424, 631, 2806, 3746, 1042, 3777, 1039], "orig_top_k_doc_id": [632, 629, 633, 630, 6424, 631, 5714, 2806, 2162, 3746, 1040, 1042, 3777, 80, 1039]}, {"qid": 1538, "question": "What baseline is used for the verb classification experiments? in Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "answer": ["No"], "top_k_doc_id": [633, 1040, 2162, 5714, 2154, 2155, 2156, 6871, 1048, 5711, 5710, 1430, 1766, 2812, 3762], "orig_top_k_doc_id": [2154, 2156, 2155, 633, 2162, 6871, 1048, 5711, 1040, 5710, 5714, 1430, 1766, 2812, 3762]}, {"qid": 1391, "question": "Are any machine translation sysems tried with these embeddings, what is the performance? in Machine Translation with Cross-lingual Word Embeddings", "answer": ["No"], "top_k_doc_id": [633, 1040, 629, 1041, 1926, 3617, 3746, 4712, 6424, 7828, 1053, 6060, 7825, 3748, 7827], "orig_top_k_doc_id": [7828, 1040, 1926, 3617, 629, 633, 3746, 1053, 1041, 6060, 7825, 3748, 7827, 6424, 4712]}, {"qid": 1392, "question": "Are any experiments performed to try this approach to word embeddings? in Machine Translation with Cross-lingual Word Embeddings", "answer": ["Yes"], "top_k_doc_id": [633, 1040, 629, 1041, 1926, 3617, 3746, 4712, 6424, 7828, 247, 7409, 5869, 1048, 786], "orig_top_k_doc_id": [629, 1040, 3746, 633, 247, 1926, 7409, 3617, 7828, 1041, 4712, 5869, 6424, 1048, 786]}]}
{"group_id": 134, "group_size": 9, "items": [{"qid": 613, "question": "What type of errors do the classifiers use? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["correct class can be directly inferred from the text content easily, even without background knowledge, correct class can be inferred from the text content, given that event-specific knowledge is provided, orrect class can be inferred from the text content if the text is interpreted correctly"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 4114, 5728, 7822, 5274, 6013, 7684, 7350, 1398], "orig_top_k_doc_id": [762, 763, 765, 766, 5728, 4115, 7822, 4533, 764, 4114, 7684, 5274, 7350, 1398, 6013]}, {"qid": 614, "question": "What neural classifiers are used? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": [" convolutional neural network (CNN) BIBREF29"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 4114, 5728, 7822, 5274, 6013, 7127, 5802, 5729], "orig_top_k_doc_id": [762, 763, 765, 766, 4115, 5728, 764, 7822, 4533, 4114, 7127, 5802, 5729, 5274, 6013]}, {"qid": 617, "question": "What dataset is used for this study? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["MH17 Twitter dataset"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 4114, 5728, 7822, 7127, 242, 7684, 1398, 4891], "orig_top_k_doc_id": [762, 763, 765, 766, 4114, 4533, 7822, 4115, 5728, 764, 7127, 242, 7684, 1398, 4891]}, {"qid": 616, "question": "What languages are included in the dataset? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["English"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 4114, 5728, 7822, 5274, 273, 7684, 4846, 1891], "orig_top_k_doc_id": [762, 763, 765, 766, 4533, 7822, 4115, 4114, 5728, 5274, 764, 273, 7684, 4846, 1891]}, {"qid": 618, "question": "What proxies for data annotation were used in previous datasets? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet, Natural Language Processing (NLP) models can be used to automatically label text content"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 4114, 5728, 7822, 5546, 5274, 5699, 7350, 7030], "orig_top_k_doc_id": [762, 763, 765, 766, 4114, 4533, 7822, 4115, 5546, 5728, 764, 5274, 5699, 7350, 7030]}, {"qid": 612, "question": "What recommendations are made to improve the performance in future? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["applying reasoning BIBREF36 or irony detection methods BIBREF37"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 4114, 5728, 7822, 7350, 1899, 3075, 4891, 6120], "orig_top_k_doc_id": [762, 763, 765, 766, 7350, 4533, 1899, 7822, 764, 3075, 4891, 4115, 4114, 6120, 5728]}, {"qid": 615, "question": "What is the hashtags does the hashtag-based baseline use? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["No"], "top_k_doc_id": [762, 763, 765, 766, 4115, 764, 4533, 5274, 5252, 5255, 2588, 2587, 5472, 2590, 5254], "orig_top_k_doc_id": [763, 762, 765, 766, 764, 5274, 5252, 5255, 2588, 2587, 5472, 4115, 2590, 5254, 4533]}, {"qid": 611, "question": "How can the classifier facilitate the annotation task for human annotators? in Mapping (Dis-)Information Flow about the MH17 Plane Crash", "answer": ["quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets"], "top_k_doc_id": [762, 763, 765, 766, 4115, 4114, 2391, 7822, 5978, 7746, 5197, 2400, 2390, 2399, 6121], "orig_top_k_doc_id": [762, 763, 765, 766, 4114, 2391, 7822, 5978, 7746, 5197, 2400, 2390, 2399, 6121, 4115]}, {"qid": 2688, "question": "What evaluation metric do they use? in Clustering Comparable Corpora of Russian and Ukrainian Academic Texts: Word Embeddings and Semantic Fingerprints", "answer": ["Accuracy", "ratio of correct `translations'"], "top_k_doc_id": [762, 763, 765, 4712, 4713, 4714, 4715, 2875, 73, 4355, 764, 5341, 7510, 982, 421], "orig_top_k_doc_id": [4712, 4713, 4714, 4715, 2875, 765, 73, 4355, 764, 5341, 7510, 982, 421, 763, 762]}]}
{"group_id": 135, "group_size": 9, "items": [{"qid": 624, "question": "What kind of information do the HMMs learn that the LSTMs don't? in Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models", "answer": ["The HMM can identify punctuation or pick up on vowels."], "top_k_doc_id": [1790, 75, 768, 2783, 3468, 7363, 3467, 4487, 5481, 7625, 7626, 373, 6888, 3980, 3207], "orig_top_k_doc_id": [3467, 3468, 1790, 7363, 768, 3980, 6888, 2783, 75, 7625, 373, 7626, 4487, 3207, 5481]}, {"qid": 626, "question": "How large is the gap in performance between the HMMs and the LSTMs? in Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models", "answer": ["With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower."], "top_k_doc_id": [1790, 75, 768, 2783, 3468, 7363, 3467, 4487, 5481, 7625, 7626, 373, 6888, 1618, 2116], "orig_top_k_doc_id": [1790, 3468, 7363, 75, 2783, 3467, 768, 373, 4487, 7625, 1618, 5481, 6888, 2116, 7626]}, {"qid": 625, "question": "Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information? in Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models", "answer": ["decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters"], "top_k_doc_id": [1790, 75, 768, 2783, 3468, 7363, 3467, 4487, 5481, 7625, 7626, 4880, 4296, 3207, 5091], "orig_top_k_doc_id": [3468, 1790, 75, 768, 2783, 4880, 7625, 7363, 7626, 4296, 3467, 4487, 5481, 3207, 5091]}, {"qid": 4374, "question": "By how much do they outperform baselines? in Learning Scripts as Hidden Markov Models", "answer": ["On r=2 SEM-HMM Approx. is 2.2% better, on r=5 SEM-HMM is 3.9% better and on r=10 SEM-HMM is 3.9% better than the best baseline", "On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests."], "top_k_doc_id": [1790, 2948, 4354, 6719, 6888, 2907, 6720, 6893, 6721, 1579, 7046, 2013, 4296, 75, 4297], "orig_top_k_doc_id": [6888, 6720, 6721, 1790, 1579, 7046, 2013, 2948, 4296, 6719, 75, 6893, 2907, 4297, 4354]}, {"qid": 4375, "question": "Which baselines do they use? in Learning Scripts as Hidden Markov Models", "answer": ["The \"frequency\" baseline, the \"conditional\" baseline, the \"BMM\" baseline and the \"BMM+EM\" baseline", "\u201cFrequency\u201d baseline, \u201cConditional\u201d baseline, BMM, BMM + EM"], "top_k_doc_id": [1790, 2948, 4354, 6719, 6888, 2907, 6720, 6893, 4674, 4676, 5318, 5430, 850, 1784, 7152], "orig_top_k_doc_id": [6888, 6893, 2948, 1790, 6720, 4354, 4674, 4676, 5318, 6719, 5430, 850, 1784, 7152, 2907]}, {"qid": 623, "question": "What kind of features are used by the HMM models, and how interpretable are those? in Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models", "answer": ["A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. \nThe interpretability of the model is shown in Figure 2. "], "top_k_doc_id": [1790, 75, 768, 2783, 3468, 7363, 4878, 3208, 3207, 5430, 2484, 3530, 5305, 5429, 4296], "orig_top_k_doc_id": [768, 4878, 3468, 3208, 3207, 5430, 2484, 3530, 2783, 1790, 7363, 5305, 75, 5429, 4296]}, {"qid": 4376, "question": "Which datasets do they evaluate on? in Learning Scripts as Hidden Markov Models", "answer": ["The Open Minds Indoor Common Sense (OMICS) corpus ", "Open Minds Indoor Common Sense (OMICS) corpus"], "top_k_doc_id": [1790, 2948, 4354, 6719, 6888, 2906, 1784, 3130, 1579, 4674, 822, 1630, 5974, 5430, 4880], "orig_top_k_doc_id": [6888, 2906, 1790, 1784, 3130, 6719, 1579, 4674, 2948, 822, 4354, 1630, 5974, 5430, 4880]}, {"qid": 1309, "question": "Which language has the lowest error rate reduction? in Fast Multi-language LSTM-based Online Handwriting Recognition", "answer": ["thai"], "top_k_doc_id": [1790, 1791, 1792, 1795, 1796, 1797, 3640, 1989, 26, 5478, 5477, 7645, 6782, 6968, 7539], "orig_top_k_doc_id": [1791, 1790, 1795, 3640, 1792, 1989, 26, 5478, 5477, 1797, 1796, 7645, 6782, 6968, 7539]}, {"qid": 1310, "question": "What datasets did they use? in Fast Multi-language LSTM-based Online Handwriting Recognition", "answer": ["IBM-UB-1 dataset BIBREF25, IAM-OnDB dataset BIBREF42, The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45, ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50"], "top_k_doc_id": [1790, 1791, 1792, 1795, 1796, 1797, 1794, 2256, 6405, 5102, 2119, 2970, 499, 6144, 3439], "orig_top_k_doc_id": [1790, 1791, 1795, 1797, 1796, 1794, 1792, 2256, 6405, 5102, 2119, 2970, 499, 6144, 3439]}]}
{"group_id": 136, "group_size": 9, "items": [{"qid": 734, "question": "How big is dataset used to train Word2Vec for the Italian Language? in An Analysis of Word2Vec for the Italian Language", "answer": ["$421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences"], "top_k_doc_id": [5946, 5947, 6853, 1050, 1049, 3267, 3266, 3760, 918, 3761, 5734, 1320, 7509, 6053, 4512], "orig_top_k_doc_id": [918, 1049, 5946, 5947, 3266, 5734, 1050, 6853, 3761, 1320, 3760, 7509, 6053, 3267, 4512]}, {"qid": 737, "question": "What dataset is used for training Word2Vec in Italian language? in An Analysis of Word2Vec for the Italian Language", "answer": ["extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)"], "top_k_doc_id": [5946, 5947, 6853, 1050, 1049, 3267, 3266, 3760, 918, 3761, 3893, 3352, 6184, 2282, 2329], "orig_top_k_doc_id": [918, 5946, 1049, 5947, 3266, 6853, 3267, 3761, 1050, 3893, 3352, 6184, 2282, 3760, 2329]}, {"qid": 736, "question": "Are the semantic analysis findings for Italian language similar to English language version? in An Analysis of Word2Vec for the Italian Language", "answer": ["No"], "top_k_doc_id": [5946, 5947, 6853, 1050, 1049, 3267, 3266, 3760, 3762, 7509, 2282, 5702, 3011, 6203, 6204], "orig_top_k_doc_id": [1049, 6853, 3762, 3266, 7509, 2282, 5702, 5947, 5946, 3011, 3760, 3267, 6203, 6204, 1050]}, {"qid": 733, "question": "Are the word embeddings evaluated? in An Analysis of Word2Vec for the Italian Language", "answer": ["Yes"], "top_k_doc_id": [5946, 5947, 6853, 1050, 1049, 3267, 4049, 5417, 2776, 3749, 2775, 52, 155, 3748, 5527], "orig_top_k_doc_id": [5947, 4049, 1049, 5946, 5417, 2776, 6853, 3749, 1050, 2775, 52, 3267, 155, 3748, 5527]}, {"qid": 732, "question": "Are the word embeddings tested on a NLP task? in An Analysis of Word2Vec for the Italian Language", "answer": ["Yes"], "top_k_doc_id": [5946, 5947, 6853, 1050, 2282, 3749, 7509, 1687, 3762, 6053, 6753, 155, 2284, 5977, 476], "orig_top_k_doc_id": [5946, 2282, 6853, 3749, 1050, 7509, 1687, 5947, 3762, 6053, 6753, 155, 2284, 5977, 476]}, {"qid": 3617, "question": "What are the contributions of this paper? in Italian Event Detection Goes Deep Learning", "answer": ["adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach, investigation on the quality of existing Italian word embeddings for this task, a comparison against a state-of-the-art discrete classifier", "(1) Using seq2seq for event detection and classification in Italian (2) Investigating quality of Italian word embeddings for this task (3) Comparison to state-of-the-art discrete classifier", "the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach, an investigation on the quality of existing Italian word embeddings for this task, a comparison against a state-of-the-art discrete classifier, pre-trained models and scripts running the system", "Adapting a seq2seq neural system to event detection and classification for Italian, investigating the quality of existing embeddings for the task, and comparing against a state-of-the-art discrete classifier."], "top_k_doc_id": [5946, 5947, 5112, 5676, 5948, 7690, 2329, 3287, 4131, 5405, 2814, 7256, 4130, 1256, 27], "orig_top_k_doc_id": [5946, 5112, 5676, 7690, 5948, 4131, 5947, 3287, 5405, 2814, 7256, 4130, 1256, 27, 2329]}, {"qid": 3619, "question": "Can the model be extended to other languages? in Italian Event Detection Goes Deep Learning", "answer": ["No", "Yes", "No", "No"], "top_k_doc_id": [5946, 5947, 5112, 5676, 5948, 7690, 2329, 3287, 3437, 6176, 5976, 2282, 7515, 1320, 7259], "orig_top_k_doc_id": [5946, 5676, 5947, 5948, 7690, 2329, 3437, 6176, 5976, 2282, 7515, 5112, 1320, 7259, 3287]}, {"qid": 735, "question": "How does different parameter settings impact the performance and semantic capacity of resulting model? in An Analysis of Word2Vec for the Italian Language", "answer": ["number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others"], "top_k_doc_id": [5946, 5947, 6853, 7424, 918, 1049, 3619, 3011, 6852, 155, 5710, 476, 3893, 661, 1450], "orig_top_k_doc_id": [5947, 6853, 7424, 918, 1049, 5946, 3619, 3011, 6852, 155, 5710, 476, 3893, 661, 1450]}, {"qid": 3618, "question": "What are the baselines this paper uses? in Italian Event Detection Goes Deep Learning", "answer": [" cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features", "FBK-HLT - a cascade of two SVM classifiers (one for detection and one for classification)", "FBK-HLT BIBREF23", "No"], "top_k_doc_id": [5946, 5947, 5112, 5676, 5948, 7690, 1256, 3927, 2404, 6894, 6176, 1026, 4359, 4353, 5572], "orig_top_k_doc_id": [5946, 5676, 5948, 1256, 5112, 5947, 7690, 3927, 2404, 6894, 6176, 1026, 4359, 4353, 5572]}]}
{"group_id": 137, "group_size": 9, "items": [{"qid": 890, "question": "How big are improvements of MMM over state of the art? in MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension", "answer": ["test accuracy of 88.9%, which exceeds the previous best by 16.9%"], "top_k_doc_id": [1512, 7727, 1145, 1141, 1143, 1146, 2839, 2840, 3416, 7589, 7359, 6932, 4277, 3633, 4189], "orig_top_k_doc_id": [1145, 1146, 1141, 1143, 2839, 2840, 1512, 6932, 7359, 3416, 7727, 7589, 4277, 3633, 4189]}, {"qid": 892, "question": "What are state of the art methods MMM is compared to? in MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension", "answer": ["FTLM++, BERT-large, XLNet"], "top_k_doc_id": [1512, 7727, 1145, 1141, 1143, 1146, 2839, 2840, 3416, 7589, 7359, 2234, 7728, 2220, 7590], "orig_top_k_doc_id": [1145, 1146, 1141, 1143, 2839, 3416, 7727, 2840, 1512, 2234, 7728, 7359, 2220, 7590, 7589]}, {"qid": 893, "question": "What four representative datasets are used for bechmark? in MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension", "answer": ["DREAM, MCTest, TOEFL, and SemEval-2018 Task 11"], "top_k_doc_id": [1512, 7727, 1145, 1141, 1143, 1146, 2839, 2840, 3416, 7589, 7590, 4189, 2759, 4188, 2661], "orig_top_k_doc_id": [1141, 1145, 1146, 1143, 7590, 1512, 3416, 2839, 4189, 7727, 7589, 2759, 2840, 4188, 2661]}, {"qid": 891, "question": "What out of domain datasets authors used for coarse-tuning stage? in MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension", "answer": ["MultiNLI BIBREF15 and SNLI BIBREF16 "], "top_k_doc_id": [1512, 7727, 1145, 1141, 1143, 1146, 2839, 2840, 1142, 1144, 3840, 3839, 2264, 2759, 4277], "orig_top_k_doc_id": [1145, 1141, 1146, 1143, 1142, 1144, 3840, 3839, 2264, 7727, 2839, 2759, 2840, 1512, 4277]}, {"qid": 4728, "question": "Do they evaluate their model on datasets other than RACE? in Dual Co-Matching Network for Multi-choice Reading Comprehension", "answer": ["Yes, they also evaluate on the ROCStories\n(Spring 2016) dataset which collects 50k five sentence commonsense stories. ", "No"], "top_k_doc_id": [1512, 7727, 1145, 1357, 2096, 4256, 7359, 7360, 7589, 7590, 7728, 2836, 4627, 7632, 3972], "orig_top_k_doc_id": [7359, 7590, 7728, 7360, 1512, 4256, 7589, 1145, 7727, 2836, 2096, 4627, 1357, 7632, 3972]}, {"qid": 4729, "question": "What is their model's performance on RACE? in Dual Co-Matching Network for Multi-choice Reading Comprehension", "answer": ["Model's performance ranges from 67.0% to 82.8%.", "67% using BERT_base, 74.1% using BERT_large, 75.8% using BERT_large, Passage, and Answer, and 82.8% using XLNET_large with Passage and Answer features"], "top_k_doc_id": [1512, 7727, 1145, 1357, 2096, 4256, 7359, 7360, 7589, 7590, 7728, 4626, 2837, 5966, 1141], "orig_top_k_doc_id": [7359, 7360, 7727, 7728, 7590, 1145, 4256, 1512, 7589, 4626, 1357, 2837, 2096, 5966, 1141]}, {"qid": 1139, "question": "What baseline models do they compare against? in Multi-Perspective Fusion Network for Commonsense Reading Comprehension", "answer": ["SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)"], "top_k_doc_id": [1512, 7727, 1822, 3825, 7728, 1516, 1513, 3972, 5970, 4277, 3827, 2004, 2220, 4278, 5473], "orig_top_k_doc_id": [1512, 1516, 1822, 1513, 7728, 3972, 5970, 4277, 3827, 7727, 2004, 2220, 4278, 5473, 3825]}, {"qid": 4966, "question": "How much improvement is given on RACE by their introduced approach? in Dynamic Fusion Networks for Machine Reading Comprehension", "answer": ["7.3% on RACE-M and 1.5% on RACE-H", "1.6%"], "top_k_doc_id": [1512, 7727, 1822, 3825, 7728, 7729, 7590, 7732, 7733, 1961, 1145, 164, 7138, 1374, 3416], "orig_top_k_doc_id": [7728, 7727, 1512, 1822, 7729, 7590, 7732, 7733, 1961, 1145, 3825, 164, 7138, 1374, 3416]}, {"qid": 1218, "question": "By how much, the proposed method improves BiDAF and DCN on SQuAD dataset? in Pay More Attention - Neural Architectures for Question-Answering", "answer": ["In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively."], "top_k_doc_id": [1512, 7727, 1651, 1652, 1357, 510, 5270, 7728, 4256, 1547, 1641, 4637, 259, 1653, 2519], "orig_top_k_doc_id": [1651, 1652, 1357, 510, 7727, 5270, 7728, 4256, 1547, 1512, 1641, 4637, 259, 1653, 2519]}]}
{"group_id": 138, "group_size": 9, "items": [{"qid": 1028, "question": "How are the coreference chain translations evaluated? in Analysing Coreference in Transformer Outputs", "answer": ["No"], "top_k_doc_id": [1348, 1347, 1352, 1349, 6042, 377, 1350, 6043, 1819, 3481, 6045, 3480, 1351, 5388, 962], "orig_top_k_doc_id": [1347, 1348, 1352, 1349, 1350, 6042, 1819, 6045, 3480, 1351, 3481, 6043, 5388, 377, 962]}, {"qid": 1029, "question": "How are the (possibly incorrect) coreference chains in the MT outputs annotated? in Analysing Coreference in Transformer Outputs", "answer": ["allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause), The mentions referring to the same discourse item are linked between each other., chain members are annotated for their correctness"], "top_k_doc_id": [1348, 1347, 1352, 1349, 6042, 377, 1350, 6043, 1819, 3481, 6045, 3482, 2425, 175, 1248], "orig_top_k_doc_id": [1347, 1349, 1348, 1350, 1352, 3481, 3482, 6042, 377, 6043, 2425, 6045, 175, 1819, 1248]}, {"qid": 1030, "question": "Which three neural machine translation systems are analyzed? in Analysing Coreference in Transformer Outputs", "answer": ["first two systems are transformer models trained on different amounts of data, The third system includes a modification to consider the information of full coreference chains"], "top_k_doc_id": [1348, 1347, 1352, 1349, 6042, 377, 1350, 6043, 3243, 493, 7191, 7669, 1124, 5792, 1248], "orig_top_k_doc_id": [1347, 1348, 1352, 1350, 1349, 6042, 6043, 3243, 377, 493, 7191, 7669, 1124, 5792, 1248]}, {"qid": 1026, "question": "What translationese effects are seen in the analysis? in Analysing Coreference in Transformer Outputs", "answer": ["potentially indicating a shining through effect, explicitation effect"], "top_k_doc_id": [1348, 1347, 1352, 1256, 1350, 3243, 7192, 1248, 1249, 2494, 980, 5869, 2051, 2491, 7191], "orig_top_k_doc_id": [1348, 1248, 1249, 1347, 3243, 7192, 1350, 2494, 1256, 1352, 980, 5869, 2051, 2491, 7191]}, {"qid": 1027, "question": "What languages are seen in the news and TED datasets? in Analysing Coreference in Transformer Outputs", "answer": ["English, German"], "top_k_doc_id": [1348, 1347, 1352, 1256, 1350, 3243, 7192, 1349, 2083, 1351, 736, 7850, 1410, 4695, 6666], "orig_top_k_doc_id": [1350, 1352, 1349, 1348, 2083, 1347, 1351, 736, 7850, 1410, 7192, 4695, 3243, 6666, 1256]}, {"qid": 1031, "question": "Which coreference phenomena are analyzed? in Analysing Coreference in Transformer Outputs", "answer": ["shining through, explicitation"], "top_k_doc_id": [1348, 1347, 1352, 1349, 6042, 7615, 3482, 1819, 717, 723, 6044, 3480, 7245, 4631, 3243], "orig_top_k_doc_id": [1348, 1347, 7615, 1349, 3482, 6042, 1819, 1352, 717, 723, 6044, 3480, 7245, 4631, 3243]}, {"qid": 992, "question": "Did the authors evaluate their system output for coherence? in Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches", "answer": ["Yes"], "top_k_doc_id": [1348, 1159, 1303, 1304, 1306, 1307, 5933, 5934, 7299, 7372, 851, 2191, 881, 1347, 880], "orig_top_k_doc_id": [1303, 1306, 1348, 5934, 5933, 1304, 1307, 7372, 851, 2191, 881, 1159, 1347, 7299, 880]}, {"qid": 993, "question": "What evaluations did the authors use on their system? in Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches", "answer": ["BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence."], "top_k_doc_id": [1348, 1159, 1303, 1304, 1306, 1307, 5933, 5934, 7299, 7372, 1244, 6261, 7687, 1246, 7784], "orig_top_k_doc_id": [1303, 1306, 1348, 7372, 1304, 5934, 1307, 5933, 1244, 1159, 6261, 7687, 1246, 7784, 7299]}, {"qid": 3648, "question": "What are the other obstacles to automatic translations which are not mentioned in the abstract? in Semantic Web for Machine Translation: Challenges and Directions", "answer": ["Excessive focus on English and European languages, limitations of SMT approaches for translating across domains, no-standard speech texts from users, morphologically rich languages, parallel data for training differs widely from real user speech", "reordering errors", "No", "reordering errors"], "top_k_doc_id": [1348, 1347, 5982, 4184, 1244, 3181, 4594, 5983, 6190, 4186, 7024, 1053, 2074, 4593, 6788], "orig_top_k_doc_id": [1347, 1348, 5982, 4184, 1244, 3181, 4594, 5983, 6190, 4186, 7024, 1053, 2074, 4593, 6788]}]}
{"group_id": 139, "group_size": 9, "items": [{"qid": 1364, "question": "Do they report results only on English data? in Is preprocessing of text really worth your time for online comment classification?", "answer": ["No"], "top_k_doc_id": [7775, 1876, 1877, 1009, 7292, 7774, 3603, 6971, 3589, 5094, 3544, 5573, 6413, 2631, 5095], "orig_top_k_doc_id": [1876, 7292, 7774, 3589, 1877, 6971, 5094, 3544, 5573, 6413, 1009, 7775, 2631, 3603, 5095]}, {"qid": 1366, "question": "What preprocessing techniques are used in the experiments? in Is preprocessing of text really worth your time for online comment classification?", "answer": ["See Figure FIGREF3"], "top_k_doc_id": [7775, 1876, 1877, 1009, 7292, 7774, 3603, 6971, 449, 4116, 5236, 2256, 502, 115, 5235], "orig_top_k_doc_id": [1876, 449, 4116, 6971, 7292, 5236, 2256, 1009, 502, 1877, 7775, 115, 3603, 7774, 5235]}, {"qid": 4995, "question": "Where do the supportive tweets about women come from? Are they automatically or manually generated? in Women, politics and Twitter: Using machine learning to change the discourse", "answer": ["Manualy (volunteers composed them)", "Volunteers submitted many of these positivitweets through an online form"], "top_k_doc_id": [7775, 4739, 7772, 7773, 7774, 5976, 5977, 6176, 3988, 6155, 5525, 4742, 934, 5949, 1957], "orig_top_k_doc_id": [7772, 7773, 7774, 7775, 5977, 6176, 6155, 5976, 4739, 5525, 4742, 934, 5949, 1957, 3988]}, {"qid": 4996, "question": "How are the hateful tweets aimed at women detected/classified? in Women, politics and Twitter: Using machine learning to change the discourse", "answer": ["The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12", "classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12"], "top_k_doc_id": [7775, 4739, 7772, 7773, 7774, 5976, 5977, 6176, 3988, 1727, 6133, 5979, 7257, 5085, 6177], "orig_top_k_doc_id": [7772, 7773, 7774, 5976, 5977, 6176, 1727, 6133, 3988, 5979, 7775, 7257, 4739, 5085, 6177]}, {"qid": 1367, "question": "What state of the art models are used in the experiments? in Is preprocessing of text really worth your time for online comment classification?", "answer": ["2) Na\u00efve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost), 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)"], "top_k_doc_id": [7775, 1876, 1877, 1009, 7292, 7774, 6413, 3545, 1008, 115, 4441, 2256, 5649, 5646, 5573], "orig_top_k_doc_id": [6413, 1877, 1009, 1876, 7774, 7775, 3545, 1008, 115, 7292, 4441, 2256, 5649, 5646, 5573]}, {"qid": 4992, "question": "Do the authors report only on English data? in Women, politics and Twitter: Using machine learning to change the discourse", "answer": ["Yes", "Yes"], "top_k_doc_id": [7775, 4739, 7772, 7773, 7774, 5976, 5977, 6176, 6177, 6402, 5764, 2038, 5812, 5783, 6101], "orig_top_k_doc_id": [7772, 7774, 7773, 7775, 4739, 6177, 6402, 5764, 5977, 5976, 6176, 2038, 5812, 5783, 6101]}, {"qid": 4993, "question": "How is the impact of ParityBOT analyzed? in Women, politics and Twitter: Using machine learning to change the discourse", "answer": [" interviewing individuals involved in government ($n=5$)", "by interviewing individuals involved in government"], "top_k_doc_id": [7775, 4739, 7772, 7773, 7774, 5641, 6804, 1725, 330, 234, 4740, 6177, 5525, 7530, 2861], "orig_top_k_doc_id": [7774, 7773, 7772, 7775, 4739, 6804, 1725, 330, 234, 4740, 6177, 5525, 7530, 5641, 2861]}, {"qid": 4994, "question": "What public online harassment datasets was the system validated on? in Women, politics and Twitter: Using machine learning to change the discourse", "answer": ["20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22", " unique tweets identified as either hateful and not hateful from previous research BIBREF22"], "top_k_doc_id": [7775, 4739, 7772, 7773, 7774, 5641, 6804, 6285, 7256, 3583, 3581, 1957, 5085, 934, 3584], "orig_top_k_doc_id": [7772, 7773, 7774, 7775, 6804, 6285, 7256, 3583, 3581, 1957, 5085, 934, 3584, 4739, 5641]}, {"qid": 1365, "question": "Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance? in Is preprocessing of text really worth your time for online comment classification?", "answer": ["No"], "top_k_doc_id": [7775, 1876, 1877, 5906, 6414, 7563, 1879, 1715, 1979, 243, 3545, 3522, 5095, 3815, 5727], "orig_top_k_doc_id": [1876, 1877, 7775, 5906, 6414, 7563, 1879, 1715, 1979, 243, 3545, 3522, 5095, 3815, 5727]}]}
{"group_id": 140, "group_size": 9, "items": [{"qid": 1461, "question": "Which 5 languages appear most frequently in AA paper titles? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["English, Chinese, French, Japanese and Arabic"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2546, 2716, 3607, 2040, 3608, 2043, 2038], "orig_top_k_doc_id": [2039, 2036, 2042, 2041, 2040, 2037, 2047, 2046, 2043, 1301, 3607, 3608, 2038, 2546, 2716]}, {"qid": 1464, "question": "How many papers are used in experiment? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["44,896 articles"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2546, 2716, 3607, 2040, 3608, 3582, 3583], "orig_top_k_doc_id": [2036, 2042, 2041, 2037, 2039, 2046, 2047, 1301, 2040, 2546, 2716, 3608, 3582, 3607, 3583]}, {"qid": 1460, "question": "Which journal and conference are cited the most in recent years? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["CL Journal and EMNLP conference"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2546, 2716, 3607, 2040, 2043, 2044, 1285], "orig_top_k_doc_id": [2042, 2036, 2037, 2043, 2046, 2041, 2044, 2047, 2546, 2716, 1301, 2040, 1285, 2039, 3607]}, {"qid": 1458, "question": "Which NLP area have the highest average citation for woman author? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2546, 2716, 3607, 2043, 3608, 1302, 2045], "orig_top_k_doc_id": [2046, 2042, 1301, 2036, 2047, 2041, 2037, 3607, 2043, 3608, 2546, 2716, 1302, 2045, 2039]}, {"qid": 1459, "question": "Which 3 NLP areas are cited the most? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["machine translation, statistical machine, sentiment analysis"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2546, 2716, 3607, 3608, 3931, 2040, 2044], "orig_top_k_doc_id": [2047, 2046, 2042, 2036, 2039, 1301, 2041, 2040, 2037, 2044, 2546, 2716, 3607, 3608, 3931]}, {"qid": 1463, "question": "Are the academically younger authors cited less than older? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["Yes"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2546, 2716, 3607, 3608, 3931, 3934, 307], "orig_top_k_doc_id": [2047, 2046, 2042, 2041, 1301, 2036, 2546, 2716, 2039, 3931, 3607, 3934, 3608, 2037, 307]}, {"qid": 990, "question": "What dataset is used? in Sentiment Analysis of Citations Using Word2vec", "answer": ["ACL Anthology Reference Corpus"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 1302, 2043, 2044, 2045, 4931, 7743, 3606, 55, 5421, 1449], "orig_top_k_doc_id": [1301, 1302, 2042, 2044, 2046, 2047, 2043, 4931, 2045, 7743, 3606, 2041, 55, 5421, 1449]}, {"qid": 991, "question": "What metrics are considered? in Sentiment Analysis of Citations Using Word2vec", "answer": ["F-score, micro-F, macro-F, weighted-F "], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 1302, 2043, 2044, 2045, 5150, 1923, 2404, 6913, 1924, 3615], "orig_top_k_doc_id": [1301, 1302, 2044, 2042, 2041, 2046, 2047, 2043, 5150, 1923, 2404, 6913, 1924, 3615, 2045]}, {"qid": 1462, "question": "What aspect of NLP research is examined? in The State of NLP Literature: A Diachronic Analysis of the ACL Anthology", "answer": ["size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender)"], "top_k_doc_id": [1301, 2041, 2042, 2046, 2047, 2036, 2037, 2039, 2040, 3582, 3583, 3608, 1808, 557, 554], "orig_top_k_doc_id": [2036, 2047, 2046, 2042, 2039, 2037, 1301, 2040, 3582, 2041, 3583, 3608, 1808, 557, 554]}]}
{"group_id": 141, "group_size": 9, "items": [{"qid": 1588, "question": "Why authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task? in A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "answer": ["No"], "top_k_doc_id": [2215, 2216, 2219, 2220, 2221, 2306, 725, 2217, 729, 3578, 726, 3152, 7005, 7472, 897], "orig_top_k_doc_id": [2215, 2220, 2221, 2216, 2219, 2217, 2306, 725, 3578, 3152, 7472, 729, 897, 726, 7005]}, {"qid": 1589, "question": "What is specific to Chinese-oriented ABSA task, how is it different from other languages? in A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "answer": ["No"], "top_k_doc_id": [2215, 2216, 2219, 2220, 2221, 2306, 725, 2217, 729, 3578, 726, 3152, 7005, 7472, 2873], "orig_top_k_doc_id": [2215, 2216, 2220, 2221, 2219, 2217, 725, 2306, 7472, 729, 726, 3152, 3578, 2873, 7005]}, {"qid": 1584, "question": "How much better is performance of the proposed model compared to the state of the art in these various experiments? in A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "answer": ["significantly improves the accuracy and F1 score of aspect polarity classification"], "top_k_doc_id": [2215, 2216, 2219, 2220, 2221, 2306, 725, 2217, 728, 893, 2970, 7472, 3154, 6401, 2310], "orig_top_k_doc_id": [2215, 2220, 2219, 2216, 2221, 2306, 2217, 725, 3154, 728, 7472, 2970, 6401, 893, 2310]}, {"qid": 1586, "question": "What was previous state-of-the-art on four Chinese reviews datasets? in A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "answer": ["GANN obtained the state-of-the-art APC performance on the Chinese review datasets"], "top_k_doc_id": [2215, 2216, 2219, 2220, 2221, 2306, 725, 2217, 729, 3578, 6640, 2874, 728, 7292, 2873], "orig_top_k_doc_id": [2215, 2220, 2219, 2216, 2221, 2217, 6640, 725, 729, 2874, 728, 2306, 7292, 3578, 2873]}, {"qid": 1587, "question": "In what four Chinese review datasets does LCF-ATEPC achieves state of the art? in A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "answer": ["Car, Phone, Notebook, Camera"], "top_k_doc_id": [2215, 2216, 2219, 2220, 2221, 2306, 725, 2217, 728, 893, 2970, 7472, 729, 6640, 6606], "orig_top_k_doc_id": [2220, 2219, 2215, 2221, 2217, 2216, 728, 725, 2306, 2970, 893, 729, 6640, 7472, 6606]}, {"qid": 588, "question": "How big is the improvement over the state-of-the-art results? in A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis", "answer": ["AGDT improves the performance by 2.4% and 1.6% in the \u201cDS\u201d part of the two dataset, Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets, In the \u201cHDS\u201d part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain"], "top_k_doc_id": [2215, 725, 728, 7472, 726, 727, 729, 1053, 3152, 606, 3578, 2306, 605, 3580, 600], "orig_top_k_doc_id": [725, 729, 728, 726, 727, 2215, 606, 7472, 3578, 2306, 1053, 605, 3580, 3152, 600]}, {"qid": 589, "question": "Is the model evaluated against other Aspect-Based models? in A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis", "answer": ["Yes"], "top_k_doc_id": [2215, 725, 728, 7472, 726, 727, 729, 1053, 3152, 6401, 5158, 597, 2216, 6472, 2217], "orig_top_k_doc_id": [725, 729, 728, 726, 727, 2215, 3152, 7472, 6401, 5158, 597, 2216, 6472, 1053, 2217]}, {"qid": 504, "question": "How are aspects identified in aspect extraction? in Basic tasks of sentiment analysis", "answer": ["apply an ensemble of deep learning and linguistics t"], "top_k_doc_id": [2215, 725, 728, 7472, 598, 597, 6183, 606, 2217, 2216, 6401, 2306, 6472, 2978, 599], "orig_top_k_doc_id": [598, 597, 2215, 7472, 6183, 606, 2217, 2216, 725, 6401, 2306, 6472, 2978, 728, 599]}, {"qid": 1585, "question": "What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset? in A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction", "answer": ["BERT-ADA, BERT-PT, AEN-BERT, SDGCN-BERT"], "top_k_doc_id": [2215, 2216, 2219, 2220, 2221, 2306, 605, 727, 7475, 895, 3154, 2874, 7005, 2308, 728], "orig_top_k_doc_id": [2215, 2221, 2219, 2220, 605, 727, 7475, 2306, 895, 3154, 2874, 2216, 7005, 2308, 728]}]}
{"group_id": 142, "group_size": 9, "items": [{"qid": 1601, "question": "How do previous methods perform on the Switchboard Dialogue Act and DailyDialog datasets? in Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition", "answer": ["Table TABREF20 , Table TABREF22, Table TABREF23"], "top_k_doc_id": [4297, 2253, 4292, 4293, 4294, 4295, 4296, 4298, 4299, 590, 2255, 2256, 2257, 589, 1713], "orig_top_k_doc_id": [2257, 2253, 2256, 4298, 2255, 4292, 4295, 590, 4297, 4294, 4296, 4293, 589, 4299, 1713]}, {"qid": 1602, "question": "What previous methods is the proposed method compared against? in Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition", "answer": ["BLSTM+Attention+BLSTM\nHierarchical BLSTM-CRF\nCRF-ASN\nHierarchical CNN (window 4)\nmLSTM-RNN\nDRLM-Conditional\nLSTM-Softmax\nRCNN\nCNN\nCRF\nLSTM\nBERT"], "top_k_doc_id": [4297, 2253, 4292, 4293, 4294, 4295, 4296, 4298, 4299, 590, 2255, 2256, 2257, 404, 2254], "orig_top_k_doc_id": [2253, 4298, 2257, 4292, 2256, 4293, 4297, 4296, 4294, 4299, 2254, 404, 2255, 590, 4295]}, {"qid": 1603, "question": "What is dialogue act recognition? in Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition", "answer": ["DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. "], "top_k_doc_id": [4297, 2253, 4292, 4293, 4294, 4295, 4296, 4298, 4299, 590, 2255, 2256, 2257, 404, 589], "orig_top_k_doc_id": [2257, 4298, 4292, 2253, 4297, 4293, 4294, 4296, 4299, 4295, 2256, 404, 590, 2255, 589]}, {"qid": 1604, "question": "Which natural language(s) are studied? in Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition", "answer": ["No"], "top_k_doc_id": [4297, 2253, 4292, 4293, 4294, 4295, 4296, 4298, 4299, 590, 2255, 2256, 2257, 404, 2254], "orig_top_k_doc_id": [2253, 2257, 4298, 4292, 4293, 4294, 4297, 4296, 2256, 4299, 2254, 404, 4295, 590, 2255]}, {"qid": 2517, "question": "By how much do they outperform state-of-the-art solutions on SWDA and MRDA? in Dialogue Act Recognition via CRF-Attentive Structured Network", "answer": ["improves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively"], "top_k_doc_id": [4297, 2253, 4292, 4293, 4294, 4295, 4296, 4298, 4299, 590, 2255, 2256, 2257, 589, 2617], "orig_top_k_doc_id": [4297, 4299, 4295, 4296, 4298, 4292, 4294, 4293, 2256, 2253, 590, 589, 2257, 2617, 2255]}, {"qid": 3932, "question": "What is the problem of session segmentation? in Dialogue Session Segmentation by Embedding-Enhanced TextTiling", "answer": ["ot all sentences in the current conversation session are equally important,  irrelevant to the current context, and should not be considered when the computer synthesizes the reply", "To retain near and context relevant dialog session utterances and to discard far, irrelevant ones.", "Retaining relevant contextual information from previous utterances. "], "top_k_doc_id": [4297, 824, 6334, 6335, 6336, 6337, 825, 4298, 6637, 6638, 2475, 6639, 6636, 4665, 1309], "orig_top_k_doc_id": [6337, 6336, 6334, 6335, 824, 6637, 4297, 6638, 2475, 6639, 6636, 4298, 825, 4665, 1309]}, {"qid": 3933, "question": "What dataset do they use? in Dialogue Session Segmentation by Embedding-Enhanced TextTiling", "answer": ["real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances", "chatting corpus from DuMi and conversation data from Douban forum", "chatting corpus from DuMi"], "top_k_doc_id": [4297, 824, 6334, 6335, 6336, 6337, 825, 4298, 7543, 6861, 6037, 4293, 1714, 7375, 6860], "orig_top_k_doc_id": [6337, 6336, 6334, 6335, 824, 4297, 7543, 6861, 825, 6037, 4293, 1714, 7375, 6860, 4298]}, {"qid": 2516, "question": "Which features do they use? in Dialogue Act Recognition via CRF-Attentive Structured Network", "answer": ["beyond localized features and have access to the entire sequence"], "top_k_doc_id": [4297, 2253, 4292, 4293, 4294, 4295, 4296, 4298, 4299, 404, 403, 6297, 1768, 7626, 7625], "orig_top_k_doc_id": [4292, 4298, 4297, 4294, 4295, 4299, 4293, 4296, 2253, 404, 403, 6297, 1768, 7626, 7625]}, {"qid": 3931, "question": "Does their model use MFCC? in Dialogue Session Segmentation by Embedding-Enhanced TextTiling", "answer": ["No", "No", "No"], "top_k_doc_id": [4297, 824, 6334, 6335, 6336, 6337, 595, 5483, 7543, 2351, 6960, 2353, 6636, 2486, 4293], "orig_top_k_doc_id": [6336, 6337, 6334, 6335, 595, 5483, 4297, 824, 7543, 2351, 6960, 2353, 6636, 2486, 4293]}]}
{"group_id": 143, "group_size": 9, "items": [{"qid": 1653, "question": "Do they single out a validation set from the fixed SRE training set? in The Intelligent Voice 2016 Speaker Recognition System", "answer": ["No"], "top_k_doc_id": [2351, 5391, 6124, 1622, 1623, 2354, 4367, 5264, 5765, 5478, 6125, 7794, 227, 2473, 4785], "orig_top_k_doc_id": [2351, 2354, 5391, 1622, 6124, 1623, 5264, 5765, 5478, 6125, 4367, 7794, 227, 2473, 4785]}, {"qid": 1654, "question": "How well does their system perform on the development set of SRE? in The Intelligent Voice 2016 Speaker Recognition System", "answer": ["EER 16.04, Cmindet 0.6012, Cdet 0.6107"], "top_k_doc_id": [2351, 5391, 6124, 1622, 1623, 2354, 4367, 5264, 5765, 5478, 6125, 483, 7799, 1064, 484], "orig_top_k_doc_id": [2351, 2354, 5391, 6124, 5765, 1622, 5264, 483, 5478, 7799, 6125, 1623, 1064, 484, 4367]}, {"qid": 3765, "question": "What was the baseline? in THUEE system description for NIST 2019 SRE CTS Challenge", "answer": ["No", "No", "No"], "top_k_doc_id": [2351, 5391, 6124, 19, 1161, 1162, 4373, 4960, 6125, 1116, 7695, 1430, 6005, 23, 7658], "orig_top_k_doc_id": [6124, 2351, 6125, 1161, 5391, 4373, 1162, 19, 4960, 7695, 1430, 1116, 6005, 23, 7658]}, {"qid": 3766, "question": "What dataset was used in this challenge? in THUEE system description for NIST 2019 SRE CTS Challenge", "answer": ["SRE18 development and SRE18 evaluation datasets", "SRE19", "SRE04/05/06/08/10/MIXER6\nLDC98S75/LDC99S79/LDC2002S06/LDC2001S13/LDC2004S07\nVoxceleb 1/2\nFisher + Switchboard I\nCallhome+Callfriend"], "top_k_doc_id": [2351, 5391, 6124, 19, 1161, 1162, 4373, 4960, 6125, 1116, 3722, 5017, 2968, 5018, 2003], "orig_top_k_doc_id": [6124, 2351, 6125, 1161, 4960, 1162, 1116, 5391, 4373, 3722, 19, 5017, 2968, 5018, 2003]}, {"qid": 1195, "question": "How much bigger is Switchboard-2000 than Switchboard-300 database? in Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard-300", "answer": ["Switchboard-2000 contains 1700 more hours of speech data."], "top_k_doc_id": [2351, 381, 3836, 1111, 1618, 1619, 1621, 4295, 4372, 4373, 1161, 5825, 5014, 1620, 589], "orig_top_k_doc_id": [1621, 1618, 4372, 1619, 381, 4373, 1161, 3836, 4295, 5825, 5014, 2351, 1620, 1111, 589]}, {"qid": 1196, "question": "How big is Switchboard-300 database? in Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard-300", "answer": ["300-hour English conversational speech"], "top_k_doc_id": [2351, 381, 3836, 1111, 1618, 1619, 1621, 4295, 4372, 4373, 1518, 4121, 373, 6649, 1171], "orig_top_k_doc_id": [1621, 1618, 1619, 4372, 3836, 381, 1518, 4373, 1111, 4295, 2351, 4121, 373, 6649, 1171]}, {"qid": 1655, "question": "Which are the novel languages on which SRE placed emphasis on? in The Intelligent Voice 2016 Speaker Recognition System", "answer": ["Cebuano and Mandarin, Tagalog and Cantonese"], "top_k_doc_id": [2351, 5391, 6124, 1622, 1623, 2354, 4367, 5264, 5765, 7794, 228, 227, 7263, 7799, 1064], "orig_top_k_doc_id": [2351, 2354, 5391, 5264, 7794, 228, 6124, 1622, 4367, 227, 1623, 7263, 7799, 5765, 1064]}, {"qid": 3767, "question": "Which subsystem outperformed the others? in THUEE system description for NIST 2019 SRE CTS Challenge", "answer": ["primary system is the linear fusion of all the above six subsystems", "eftdnn ", "eftdnn"], "top_k_doc_id": [2351, 5391, 6124, 19, 1161, 1162, 4373, 4960, 6125, 6441, 553, 2285, 6176, 4610, 611], "orig_top_k_doc_id": [6124, 6125, 2351, 1161, 1162, 6441, 5391, 4960, 4373, 553, 19, 2285, 6176, 4610, 611]}, {"qid": 2383, "question": "How are sentence embeddings incorporated into the speech recognition system? in Gated Embeddings in End-to-End Speech Recognition for Conversational-Context Fusion", "answer": ["BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer."], "top_k_doc_id": [2351, 381, 3836, 3838, 3834, 3835, 7244, 3837, 2354, 373, 2238, 4842, 5053, 7351, 7793], "orig_top_k_doc_id": [3838, 3836, 3834, 3835, 7244, 3837, 381, 2351, 2354, 373, 2238, 4842, 5053, 7351, 7793]}]}
{"group_id": 144, "group_size": 9, "items": [{"qid": 1941, "question": "How they compute similarity between the representations? in Evaluating Multimodal Representations on Visual Semantic Textual Similarity", "answer": ["similarity is computed as the cosine of the produced $h_{L}$ and $h_{R}$ sentence/image representations"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 290, 7140, 7560, 80, 81, 2902, 7141, 7138, 4744], "orig_top_k_doc_id": [2899, 2904, 2900, 2903, 2905, 7000, 7560, 290, 7140, 2902, 81, 7141, 80, 7138, 4744]}, {"qid": 4456, "question": "In what language are the captions written in? in Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset", "answer": ["No", "No"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 80, 81, 2418, 2901, 2902, 4744, 7001, 2417, 5966], "orig_top_k_doc_id": [2899, 2900, 7000, 2905, 2904, 2903, 7001, 80, 81, 4744, 2901, 2417, 2418, 2902, 5966]}, {"qid": 4457, "question": "What is the average length of the captions? in Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset", "answer": ["No", "No"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 80, 81, 2418, 2901, 2902, 4744, 7001, 7560, 4756], "orig_top_k_doc_id": [2899, 2900, 7000, 2905, 2904, 2903, 7001, 2901, 81, 2418, 80, 2902, 4744, 7560, 4756]}, {"qid": 4459, "question": "What is the size of the dataset? in Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset", "answer": ["829 instances", "819"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 290, 7140, 7560, 80, 81, 2902, 7001, 4759, 1237], "orig_top_k_doc_id": [2899, 2900, 7000, 2905, 2904, 2903, 7001, 7560, 290, 2902, 7140, 80, 81, 4759, 1237]}, {"qid": 4460, "question": "What is the source of the images and textual captions? in Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset", "answer": [" Image Descriptions dataset, which is a subset of 8k-picture of Flickr, Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16", "PASCAL VOC-2008 dataset, 8k-Flicker"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 80, 81, 2418, 2901, 2902, 4744, 7001, 2417, 5966], "orig_top_k_doc_id": [2899, 2900, 7000, 2905, 2904, 2903, 7001, 80, 2901, 81, 2418, 2902, 5966, 4744, 2417]}, {"qid": 1942, "question": "How big is vSTS training data? in Evaluating Multimodal Representations on Visual Semantic Textual Similarity", "answer": ["1338 pairs for training"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 290, 7140, 7560, 7001, 416, 4744, 7138, 414, 3176], "orig_top_k_doc_id": [2899, 2900, 2905, 2904, 7000, 2903, 7001, 416, 7560, 7140, 4744, 7138, 414, 290, 3176]}, {"qid": 4458, "question": "Does each image have one caption? in Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset", "answer": ["Yes", "Yes"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 7000, 80, 81, 2418, 2901, 2902, 4744, 7001, 82, 413], "orig_top_k_doc_id": [2899, 2900, 7000, 2904, 2903, 7001, 2905, 81, 80, 2901, 4744, 2418, 82, 413, 2902]}, {"qid": 1939, "question": "What multimodal representations are used in the experiments? in Evaluating Multimodal Representations on Visual Semantic Textual Similarity", "answer": ["The second method it to learn a common space for the two modalities before concatenation (project), The first method is concatenation of the text and image representation (concat)"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 80, 3175, 7138, 7140, 7141, 3176, 7560, 7143, 413, 7148], "orig_top_k_doc_id": [2899, 2904, 2905, 2900, 2903, 3176, 7140, 3175, 7560, 7138, 7143, 7141, 80, 413, 7148]}, {"qid": 1940, "question": "How much better is inference that has addition of image representation compared to text-only representations?  in Evaluating Multimodal Representations on Visual Semantic Textual Similarity", "answer": [" largest improvement ($22-26\\%$ E.R) when text-based unsupervised models are combined with image representations"], "top_k_doc_id": [2899, 2900, 2903, 2904, 2905, 80, 3175, 7138, 7140, 7141, 7000, 3180, 4744, 2418, 7139], "orig_top_k_doc_id": [2899, 2905, 2900, 2904, 7000, 2903, 7140, 7138, 3175, 80, 3180, 7141, 4744, 2418, 7139]}]}
{"group_id": 145, "group_size": 9, "items": [{"qid": 2032, "question": "Do they fine-tune the used word embeddings on their medical texts? in Clustering of Medical Free-Text Records Based on Word Embeddings", "answer": ["No"], "top_k_doc_id": [1394, 3076, 4996, 5003, 3077, 3079, 165, 168, 1395, 2640, 4995, 2641, 1401, 1404, 4867], "orig_top_k_doc_id": [3076, 1394, 3079, 2640, 5003, 165, 3077, 2641, 168, 1401, 1404, 4996, 1395, 4995, 4867]}, {"qid": 2033, "question": "Which word embeddings do they use to represent medical visits? in Clustering of Medical Free-Text Records Based on Word Embeddings", "answer": ["GloVe, concatenation of average embeddings calculated separately for the interview and for the medical examination"], "top_k_doc_id": [1394, 3076, 4996, 5003, 3077, 3079, 165, 168, 1395, 2640, 4995, 3078, 5827, 5830, 5627], "orig_top_k_doc_id": [3076, 3079, 3077, 3078, 1394, 5827, 168, 165, 5003, 5830, 2640, 4996, 1395, 4995, 5627]}, {"qid": 1064, "question": "How are content clusters used to improve the prediction of incident severity? in Extracting information from free text through unsupervised graph-based clustering: an application to patient incident records", "answer": ["they are used as additional features in a supervised classification task"], "top_k_doc_id": [1394, 3076, 4996, 5003, 1395, 1399, 1401, 1402, 1403, 1404, 4995, 5004, 3752, 5001, 4999], "orig_top_k_doc_id": [1394, 4995, 1395, 5003, 1404, 4996, 1399, 5004, 1403, 1402, 5001, 1401, 3752, 4999, 3076]}, {"qid": 1065, "question": "What cluster identification method is used in this paper? in Extracting information from free text through unsupervised graph-based clustering: an application to patient incident records", "answer": ["A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18"], "top_k_doc_id": [1394, 3076, 4996, 5003, 1395, 1399, 1401, 1402, 1403, 1404, 4995, 5004, 3752, 2794, 4475], "orig_top_k_doc_id": [1394, 4995, 1395, 5003, 1399, 4996, 1404, 1403, 5004, 3076, 3752, 1401, 1402, 2794, 4475]}, {"qid": 2035, "question": "Which clustering technique do they use on partients' visits texts? in Clustering of Medical Free-Text Records Based on Word Embeddings", "answer": ["k-means, hierarchical clustering with Ward's method for merging clusters BIBREF23"], "top_k_doc_id": [1394, 3076, 4996, 5003, 3077, 3079, 165, 168, 3078, 5827, 1401, 6657, 4712, 5674, 7832], "orig_top_k_doc_id": [3076, 3079, 3077, 3078, 1394, 4996, 168, 5827, 5003, 1401, 6657, 4712, 5674, 165, 7832]}, {"qid": 2034, "question": "Do they explore similarity of texts across different doctors? in Clustering of Medical Free-Text Records Based on Word Embeddings", "answer": ["Yes"], "top_k_doc_id": [1394, 3076, 4996, 5003, 3077, 3079, 3078, 4995, 6656, 5674, 1395, 1401, 5002, 1399, 2641], "orig_top_k_doc_id": [3076, 3077, 3079, 1394, 4996, 5003, 3078, 4995, 6656, 5674, 1395, 1401, 5002, 1399, 2641]}, {"qid": 2853, "question": "Which text embedding methodologies are used? in From Free Text to Clusters of Content in Health Records: An Unsupervised Graph Partitioning Approach", "answer": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "top_k_doc_id": [1394, 3076, 4996, 5003, 1395, 1399, 1401, 1402, 1403, 1404, 4995, 5004, 5001, 1400, 6194], "orig_top_k_doc_id": [5003, 4995, 1394, 1395, 1403, 4996, 1404, 1399, 1401, 1402, 3076, 5004, 5001, 1400, 6194]}, {"qid": 1642, "question": "Does the proposed method outperform a baseline? in Deep Health Care Text Classification", "answer": ["No"], "top_k_doc_id": [1394, 1404, 2411, 4995, 4297, 1341, 462, 5404, 3300, 6606, 5111, 6195, 6027, 463, 7289], "orig_top_k_doc_id": [2411, 1404, 4995, 4297, 1394, 1341, 462, 5404, 3300, 6606, 5111, 6195, 6027, 463, 7289]}, {"qid": 1643, "question": "What type of RNN is used? in Deep Health Care Text Classification", "answer": ["RNN, LSTM"], "top_k_doc_id": [1394, 1404, 2411, 4995, 5737, 2332, 2410, 5258, 768, 5740, 2140, 1757, 3431, 5396, 2621], "orig_top_k_doc_id": [2411, 5737, 2332, 2410, 1404, 4995, 5258, 768, 5740, 2140, 1757, 3431, 5396, 2621, 1394]}]}
{"group_id": 146, "group_size": 9, "items": [{"qid": 2816, "question": "What hyperparameters are explored? in Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks", "answer": ["Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.", "Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs."], "top_k_doc_id": [7424, 436, 4931, 4932, 6697, 4933, 987, 991, 1874, 6106, 5663, 6104, 1862, 159, 6105], "orig_top_k_doc_id": [4931, 4932, 6697, 7424, 987, 6106, 4933, 991, 5663, 6104, 436, 1874, 1862, 159, 6105]}, {"qid": 2818, "question": "What sentiment analysis dataset is used? in Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks", "answer": ["IMDb dataset of movie reviews", "IMDb"], "top_k_doc_id": [7424, 436, 4931, 4932, 6697, 4933, 987, 991, 1874, 155, 3204, 5303, 2886, 3614, 5006], "orig_top_k_doc_id": [4931, 7424, 6697, 4932, 987, 1874, 4933, 155, 3204, 5303, 436, 2886, 991, 3614, 5006]}, {"qid": 4765, "question": "What downstream tasks are explored? in Factors Influencing the Surprising Instability of Word Embeddings", "answer": ["word similarity, POS tagging", "word similarity, POS tagging"], "top_k_doc_id": [7424, 5563, 2335, 7428, 7684, 7685, 4229, 5217, 4603, 2238, 7220, 4932, 7141, 6035, 7140], "orig_top_k_doc_id": [7424, 7428, 2335, 7684, 4603, 5217, 7685, 4229, 2238, 7220, 4932, 7141, 6035, 5563, 7140]}, {"qid": 4766, "question": "What factors contribute to the stability of the word embeddings? in Factors Influencing the Surprising Instability of Word Embeddings", "answer": ["curriculum learning, POS, domains.", "POS is one of the biggest factors in stability"], "top_k_doc_id": [7424, 5563, 2335, 7428, 7684, 7685, 5446, 7425, 7427, 5084, 7140, 7855, 7858, 5221, 1610], "orig_top_k_doc_id": [7424, 7428, 7425, 7427, 2335, 7684, 7685, 5084, 5563, 7140, 5446, 7855, 7858, 5221, 1610]}, {"qid": 4767, "question": "How is unstability defined? in Factors Influencing the Surprising Instability of Word Embeddings", "answer": ["We define stability as the percent overlap between nearest neighbors in an embedding space., 0% stability indicates complete disagreement", "An embedding is unstable if it has a low number of nearest neighbor embeddings of the words within the same frequency bucket."], "top_k_doc_id": [7424, 5563, 2335, 7428, 7684, 7685, 5446, 6206, 1380, 4603, 5307, 7822, 1726, 7743, 3498], "orig_top_k_doc_id": [7424, 7684, 7428, 5563, 7685, 2335, 6206, 1380, 4603, 5307, 5446, 7822, 1726, 7743, 3498]}, {"qid": 4768, "question": "What embedding algorithms are explored? in Factors Influencing the Surprising Instability of Word Embeddings", "answer": [" word2vec, GloVe, and PPMI", "word2vec, GloVe, PPMI"], "top_k_doc_id": [7424, 5563, 2335, 7428, 7684, 7685, 4229, 5217, 5307, 7425, 6853, 3183, 1826, 4522, 7426], "orig_top_k_doc_id": [7424, 2335, 7684, 7685, 7428, 5307, 4229, 7425, 6853, 3183, 5563, 5217, 1826, 4522, 7426]}, {"qid": 2819, "question": "Do they test both skipgram and c-bow? in Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks", "answer": ["Yes", "Yes"], "top_k_doc_id": [7424, 436, 4931, 4932, 6697, 4933, 1447, 6106, 1354, 7680, 7274, 7273, 3700, 7683, 3614], "orig_top_k_doc_id": [4931, 4932, 1447, 6697, 7424, 6106, 1354, 7680, 7274, 4933, 7273, 3700, 436, 7683, 3614]}, {"qid": 2817, "question": "What Named Entity Recognition dataset is used? in Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks", "answer": ["Groningen Meaning Bank", "Groningen Meaning Bank (GMB)"], "top_k_doc_id": [7424, 436, 4931, 4932, 6697, 5083, 22, 987, 4750, 2885, 4749, 3505, 7553, 2886, 5184], "orig_top_k_doc_id": [4931, 5083, 22, 987, 436, 4750, 2885, 4749, 3505, 6697, 7424, 4932, 7553, 2886, 5184]}, {"qid": 3322, "question": "What other factors affect the performance? in Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP", "answer": ["architecture of the classifier, sentence length,  input domain"], "top_k_doc_id": [7424, 5563, 5562, 5558, 5581, 5664, 491, 5251, 6714, 5352, 1339, 643, 2334, 4202, 5157], "orig_top_k_doc_id": [5562, 5558, 5563, 5581, 5664, 491, 5251, 6714, 5352, 1339, 643, 2334, 4202, 7424, 5157]}]}
{"group_id": 147, "group_size": 9, "items": [{"qid": 3158, "question": "Which two news domains are country-independent? in A multi-layer approach to disinformation detection on Twitter", "answer": ["mainstream news and disinformation", "mainstream and disinformation news"], "top_k_doc_id": [1494, 5321, 5322, 1172, 762, 1499, 5325, 5326, 1173, 1174, 3287, 6172, 5927, 6834, 6455], "orig_top_k_doc_id": [5326, 5325, 5321, 5322, 1173, 1494, 1172, 1499, 6172, 5927, 762, 6834, 6455, 3287, 1174]}, {"qid": 3160, "question": "What are the two large-scale datasets used? in A multi-layer approach to disinformation detection on Twitter", "answer": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "top_k_doc_id": [1494, 5321, 5322, 1172, 762, 1499, 5325, 5326, 1173, 1174, 3287, 3860, 5324, 4130, 6740], "orig_top_k_doc_id": [5325, 5326, 5321, 5322, 762, 1494, 3860, 1499, 1172, 5324, 3287, 1174, 1173, 4130, 6740]}, {"qid": 912, "question": "What is the architecture of their model? in Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources", "answer": ["we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."], "top_k_doc_id": [1494, 5321, 5322, 1172, 1173, 1174, 2157, 3879, 5784, 5927, 5928, 6555, 7545, 1517, 2083], "orig_top_k_doc_id": [1172, 1174, 1173, 5928, 5322, 1494, 6555, 3879, 5321, 5927, 7545, 1517, 5784, 2157, 2083]}, {"qid": 913, "question": "What are the nine types? in Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources", "answer": ["agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other"], "top_k_doc_id": [1494, 5321, 5322, 1172, 1173, 1174, 2157, 3879, 5784, 5927, 5928, 6555, 7545, 6916, 3878], "orig_top_k_doc_id": [1174, 1172, 1173, 1494, 5322, 5928, 7545, 3879, 6555, 5321, 5927, 2157, 6916, 5784, 3878]}, {"qid": 3159, "question": "How is the political bias of different sources included in the model? in A multi-layer approach to disinformation detection on Twitter", "answer": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "top_k_doc_id": [1494, 5321, 5322, 1172, 762, 1499, 5325, 5326, 1173, 1174, 606, 5145, 3596, 3860, 5927], "orig_top_k_doc_id": [5325, 5321, 5322, 5326, 1494, 1173, 1172, 762, 1174, 606, 1499, 5145, 3596, 3860, 5927]}, {"qid": 911, "question": "How is speed measured? in Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources", "answer": ["time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred"], "top_k_doc_id": [1494, 5321, 5322, 1172, 1173, 1174, 2157, 3879, 5784, 5927, 5928, 6555, 7545, 3486, 5929], "orig_top_k_doc_id": [1172, 1173, 1174, 3879, 5322, 5928, 1494, 5927, 7545, 5321, 2157, 5784, 3486, 6555, 5929]}, {"qid": 3161, "question": "What are the global network features which quantify different aspects of the sharing process? in A multi-layer approach to disinformation detection on Twitter", "answer": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "top_k_doc_id": [1494, 5321, 5322, 1172, 762, 1499, 5325, 5326, 6665, 5324, 5323, 5927, 6666, 6667, 1498], "orig_top_k_doc_id": [5321, 5325, 5326, 5322, 6665, 1494, 762, 5324, 5323, 5927, 1499, 1172, 6666, 6667, 1498]}, {"qid": 1130, "question": "What is the baseline? in An Emotional Analysis of False Information in Social Media and News Articles", "answer": ["Majority Class baseline (MC) , Random selection baseline (RAN)"], "top_k_doc_id": [1494, 5321, 5322, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 3926, 6005, 5930, 3015, 6458], "orig_top_k_doc_id": [1494, 1495, 1501, 1500, 1499, 1498, 6005, 1497, 1496, 5321, 5322, 5930, 3926, 3015, 6458]}, {"qid": 1131, "question": "What datasets did they use? in An Emotional Analysis of False Information in Social Media and News Articles", "answer": ["News Articles, Twitter"], "top_k_doc_id": [1494, 5321, 5322, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 3926, 6005, 6743, 5470, 3486], "orig_top_k_doc_id": [1494, 1495, 1501, 1500, 1496, 6005, 1499, 1497, 5322, 5321, 1498, 3926, 6743, 5470, 3486]}]}
{"group_id": 148, "group_size": 9, "items": [{"qid": 3338, "question": "What is the performance of the baseline? in X-Stance: A Multilingual Multi-Target Dataset for Stance Detection", "answer": ["M-Bert had 76.6 F1 macro score.", "75.1% and 75.6% accuracy"], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 5572, 5573, 5574, 6743, 5181, 5182, 6667, 6663, 4116, 6457], "orig_top_k_doc_id": [5570, 5573, 5574, 5572, 7500, 5180, 5181, 3860, 6667, 6663, 5182, 2874, 4116, 6743, 6457]}, {"qid": 3340, "question": "What was the performance of multilingual BERT? in X-Stance: A Multilingual Multi-Target Dataset for Stance Detection", "answer": ["BERT had 76.6 F1 macro score on x-stance dataset."], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 5572, 5573, 5574, 6743, 5181, 5182, 6667, 6663, 3273, 2409], "orig_top_k_doc_id": [5570, 5573, 5574, 5572, 3860, 7500, 6663, 2874, 6667, 5181, 5180, 3273, 5182, 2409, 6743]}, {"qid": 3341, "question": "What annotations are present in dataset? in X-Stance: A Multilingual Multi-Target Dataset for Stance Detection", "answer": ["answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters"], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 5572, 5573, 5574, 6743, 5181, 5182, 6667, 6740, 7599, 7499], "orig_top_k_doc_id": [5570, 5573, 5574, 7500, 5572, 6743, 2874, 5182, 5180, 5181, 3860, 6740, 7599, 6667, 7499]}, {"qid": 3339, "question": "Did they pefrorm any cross-lingual vs single language evaluation? in X-Stance: A Multilingual Multi-Target Dataset for Stance Detection", "answer": ["Yes"], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 5572, 5573, 5574, 6743, 5181, 5182, 5571, 5702, 7628, 398], "orig_top_k_doc_id": [5570, 5573, 5572, 5571, 5574, 2874, 5182, 3860, 7500, 5181, 5180, 5702, 6743, 7628, 398]}, {"qid": 3337, "question": "Does the paper report the performance of the model for each individual language? in X-Stance: A Multilingual Multi-Target Dataset for Stance Detection", "answer": ["Yes", "No", "Yes"], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 5572, 5573, 5574, 6743, 7628, 5571, 6457, 3861, 6663, 4111], "orig_top_k_doc_id": [5570, 5573, 5574, 5572, 3860, 7500, 6743, 2874, 7628, 5180, 5571, 6457, 3861, 6663, 4111]}, {"qid": 4818, "question": "do they compare their system with other systems? in 360{\\deg} Stance Detection", "answer": ["Yes", "Yes"], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 2827, 2828, 5182, 7499, 5181, 6745, 6740, 2873, 3861, 3543], "orig_top_k_doc_id": [7500, 7499, 2874, 5180, 5182, 5181, 3860, 2828, 2827, 5570, 6745, 6740, 2873, 3861, 3543]}, {"qid": 4820, "question": "what dataset did they use for this tool? in 360{\\deg} Stance Detection", "answer": ["They collect data using the AYLIEN News API,  which provides search capabilities for news articles enriched with extracted entities and other metadata and take a step to compile a curated list of topics. The final dataset consists of 32,227 pairs of news articles and topics annotated with their stance. ", "dataset consists of 32,227 pairs of news articles and topics annotated with their stance"], "top_k_doc_id": [3860, 7500, 2874, 5180, 5570, 2827, 2828, 5182, 7499, 4137, 6743, 2396, 5573, 5571, 7174], "orig_top_k_doc_id": [7500, 7499, 2874, 3860, 4137, 6743, 2827, 2396, 5573, 5570, 5571, 2828, 5180, 7174, 5182]}, {"qid": 2391, "question": "What are the state-of-the-art models for the task? in Taking a Stance on Fake News: Towards Automatic Disinformation Assessment via Deep Bidirectional Transformer Language Models for Stance Detection", "answer": ["To the best of our knowledge, our method achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset"], "top_k_doc_id": [3860, 7500, 3273, 6663, 6665, 6666, 6667, 7499, 3861, 6740, 3542, 7625, 6664, 3287, 3862], "orig_top_k_doc_id": [3860, 3861, 6663, 6666, 3273, 6740, 7500, 6667, 6665, 3542, 7625, 7499, 6664, 3287, 3862]}, {"qid": 4819, "question": "what is the architecture of their model? in 360{\\deg} Stance Detection", "answer": ["bidirectional LSTM", "a Bidirectional Encoding model BIBREF2"], "top_k_doc_id": [3860, 7500, 3273, 6663, 6665, 6666, 6667, 7499, 5573, 2874, 5180, 5570, 312, 4392, 4113], "orig_top_k_doc_id": [7500, 7499, 5573, 2874, 6667, 5180, 5570, 3860, 6663, 6665, 312, 3273, 6666, 4392, 4113]}]}
{"group_id": 149, "group_size": 9, "items": [{"qid": 3397, "question": "What languages are the model transferred to? in From English To Foreign Languages: Transferring Pre-trained Language Models", "answer": ["French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)", "French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)"], "top_k_doc_id": [5623, 5622, 5624, 4752, 5621, 6060, 6310, 1778, 2789, 4027, 518, 2997, 6064, 6208, 5844], "orig_top_k_doc_id": [5622, 5623, 5624, 5621, 6310, 2789, 4027, 6060, 4752, 1778, 2997, 6208, 518, 6064, 5844]}, {"qid": 3398, "question": "How is the model transferred to other languages? in From English To Foreign Languages: Transferring Pre-trained Language Models", "answer": ["Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM."], "top_k_doc_id": [5623, 5622, 5624, 4752, 5621, 6060, 6310, 1778, 2789, 4027, 518, 2997, 6064, 7408, 6871], "orig_top_k_doc_id": [5622, 5623, 5624, 5621, 6060, 4027, 2789, 6310, 1778, 4752, 2997, 7408, 6064, 518, 6871]}, {"qid": 3395, "question": "How much training data from the non-English language is used by the system? in From English To Foreign Languages: Transferring Pre-trained Language Models", "answer": ["No data. Pretrained model is used."], "top_k_doc_id": [5623, 5622, 5624, 4752, 5621, 6060, 6310, 1778, 2789, 4027, 6945, 783, 6871, 5276, 1884], "orig_top_k_doc_id": [5622, 5623, 5621, 5624, 6310, 4027, 6060, 6945, 1778, 2789, 4752, 783, 6871, 5276, 1884]}, {"qid": 3399, "question": "What metrics are used for evaluation? in From English To Foreign Languages: Transferring Pre-trained Language Models", "answer": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "top_k_doc_id": [5623, 5622, 5624, 4752, 5621, 6060, 6310, 1778, 2789, 783, 2137, 5802, 6063, 6064, 6062], "orig_top_k_doc_id": [5622, 5623, 5624, 5621, 6063, 2789, 6310, 6060, 1778, 4752, 783, 6062, 2137, 6064, 5802]}, {"qid": 3400, "question": "What datasets are used for evaluation? in From English To Foreign Languages: Transferring Pre-trained Language Models", "answer": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "top_k_doc_id": [5623, 5622, 5624, 4752, 5621, 6060, 6310, 1778, 2789, 783, 2137, 5802, 6063, 6064, 1000], "orig_top_k_doc_id": [5622, 5623, 5624, 6310, 5621, 6060, 4752, 2789, 2137, 783, 1778, 6063, 1000, 6064, 5802]}, {"qid": 3396, "question": "Is the system tested on low-resource languages? in From English To Foreign Languages: Transferring Pre-trained Language Models", "answer": ["Yes", "Yes"], "top_k_doc_id": [5623, 5622, 5624, 4752, 5621, 6060, 6310, 1777, 783, 4027, 1781, 6945, 4290, 4592, 2795], "orig_top_k_doc_id": [5622, 5623, 5621, 6310, 4752, 5624, 1777, 6060, 783, 4027, 1781, 6945, 4290, 4592, 2795]}, {"qid": 2276, "question": "How big is the provided treebank? in Universal Dependency Parsing for Hindi-English Code-switching", "answer": ["1448 sentences more than the dataset from Bhat et al., 2017"], "top_k_doc_id": [5623, 1773, 1991, 3258, 3598, 3599, 3600, 3602, 4290, 5977, 7327, 5622, 3601, 5821, 7172], "orig_top_k_doc_id": [3598, 3600, 3599, 1991, 5977, 3602, 7327, 5622, 4290, 3601, 1773, 5623, 5821, 3258, 7172]}, {"qid": 2277, "question": "What is LAS metric? in Universal Dependency Parsing for Hindi-English Code-switching", "answer": ["No"], "top_k_doc_id": [5623, 1773, 1991, 3258, 3598, 3599, 3600, 3602, 4290, 5977, 5624, 6938, 6280, 7288, 4050], "orig_top_k_doc_id": [3600, 3602, 3598, 3599, 5624, 1991, 5977, 6938, 1773, 3258, 5623, 6280, 7288, 4050, 4290]}, {"qid": 3525, "question": "What is the size of the parallel corpus used to train the model constraints? in Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments", "answer": ["1500 sentences", "1500 sentences"], "top_k_doc_id": [5623, 5622, 5624, 5819, 5821, 1189, 2807, 661, 662, 3760, 5361, 437, 438, 1457, 1721], "orig_top_k_doc_id": [5819, 5821, 5623, 1189, 5624, 2807, 5622, 661, 662, 3760, 5361, 437, 438, 1457, 1721]}]}
{"group_id": 150, "group_size": 9, "items": [{"qid": 3437, "question": "How better does new approach behave than existing solutions? in Exploration Based Language Learning for Text-Based Games", "answer": [" On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model", "On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.\nOn Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin."], "top_k_doc_id": [2867, 2869, 2871, 4716, 5683, 5684, 5685, 5686, 5687, 5688, 2868, 4718, 4717, 806, 1284], "orig_top_k_doc_id": [5683, 5688, 5687, 4716, 2867, 5685, 5684, 5686, 4717, 2871, 2869, 4718, 2868, 806, 1284]}, {"qid": 3439, "question": "On what Text-Based Games are experiments performed? in Exploration Based Language Learning for Text-Based Games", "answer": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "top_k_doc_id": [2867, 2869, 2871, 4716, 5683, 5684, 5685, 5686, 5687, 5688, 2868, 4718, 2870, 558, 2689], "orig_top_k_doc_id": [5683, 5687, 4716, 5688, 5685, 5686, 2867, 2870, 2869, 5684, 2871, 2868, 4718, 558, 2689]}, {"qid": 2689, "question": "What are the results from these proposed strategies? in How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents", "answer": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "top_k_doc_id": [2867, 2869, 2871, 4716, 558, 1443, 1719, 2868, 2870, 4717, 4718, 410, 6344, 6345, 5683], "orig_top_k_doc_id": [4716, 4718, 2871, 2867, 2869, 558, 1719, 6344, 6345, 1443, 2870, 410, 5683, 4717, 2868]}, {"qid": 2691, "question": "What are the two new strategies? in How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents", "answer": ["a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space", "KG-A2C-chained, KG-A2C-Explore"], "top_k_doc_id": [2867, 2869, 2871, 4716, 558, 1443, 1719, 2868, 2870, 4717, 4718, 410, 6344, 6345, 5250], "orig_top_k_doc_id": [4716, 4718, 2867, 2871, 2869, 1719, 558, 6344, 4717, 2870, 5250, 6345, 2868, 410, 1443]}, {"qid": 3440, "question": "How do the authors show that their learned policy generalize better than existing solutions to unseen games? in Exploration Based Language Learning for Text-Based Games", "answer": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "top_k_doc_id": [2867, 2869, 2871, 4716, 5683, 5684, 5685, 5686, 5687, 5688, 2868, 4717, 2872, 410, 3772], "orig_top_k_doc_id": [5683, 5688, 5687, 5685, 5684, 5686, 2867, 4716, 2868, 2871, 4717, 2869, 2872, 410, 3772]}, {"qid": 2690, "question": "What are the baselines? in How To Avoid Being Eaten By a Grue: Exploration Strategies for Text-Adventure Agents", "answer": ["a score of 40", "KG-A2C, A2C, A2C-chained, A2C-Explore"], "top_k_doc_id": [2867, 2869, 2871, 4716, 558, 1443, 1719, 2868, 2870, 4717, 4718, 5683, 2872, 5793, 1718], "orig_top_k_doc_id": [4716, 4718, 2871, 2867, 2869, 558, 1719, 2870, 5683, 1443, 2868, 2872, 4717, 5793, 1718]}, {"qid": 3438, "question": "How is trajectory with how rewards extracted? in Exploration Based Language Learning for Text-Based Games", "answer": ["explores the state space through keeping track of previously visited states by maintaining an archive"], "top_k_doc_id": [2867, 2869, 2871, 4716, 5683, 5684, 5685, 5686, 5687, 5688, 4718, 4717, 3475, 4454, 3476], "orig_top_k_doc_id": [5686, 5685, 5684, 5687, 5683, 4718, 4717, 4716, 2867, 3475, 5688, 4454, 3476, 2869, 2871]}, {"qid": 1925, "question": "What games are used to test author's methods? in Transfer in Deep Reinforcement Learning using Knowledge Graphs", "answer": ["Lurking Horror, Afflicted, Anchorhead, 9:05, TextWorld games"], "top_k_doc_id": [2867, 2869, 2871, 4716, 2868, 2872, 4978, 4979, 2870, 560, 5688, 537, 215, 562, 558], "orig_top_k_doc_id": [2867, 2868, 2872, 2871, 4716, 2870, 560, 5688, 537, 2869, 215, 562, 4979, 558, 4978]}, {"qid": 1926, "question": "How is the domain knowledge transfer represented as knowledge graph? in Transfer in Deep Reinforcement Learning using Knowledge Graphs", "answer": ["the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7"], "top_k_doc_id": [2867, 2869, 2871, 4716, 2868, 2872, 4978, 4979, 4983, 3628, 7514, 4982, 3269, 5966, 1547], "orig_top_k_doc_id": [2867, 2872, 2868, 2871, 4983, 3628, 7514, 4979, 4978, 4982, 3269, 4716, 2869, 5966, 1547]}]}
{"group_id": 151, "group_size": 9, "items": [{"qid": 3479, "question": "How big is imbalance in analyzed corpora? in Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance", "answer": ["Women represent 33.16% of the speakers"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 5766, 5767, 5768, 608, 2788, 7408, 609, 5980, 616], "orig_top_k_doc_id": [5764, 5765, 1899, 1901, 5767, 5766, 1902, 5768, 1903, 609, 608, 5980, 616, 7408, 2788]}, {"qid": 3480, "question": "What are four major corpora of French broadcast? in Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance", "answer": ["ESTER1, ESTER2, ETAPE, REPERE"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 5766, 5767, 5768, 608, 2788, 7408, 2797, 1266, 2486], "orig_top_k_doc_id": [5764, 5765, 5767, 5768, 5766, 1901, 1899, 2788, 1903, 2797, 608, 1266, 7408, 1902, 2486]}, {"qid": 3477, "question": "Which corpora does this paper analyse? in Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance", "answer": ["ESTER1, ESTER2, ETAPE, REPERE", "ESTER1, ESTER2, ETAPE, REPERE"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 5766, 5767, 5768, 608, 2788, 4881, 1900, 607, 224], "orig_top_k_doc_id": [5764, 5765, 5767, 1901, 1899, 5766, 5768, 1902, 1903, 4881, 608, 1900, 607, 2788, 224]}, {"qid": 3476, "question": "What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role? in Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance", "answer": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 5766, 5767, 5768, 608, 607, 1266, 6793, 7266, 5006], "orig_top_k_doc_id": [5765, 5768, 5764, 5767, 5766, 1899, 1901, 1903, 607, 1902, 608, 1266, 6793, 7266, 5006]}, {"qid": 1380, "question": "What representations are presented by this paper? in Gender Representation in Open Source Speech Resources", "answer": ["the number of speakers of each gender category, their speech duration"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 224, 1900, 6463, 7351, 3547, 5716, 2351, 7772, 876], "orig_top_k_doc_id": [1899, 1903, 1900, 224, 5764, 1902, 5765, 1901, 3547, 6463, 5716, 2351, 7351, 7772, 876]}, {"qid": 1382, "question": "What natural languages are represented in the speech resources studied? in Gender Representation in Open Source Speech Resources", "answer": ["No"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 224, 1900, 6463, 7351, 221, 223, 6464, 6468, 4864], "orig_top_k_doc_id": [1899, 6463, 1903, 1901, 1900, 5764, 1902, 221, 223, 224, 6464, 7351, 6468, 4864, 5765]}, {"qid": 3478, "question": "How many categories do authors define for speaker role? in Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance", "answer": [" two salient roles called Anchors and Punctual speakers"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 5766, 5767, 5768, 1900, 6485, 6793, 224, 7408, 5565], "orig_top_k_doc_id": [5765, 5766, 5764, 5767, 5768, 1901, 1899, 1902, 1903, 1900, 6485, 6793, 224, 7408, 5565]}, {"qid": 1381, "question": "What corpus characteristics correlate with more equitable gender balance? in Gender Representation in Open Source Speech Resources", "answer": ["No"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 224, 1900, 5768, 2045, 3007, 7772, 222, 6559, 221], "orig_top_k_doc_id": [1899, 1903, 1902, 1900, 1901, 5764, 5768, 5765, 2045, 3007, 7772, 222, 224, 6559, 221]}, {"qid": 3475, "question": "What tasks did they use to evaluate performance for male and female speakers? in Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance", "answer": ["ASR", "No"], "top_k_doc_id": [1899, 1901, 1902, 1903, 5764, 5765, 5766, 5767, 5768, 1900, 6738, 6341, 6737, 5949, 6964], "orig_top_k_doc_id": [5765, 5764, 5767, 5768, 1899, 5766, 1901, 1902, 1900, 1903, 6738, 6341, 6737, 5949, 6964]}]}
{"group_id": 152, "group_size": 9, "items": [{"qid": 4001, "question": "Are results reported only on English data? in TexTrolls: Identifying Russian Trolls on Twitter from a Textual Perspective", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6455, 6458, 6817, 6821, 3526, 6456, 6457, 7772, 1205, 765, 1726, 6818, 1735, 6820, 1208], "orig_top_k_doc_id": [6455, 6456, 3526, 6458, 6457, 7772, 765, 6817, 1205, 6821, 1726, 1735, 6820, 6818, 1208]}, {"qid": 4002, "question": "What type of model were the features used in? in TexTrolls: Identifying Russian Trolls on Twitter from a Textual Perspective", "answer": ["character-based Bidirectional Gated Recurrent neural network", "Random Selection, Majority Class, bag-of-words, Tweet2vec BIBREF32", "Logistic Regression classifier"], "top_k_doc_id": [6455, 6458, 6817, 6821, 3526, 6456, 6457, 7772, 1205, 1494, 1500, 6459, 6820, 765, 1208], "orig_top_k_doc_id": [6455, 6456, 6458, 6817, 6821, 6457, 1500, 7772, 1494, 6459, 765, 3526, 6820, 1205, 1208]}, {"qid": 4003, "question": "What unsupervised approach was used to deduce the thematic information? in TexTrolls: Identifying Russian Trolls on Twitter from a Textual Perspective", "answer": [" Latent Dirichlet Allocation (LDA)", "Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8", "Latent Dirichlet Allocation (LDA) topic modeling"], "top_k_doc_id": [6455, 6458, 6817, 6821, 3526, 6456, 6457, 7772, 1205, 765, 1726, 6818, 6459, 6816, 1875], "orig_top_k_doc_id": [6455, 6458, 6456, 6457, 6817, 3526, 6821, 7772, 6459, 6818, 1205, 6816, 1875, 765, 1726]}, {"qid": 4005, "question": "What textual features are used? in TexTrolls: Identifying Russian Trolls on Twitter from a Textual Perspective", "answer": ["eight Plutchik's emotions, positive and negative, list of bad and sexual words from BIBREF10, belief, denial, doubt, fake, knowledge, negation, question, and report, assertives verbs BIBREF14, bias BIBREF15, factive verbs BIBREF16, implicative verbs BIBREF17, hedges BIBREF18, report verbs BIBREF15, pronoun, anx, cogmech, insight, cause, discrep, tentat, certain, inhib, incl, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation", "emotion features, bad and sexual language features, stance towards some topics, bias cues, linguistic features from LIWC and morality cues"], "top_k_doc_id": [6455, 6458, 6817, 6821, 3526, 6456, 6457, 7772, 1205, 1494, 1500, 6459, 6820, 6818, 5906], "orig_top_k_doc_id": [6455, 6456, 6458, 6459, 1205, 6817, 6457, 6821, 1500, 7772, 3526, 6820, 1494, 6818, 5906]}, {"qid": 4308, "question": "How large is the dataset? in Predicting the Role of Political Trolls in Social Media", "answer": ["2973371 tweets by 2848 Twitter users", "2973371 tweets"], "top_k_doc_id": [6455, 6458, 6817, 6821, 1735, 5321, 6459, 6816, 6818, 7773, 1726, 6820, 5468, 3486, 5463], "orig_top_k_doc_id": [6817, 6821, 6816, 6818, 1735, 6455, 7773, 5468, 1726, 3486, 5321, 5463, 6458, 6820, 6459]}, {"qid": 4309, "question": "How are labels for trolls obtained? in Predicting the Role of Political Trolls in Social Media", "answer": ["the ground truth labels for the troll users are available, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves", "We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.", "manual labeling, more realistic scenario assumes that labels for troll accounts are not available"], "top_k_doc_id": [6455, 6458, 6817, 6821, 1735, 5321, 6459, 6816, 6818, 7773, 1726, 6820, 6456, 1205, 6819], "orig_top_k_doc_id": [6821, 6817, 6816, 6818, 1735, 6458, 6455, 6459, 7773, 6456, 6820, 5321, 1205, 6819, 1726]}, {"qid": 4307, "question": "What is the state-of-the-art? in Predicting the Role of Political Trolls in Social Media", "answer": ["BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches", "BIBREF2"], "top_k_doc_id": [6455, 6458, 6817, 6821, 1735, 5321, 6459, 6816, 6818, 7773, 7671, 1387, 5463, 5644, 1205], "orig_top_k_doc_id": [6821, 6816, 6817, 6818, 1735, 7773, 7671, 6455, 1387, 5321, 6459, 5463, 5644, 6458, 1205]}, {"qid": 4004, "question": "What profile features are used? in TexTrolls: Identifying Russian Trolls on Twitter from a Textual Perspective", "answer": ["Part-of-speech tags (POS), syntactic dependency relations, count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, tweet length", "bag of stopwords, Part-of-speech tags, syntactic dependency relations, count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, tweet length", "bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL), count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio and the tweet length"], "top_k_doc_id": [6455, 6458, 6817, 6821, 3526, 6456, 6457, 7772, 2343, 521, 1486, 2346, 2345, 520, 7029], "orig_top_k_doc_id": [6455, 6458, 6456, 6817, 6457, 2343, 6821, 521, 1486, 7772, 2346, 2345, 3526, 520, 7029]}, {"qid": 4310, "question": "Do they only look at tweets? in Predicting the Role of Political Trolls in Social Media", "answer": ["Yes", "No"], "top_k_doc_id": [6455, 6458, 6817, 6821, 1735, 5321, 6459, 6816, 6818, 7773, 5783, 447, 1726, 330, 7772], "orig_top_k_doc_id": [6817, 6821, 6818, 6816, 7773, 1735, 6455, 5783, 447, 1726, 5321, 6459, 330, 6458, 7772]}]}
{"group_id": 153, "group_size": 9, "items": [{"qid": 4843, "question": "What syntactic and semantic features are proposed? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["Opinion Words, Vulgar Words, Emoticons, Speech Act Verbs, N-grams, Punctuations, Twitter-specific Characters, Abbreviations, Dependency Sub-trees, Part-of-speech", "Semantic Features : Opinion Words, Vulgar Words, Emoticons,  Speech Act Verbs,  N-grams.\nSyntactic Features: Punctuations, Twitter-specific Characters, Abbreviations, Dependency Sub-trees,  Part-of-speech."], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 2343, 6752, 6753, 6630, 6512], "orig_top_k_doc_id": [7534, 7536, 7535, 1712, 3669, 1713, 3543, 1714, 2343, 1711, 6630, 1715, 6752, 6512, 6753]}, {"qid": 4850, "question": "what syntactic features are proposed? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["Punctuations, Twitter-specific Characters, Abbreviations, Dependency Sub-trees, Part-of-speech", "Binary features indicating appeance of punctuations, twitter-specific characters - @, #, and RT, abbreviations, length one and two sub-trees extracted from dependency sub-tree and Part-of-speech - adjectives and interjections."], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 2343, 6752, 6753, 4138, 589], "orig_top_k_doc_id": [7534, 7536, 7535, 1712, 3669, 3543, 1714, 1713, 1711, 1715, 6752, 4138, 2343, 589, 6753]}, {"qid": 4844, "question": "Which six speech acts are included in the taxonomy? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["Assertion, Recommendation , Expression, Question, Request, Miscellaneous", "Assertion, Recommendation Expression, Question, Request, and Miscellaneous"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 4136, 4138, 589, 1718, 4295], "orig_top_k_doc_id": [7534, 1712, 7536, 1713, 7535, 1711, 1714, 3669, 1715, 589, 3543, 1718, 4295, 4138, 4136]}, {"qid": 4849, "question": "what are the proposed semantic features? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["Opinion Words, Vulgar Words, Emoticons, Speech Act Verbs, N-grams", "Binary features indicating opinion words, vulgar words, emoticons, speech act verbs and unigram, bigram and trigram that appear at least five times in the dataset"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 2343, 6630, 4138, 4323, 1717], "orig_top_k_doc_id": [7534, 7536, 7535, 1712, 3669, 1713, 3543, 1714, 1711, 6630, 1715, 4138, 2343, 4323, 1717]}, {"qid": 4851, "question": "what datasets were used? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["A dataset they annotated, \"Harvard General Inquirer\" Lexicon for opinion words, a collection of vulgar words, an online collection of text-based emoticons, and Wierzbicka's collection of English speech act verbs", "Twitter data"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 4136, 4138, 589, 770, 1172], "orig_top_k_doc_id": [7534, 7535, 7536, 3669, 1712, 1714, 3543, 1713, 4138, 1711, 1715, 589, 770, 4136, 1172]}, {"qid": 4845, "question": "what classifier had better performance? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["logistic regression", "topic-specific classifier"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 4138, 6378, 3045, 1717, 3989], "orig_top_k_doc_id": [7534, 7536, 3669, 7535, 3543, 1712, 1714, 4138, 1715, 6378, 1713, 3045, 1711, 1717, 3989]}, {"qid": 4847, "question": "how many annotators were there? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["three", "three"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 1713, 1715, 4136, 4138, 3046, 1756, 5170], "orig_top_k_doc_id": [7534, 7535, 7536, 3669, 1714, 1713, 1712, 4138, 4136, 1711, 3543, 1715, 3046, 1756, 5170]}, {"qid": 4846, "question": "how many tweets were labeled? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["7,563", "7,563"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 4136, 5169, 4138, 6376, 1756, 4990, 3989], "orig_top_k_doc_id": [7534, 7535, 3669, 7536, 1712, 3543, 1714, 4136, 5169, 4138, 6376, 1711, 1756, 4990, 3989]}, {"qid": 4848, "question": "who labelled the tweets? in Tweet Acts: A Speech Act Classifier for Twitter", "answer": ["three undergraduate annotators ", "three undergraduate annotators"], "top_k_doc_id": [1711, 1712, 1714, 3543, 3669, 7534, 7535, 7536, 4138, 7308, 6631, 6520, 6176, 3670, 6141], "orig_top_k_doc_id": [7534, 3669, 7535, 7536, 1712, 4138, 1714, 7308, 3543, 1711, 6631, 6520, 6176, 3670, 6141]}]}
{"group_id": 154, "group_size": 9, "items": [{"qid": 4954, "question": "In their nonsymbolic representation can they represent two same string differently depending on the context? in Nonsymbolic Text Representation", "answer": ["No", "No"], "top_k_doc_id": [7686, 7680, 7681, 7682, 7685, 7688, 5885, 5886, 3764, 6047, 3411, 3108, 1286, 5103, 3054], "orig_top_k_doc_id": [7680, 7688, 7681, 7682, 7686, 7685, 5885, 5886, 3764, 6047, 3411, 3108, 1286, 5103, 3054]}, {"qid": 4955, "question": "On which datasets do they evaluate their models? in Nonsymbolic Text Representation", "answer": ["3 gigabyte English Wikipedia corpus", "entity dataset released by xie16entitydesc2"], "top_k_doc_id": [7686, 7680, 7681, 7682, 7685, 7688, 2899, 3414, 2223, 1692, 3743, 214, 4006, 3068, 123], "orig_top_k_doc_id": [7688, 7680, 7682, 7681, 7685, 7686, 2899, 3414, 2223, 1692, 3743, 214, 4006, 3068, 123]}, {"qid": 4952, "question": "Do they have an elementary unit of text? in Nonsymbolic Text Representation", "answer": ["No", "No"], "top_k_doc_id": [7686, 7680, 7681, 7682, 7685, 7688, 5032, 5663, 3183, 6050, 977, 1909, 7072, 3182, 1273], "orig_top_k_doc_id": [7680, 7681, 7688, 7682, 7685, 5032, 5663, 3183, 6050, 7686, 977, 1909, 7072, 3182, 1273]}, {"qid": 1192, "question": "What text classification task is considered? in The emergent algebraic structure of RNNs and embeddings in NLP", "answer": ["To classify a text as belonging to one of the ten possible classes."], "top_k_doc_id": [7686, 425, 1609, 1610, 1611, 1612, 1613, 1616, 1617, 7116, 1614, 3860, 433, 7110, 7687], "orig_top_k_doc_id": [1609, 1610, 1611, 1617, 1613, 1612, 1616, 425, 1614, 7116, 7686, 3860, 433, 7110, 7687]}, {"qid": 1193, "question": "What novel class of recurrent-like networks is proposed? in The emergent algebraic structure of RNNs and embeddings in NLP", "answer": ["A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state."], "top_k_doc_id": [7686, 425, 1609, 1610, 1611, 1612, 1613, 1616, 1617, 7116, 3825, 7232, 5938, 689, 1768], "orig_top_k_doc_id": [1609, 1610, 1617, 1611, 1613, 1616, 425, 7686, 3825, 1612, 7232, 7116, 5938, 689, 1768]}, {"qid": 1244, "question": "How many parameters does the model have? in Bridging the Gap for Tokenizer-Free Language Models", "answer": ["model has around 836M parameters"], "top_k_doc_id": [7686, 1691, 1692, 1693, 1742, 6159, 6969, 34, 1320, 1741, 4922, 4689, 1639, 7680, 7813], "orig_top_k_doc_id": [1742, 1693, 1692, 6159, 1691, 34, 6969, 1320, 7686, 1741, 4922, 4689, 1639, 7680, 7813]}, {"qid": 1245, "question": "How many characters are accepted as input of the language model? in Bridging the Gap for Tokenizer-Free Language Models", "answer": ["input byte embedding matrix has dimensionality 256"], "top_k_doc_id": [7686, 1691, 1692, 1693, 1742, 6159, 6969, 2677, 4515, 2302, 855, 5564, 7688, 1790, 2790], "orig_top_k_doc_id": [1691, 7686, 1693, 1692, 6159, 1742, 2677, 4515, 2302, 855, 5564, 7688, 1790, 2790, 6969]}, {"qid": 4953, "question": "By how much do they outpeform existing text denoising models? in Nonsymbolic Text Representation", "answer": ["Their F1 score outperforms an existing model by 0.017 on average for the random segmentation experiment.", "Answer with content missing: (Table 4) Mean reciprocal rank of proposed model is 0.76 compared to 0.64 of bag-of-ngrams."], "top_k_doc_id": [7686, 7680, 7681, 7682, 7685, 7688, 6426, 7273, 6314, 96, 97, 1668, 309, 6424, 5212], "orig_top_k_doc_id": [7688, 7680, 7681, 6426, 7273, 7685, 6314, 7682, 7686, 96, 97, 1668, 309, 6424, 5212]}, {"qid": 1194, "question": "Is there a formal proof that the RNNs form a representation of the group? in The emergent algebraic structure of RNNs and embeddings in NLP", "answer": ["No"], "top_k_doc_id": [7686, 425, 1609, 1610, 1611, 1612, 1613, 1616, 1617, 1615, 7394, 2302, 7687, 7391, 3556], "orig_top_k_doc_id": [1610, 1609, 1617, 1611, 1616, 425, 1615, 1613, 7686, 1612, 7394, 2302, 7687, 7391, 3556]}]}
{"group_id": 155, "group_size": 8, "items": [{"qid": 40, "question": "How many attention layers are there in their model? in Saliency Maps Generation for Automatic Text Summarization", "answer": ["one"], "top_k_doc_id": [37, 7134, 7136, 7137, 38, 39, 40, 3198, 6927, 4267, 256, 257, 4268, 6068, 3200], "orig_top_k_doc_id": [37, 38, 39, 40, 7134, 3198, 6927, 256, 4267, 4268, 7137, 7136, 257, 6068, 3200]}, {"qid": 41, "question": "Is the explanation from saliency map correct? in Saliency Maps Generation for Automatic Text Summarization", "answer": ["No"], "top_k_doc_id": [37, 7134, 7136, 7137, 38, 39, 40, 3198, 6927, 4267, 256, 257, 4268, 7135, 3201], "orig_top_k_doc_id": [37, 38, 39, 40, 7134, 257, 256, 7137, 4267, 7136, 4268, 3198, 7135, 6927, 3201]}, {"qid": 4572, "question": "What is the architecture of the decoder? in Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models", "answer": ["self-attention module, a context-attention module, and a two-layer feed-forward network", "M blocks, each consisting of self-attention module, context-attention module, and a two-layer feed-forward network."], "top_k_doc_id": [37, 7134, 7136, 7137, 4760, 5804, 6928, 5278, 5540, 6927, 2148, 4761, 5277, 7135, 5541], "orig_top_k_doc_id": [7134, 7137, 7136, 6927, 37, 7135, 5804, 6928, 5277, 4760, 5278, 4761, 2148, 5541, 5540]}, {"qid": 4573, "question": "What is the architecture of the encoder? in Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models", "answer": ["M blocks, each consisting of self-attention module and a two-layer feed-forward network.", "encoder block consists of a self-attention module and a two-layer feed-forward network"], "top_k_doc_id": [37, 7134, 7136, 7137, 4760, 5804, 6928, 5278, 5540, 6927, 2148, 4761, 5277, 7135, 730], "orig_top_k_doc_id": [7134, 7137, 7136, 6927, 37, 7135, 5278, 6928, 5277, 4760, 5804, 5540, 4761, 2148, 730]}, {"qid": 39, "question": "Which baselines did they compare? in Saliency Maps Generation for Automatic Text Summarization", "answer": ["The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.", "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."], "top_k_doc_id": [37, 7134, 7136, 7137, 38, 39, 40, 3198, 6927, 4267, 3201, 1255, 3200, 4382, 1866], "orig_top_k_doc_id": [40, 39, 37, 38, 7134, 7137, 4267, 6927, 3201, 1255, 3200, 4382, 3198, 1866, 7136]}, {"qid": 4571, "question": "What is the previous state-of-the-art? in Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models", "answer": ["Transformer-based encoder-decoder", "BART LARGE"], "top_k_doc_id": [37, 7134, 7136, 7137, 4760, 5804, 6928, 5278, 5540, 6927, 4482, 1132, 3202, 5079, 4478], "orig_top_k_doc_id": [7134, 7137, 7136, 6927, 4482, 1132, 4760, 3202, 37, 5804, 5540, 5079, 6928, 5278, 4478]}, {"qid": 4574, "question": "What are the languages of the datasets? in Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models", "answer": ["No", "No"], "top_k_doc_id": [37, 7134, 7136, 7137, 4760, 5804, 6928, 3202, 4482, 6060, 7135, 7243, 5276, 4396, 2148], "orig_top_k_doc_id": [7134, 7137, 7136, 37, 3202, 4760, 4482, 6060, 7135, 6928, 7243, 5276, 4396, 2148, 5804]}, {"qid": 4575, "question": "What is the architecture of the saliency model? in Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models", "answer": ["basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network", "M blocks, each consisting of a self-attention module and two-layer feed-forward network, combined with a  single-layer feed-forward network."], "top_k_doc_id": [37, 7134, 7136, 7137, 38, 39, 40, 3198, 6927, 7135, 3202, 3201, 4268, 6928, 5079], "orig_top_k_doc_id": [7134, 7137, 7136, 37, 7135, 38, 39, 3202, 6927, 3201, 4268, 6928, 3198, 40, 5079]}]}
{"group_id": 156, "group_size": 8, "items": [{"qid": 120, "question": "What are the country-specific drivers of international development rhetoric? in What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "answer": ["wealth , democracy , population, levels of ODA, conflict "], "top_k_doc_id": [4885, 140, 3594, 143, 2076, 3595, 4884, 4887, 4888, 141, 142, 2074, 2077, 2409, 4800], "orig_top_k_doc_id": [140, 143, 4887, 4884, 4888, 2074, 3595, 141, 2077, 2076, 3594, 142, 4885, 2409, 4800]}, {"qid": 121, "question": "Is the dataset multilingual? in What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "answer": ["No", "No"], "top_k_doc_id": [4885, 140, 3594, 143, 2076, 3595, 4884, 4887, 4888, 141, 142, 2074, 2077, 2409, 3598], "orig_top_k_doc_id": [140, 4888, 4887, 4884, 143, 2074, 3594, 2076, 3595, 4885, 141, 2409, 142, 2077, 3598]}, {"qid": 122, "question": "How are the main international development topics that states raise identified? in What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "answer": [" They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence."], "top_k_doc_id": [4885, 140, 3594, 143, 2076, 3595, 4884, 4887, 4888, 141, 142, 2074, 2077, 3597, 2078], "orig_top_k_doc_id": [140, 143, 2074, 142, 4888, 4884, 4887, 141, 3594, 3595, 2077, 2076, 4885, 3597, 2078]}, {"qid": 2275, "question": "how many speeches are in the dataset? in Automated Speech Generation from UN General Assembly Statements: Mapping Risks in AI Generated Texts", "answer": ["7,507"], "top_k_doc_id": [4885, 140, 3594, 143, 2076, 3595, 4884, 4887, 4888, 3596, 3333, 4886, 2409, 285, 6361], "orig_top_k_doc_id": [3596, 3594, 4887, 140, 3595, 4888, 4885, 2076, 4884, 3333, 4886, 2409, 143, 285, 6361]}, {"qid": 2789, "question": "Do they use number of votes as an indicator of preference? in Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes", "answer": ["No", "Yes"], "top_k_doc_id": [4885, 1382, 2074, 2076, 2340, 4884, 4886, 4887, 4889, 4892, 4893, 4894, 4890, 3801, 743], "orig_top_k_doc_id": [4884, 4886, 4887, 4893, 4892, 4885, 4894, 4889, 2340, 1382, 2076, 4890, 2074, 3801, 743]}, {"qid": 2790, "question": "What does a node in the network approach repesent? in Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes", "answer": ["No", "No"], "top_k_doc_id": [4885, 1382, 2074, 2076, 2340, 4884, 4886, 4887, 4889, 4892, 4893, 4894, 4890, 1380, 2850], "orig_top_k_doc_id": [4884, 4886, 4887, 4893, 4892, 4894, 4889, 4885, 1382, 2074, 2076, 4890, 2340, 1380, 2850]}, {"qid": 1483, "question": "What are the potentials risks of this approach? in Data Innovation for International Development: An overview of natural language processing for qualitative data analysis", "answer": ["No"], "top_k_doc_id": [4885, 140, 3594, 2074, 2077, 3597, 3596, 2078, 7773, 6712, 34, 1035, 3581, 4877, 236], "orig_top_k_doc_id": [2074, 2077, 3597, 3596, 2078, 140, 3594, 7773, 6712, 34, 1035, 4885, 3581, 4877, 236]}, {"qid": 2791, "question": "Which dataset do they use? in Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes", "answer": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "top_k_doc_id": [4885, 1382, 2074, 2076, 2340, 4884, 4886, 4887, 4889, 4892, 4893, 4894, 2157, 1931, 6817], "orig_top_k_doc_id": [4884, 4887, 4886, 4893, 4892, 4885, 4894, 4889, 2074, 2076, 1382, 2157, 1931, 2340, 6817]}]}
{"group_id": 157, "group_size": 8, "items": [{"qid": 154, "question": "What are the benchmark models? in CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "answer": ["BERTNLU from ConvLab-2, a rule-based model (RuleDST) , TRADE (Transferable Dialogue State Generator) , a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)"], "top_k_doc_id": [202, 5744, 196, 2276, 197, 200, 680, 965, 3193, 3679, 6036, 2221, 6703, 3509, 3191], "orig_top_k_doc_id": [196, 200, 197, 202, 3679, 3193, 6036, 965, 2221, 680, 5744, 2276, 6703, 3509, 3191]}, {"qid": 155, "question": "How was the corpus annotated? in CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "answer": ["The workers were also asked to annotate both user states and system states, we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories"], "top_k_doc_id": [202, 5744, 196, 2276, 197, 200, 680, 965, 3193, 3679, 6036, 2221, 5850, 573, 6039], "orig_top_k_doc_id": [196, 197, 202, 200, 965, 2276, 5744, 3193, 3679, 5850, 6036, 573, 6039, 680, 2221]}, {"qid": 1914, "question": "How better is gCAS approach compared to other approaches? in Modeling Multi-Action Policy for Task-Oriented Dialogues", "answer": ["For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52"], "top_k_doc_id": [202, 5744, 196, 2276, 1674, 1678, 3475, 6588, 2841, 2842, 2843, 1677, 4669, 3507, 3185], "orig_top_k_doc_id": [2276, 1674, 2841, 2842, 5744, 3475, 196, 2843, 6588, 1677, 4669, 202, 3507, 1678, 3185]}, {"qid": 1915, "question": "What is specific to gCAS cell? in Modeling Multi-Action Policy for Task-Oriented Dialogues", "answer": ["It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner."], "top_k_doc_id": [202, 5744, 196, 2276, 1674, 1678, 3475, 6588, 2841, 2842, 2843, 1676, 201, 7840, 7584], "orig_top_k_doc_id": [2841, 2842, 2843, 1674, 5744, 196, 2276, 202, 6588, 1676, 201, 1678, 7840, 7584, 3475]}, {"qid": 153, "question": "How was the dataset collected? in CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "answer": ["Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ", "They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. "], "top_k_doc_id": [202, 5744, 196, 2276, 197, 200, 680, 965, 3193, 3679, 6036, 1070, 3684, 4669, 3507], "orig_top_k_doc_id": [196, 197, 200, 202, 3679, 3193, 1070, 965, 6036, 3684, 5744, 680, 2276, 4669, 3507]}, {"qid": 1913, "question": "What datasets are used for training/testing models?  in Modeling Multi-Action Policy for Task-Oriented Dialogues", "answer": ["Microsoft Research dataset containing movie, taxi and restaurant domains."], "top_k_doc_id": [202, 5744, 196, 2276, 1674, 1678, 3475, 6588, 1677, 3191, 7840, 201, 3679, 1676, 200], "orig_top_k_doc_id": [196, 1674, 6588, 5744, 2276, 202, 1677, 3475, 1678, 3191, 7840, 201, 3679, 1676, 200]}, {"qid": 3465, "question": "by how much did nus outperform abus? in Neural User Simulation for Corpus-based Policy Optimisation for Spoken Dialogue Systems", "answer": ["Average success rate is higher by 2.6 percent points."], "top_k_doc_id": [202, 5744, 3187, 3188, 3679, 5745, 5748, 5749, 6588, 7584, 5746, 3507, 4656, 196, 5258], "orig_top_k_doc_id": [5745, 5744, 5749, 5748, 5746, 3187, 202, 3507, 4656, 3188, 6588, 3679, 7584, 196, 5258]}, {"qid": 3466, "question": "what corpus is used to learn behavior? in Neural User Simulation for Corpus-based Policy Optimisation for Spoken Dialogue Systems", "answer": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "top_k_doc_id": [202, 5744, 3187, 3188, 3679, 5745, 5748, 5749, 6588, 7584, 7842, 7843, 7839, 5441, 3185], "orig_top_k_doc_id": [5745, 5744, 3187, 202, 7842, 5749, 6588, 3188, 7843, 7839, 3679, 7584, 5441, 5748, 3185]}]}
{"group_id": 158, "group_size": 8, "items": [{"qid": 173, "question": "How is face and audio data analysis evaluated? in Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning", "answer": ["confusion matrices, $\\text{F}_1$ score"], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 227, 228, 229, 230, 3439, 5264, 589, 5015, 7794, 521], "orig_top_k_doc_id": [226, 230, 229, 3438, 3437, 5481, 227, 5264, 228, 5484, 5015, 7794, 589, 521, 3439]}, {"qid": 175, "question": "What are the emotion detection tools used for audio and face input? in Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning", "answer": ["We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)", "cannot be disclosed due to licensing restrictions"], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 227, 228, 229, 230, 3439, 5264, 589, 5015, 6005, 5482], "orig_top_k_doc_id": [226, 230, 3438, 229, 3437, 5481, 227, 5264, 6005, 228, 3439, 5484, 589, 5015, 5482]}, {"qid": 174, "question": "What is the baseline method for the task? in Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning", "answer": ["For the emotion recognition from text they use described neural network as baseline.\nFor audio and face there is no baseline."], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 227, 228, 229, 230, 3439, 5264, 589, 6005, 929, 7623], "orig_top_k_doc_id": [226, 230, 3437, 229, 3438, 227, 5264, 3439, 5481, 6005, 929, 589, 228, 7623, 5484]}, {"qid": 3267, "question": "Do they use datasets with transcribed text or do they determine text from the audio? in Multimodal Speech Emotion Recognition Using Audio and Text", "answer": ["They use text transcription.", "both"], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 589, 5015, 5482, 5483, 230, 620, 7557, 7556, 4875, 4862], "orig_top_k_doc_id": [226, 7557, 5481, 5484, 5015, 5483, 589, 7556, 4875, 620, 5482, 3437, 4862, 230, 3438]}, {"qid": 3268, "question": "By how much does their model outperform the state of the art results? in Multimodal Speech Emotion Recognition Using Audio and Text", "answer": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 589, 5015, 5482, 5483, 230, 620, 7138, 2119, 2120, 229], "orig_top_k_doc_id": [5484, 5481, 226, 589, 230, 7138, 5482, 2119, 5483, 620, 3437, 5015, 3438, 2120, 229]}, {"qid": 172, "question": "Does the paper evaluate any adjustment to improve the predicion accuracy of face and audio features? in Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning", "answer": ["No"], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 227, 228, 229, 230, 3439, 5264, 2800, 5015, 4671, 7138], "orig_top_k_doc_id": [226, 229, 3438, 3437, 230, 227, 5481, 5484, 228, 3439, 5264, 2800, 5015, 4671, 7138]}, {"qid": 3269, "question": "How do they combine audio and text sequences in their RNN? in Multimodal Speech Emotion Recognition Using Audio and Text", "answer": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "top_k_doc_id": [3437, 3438, 226, 5481, 5484, 589, 5015, 5482, 5483, 7138, 5667, 380, 6314, 6371, 1812], "orig_top_k_doc_id": [5481, 5482, 226, 5484, 5483, 5015, 7138, 5667, 589, 380, 6314, 3437, 6371, 1812, 3438]}, {"qid": 2210, "question": "Which four languages do they experiment with? in Cross Lingual Cross Corpus Speech Emotion Recognition", "answer": ["German, English, Italian, Chinese"], "top_k_doc_id": [3437, 3438, 3439, 247, 1112, 221, 6036, 2329, 3618, 627, 249, 1365, 6031, 230, 3617], "orig_top_k_doc_id": [3437, 3439, 3438, 247, 1112, 221, 6036, 2329, 3618, 627, 249, 1365, 6031, 230, 3617]}]}
{"group_id": 159, "group_size": 8, "items": [{"qid": 216, "question": "What benchmark datasets are used for the link prediction task? in Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction", "answer": ["WN18RR, FB15k-237, YAGO3-10", "WN18RR BIBREF26, FB15k-237 BIBREF18, YAGO3-10 BIBREF27"], "top_k_doc_id": [3633, 4160, 267, 270, 4981, 268, 272, 4161, 4164, 4490, 4720, 4163, 3632, 178, 469], "orig_top_k_doc_id": [267, 272, 270, 4981, 4160, 4490, 4161, 4164, 4163, 268, 3632, 3633, 178, 4720, 469]}, {"qid": 217, "question": "What are state-of-the art models for this task? in Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction", "answer": ["TransE, DistMult, ComplEx, ConvE, RotatE"], "top_k_doc_id": [3633, 4160, 267, 270, 4981, 268, 272, 178, 271, 3632, 4982, 6579, 4490, 4161, 4720], "orig_top_k_doc_id": [267, 272, 268, 270, 4981, 4982, 3633, 4160, 178, 271, 4490, 4161, 3632, 6579, 4720]}, {"qid": 218, "question": "How better does HAKE model peform than state-of-the-art methods? in Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction", "answer": ["0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively, doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively"], "top_k_doc_id": [3633, 4160, 267, 270, 4981, 268, 272, 178, 271, 3632, 4982, 6579, 269, 5215, 2853], "orig_top_k_doc_id": [272, 267, 270, 268, 271, 269, 3633, 4981, 4160, 4982, 5215, 3632, 2853, 6579, 178]}, {"qid": 219, "question": "How are entities mapped onto polar coordinate system? in Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction", "answer": ["radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively"], "top_k_doc_id": [3633, 4160, 267, 270, 4981, 268, 272, 4161, 4164, 4490, 4720, 269, 271, 4317, 4555], "orig_top_k_doc_id": [267, 272, 269, 271, 268, 4317, 4160, 270, 4164, 4490, 4981, 4161, 3633, 4555, 4720]}, {"qid": 2143, "question": "What datasets are used to evaluate the model? in Link Prediction using Embedded Knowledge Graphs", "answer": ["WN18 and FB15k"], "top_k_doc_id": [3633, 4160, 267, 270, 4981, 178, 182, 3632, 4161, 4490, 4164, 3414, 4163, 6579, 7800], "orig_top_k_doc_id": [4160, 3632, 4490, 4981, 182, 4161, 3633, 178, 267, 4164, 3414, 270, 4163, 6579, 7800]}, {"qid": 2306, "question": "What datasets are used to evaluate the model? in MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs", "answer": ["WN18 and FB15k"], "top_k_doc_id": [3633, 4160, 267, 270, 4981, 178, 182, 3632, 4161, 4490, 3677, 3672, 3676, 3673, 3634], "orig_top_k_doc_id": [3677, 3672, 3676, 3673, 3632, 4160, 267, 3633, 270, 4981, 4490, 4161, 182, 3634, 178]}, {"qid": 2477, "question": "What meta-information is being transferred? in Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs", "answer": ["high-order representation of a relation, loss gradient of relation meta"], "top_k_doc_id": [3633, 4160, 4161, 4162, 4163, 4164, 4165, 7166, 7167, 7231, 7283, 3571, 3568, 7230, 3572], "orig_top_k_doc_id": [4160, 4164, 4165, 4161, 4163, 4162, 7166, 7231, 7283, 3571, 7167, 3633, 3568, 7230, 3572]}, {"qid": 2478, "question": "What datasets are used to evaluate the approach? in Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs", "answer": ["NELL-One, Wiki-One"], "top_k_doc_id": [3633, 4160, 4161, 4162, 4163, 4164, 4165, 7166, 7167, 4216, 182, 3414, 3634, 4490, 3632], "orig_top_k_doc_id": [4160, 4164, 4165, 4161, 4163, 4162, 4216, 3633, 182, 3414, 3634, 4490, 3632, 7166, 7167]}]}
{"group_id": 160, "group_size": 8, "items": [{"qid": 222, "question": "Do they compare to other models? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["No"], "top_k_doc_id": [2717, 2922, 4526, 3358, 4412, 3823, 7493, 2117, 2116, 3357, 6320, 2301, 2547, 7664, 3637], "orig_top_k_doc_id": [4526, 4412, 3358, 3357, 7493, 2717, 2117, 3823, 6320, 2301, 2922, 2116, 2547, 7664, 3637]}, {"qid": 224, "question": "How long are expressions in layman's language? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["No"], "top_k_doc_id": [2717, 2922, 4526, 3358, 4412, 3823, 7493, 2117, 2116, 3357, 6399, 273, 7490, 2448, 7091], "orig_top_k_doc_id": [6399, 3358, 2717, 2117, 4412, 7493, 3823, 2922, 2116, 3357, 273, 4526, 7490, 2448, 7091]}, {"qid": 221, "question": "What dataset do they use? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["A parallel corpus where the source is an English expression of code and the target is Python code.", " text-code parallel corpus"], "top_k_doc_id": [2717, 2922, 4526, 3358, 4412, 3823, 7493, 2117, 273, 6943, 2910, 5769, 6593, 6320, 2301], "orig_top_k_doc_id": [2717, 2922, 273, 7493, 4412, 4526, 3358, 2117, 6943, 3823, 2910, 5769, 6593, 6320, 2301]}, {"qid": 220, "question": "What additional techniques are incorporated? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["No", "incorporating coding syntax tree model"], "top_k_doc_id": [2717, 2922, 4526, 274, 7847, 3426, 4212, 6399, 6943, 3705, 7493, 245, 4412, 273, 3358], "orig_top_k_doc_id": [3426, 4212, 2717, 2922, 274, 6943, 4526, 3705, 6399, 7493, 7847, 245, 4412, 273, 3358]}, {"qid": 223, "question": "What is the architecture of the system? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["seq2seq translation"], "top_k_doc_id": [2717, 2922, 4526, 3358, 4412, 3823, 7493, 6399, 3357, 4212, 6927, 5769, 7818, 3637, 1329], "orig_top_k_doc_id": [2922, 6399, 4526, 7493, 3358, 3357, 4412, 3823, 2717, 4212, 6927, 5769, 7818, 3637, 1329]}, {"qid": 225, "question": "What additional techniques could be incorporated to further improve accuracy? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["phrase-based word embedding, Abstract Syntax Tree(AST)"], "top_k_doc_id": [2717, 2922, 4526, 274, 7847, 3426, 4212, 6399, 6943, 7818, 7664, 7263, 353, 2293, 6770], "orig_top_k_doc_id": [7847, 2922, 3426, 6943, 6399, 2717, 7818, 7664, 4212, 4526, 274, 7263, 353, 2293, 6770]}, {"qid": 226, "question": "What programming language is target language? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["Python"], "top_k_doc_id": [2717, 2922, 4526, 274, 7847, 273, 2270, 6445, 3823, 2273, 353, 2730, 4412, 275, 3358], "orig_top_k_doc_id": [273, 274, 2270, 6445, 3823, 2273, 353, 2922, 2730, 2717, 4412, 275, 4526, 7847, 3358]}, {"qid": 227, "question": "What dataset is used to measure accuracy? in Machine Translation from Natural Language to Code using Long-Short Term Memory", "answer": ["validation data"], "top_k_doc_id": [2717, 2922, 4526, 3358, 4412, 6399, 4841, 353, 3637, 3071, 245, 5769, 6750, 6320, 3305], "orig_top_k_doc_id": [6399, 2922, 3358, 4526, 4841, 353, 3637, 3071, 4412, 245, 5769, 6750, 2717, 6320, 3305]}]}
{"group_id": 161, "group_size": 8, "items": [{"qid": 236, "question": "Are there datasets with relation tuples annotated, how big are datasets available? in Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction", "answer": ["Yes"], "top_k_doc_id": [3344, 295, 299, 301, 7251, 300, 1763, 2856, 2855, 5212, 1533, 296, 2859, 1853, 7610], "orig_top_k_doc_id": [295, 301, 299, 300, 3344, 2855, 5212, 1763, 1533, 296, 2856, 2859, 1853, 7251, 7610]}, {"qid": 237, "question": "Which one of two proposed approaches performed better in experiments? in Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction", "answer": ["WordDecoding (WDec) model"], "top_k_doc_id": [3344, 295, 299, 301, 7251, 1760, 5212, 7056, 7250, 7254, 2995, 7083, 6927, 7055, 300], "orig_top_k_doc_id": [301, 295, 7251, 2995, 3344, 1760, 7083, 6927, 299, 5212, 7055, 7254, 7056, 7250, 300]}, {"qid": 238, "question": "What is previous work authors reffer to? in Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction", "answer": ["SPTree, Tagging, CopyR, HRL, GraphR, N-gram Attention"], "top_k_doc_id": [3344, 295, 299, 301, 7251, 1760, 5212, 7056, 7250, 7254, 7255, 467, 7351, 7553, 1763], "orig_top_k_doc_id": [295, 301, 299, 7255, 7251, 7254, 3344, 5212, 467, 7056, 1760, 7351, 7250, 7553, 1763]}, {"qid": 239, "question": "How higher are F1 scores compared to previous work? in Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction", "answer": ["WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively", "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, In the ensemble scenario, compared to HRL, WDec achieves $4.2\\%$ and $3.5\\%$ higher F1 scores"], "top_k_doc_id": [3344, 295, 299, 301, 7251, 300, 1763, 2856, 7254, 1300, 5496, 3348, 6738, 7056, 6968], "orig_top_k_doc_id": [301, 299, 300, 1763, 295, 7254, 1300, 7251, 5496, 3348, 6738, 7056, 3344, 2856, 6968]}, {"qid": 2180, "question": "Do they repot results only on English data? in Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "answer": ["Yes"], "top_k_doc_id": [3344, 1760, 1763, 3345, 3348, 7254, 1764, 2694, 4601, 4602, 7609, 3540, 7055, 3538, 5675], "orig_top_k_doc_id": [3344, 3348, 3345, 3540, 1763, 1760, 4601, 2694, 4602, 7254, 7055, 3538, 7609, 5675, 1764]}, {"qid": 2181, "question": "What were the variables in the ablation study? in Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "answer": ["(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind"], "top_k_doc_id": [3344, 1760, 1763, 3345, 3348, 7254, 1764, 2694, 4601, 4602, 7609, 3346, 6113, 300, 4357], "orig_top_k_doc_id": [3348, 3344, 3345, 1760, 7254, 1764, 4602, 4601, 3346, 2694, 7609, 6113, 1763, 300, 4357]}, {"qid": 2182, "question": "How many shared layers are in the system? in Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "answer": ["1"], "top_k_doc_id": [3344, 1760, 1763, 3345, 3348, 7254, 3346, 3349, 3538, 3540, 7250, 7251, 4601, 7253, 3541], "orig_top_k_doc_id": [3348, 3344, 3345, 1760, 3346, 7254, 7250, 3538, 3540, 7251, 1763, 4601, 7253, 3349, 3541]}, {"qid": 2183, "question": "How many additional task-specific layers are introduced? in Deeper Task-Specificity Improves Joint Entity and Relation Extraction", "answer": ["2 for the ADE dataset and 3 for the CoNLL04 dataset"], "top_k_doc_id": [3344, 1760, 1763, 3345, 3348, 7254, 3346, 3349, 3538, 3540, 7250, 7251, 7055, 7056, 1603], "orig_top_k_doc_id": [3348, 3344, 3345, 3346, 1760, 7254, 3540, 1763, 3538, 7250, 7251, 7055, 3349, 7056, 1603]}]}
{"group_id": 162, "group_size": 8, "items": [{"qid": 418, "question": "Are other pretrained language models also evaluated for contextual augmentation?  in Conditional BERT Contextual Augmentation", "answer": ["No"], "top_k_doc_id": [5693, 485, 486, 487, 488, 489, 4329, 5501, 5692, 6925, 5690, 5500, 4649, 5694, 5231], "orig_top_k_doc_id": [485, 488, 487, 489, 5690, 5692, 5693, 6925, 5501, 5500, 4649, 486, 5694, 4329, 5231]}, {"qid": 419, "question": "Do the authors report performance of conditional bert on tasks without data augmentation? in Conditional BERT Contextual Augmentation", "answer": ["Yes"], "top_k_doc_id": [5693, 485, 486, 487, 488, 489, 4329, 5501, 5692, 6925, 5690, 5500, 4649, 2446, 7818], "orig_top_k_doc_id": [488, 485, 487, 489, 5693, 486, 5692, 5690, 6925, 4329, 5501, 5500, 4649, 2446, 7818]}, {"qid": 415, "question": "On what datasets is the new model evaluated on? in Conditional BERT Contextual Augmentation", "answer": ["SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types"], "top_k_doc_id": [5693, 485, 486, 487, 488, 489, 4329, 5501, 5692, 6925, 5690, 5500, 2746, 5694, 6926], "orig_top_k_doc_id": [488, 487, 489, 485, 6925, 5500, 5690, 486, 5692, 5501, 5693, 4329, 2746, 5694, 6926]}, {"qid": 416, "question": "How do the authors measure performance? in Conditional BERT Contextual Augmentation", "answer": ["Accuracy across six datasets"], "top_k_doc_id": [5693, 485, 486, 487, 488, 489, 4329, 5501, 5692, 6925, 5690, 4649, 4330, 2052, 4333], "orig_top_k_doc_id": [488, 485, 489, 487, 4329, 5692, 4649, 5690, 4330, 6925, 486, 5501, 5693, 2052, 4333]}, {"qid": 2744, "question": "How are weights dynamically adjusted? in Dice Loss for Data-imbalanced NLP Tasks", "answer": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "top_k_doc_id": [5693, 3691, 5690, 5694, 4788, 5498, 5499, 6917, 4789, 4791, 4792, 2982, 4790, 1149, 5264], "orig_top_k_doc_id": [4788, 4789, 4791, 4792, 5694, 2982, 5499, 5498, 4790, 1149, 3691, 6917, 5264, 5690, 5693]}, {"qid": 3441, "question": "How much is classification performance improved in experiments for low data regime and class-imbalance problems? in Learning Data Manipulation for Augmentation and Weighting", "answer": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "top_k_doc_id": [5693, 3691, 5690, 5694, 4788, 5498, 5499, 6917, 5695, 5692, 5501, 5691, 586, 6178, 5263], "orig_top_k_doc_id": [5690, 5695, 5694, 5693, 5692, 5501, 5691, 5499, 586, 4788, 3691, 5498, 6178, 6917, 5263]}, {"qid": 417, "question": "Does the new objective perform better than the original objective bert is trained on? in Conditional BERT Contextual Augmentation", "answer": ["Yes"], "top_k_doc_id": [5693, 485, 486, 487, 488, 489, 4329, 5501, 5692, 6925, 5232, 7118, 3499, 5540, 5100], "orig_top_k_doc_id": [485, 487, 488, 486, 489, 5692, 5232, 5501, 5693, 7118, 4329, 3499, 5540, 5100, 6925]}, {"qid": 3442, "question": "What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted? in Learning Data Manipulation for Augmentation and Weighting", "answer": ["BIBREF7", " reward learning algorithm BIBREF7"], "top_k_doc_id": [5693, 3691, 5690, 5694, 5695, 5692, 5691, 6689, 7843, 7842, 7444, 7447, 5744, 7446, 6688], "orig_top_k_doc_id": [5695, 5690, 5692, 5691, 5693, 5694, 6689, 7843, 7842, 7444, 3691, 7447, 5744, 7446, 6688]}]}
{"group_id": 163, "group_size": 8, "items": [{"qid": 490, "question": "What was the baseline? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["No"], "top_k_doc_id": [579, 580, 581, 582, 3797, 3135, 5470, 7530, 6006, 243, 6005, 2077, 3878, 5928, 6458], "orig_top_k_doc_id": [581, 582, 579, 3797, 6005, 7530, 243, 3135, 6006, 2077, 5470, 580, 3878, 5928, 6458]}, {"qid": 491, "question": "Are the tweets specific to a region? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["No"], "top_k_doc_id": [579, 580, 581, 582, 3797, 3135, 5470, 7530, 6006, 243, 6005, 3066, 6805, 1481, 5085], "orig_top_k_doc_id": [581, 582, 579, 243, 6005, 5470, 3066, 3797, 3135, 6805, 7530, 6006, 1481, 580, 5085]}, {"qid": 489, "question": "How many categories are there? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["No"], "top_k_doc_id": [579, 580, 581, 582, 3797, 3135, 5470, 7530, 6006, 243, 5135, 5085, 5627, 59, 2077], "orig_top_k_doc_id": [581, 582, 579, 7530, 3135, 243, 5470, 5135, 6006, 5085, 5627, 59, 3797, 2077, 580]}, {"qid": 485, "question": "Is the Android application publicly available? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["No"], "top_k_doc_id": [579, 580, 581, 582, 3797, 3135, 5470, 7530, 6006, 243, 5391, 2077, 6074, 343, 6155], "orig_top_k_doc_id": [581, 582, 579, 580, 5391, 3797, 6006, 7530, 2077, 243, 3135, 6074, 343, 6155, 5470]}, {"qid": 484, "question": "Is the web interface publicly accessible? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["No"], "top_k_doc_id": [579, 580, 581, 582, 3797, 3135, 5470, 7530, 6006, 6140, 6005, 3667, 251, 4302, 3878], "orig_top_k_doc_id": [581, 582, 579, 580, 7530, 6140, 6005, 3667, 6006, 5470, 3797, 3135, 251, 4302, 3878]}, {"qid": 486, "question": "What classifier is used for emergency categorization? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["multi-class Naive Bayes"], "top_k_doc_id": [579, 580, 581, 582, 3797, 4280, 4780, 5085, 6005, 6006, 4287, 7530, 2343, 4119, 3135], "orig_top_k_doc_id": [581, 582, 579, 6005, 580, 6006, 4287, 4780, 3797, 4280, 5085, 7530, 2343, 4119, 3135]}, {"qid": 487, "question": "What classifier is used for emergency detection? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["SVM"], "top_k_doc_id": [579, 580, 581, 582, 3797, 4280, 4780, 5085, 6005, 6006, 4279, 5928, 3736, 4282, 59], "orig_top_k_doc_id": [581, 582, 579, 580, 6005, 4280, 6006, 4279, 5928, 4780, 3797, 3736, 4282, 59, 5085]}, {"qid": 488, "question": "Do the tweets come from any individual? in Civique: Using Social Media to Detect Urban Emergencies", "answer": ["Yes"], "top_k_doc_id": [579, 580, 581, 582, 3797, 3135, 5470, 7530, 243, 7029, 521, 6805, 4139, 5144, 4948], "orig_top_k_doc_id": [581, 582, 579, 7530, 3135, 243, 7029, 5470, 521, 3797, 6805, 4139, 5144, 580, 4948]}]}
{"group_id": 164, "group_size": 8, "items": [{"qid": 531, "question": "How does the word segmentation method work? in Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation", "answer": ["morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12"], "top_k_doc_id": [648, 650, 649, 1583, 4472, 6782, 1374, 4849, 4850, 4851, 4471, 4852, 3253, 1370, 6280], "orig_top_k_doc_id": [650, 648, 649, 6782, 1583, 4472, 4850, 4849, 4471, 4851, 4852, 3253, 1370, 1374, 6280]}, {"qid": 532, "question": "Is the word segmentation method independently evaluated? in Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [648, 650, 649, 1583, 4472, 6782, 1374, 4849, 4850, 4851, 4471, 4852, 2587, 6767, 4848], "orig_top_k_doc_id": [648, 650, 6782, 649, 1583, 4472, 4849, 4850, 2587, 4851, 4852, 1374, 4471, 6767, 4848]}, {"qid": 529, "question": "How many linguistic and semantic features are learned? in Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [648, 650, 649, 1583, 4472, 6782, 1374, 4849, 4850, 4851, 5965, 6280, 5836, 5835, 659], "orig_top_k_doc_id": [648, 650, 1583, 6782, 1374, 4472, 649, 5965, 6280, 4849, 4851, 5836, 4850, 5835, 659]}, {"qid": 530, "question": "How is morphology knowledge implemented in the method? in Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation", "answer": ["A BPE model is applied to the stem after morpheme segmentation."], "top_k_doc_id": [648, 650, 649, 1583, 4472, 6782, 659, 6280, 4852, 4471, 923, 660, 655, 925, 920], "orig_top_k_doc_id": [650, 648, 649, 1583, 659, 6782, 4472, 6280, 4852, 4471, 923, 660, 655, 925, 920]}, {"qid": 2769, "question": "By how much do they improve the efficacy of the attention mechanism? in Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages", "answer": ["No", "No"], "top_k_doc_id": [648, 650, 4846, 4849, 4851, 4852, 5026, 6215, 7825, 6216, 1583, 4298, 5836, 7046, 649], "orig_top_k_doc_id": [4851, 4852, 6216, 4846, 6215, 650, 1583, 648, 5026, 4298, 7825, 4849, 5836, 7046, 649]}, {"qid": 2770, "question": "How were the human judgements assembled? in Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages", "answer": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "top_k_doc_id": [648, 650, 4846, 4849, 4851, 4852, 5026, 6215, 7825, 7342, 5985, 7353, 982, 5983, 48], "orig_top_k_doc_id": [4852, 4846, 4851, 4849, 650, 5026, 7342, 6215, 648, 7825, 5985, 7353, 982, 5983, 48]}, {"qid": 1180, "question": "What are the three languages studied in the paper? in A Latent Morphology Model for Open-Vocabulary Neural Machine Translation", "answer": ["Arabic, Czech and Turkish"], "top_k_doc_id": [648, 650, 1584, 6251, 7509, 1583, 4864, 697, 659, 6782, 5841, 920, 273, 6498, 919], "orig_top_k_doc_id": [1583, 1584, 650, 648, 4864, 697, 659, 6782, 5841, 920, 273, 7509, 6498, 6251, 919]}, {"qid": 3871, "question": "Is there a difference between the model's performance for morphologically impoverished and morphologically complex languages? in A Joint Model for Word Embedding and Word Morphology", "answer": ["They did not report results for English but expect that morphologically complex languages will perform better.", "No", "Yes"], "top_k_doc_id": [648, 650, 1584, 6251, 7509, 6255, 656, 6279, 4472, 6254, 6215, 4852, 7318, 7512, 6165], "orig_top_k_doc_id": [6255, 7509, 650, 656, 6279, 4472, 6254, 6215, 648, 4852, 7318, 1584, 7512, 6251, 6165]}]}
{"group_id": 165, "group_size": 8, "items": [{"qid": 561, "question": "What are the performance metrics used? in Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation", "answer": ["joint goal accuracy"], "top_k_doc_id": [1738, 680, 3193, 3507, 3679, 573, 684, 705, 3190, 685, 3359, 3683, 7371, 7374, 3192], "orig_top_k_doc_id": [680, 3193, 684, 3190, 573, 1738, 3679, 685, 7371, 705, 3507, 7374, 3683, 3359, 3192]}, {"qid": 562, "question": "Which datasets are used to evaluate performance? in Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation", "answer": ["the single domain dataset, WoZ2.0 , the multi-domain dataset, MultiWoZ"], "top_k_doc_id": [1738, 680, 3193, 3507, 3679, 573, 684, 705, 3190, 685, 3359, 3683, 7371, 7374, 1736], "orig_top_k_doc_id": [680, 684, 3679, 3190, 3193, 7371, 7374, 1738, 685, 3683, 705, 573, 3507, 3359, 1736]}, {"qid": 560, "question": "Does this approach perform better in the multi-domain or single-domain setting? in Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation", "answer": ["single-domain setting"], "top_k_doc_id": [1738, 680, 3193, 3507, 3679, 573, 684, 705, 3190, 196, 2276, 3508, 401, 2227, 2234], "orig_top_k_doc_id": [680, 684, 3507, 3193, 3679, 196, 705, 2276, 3508, 401, 1738, 573, 3190, 2227, 2234]}, {"qid": 1266, "question": "What network architecture do they use for SIM? in SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "answer": ["convolutional neural networks (CNN)"], "top_k_doc_id": [1738, 684, 1736, 2234, 2554, 2555, 7584, 7588, 199, 1737, 3507, 3508, 7585, 2550, 2228], "orig_top_k_doc_id": [1738, 1736, 7588, 2234, 7584, 1737, 2555, 199, 2554, 3507, 7585, 2550, 684, 3508, 2228]}, {"qid": 1267, "question": "How do they measure model size? in SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "answer": ["By the number of parameters."], "top_k_doc_id": [1738, 684, 1736, 2234, 2554, 2555, 7584, 7588, 199, 1737, 3507, 3508, 2227, 7586, 2276], "orig_top_k_doc_id": [1738, 1736, 7588, 1737, 7584, 2234, 2554, 3507, 684, 2555, 2227, 3508, 7586, 199, 2276]}, {"qid": 4886, "question": "Which part of their architecture provides the most speedup in comparison to existing approaches? in Scalable Neural Dialogue State Tracking", "answer": ["No", "No"], "top_k_doc_id": [1738, 680, 3193, 3507, 3679, 1736, 2233, 2234, 2276, 3508, 3683, 3684, 7584, 7585, 196], "orig_top_k_doc_id": [3679, 2276, 3193, 3684, 7584, 2234, 3507, 680, 7585, 196, 1738, 2233, 3508, 1736, 3683]}, {"qid": 4887, "question": "Do they consistently outperform existing systems in terms of accuracy? in Scalable Neural Dialogue State Tracking", "answer": ["No", "No"], "top_k_doc_id": [1738, 680, 3193, 3507, 3679, 1736, 2233, 2234, 2276, 3508, 3683, 3684, 7584, 684, 1987], "orig_top_k_doc_id": [3507, 1738, 3679, 2276, 1736, 684, 2234, 2233, 3193, 3684, 3683, 680, 7584, 3508, 1987]}, {"qid": 1265, "question": "How do they prevent the model complexity increasing with the increased number of slots? in SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "answer": ["They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN)."], "top_k_doc_id": [1738, 684, 1736, 2234, 2554, 2555, 7584, 7588, 3679, 2276, 2277, 7585, 7586, 2278, 680], "orig_top_k_doc_id": [1736, 7584, 1738, 7588, 684, 3679, 2276, 2277, 7585, 2234, 7586, 2278, 680, 2555, 2554]}]}
{"group_id": 166, "group_size": 8, "items": [{"qid": 595, "question": "How better are results compared to baseline models? in The Role of Pragmatic and Discourse Context in Determining Argument Impact", "answer": ["F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61."], "top_k_doc_id": [3143, 5380, 742, 743, 746, 5383, 5388, 5389, 745, 2957, 4782, 4783, 864, 744, 7350], "orig_top_k_doc_id": [742, 743, 746, 5389, 4783, 745, 2957, 3143, 4782, 864, 5380, 5383, 5388, 744, 7350]}, {"qid": 597, "question": "How is pargmative and discourse context added to the dataset? in The Role of Pragmatic and Discourse Context in Determining Argument Impact", "answer": ["While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument."], "top_k_doc_id": [3143, 5380, 742, 743, 746, 5383, 5388, 5389, 745, 2957, 4782, 4783, 4632, 5375, 1905], "orig_top_k_doc_id": [742, 743, 746, 4782, 3143, 745, 5380, 4632, 4783, 2957, 5389, 5388, 5383, 5375, 1905]}, {"qid": 2068, "question": "what experiments are conducted? in Annotating Student Talk in Text-based Classroom Discussions", "answer": ["a reliability study for the proposed scheme "], "top_k_doc_id": [3143, 5380, 2335, 3144, 3145, 4619, 5613, 739, 2389, 4242, 4243, 7223, 3627, 4244, 5376], "orig_top_k_doc_id": [3145, 3143, 3144, 4243, 4619, 2335, 2389, 3627, 4244, 739, 5380, 5376, 4242, 7223, 5613]}, {"qid": 2069, "question": "what opportunities are highlighted? in Annotating Student Talk in Text-based Classroom Discussions", "answer": ["Our annotation scheme introduces opportunities for the educational community to conduct further research , Once automated classifiers are developed, such relations between talk and learning can be examined at scale,  automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potentially lead to automated tools for teachers and students, collecting and annotating corpora that can be used by the NLP community to advance the field in this particular area"], "top_k_doc_id": [3143, 5380, 2335, 3144, 3145, 4619, 5613, 739, 2389, 4242, 4243, 7223, 3593, 2861, 5722], "orig_top_k_doc_id": [3145, 3143, 3144, 2335, 3593, 4619, 4243, 739, 7223, 5613, 5380, 4242, 2389, 2861, 5722]}, {"qid": 596, "question": "What models that rely only on claim-specific linguistic features are used as baselines? in The Role of Pragmatic and Discourse Context in Determining Argument Impact", "answer": ["SVM with RBF kernel"], "top_k_doc_id": [3143, 5380, 742, 743, 746, 5383, 5388, 5389, 745, 744, 5375, 5384, 5537, 3144, 5386], "orig_top_k_doc_id": [742, 743, 746, 745, 744, 5375, 3143, 5383, 5389, 5384, 5380, 5537, 3144, 5386, 5388]}, {"qid": 2070, "question": "how do they measure discussion quality? in Annotating Student Talk in Text-based Classroom Discussions", "answer": ["Measuring three aspects: argumentation, specificity and knowledge domain."], "top_k_doc_id": [3143, 5380, 2335, 3144, 3145, 4619, 5613, 739, 2389, 2861, 7857, 3593, 444, 442, 5379], "orig_top_k_doc_id": [3145, 3143, 3144, 5613, 739, 2861, 7857, 2335, 5380, 3593, 444, 2389, 442, 4619, 5379]}, {"qid": 598, "question": "What annotations are available in the dataset? in The Role of Pragmatic and Discourse Context in Determining Argument Impact", "answer": ["5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact"], "top_k_doc_id": [3143, 5380, 742, 743, 746, 5383, 5388, 5389, 2957, 5375, 864, 3145, 898, 781, 4345], "orig_top_k_doc_id": [742, 743, 5383, 2957, 5389, 746, 5375, 3143, 864, 5380, 3145, 5388, 898, 781, 4345]}, {"qid": 2071, "question": "do they use a crowdsourcing platform? in Annotating Student Talk in Text-based Classroom Discussions", "answer": ["No"], "top_k_doc_id": [3143, 5380, 2335, 3144, 3145, 4619, 5613, 7222, 3587, 5722, 3627, 2861, 5949, 5038, 4243], "orig_top_k_doc_id": [3145, 3143, 3144, 7222, 3587, 5722, 5613, 3627, 5380, 2335, 4619, 2861, 5949, 5038, 4243]}]}
{"group_id": 167, "group_size": 8, "items": [{"qid": 638, "question": "How much more coverage is in the new dataset? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["278 more annotations"], "top_k_doc_id": [780, 781, 782, 6142, 3150, 2919, 5720, 5965, 862, 861, 2920, 2921, 5718, 1423, 328], "orig_top_k_doc_id": [782, 780, 781, 3150, 2920, 862, 2921, 5720, 2919, 861, 5965, 5718, 6142, 1423, 328]}, {"qid": 645, "question": "How big is the dataset? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["1593 annotations"], "top_k_doc_id": [780, 781, 782, 6142, 3150, 2919, 5720, 5965, 862, 861, 2920, 2921, 1109, 1422, 325], "orig_top_k_doc_id": [782, 780, 781, 2919, 5965, 2920, 6142, 862, 3150, 5720, 1109, 1422, 861, 2921, 325]}, {"qid": 641, "question": "How was the corpus obtained? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": [" trained annotators BIBREF4, crowdsourcing BIBREF5 "], "top_k_doc_id": [780, 781, 782, 6142, 3150, 2919, 5720, 5965, 862, 861, 5357, 5718, 325, 6763, 10], "orig_top_k_doc_id": [780, 782, 781, 5720, 862, 3150, 5357, 5965, 5718, 2919, 325, 6142, 861, 6763, 10]}, {"qid": 639, "question": "How was coverage measured? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["QA pairs per predicate"], "top_k_doc_id": [780, 781, 782, 6142, 3150, 2919, 5720, 5965, 584, 1422, 2920, 5331, 7864, 862, 325], "orig_top_k_doc_id": [780, 782, 781, 6142, 862, 5965, 5720, 3150, 5331, 7864, 2919, 1422, 2920, 584, 325]}, {"qid": 640, "question": "How was quality measured? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations."], "top_k_doc_id": [780, 781, 782, 6142, 3150, 2919, 5720, 5965, 584, 1422, 2920, 5331, 7864, 3225, 2969], "orig_top_k_doc_id": [780, 782, 781, 6142, 5720, 5965, 5331, 7864, 2919, 3150, 1422, 2920, 584, 3225, 2969]}, {"qid": 644, "question": "How was the previous dataset annotated? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["the annotation machinery of BIBREF5"], "top_k_doc_id": [780, 781, 782, 6142, 3150, 2919, 5720, 5965, 862, 584, 585, 10, 5357, 328, 6805], "orig_top_k_doc_id": [780, 782, 781, 6142, 584, 585, 862, 10, 5720, 3150, 5965, 2919, 5357, 328, 6805]}, {"qid": 643, "question": "What is different in the improved annotation protocol? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["a trained worker consolidates existing annotations "], "top_k_doc_id": [780, 781, 782, 6142, 3150, 5718, 862, 861, 10, 1056, 5037, 7823, 3623, 5145, 6808], "orig_top_k_doc_id": [780, 781, 782, 5718, 862, 861, 10, 6142, 1056, 5037, 7823, 3623, 5145, 6808, 3150]}, {"qid": 642, "question": "How are workers trained? in Crowdsourcing a High-Quality Gold Standard for QA-SRL", "answer": ["extensive personal feedback"], "top_k_doc_id": [780, 781, 782, 6142, 5720, 584, 585, 6805, 6806, 5039, 6143, 6140, 5038, 3225, 1244], "orig_top_k_doc_id": [780, 782, 781, 6142, 5720, 584, 585, 6805, 6806, 5039, 6143, 6140, 5038, 3225, 1244]}]}
{"group_id": 168, "group_size": 8, "items": [{"qid": 660, "question": "What is the baseline? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["random method , LSTM "], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 1277, 4781, 5385, 742, 4780, 5374, 5378, 5377, 4942], "orig_top_k_doc_id": [807, 809, 808, 811, 812, 810, 1277, 5385, 5374, 4780, 742, 4781, 5377, 5378, 4942]}, {"qid": 663, "question": "What metrics are used in evaluation of this task? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["F1 score"], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 1277, 4781, 5385, 742, 4780, 5374, 5378, 5377, 1692], "orig_top_k_doc_id": [807, 809, 808, 811, 812, 810, 5377, 1277, 742, 4780, 5385, 1692, 5374, 5378, 4781]}, {"qid": 662, "question": "What are overall baseline results on new this new task? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["all of our models outperform the random baseline by a wide margin, he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)"], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 1277, 4781, 5385, 742, 4780, 5374, 5378, 329, 4942], "orig_top_k_doc_id": [807, 809, 811, 808, 810, 812, 5374, 742, 329, 5378, 4780, 1277, 4781, 4942, 5385]}, {"qid": 661, "question": "What are their proposed features? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties"], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 1277, 4781, 5385, 742, 4780, 5374, 5377, 3863, 5386], "orig_top_k_doc_id": [807, 809, 811, 808, 812, 810, 5377, 1277, 5385, 742, 4780, 5374, 3863, 5386, 4781]}, {"qid": 665, "question": "What features are proposed? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC, General OP/PC properties"], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 1277, 4781, 5385, 742, 4780, 5374, 5377, 3863, 5386], "orig_top_k_doc_id": [807, 809, 811, 808, 812, 810, 5377, 1277, 742, 4780, 5385, 5374, 3863, 4781, 5386]}, {"qid": 659, "question": "What non-contextual properties do they refer to? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["These features are derived directly from the word and capture the general tendency of a word being echoed in explanations."], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 1277, 4781, 5385, 5377, 5379, 5378, 3863, 253, 5384], "orig_top_k_doc_id": [807, 809, 811, 808, 812, 810, 5377, 1277, 5379, 5385, 5378, 3863, 253, 4781, 5384]}, {"qid": 664, "question": "Do authors provide any explanation for intriguing patterns of word being echoed? in What Gets Echoed? Understanding the\"Pointers\"in Explanations of Persuasive Arguments", "answer": ["No"], "top_k_doc_id": [807, 808, 809, 810, 811, 812, 7074, 3589, 7073, 4780, 253, 252, 5380, 4497, 3532], "orig_top_k_doc_id": [807, 809, 811, 808, 812, 810, 7074, 3589, 7073, 4780, 253, 252, 5380, 4497, 3532]}, {"qid": 2564, "question": "what are the three methods presented in the paper? in Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation Generation", "answer": ["Optimized TF-IDF, iterated TF-IDF, BERT re-ranking."], "top_k_doc_id": [807, 808, 4497, 4496, 2181, 4831, 3789, 914, 4631, 1930, 3175, 2618, 7154, 1410, 7514], "orig_top_k_doc_id": [4497, 4496, 2181, 4831, 808, 3789, 914, 4631, 807, 1930, 3175, 2618, 7154, 1410, 7514]}]}
{"group_id": 169, "group_size": 8, "items": [{"qid": 711, "question": "What evaluation metric is used? in MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "answer": ["The BLEU metric "], "top_k_doc_id": [891, 890, 888, 889, 2906, 1411, 2528, 4973, 5682, 7510, 1670, 6928, 3181, 3749, 4570], "orig_top_k_doc_id": [890, 891, 888, 889, 2906, 7510, 1670, 2528, 6928, 3181, 1411, 4973, 5682, 3749, 4570]}, {"qid": 712, "question": "What datasets are used? in MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "answer": ["WMT14 En-Fr and En-De datasets, IWSLT De-En and En-Vi datasets"], "top_k_doc_id": [891, 890, 888, 889, 2906, 1411, 2528, 4973, 5682, 5681, 6035, 1744, 4754, 1768, 1048], "orig_top_k_doc_id": [890, 891, 888, 889, 2906, 5681, 2528, 6035, 1411, 4973, 5682, 1744, 4754, 1768, 1048]}, {"qid": 713, "question": "What are three main machine translation tasks? in MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "answer": ["De-En, En-Fr and En-Vi translation tasks"], "top_k_doc_id": [891, 890, 888, 889, 2906, 1768, 2998, 7825, 5841, 3416, 5681, 1743, 6291, 1048, 4415], "orig_top_k_doc_id": [891, 890, 888, 889, 1768, 2906, 2998, 7825, 5841, 3416, 5681, 1743, 6291, 1048, 4415]}, {"qid": 126, "question": "How they measure robustness in experiments? in A simple discriminative training method for machine translation with large-scale features", "answer": ["We empirically provide a formula to measure the richness in the scenario of machine translation.", "boost the training BLEU very greatly, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$"], "top_k_doc_id": [891, 3617, 7658, 2162, 3567, 5871, 7661, 1238, 6616, 2375, 3918, 4206, 1729, 3686, 4312], "orig_top_k_doc_id": [3617, 5871, 1238, 7658, 6616, 2375, 7661, 3918, 2162, 4206, 1729, 3567, 3686, 891, 4312]}, {"qid": 127, "question": "Is new method inferior in terms of robustness to MIRAs in experiments? in A simple discriminative training method for machine translation with large-scale features", "answer": ["No"], "top_k_doc_id": [891, 3617, 7658, 2162, 3567, 5871, 7661, 147, 627, 5478, 84, 500, 3640, 3030, 6560], "orig_top_k_doc_id": [147, 3617, 627, 7658, 5871, 5478, 3567, 84, 500, 3640, 891, 2162, 7661, 3030, 6560]}, {"qid": 714, "question": "How big is improvement in performance over Transformers? in MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "answer": ["2.2 BLEU gains"], "top_k_doc_id": [891, 890, 888, 889, 3002, 3781, 1768, 4415, 3001, 5411, 1138, 651, 4760, 7056, 7847], "orig_top_k_doc_id": [890, 891, 888, 3002, 3781, 889, 1768, 4415, 3001, 5411, 1138, 651, 4760, 7056, 7847]}, {"qid": 128, "question": "What experiments with large-scale features are performed? in A simple discriminative training method for machine translation with large-scale features", "answer": ["Plackett-Luce Model for SMT Reranking"], "top_k_doc_id": [891, 3617, 7658, 4731, 4918, 1238, 1185, 2375, 6021, 2836, 890, 3640, 2528, 4412, 3918], "orig_top_k_doc_id": [4731, 3617, 891, 4918, 1238, 1185, 2375, 6021, 2836, 7658, 890, 3640, 2528, 4412, 3918]}, {"qid": 4548, "question": "Do they evaluate whether local or global context proves more important? in Contextual Encoding for Translation Quality Estimation", "answer": ["No", "No"], "top_k_doc_id": [891, 890, 7108, 214, 209, 5218, 3253, 4847, 5217, 4827, 989, 4829, 2983, 4828, 4553], "orig_top_k_doc_id": [891, 7108, 214, 209, 5218, 3253, 4847, 890, 5217, 4827, 989, 4829, 2983, 4828, 4553]}]}
{"group_id": 170, "group_size": 8, "items": [{"qid": 822, "question": "What dataset do they use for experiments? in Controlling the Output Length of Neural Machine Translation", "answer": ["English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)"], "top_k_doc_id": [1028, 28, 1024, 7339, 379, 4615, 6238, 274, 1244, 1371, 2135, 2491, 4814, 6943, 6612], "orig_top_k_doc_id": [28, 6238, 1024, 6943, 1028, 1244, 1371, 274, 4615, 4814, 379, 2491, 6612, 2135, 7339]}, {"qid": 826, "question": "What dataset do they use? in Controlling the Output Length of Neural Machine Translation", "answer": ["English$\\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)"], "top_k_doc_id": [1028, 28, 1024, 7339, 379, 4615, 6238, 274, 1244, 1371, 2135, 2491, 4814, 6943, 117], "orig_top_k_doc_id": [28, 1024, 6238, 1028, 274, 6943, 379, 1244, 1371, 2135, 7339, 4615, 117, 2491, 4814]}, {"qid": 824, "question": "How do they condition the output to a given target-source class? in Controlling the Output Length of Neural Machine Translation", "answer": ["They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."], "top_k_doc_id": [1028, 28, 1024, 7339, 379, 4615, 6238, 274, 1027, 7617, 2188, 1026, 3821, 2373, 3820], "orig_top_k_doc_id": [1024, 1028, 1027, 379, 28, 7617, 6238, 274, 2188, 1026, 7339, 4615, 3821, 2373, 3820]}, {"qid": 825, "question": "Which languages do they focus on? in Controlling the Output Length of Neural Machine Translation", "answer": ["two translation directions (En-It and En-De)"], "top_k_doc_id": [1028, 28, 1024, 7339, 379, 4615, 6238, 1347, 1370, 5835, 774, 7366, 7190, 4766, 1371], "orig_top_k_doc_id": [28, 7339, 1024, 1347, 379, 6238, 1370, 4615, 5835, 774, 1028, 7366, 7190, 4766, 1371]}, {"qid": 827, "question": "Do they experiment with combining both methods? in Controlling the Output Length of Neural Machine Translation", "answer": ["Yes"], "top_k_doc_id": [1028, 28, 1024, 7339, 379, 4615, 5835, 1027, 7420, 6943, 3825, 4822, 7447, 2757, 2636], "orig_top_k_doc_id": [1024, 7339, 5835, 1027, 7420, 1028, 28, 6943, 3825, 4822, 7447, 2757, 2636, 379, 4615]}, {"qid": 821, "question": "Do they conduct any human evaluation? in Controlling the Output Length of Neural Machine Translation", "answer": ["Yes"], "top_k_doc_id": [1028, 28, 1024, 7339, 1245, 1244, 5835, 111, 6620, 3686, 2373, 6238, 1246, 2435, 1027], "orig_top_k_doc_id": [1028, 1245, 28, 1024, 1244, 5835, 111, 6620, 3686, 7339, 2373, 6238, 1246, 2435, 1027]}, {"qid": 823, "question": "How do they enrich the positional embedding with length information in Controlling the Output Length of Neural Machine Translation", "answer": ["They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."], "top_k_doc_id": [1028, 28, 1024, 6927, 1370, 7490, 2254, 2188, 3821, 1781, 1348, 6238, 124, 2253, 2940], "orig_top_k_doc_id": [1024, 1028, 6927, 1370, 28, 7490, 2254, 2188, 3821, 1781, 1348, 6238, 124, 2253, 2940]}, {"qid": 4550, "question": "How did their model rank in three CMU WMT2018 tracks it didn't rank first? in Contextual Encoding for Translation Quality Estimation", "answer": ["Second on De-En and En-De (NMT) tasks, and third on En-De (SMT) task.", "3rd in En-De (SMT), 2nd in En-De (NNT) and 2nd ibn De-En"], "top_k_doc_id": [1028, 1555, 6408, 7109, 5736, 7864, 6160, 3182, 7674, 6620, 1265, 6619, 7865, 5619, 3737], "orig_top_k_doc_id": [1555, 6408, 7109, 5736, 7864, 6160, 3182, 7674, 6620, 1265, 6619, 7865, 1028, 5619, 3737]}]}
{"group_id": 171, "group_size": 8, "items": [{"qid": 840, "question": "How is dialogue guided to avoid interactions that breach procedures and processes only known to experts? in CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "answer": ["pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction"], "top_k_doc_id": [1070, 1071, 1072, 1074, 1075, 3683, 1073, 3508, 3679, 5118, 7586, 3185, 2234, 199, 5426], "orig_top_k_doc_id": [1070, 1071, 1072, 1075, 1074, 3185, 3683, 3679, 1073, 2234, 199, 5118, 3508, 5426, 7586]}, {"qid": 841, "question": "What is meant by semiguided dialogue, what part of dialogue is guided? in CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "answer": ["The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard."], "top_k_doc_id": [1070, 1071, 1072, 1074, 1075, 3683, 1073, 3508, 3679, 2278, 6584, 2234, 2231, 1443, 199], "orig_top_k_doc_id": [1072, 1070, 1071, 1074, 1075, 1073, 3679, 3683, 3508, 2234, 2231, 2278, 1443, 199, 6584]}, {"qid": 842, "question": "Is CRWIZ already used for data collection, what are the results? in CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "answer": ["Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant."], "top_k_doc_id": [1070, 1071, 1072, 1074, 1075, 3683, 1073, 3508, 3679, 2278, 6584, 5118, 198, 5434, 898], "orig_top_k_doc_id": [1070, 1072, 1071, 1074, 1075, 1073, 5118, 3683, 198, 3679, 2278, 3508, 6584, 5434, 898]}, {"qid": 843, "question": "How does framework made sure that dialogue will not breach procedures? in CRWIZ: A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues", "answer": ["The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions."], "top_k_doc_id": [1070, 1071, 1072, 1074, 1075, 3683, 1073, 3508, 3679, 5118, 7586, 2278, 5440, 3359, 7379], "orig_top_k_doc_id": [1070, 1072, 1071, 1075, 1074, 3508, 1073, 3679, 2278, 5118, 5440, 3359, 7379, 3683, 7586]}, {"qid": 2653, "question": "How does the IPA label data after interacting with users? in Multipurpose Intelligent Process Automation via Conversational Assistant", "answer": ["It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).", "Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)"], "top_k_doc_id": [1070, 898, 899, 1706, 4663, 4669, 6583, 6584, 6876, 705, 900, 3772, 5118, 5425, 3891], "orig_top_k_doc_id": [4663, 899, 3772, 898, 6584, 4669, 5425, 705, 1706, 6583, 1070, 6876, 900, 5118, 3891]}, {"qid": 2654, "question": "What kind of repetitive and time-consuming activities does their assistant handle? in Multipurpose Intelligent Process Automation via Conversational Assistant", "answer": ["No", " What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation"], "top_k_doc_id": [1070, 898, 899, 1706, 4663, 4669, 6583, 6584, 6876, 705, 900, 3772, 5118, 4667, 1072], "orig_top_k_doc_id": [4663, 4669, 899, 1706, 6584, 900, 4667, 6876, 3772, 1072, 898, 6583, 705, 1070, 5118]}, {"qid": 2652, "question": "Do they use off-the-shelf NLP systems to build their assitant? in Multipurpose Intelligent Process Automation via Conversational Assistant", "answer": ["No", "No"], "top_k_doc_id": [1070, 898, 899, 1706, 4663, 4669, 6583, 6584, 6876, 228, 5425, 402, 2998, 6050, 1072], "orig_top_k_doc_id": [6584, 4663, 228, 4669, 6583, 899, 898, 5425, 402, 2998, 1070, 1706, 6050, 1072, 6876]}, {"qid": 2947, "question": "What is a wizard of oz setup? in Macaw: An Extensible Conversational Information Seeking Platform", "answer": ["seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions", "a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message"], "top_k_doc_id": [1070, 1071, 1072, 1074, 1075, 3683, 5118, 5119, 5120, 724, 898, 6851, 6584, 2278, 7226], "orig_top_k_doc_id": [5118, 5119, 5120, 1070, 1075, 724, 1074, 1072, 898, 6851, 6584, 2278, 1071, 3683, 7226]}]}
{"group_id": 172, "group_size": 8, "items": [{"qid": 882, "question": "How does new evaluation metric considers critical informative entities? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities"], "top_k_doc_id": [1132, 1134, 1135, 1255, 3715, 5142, 5554, 6862, 5079, 5557, 4308, 6840, 4478, 6928, 4826], "orig_top_k_doc_id": [1135, 1134, 1132, 1255, 5142, 5079, 5554, 5557, 4308, 6840, 3715, 4478, 6928, 6862, 4826]}, {"qid": 883, "question": "Is new evaluation metric extension of ROGUE? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["No"], "top_k_doc_id": [1132, 1134, 1135, 1255, 3715, 5142, 5554, 6862, 5140, 5141, 5143, 6574, 3718, 4424, 6573], "orig_top_k_doc_id": [1135, 1132, 1134, 5142, 5140, 5141, 5143, 6574, 1255, 3718, 4424, 6862, 3715, 5554, 6573]}, {"qid": 876, "question": "By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["SPNet vs best baseline:\nROUGE-1: 90.97 vs 90.68\nCIC: 70.45 vs 70.25"], "top_k_doc_id": [1132, 1134, 1135, 1138, 3160, 5142, 5554, 3715, 4482, 6840, 4829, 6496, 4760, 3157, 4304], "orig_top_k_doc_id": [1135, 1132, 1134, 3160, 3715, 5554, 5142, 1138, 4482, 6840, 4829, 6496, 4760, 3157, 4304]}, {"qid": 877, "question": "What automatic and human evaluation metrics are used to compare SPNet to its counterparts? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["ROUGE and CIC, relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair"], "top_k_doc_id": [1132, 1134, 1135, 1138, 3160, 5142, 5554, 6927, 1133, 1969, 1971, 3231, 5920, 1970, 2595], "orig_top_k_doc_id": [1135, 1132, 1134, 6927, 1133, 1969, 1138, 1971, 3231, 5920, 3160, 1970, 2595, 5142, 5554]}, {"qid": 878, "question": "Is proposed abstractive dialog summarization dataset open source? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["No"], "top_k_doc_id": [1132, 1134, 1135, 1255, 3715, 4424, 7137, 4307, 3157, 4478, 7134, 4304, 5544, 7762, 1970], "orig_top_k_doc_id": [1132, 1135, 1134, 4424, 7137, 4307, 3157, 4478, 1255, 7134, 3715, 4304, 5544, 7762, 1970]}, {"qid": 881, "question": "What are previous state-of-the-art document summarization methods used? in Abstractive Dialog Summarization with Semantic Scaffolds", "answer": ["Pointer-Generator, Transformer"], "top_k_doc_id": [1132, 3715, 4478, 5554, 6716, 6927, 5540, 6571, 1135, 1134, 4760, 3202, 7242, 5804, 2334], "orig_top_k_doc_id": [1132, 1135, 1134, 4760, 3202, 5554, 3715, 5540, 6571, 6927, 4478, 7242, 5804, 2334, 6716]}, {"qid": 4112, "question": "Do they compare against state-of-the-art summarization approaches? in Using Statistical and Semantic Models for Multi-Document Summarization", "answer": ["Yes", "No"], "top_k_doc_id": [1132, 3715, 4478, 5554, 6716, 6927, 5540, 6571, 5997, 6573, 5541, 1969, 2335, 6574, 4829], "orig_top_k_doc_id": [1132, 6571, 5554, 5997, 6573, 6716, 5541, 1969, 5540, 2335, 6574, 3715, 4829, 4478, 6927]}, {"qid": 2063, "question": "What evaluation metrics do they use? in Attention Optimization for Abstractive Document Summarization", "answer": ["ROUGE F1, METEOR"], "top_k_doc_id": [1132, 3715, 4478, 5554, 6716, 6927, 6928, 4619, 7763, 1255, 6955, 4826, 1973, 4828, 4763], "orig_top_k_doc_id": [6928, 5554, 1132, 3715, 4478, 4619, 7763, 6716, 1255, 6927, 6955, 4826, 1973, 4828, 4763]}]}
{"group_id": 173, "group_size": 8, "items": [{"qid": 914, "question": "How do they represent input features of their model to train embeddings? in Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches", "answer": ["a vector of frame-level acoustic features"], "top_k_doc_id": [5217, 227, 380, 1175, 1176, 1812, 2293, 1668, 1177, 3951, 3338, 2119, 1365, 4447, 6370], "orig_top_k_doc_id": [2293, 3338, 1176, 5217, 2119, 1175, 380, 1365, 1812, 4447, 1668, 1177, 6370, 227, 3951]}, {"qid": 915, "question": "Which dimensionality do they use for their embeddings? in Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches", "answer": ["1061"], "top_k_doc_id": [5217, 227, 380, 1175, 1176, 1812, 2293, 1668, 1177, 3951, 1179, 2065, 6300, 5582, 1664], "orig_top_k_doc_id": [1176, 1177, 1179, 5217, 2293, 227, 2065, 1175, 6300, 5582, 1664, 380, 1668, 1812, 3951]}, {"qid": 3070, "question": "Which approach out of two proposed in the paper performed better in experiments? in Contextual Joint Factor Acoustic Embeddings", "answer": ["CJFA encoder ", "CJFA encoder"], "top_k_doc_id": [5217, 3836, 5218, 5219, 5221, 4450, 373, 375, 376, 6968, 2486, 6517, 1812, 2352, 2995], "orig_top_k_doc_id": [5221, 5217, 5219, 4450, 3836, 5218, 6968, 2486, 376, 373, 6517, 1812, 2352, 375, 2995]}, {"qid": 3072, "question": "What TIMIT datasets are used for testing? in Contextual Joint Factor Acoustic Embeddings", "answer": ["Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H", " this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each"], "top_k_doc_id": [5217, 3836, 5218, 5219, 5221, 4450, 373, 375, 376, 6968, 621, 622, 5220, 2119, 6313], "orig_top_k_doc_id": [5219, 5221, 5217, 5218, 375, 376, 3836, 621, 622, 373, 5220, 2119, 6968, 6313, 4450]}, {"qid": 916, "question": "Which dataset do they use? in Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches", "answer": ["Switchboard conversational English corpus"], "top_k_doc_id": [5217, 227, 380, 1175, 1176, 1812, 2293, 1668, 381, 575, 2119, 2065, 4371, 373, 3338], "orig_top_k_doc_id": [2293, 1812, 380, 227, 381, 5217, 575, 1176, 1668, 2119, 2065, 1175, 4371, 373, 3338]}, {"qid": 3071, "question": "What classification baselines are used for comparison? in Contextual Joint Factor Acoustic Embeddings", "answer": ["VAE", "VAE based phone classification"], "top_k_doc_id": [5217, 3836, 5218, 5219, 5221, 4450, 5015, 2119, 4373, 1567, 3849, 3848, 1177, 2600, 1851], "orig_top_k_doc_id": [5221, 5217, 5219, 5218, 5015, 2119, 3836, 4373, 1567, 3849, 4450, 3848, 1177, 2600, 1851]}, {"qid": 917, "question": "By how much do they outpeform previous results on the word discrimination task? in Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches", "answer": ["Their best average precision tops previous best result by 0.202"], "top_k_doc_id": [5217, 227, 380, 1175, 1176, 1812, 2293, 1179, 1177, 4673, 5264, 2295, 3338, 1345, 4371], "orig_top_k_doc_id": [1175, 1179, 2293, 1176, 1177, 4673, 1812, 5264, 5217, 227, 2295, 3338, 380, 1345, 4371]}, {"qid": 3073, "question": "How does this approach compares to the state-of-the-art results on these tasks? in Contextual Joint Factor Acoustic Embeddings", "answer": ["No"], "top_k_doc_id": [5217, 3836, 5218, 5219, 5221, 1812, 3293, 3297, 5286, 7318, 7116, 1338, 683, 7403, 4602], "orig_top_k_doc_id": [5217, 5221, 1812, 5218, 5219, 3836, 3293, 3297, 5286, 7318, 7116, 1338, 683, 7403, 4602]}]}
{"group_id": 174, "group_size": 8, "items": [{"qid": 924, "question": "Do they evaluate only on English data? in Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "answer": ["Yes"], "top_k_doc_id": [1191, 1192, 879, 1190, 1193, 6405, 4670, 7066, 5783, 6026, 7307, 7234, 7308, 4002, 7257], "orig_top_k_doc_id": [1192, 1191, 1190, 1193, 879, 6405, 4670, 5783, 7066, 6026, 7307, 7234, 7308, 4002, 7257]}, {"qid": 925, "question": "How strong was the correlation between exercise and diabetes? in Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "answer": ["weak correlation with p-value of 0.08"], "top_k_doc_id": [1191, 1192, 879, 1190, 1193, 6405, 4670, 7066, 6025, 394, 7067, 17, 4664, 6027, 5946], "orig_top_k_doc_id": [1192, 1191, 1190, 1193, 879, 6025, 394, 4670, 6405, 7066, 7067, 17, 4664, 6027, 5946]}, {"qid": 705, "question": "Do the authors report results only on English data? in Yoga-Veganism: Correlation Mining of Twitter Health Data", "answer": ["Yes"], "top_k_doc_id": [1191, 1192, 879, 881, 882, 2827, 3135, 3300, 4139, 6195, 6207, 6155, 7755, 1193, 880], "orig_top_k_doc_id": [882, 879, 1192, 881, 1191, 6195, 2827, 6207, 6155, 7755, 3135, 4139, 3300, 1193, 880]}, {"qid": 706, "question": "What other interesting correlations are observed? in Yoga-Veganism: Correlation Mining of Twitter Health Data", "answer": ["Women-Yoga"], "top_k_doc_id": [1191, 1192, 879, 881, 882, 2827, 3135, 3300, 4139, 7031, 1386, 7033, 6835, 7032, 612], "orig_top_k_doc_id": [882, 879, 1192, 881, 1191, 3135, 2827, 7031, 4139, 1386, 7033, 6835, 7032, 612, 3300]}, {"qid": 926, "question": "How were topics of interest about DDEO identified? in Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "answer": ["using topic modeling model Latent Dirichlet Allocation (LDA)"], "top_k_doc_id": [1191, 1192, 879, 1190, 1193, 6405, 2077, 6025, 1733, 3486, 6817, 6026, 2074, 5783, 955], "orig_top_k_doc_id": [1192, 1191, 1190, 1193, 879, 2077, 6025, 6405, 1733, 3486, 6817, 6026, 2074, 5783, 955]}, {"qid": 1282, "question": "How were breast cancer related posts compiled from the Twitter streaming API? in A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter", "answer": ["By using  keywords `breast' AND `cancer' in tweet collecting process. \n"], "top_k_doc_id": [1191, 1192, 1190, 1754, 1755, 1756, 1757, 1758, 1759, 5057, 5489, 4682, 5056, 6155, 7752], "orig_top_k_doc_id": [1756, 1757, 1754, 1758, 1755, 1192, 1759, 1190, 1191, 5057, 4682, 5056, 6155, 7752, 5489]}, {"qid": 1283, "question": "What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences? in A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter", "answer": ["ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.\nNLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing \"retweets\", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation."], "top_k_doc_id": [1191, 1192, 1190, 1754, 1755, 1756, 1757, 1758, 1759, 5057, 5489, 4682, 5056, 3588, 5740], "orig_top_k_doc_id": [1754, 1756, 1757, 1758, 1755, 1759, 1192, 1191, 5057, 1190, 5489, 4682, 3588, 5056, 5740]}, {"qid": 1281, "question": "Do the authors report results only on English datasets? in A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter", "answer": ["Yes"], "top_k_doc_id": [1191, 1192, 1190, 1754, 1755, 1756, 1757, 1758, 1759, 5057, 5489, 3588, 5004, 5059, 6206], "orig_top_k_doc_id": [1758, 1757, 1756, 1754, 1755, 1192, 5057, 3588, 1190, 5489, 5004, 1759, 1191, 5059, 6206]}]}
{"group_id": 175, "group_size": 8, "items": [{"qid": 934, "question": "Do they experiment with the dataset? in Modeling Trolling in Social Media Conversations", "answer": ["Yes"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6816, 1210, 3584, 243, 6285, 6817, 4948], "orig_top_k_doc_id": [1205, 1206, 1209, 1207, 3522, 3523, 3521, 3526, 6817, 6816, 4948, 3584, 1210, 243, 6285]}, {"qid": 938, "question": "What is the size of the dataset? in Modeling Trolling in Social Media Conversations", "answer": ["1000 conversations composed of 6833 sentences and 88047 tokens"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6816, 1210, 3584, 243, 6285, 6793, 2076], "orig_top_k_doc_id": [1205, 1206, 1207, 3521, 3523, 3522, 1209, 6285, 3526, 6816, 3584, 6793, 1210, 243, 2076]}, {"qid": 2244, "question": "how was annotation done? in A Trolling Hierarchy in Social Media and A Conditional Random Field For Trolling Detection", "answer": ["Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6816, 1210, 3584, 3524, 4948, 6817, 5168], "orig_top_k_doc_id": [4948, 3522, 1205, 3524, 1206, 3521, 1209, 3584, 5168, 3523, 6816, 6817, 1207, 3526, 1210]}, {"qid": 2245, "question": "what is the source of the new dataset? in A Trolling Hierarchy in Social Media and A Conditional Random Field For Trolling Detection", "answer": ["Reddit"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6816, 1210, 3584, 3524, 4948, 6817, 3445], "orig_top_k_doc_id": [3522, 1205, 1206, 3521, 1209, 3524, 3584, 1207, 6816, 4948, 3523, 6817, 1210, 3526, 3445]}, {"qid": 936, "question": "What is an example of a difficult-to-classify case? in Modeling Trolling in Social Media Conversations", "answer": ["The lack of background, Non-cursing aggressions and insults, the presence of controversial topic words ,  shallow meaning representation, directly ask the suspected troll if he/she is trolling or not, a blurry line between \u201cFrustrate\u201d and \u201cNeutralize\u201d, distinction between the classes \u201cTroll\u201d and \u201cEngage\u201d"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6816, 1210, 3584, 2076, 7625, 5258, 441], "orig_top_k_doc_id": [1205, 1206, 1209, 1207, 3521, 3522, 3523, 2076, 6816, 3584, 7625, 1210, 5258, 3526, 441]}, {"qid": 937, "question": "What potential solutions are suggested? in Modeling Trolling in Social Media Conversations", "answer": [" inclusion of longer parts of the conversation"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6816, 6285, 6817, 5427, 6159, 4739, 5906], "orig_top_k_doc_id": [1205, 1206, 3521, 1207, 6816, 3522, 3523, 1209, 6285, 6817, 3526, 5427, 6159, 4739, 5906]}, {"qid": 935, "question": "Do they use a crowdsourcing platform for annotation? in Modeling Trolling in Social Media Conversations", "answer": ["No"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 1209, 6140, 2800, 5037, 3587, 3623, 7625, 5910], "orig_top_k_doc_id": [1205, 1206, 3522, 3523, 1207, 1209, 6140, 2800, 3521, 5037, 3587, 3623, 7625, 3526, 5910]}, {"qid": 939, "question": "What Reddit communities do they look at? in Modeling Trolling in Social Media Conversations", "answer": ["No"], "top_k_doc_id": [1205, 1206, 1207, 3521, 3522, 3523, 3526, 4581, 441, 6285, 5908, 14, 5291, 5907, 6286], "orig_top_k_doc_id": [1205, 3522, 1206, 1207, 3521, 4581, 3523, 441, 6285, 5908, 14, 3526, 5291, 5907, 6286]}]}
{"group_id": 176, "group_size": 8, "items": [{"qid": 976, "question": "How do they explore domain mismatch? in MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge", "answer": ["No"], "top_k_doc_id": [1266, 1320, 2618, 2788, 2797, 1267, 1268, 1269, 2798, 85, 1321, 4002, 4834, 2469, 1318], "orig_top_k_doc_id": [1266, 1268, 1269, 2788, 1267, 2798, 2797, 1320, 2618, 1321, 4834, 2469, 4002, 1318, 85]}, {"qid": 977, "question": "How do they explore dialect variability? in MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge", "answer": ["No"], "top_k_doc_id": [1266, 1320, 2618, 2788, 2797, 1267, 1268, 1269, 2798, 85, 1321, 4002, 4005, 2785, 4007], "orig_top_k_doc_id": [1266, 1269, 1268, 2788, 2797, 1267, 2798, 2618, 4002, 4005, 1321, 1320, 85, 2785, 4007]}, {"qid": 975, "question": "What is the architecture of the siamese neural network? in MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge", "answer": ["two parallel convolutional networks, INLINEFORM0 , that share the same set of weights"], "top_k_doc_id": [1266, 1320, 2618, 2788, 2797, 1267, 1268, 1269, 2798, 4834, 6731, 2783, 2072, 3648, 2785], "orig_top_k_doc_id": [1266, 1268, 1269, 1267, 2788, 2798, 2797, 2618, 4834, 6731, 2783, 2072, 3648, 2785, 1320]}, {"qid": 978, "question": "Which are the four Arabic dialects? in MIT-QCRI Arabic Dialect Identification System for the 2017 Multi-Genre Broadcast Challenge", "answer": ["Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR)"], "top_k_doc_id": [1266, 1320, 2618, 2788, 2797, 1267, 1268, 1269, 2798, 5168, 4223, 5169, 85, 2796, 87], "orig_top_k_doc_id": [1266, 1268, 2797, 2788, 1267, 2798, 1269, 2618, 1320, 5168, 4223, 5169, 85, 2796, 87]}, {"qid": 3876, "question": "what are the baselines? in Two-stage Training for Chinese Dialect Recognition", "answer": ["one-stage RNN system containing 2-layer BLSTM", "one-stage RNN system", "a one-stage RNN system"], "top_k_doc_id": [1266, 1288, 4002, 4750, 6262, 6263, 6264, 974, 1267, 2839, 4790, 6036, 6062, 22, 1269], "orig_top_k_doc_id": [6264, 6263, 6262, 2839, 4790, 6036, 4002, 1266, 4750, 6062, 1288, 1267, 22, 974, 1269]}, {"qid": 3878, "question": "what chinese dialects are explored? in Two-stage Training for Chinese Dialect Recognition", "answer": ["Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian", "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka, Fujian", "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian"], "top_k_doc_id": [1266, 1288, 4002, 4750, 6262, 6263, 6264, 974, 1267, 2797, 4007, 1287, 2796, 2775, 1376], "orig_top_k_doc_id": [6264, 6263, 6262, 2797, 1266, 1288, 1267, 4007, 4002, 1287, 2796, 2775, 1376, 4750, 974]}, {"qid": 1794, "question": "Which language is divided into six dialects in the task mentioned in the paper? in Experiments in Cuneiform Language Identification", "answer": ["Akkadian."], "top_k_doc_id": [1266, 1320, 2618, 2788, 2797, 2619, 1287, 1286, 2775, 6262, 4002, 3664, 4223, 78, 4007], "orig_top_k_doc_id": [2618, 2619, 1287, 1286, 2775, 1320, 1266, 2797, 6262, 2788, 4002, 3664, 4223, 78, 4007]}, {"qid": 3877, "question": "what results do they achieve? in Two-stage Training for Chinese Dialect Recognition", "answer": [" relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline, accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi", "state-of-the-art in the Chinese dialect recognition task", " The relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task (88.88 and 87.24)  relative to the baseline  78.85."], "top_k_doc_id": [1266, 1288, 4002, 4750, 6262, 6263, 6264, 2797, 22, 21, 2798, 482, 5765, 4373, 2839], "orig_top_k_doc_id": [6264, 6263, 6262, 2797, 22, 21, 2798, 4002, 1266, 482, 5765, 1288, 4750, 4373, 2839]}]}
{"group_id": 177, "group_size": 8, "items": [{"qid": 1400, "question": "What is MRR? in Mixed Membership Word Embeddings for Computational Social Science", "answer": ["mean reciprocal rank"], "top_k_doc_id": [1937, 1934, 1935, 1936, 1938, 1939, 3336, 1067, 1190, 5537, 2192, 3581, 5906, 2193, 241], "orig_top_k_doc_id": [1934, 1935, 1936, 1937, 1938, 1939, 2192, 3581, 5537, 1067, 5906, 1190, 2193, 241, 3336]}, {"qid": 1401, "question": "Which techniques for word embeddings and topic models are used? in Mixed Membership Word Embeddings for Computational Social Science", "answer": [" skip-gram, LDA"], "top_k_doc_id": [1937, 1934, 1935, 1936, 1938, 1939, 3336, 1067, 1190, 5537, 554, 3612, 6817, 5949, 7743], "orig_top_k_doc_id": [1934, 1935, 1936, 1937, 1938, 1939, 1190, 554, 3336, 5537, 3612, 1067, 6817, 5949, 7743]}, {"qid": 150, "question": "what are the advantages of the proposed model? in Learning Supervised Topic Models for Classification and Regression from Crowds", "answer": ["he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."], "top_k_doc_id": [1937, 9, 185, 186, 187, 188, 194, 5116, 5117, 5737, 7536, 190, 4417, 1877, 191], "orig_top_k_doc_id": [185, 194, 187, 186, 5117, 188, 9, 5116, 1877, 191, 5737, 1937, 4417, 190, 7536]}, {"qid": 152, "question": "what datasets were used? in Learning Supervised Topic Models for Classification and Regression from Crowds", "answer": ["Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 ", " 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe"], "top_k_doc_id": [1937, 9, 185, 186, 187, 188, 194, 5116, 5117, 5737, 7536, 190, 4417, 5727, 599], "orig_top_k_doc_id": [185, 194, 187, 5117, 1937, 9, 5116, 5737, 188, 186, 190, 5727, 599, 7536, 4417]}, {"qid": 1399, "question": "What supervised learning tasks are attempted with these representations? in Mixed Membership Word Embeddings for Computational Social Science", "answer": ["document categorization, regression tasks"], "top_k_doc_id": [1937, 1934, 1935, 1936, 3581, 3598, 5525, 5910, 6208, 1067, 1938, 1939, 5537, 6805, 186], "orig_top_k_doc_id": [1934, 1935, 1937, 1936, 1067, 1938, 3598, 1939, 5537, 3581, 5910, 6805, 6208, 186, 5525]}, {"qid": 1402, "question": "Why is big data not appropriate for this task? in Mixed Membership Word Embeddings for Computational Social Science", "answer": ["Training embeddings from small-corpora can increase the performance of some tasks"], "top_k_doc_id": [1937, 1934, 1935, 1936, 1938, 1939, 3336, 3581, 237, 5272, 1378, 5906, 807, 3598, 4235], "orig_top_k_doc_id": [1934, 1935, 1936, 1937, 1938, 1939, 3336, 3581, 237, 5272, 1378, 5906, 807, 3598, 4235]}, {"qid": 1403, "question": "What is an example of a computational social science NLP task? in Mixed Membership Word Embeddings for Computational Social Science", "answer": ["Visualization of State of the union addresses"], "top_k_doc_id": [1937, 1934, 1935, 1936, 3581, 3598, 5525, 5910, 6208, 742, 5906, 5373, 4581, 554, 807], "orig_top_k_doc_id": [1934, 1935, 1936, 1937, 3581, 742, 5906, 5525, 5373, 3598, 4581, 6208, 5910, 554, 807]}, {"qid": 151, "question": "what are the state of the art approaches? in Learning Supervised Topic Models for Classification and Regression from Crowds", "answer": ["Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)"], "top_k_doc_id": [1937, 9, 185, 186, 187, 188, 194, 5116, 5117, 5737, 7536, 5421, 6448, 7116, 5099], "orig_top_k_doc_id": [185, 194, 5117, 187, 186, 5421, 6448, 7116, 9, 5116, 7536, 188, 5099, 5737, 1937]}]}
{"group_id": 178, "group_size": 8, "items": [{"qid": 1434, "question": "What were their accuracy results on the task? in Build Fast and Accurate Lemmatization for Arabic", "answer": ["97.32%"], "top_k_doc_id": [4226, 4227, 1731, 1984, 1985, 1986, 2810, 2811, 5168, 4223, 1732, 7097, 7320, 1439, 5624], "orig_top_k_doc_id": [1986, 1985, 2811, 1984, 4226, 4227, 2810, 1731, 5168, 4223, 7097, 1439, 5624, 7320, 1732]}, {"qid": 1436, "question": "How was the dataset annotated? in Build Fast and Accurate Lemmatization for Arabic", "answer": ["Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization"], "top_k_doc_id": [4226, 4227, 1731, 1984, 1985, 1986, 2810, 2811, 5168, 4223, 1732, 7097, 7320, 5169, 4224], "orig_top_k_doc_id": [1986, 1985, 4226, 1984, 2810, 2811, 4227, 4223, 5168, 7320, 1731, 1732, 5169, 7097, 4224]}, {"qid": 1437, "question": "What is the size of the dataset? in Build Fast and Accurate Lemmatization for Arabic", "answer": ["Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each"], "top_k_doc_id": [4226, 4227, 1731, 1984, 1985, 1986, 2810, 2811, 5168, 4223, 1732, 7097, 5622, 5623, 4228], "orig_top_k_doc_id": [1986, 1985, 4226, 1984, 4227, 2811, 2810, 5622, 1731, 4223, 5623, 4228, 7097, 5168, 1732]}, {"qid": 1435, "question": "What is the state of the art? in Build Fast and Accurate Lemmatization for Arabic", "answer": [" MADAMIRA BIBREF6 system"], "top_k_doc_id": [4226, 4227, 1731, 1984, 1985, 1986, 2810, 2811, 5168, 4223, 7320, 5624, 5623, 1439, 5622], "orig_top_k_doc_id": [1986, 1985, 1984, 4226, 2811, 7320, 4227, 4223, 5624, 5168, 5623, 1731, 1439, 2810, 5622]}, {"qid": 1433, "question": "How was speed measured? in Build Fast and Accurate Lemmatization for Arabic", "answer": ["how long it takes the system to lemmatize a set number of words"], "top_k_doc_id": [4226, 4227, 1731, 1984, 1985, 1986, 2810, 2811, 5168, 4223, 7131, 5622, 2174, 1441, 1732], "orig_top_k_doc_id": [1986, 1985, 4226, 2810, 1984, 7131, 5622, 2174, 2811, 4223, 4227, 5168, 1441, 1732, 1731]}, {"qid": 1438, "question": "Where did they collect their dataset from? in Build Fast and Accurate Lemmatization for Arabic", "answer": ["from Arabic WikiNews site https://ar.wikinews.org/wiki"], "top_k_doc_id": [4226, 4227, 1731, 1984, 1985, 1986, 2810, 2811, 5168, 5622, 7097, 2174, 6805, 5172, 7320], "orig_top_k_doc_id": [1986, 1985, 1984, 4226, 2810, 5622, 5168, 7097, 2174, 2811, 1731, 6805, 5172, 4227, 7320]}, {"qid": 2499, "question": "How does the semi-automatic construction process work? in TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus", "answer": ["Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus"], "top_k_doc_id": [4226, 4227, 4223, 4224, 4225, 4228, 6110, 2104, 7725, 5044, 6858, 2498, 7072, 4464, 7718], "orig_top_k_doc_id": [4227, 4223, 4224, 4228, 4225, 4226, 2104, 7725, 5044, 6858, 2498, 7072, 4464, 6110, 7718]}, {"qid": 2500, "question": "Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words? in TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus", "answer": ["Yes"], "top_k_doc_id": [4226, 4227, 4223, 4224, 4225, 4228, 6110, 85, 2797, 2798, 86, 2789, 776, 6791, 7267], "orig_top_k_doc_id": [4223, 4227, 4224, 4226, 4225, 4228, 85, 2797, 6110, 2798, 86, 2789, 776, 6791, 7267]}]}
{"group_id": 179, "group_size": 8, "items": [{"qid": 1611, "question": "Where is the dataset from? in Schema-Guided Dialogue State Tracking Task at DSTC8", "answer": ["dialogue simulator"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 200, 2234, 2280, 2550, 3682, 2278, 2279, 196, 2555], "orig_top_k_doc_id": [2276, 3679, 2280, 3684, 3681, 2277, 3683, 2234, 3682, 2550, 196, 200, 2555, 2279, 2278]}, {"qid": 1612, "question": "What data augmentation techniques are used? in Schema-Guided Dialogue State Tracking Task at DSTC8", "answer": ["back translation between English and Chinese"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 200, 2234, 2280, 2550, 3682, 196, 2555, 2279, 7584], "orig_top_k_doc_id": [2280, 2276, 3679, 3684, 3681, 2277, 3683, 2234, 3682, 2279, 2555, 2550, 196, 200, 7584]}, {"qid": 1615, "question": "What is the baseline model? in Schema-Guided Dialogue State Tracking Task at DSTC8", "answer": ["No"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 200, 2234, 2280, 2550, 3682, 196, 2555, 2279, 3193], "orig_top_k_doc_id": [2276, 2280, 3679, 3684, 3681, 2277, 3683, 2234, 3682, 2555, 2550, 2279, 196, 200, 3193]}, {"qid": 1616, "question": "What domains are present in the data? in Schema-Guided Dialogue State Tracking Task at DSTC8", "answer": ["Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 200, 2234, 2280, 2550, 3682, 2278, 2279, 196, 3507], "orig_top_k_doc_id": [2280, 3679, 2276, 3681, 3684, 2277, 3683, 2234, 3682, 196, 2550, 2278, 200, 3507, 2279]}, {"qid": 1613, "question": "Do all teams use neural networks for their models? in Schema-Guided Dialogue State Tracking Task at DSTC8", "answer": ["No"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 200, 2234, 2280, 2550, 3682, 2278, 2279, 3507, 2555], "orig_top_k_doc_id": [2280, 2276, 3681, 3679, 2277, 3684, 2234, 2279, 2278, 2550, 3683, 3682, 3507, 200, 2555]}, {"qid": 1614, "question": "How are the models evaluated? in Schema-Guided Dialogue State Tracking Task at DSTC8", "answer": ["Active Intent Accuracy, Requested Slot F1, Average Goal Accuracy, Joint Goal Accuracy"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 200, 2234, 2280, 2550, 3682, 196, 2555, 3193, 3359], "orig_top_k_doc_id": [2276, 2280, 3679, 3681, 3684, 2277, 3683, 2234, 3682, 200, 2550, 196, 2555, 3193, 3359]}, {"qid": 2307, "question": "How did they gather the data? in Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset", "answer": ["simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers "], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 1070, 1171, 1711, 1712, 2278, 6590, 1718, 6583, 7299], "orig_top_k_doc_id": [3679, 2276, 3684, 6590, 1712, 1718, 2277, 3681, 1171, 1711, 1070, 2278, 3683, 6583, 7299]}, {"qid": 2308, "question": "What are the domains covered in the dataset? in Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset", "answer": ["Alarm\nBank\nBus\nCalendar\nEvent\nFlight\nHome\nHotel\nMedia\nMovie\nMusic\nRentalCar\nRestaurant\nRideShare\nService\nTravel\nWeather"], "top_k_doc_id": [2276, 2277, 3679, 3681, 3683, 3684, 1070, 1171, 1711, 1712, 2278, 2234, 3680, 2280, 575], "orig_top_k_doc_id": [3679, 2276, 3681, 2277, 2234, 2278, 3684, 3683, 1712, 1711, 1171, 1070, 3680, 2280, 575]}]}
{"group_id": 180, "group_size": 8, "items": [{"qid": 1662, "question": "How large is the improvement over the baseline? in Unsupervised Question Decomposition for Question Answering", "answer": ["3.1 F1 gain on the original dev set, 11 F1 gain on the multi-hop dev set, 10 F1 gain on the out-of-domain dev set."], "top_k_doc_id": [2370, 2366, 2369, 2371, 2367, 2368, 2372, 7610, 3467, 3839, 2052, 947, 5580, 4497, 2442], "orig_top_k_doc_id": [2366, 2370, 2372, 2371, 2369, 2367, 7610, 2052, 2368, 3467, 947, 5580, 3839, 4497, 2442]}, {"qid": 1663, "question": "What is the strong baseline that this work outperforms? in Unsupervised Question Decomposition for Question Answering", "answer": ["RoBERTa baseline"], "top_k_doc_id": [2370, 2366, 2369, 2371, 2367, 2368, 2372, 7610, 3467, 3839, 5327, 3617, 2661, 5729, 4455], "orig_top_k_doc_id": [2366, 2371, 2370, 2369, 2367, 2372, 3839, 5327, 3617, 2661, 5729, 4455, 7610, 3467, 2368]}, {"qid": 4405, "question": "How much did the model outperform in Reinforced Multi-task Approach for Multi-hop Question Generation", "answer": ["the absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric", "Automatic evaluation metrics show  relative improvements of  11.11, 6.07, 19.29 for BLEU-4, ROUGE-L and SF Coverage respectively (over average baseline). \nHuman evaluation  relative improvement for Difficulty, Naturalness and SF Coverage are 8.44,  32.64,  13.57 respectively."], "top_k_doc_id": [2370, 2366, 2369, 2371, 325, 6932, 6935, 6936, 3810, 4496, 4497, 6933, 2372, 1240, 7678], "orig_top_k_doc_id": [6936, 2370, 2371, 6932, 2366, 6935, 2372, 6933, 4497, 3810, 4496, 2369, 325, 1240, 7678]}, {"qid": 4406, "question": "What language is in the dataset? in Reinforced Multi-task Approach for Multi-hop Question Generation", "answer": ["English", "English"], "top_k_doc_id": [2370, 2366, 2369, 2371, 325, 6932, 6935, 6936, 3810, 4496, 4497, 6933, 4640, 2368, 2096], "orig_top_k_doc_id": [6936, 6932, 2366, 6935, 2371, 2370, 325, 6933, 4640, 2368, 3810, 2369, 4496, 2096, 4497]}, {"qid": 1661, "question": "What off-the-shelf QA model was used to answer sub-questions? in Unsupervised Question Decomposition for Question Answering", "answer": ["$\\textsc {BERT}_{\\textsc {BASE}}$ ensemble from BIBREF3"], "top_k_doc_id": [2370, 2366, 2369, 2371, 2367, 2368, 2372, 7610, 4535, 7609, 4463, 7605, 2661, 1120, 1121], "orig_top_k_doc_id": [2366, 2371, 2367, 2370, 2369, 7610, 2368, 4535, 7609, 2372, 4463, 7605, 2661, 1120, 1121]}, {"qid": 4407, "question": "How big is the HotPotQA dataset? in Reinforced Multi-task Approach for Multi-hop Question Generation", "answer": [" over 113k Wikipedia-based question-answer pairs", "113k Wikipedia-based question-answer pairs"], "top_k_doc_id": [2370, 2366, 2369, 2371, 325, 6932, 6935, 6936, 2368, 2367, 1240, 2372, 1242, 329, 6934], "orig_top_k_doc_id": [6936, 6935, 6932, 2370, 2371, 2366, 325, 2369, 2368, 2367, 1240, 2372, 1242, 329, 6934]}, {"qid": 4520, "question": "What does it mean for sentences to be \"lexically overlapping\"? in WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-Hop Inference", "answer": ["They share words.", "share words"], "top_k_doc_id": [2370, 325, 884, 4496, 6932, 7071, 7072, 7073, 7074, 7075, 7076, 2369, 6933, 4704, 2371], "orig_top_k_doc_id": [4496, 7075, 7071, 7076, 7072, 7073, 7074, 6932, 884, 325, 2369, 6933, 4704, 2371, 2370]}, {"qid": 4521, "question": "How many tables are in the tablestore? in WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-Hop Inference", "answer": ["62", "62"], "top_k_doc_id": [2370, 325, 884, 4496, 6932, 7071, 7072, 7073, 7074, 7075, 7076, 4463, 4498, 4216, 345], "orig_top_k_doc_id": [7073, 7075, 4496, 7076, 7072, 7071, 7074, 325, 4463, 884, 2370, 4498, 6932, 4216, 345]}]}
{"group_id": 181, "group_size": 8, "items": [{"qid": 1672, "question": "What dataset of tweets is used? in Sentiment Analysis for Twitter : Going Beyond Tweet Text", "answer": ["tweets about `ObamaCare' in USA collected during march 2010"], "top_k_doc_id": [447, 448, 449, 6752, 6873, 7307, 330, 927, 502, 7752, 1688, 5879, 3246, 4895, 928], "orig_top_k_doc_id": [448, 447, 6873, 449, 6752, 927, 502, 1688, 5879, 7307, 3246, 330, 7752, 4895, 928]}, {"qid": 1674, "question": "What linguistic features are used? in Sentiment Analysis for Twitter : Going Beyond Tweet Text", "answer": ["Parts of Speech (POS) tags, Prior polarity of the words, Capitalization, Negation, Text Feature"], "top_k_doc_id": [447, 448, 449, 6752, 6873, 7307, 330, 927, 502, 7752, 7534, 1731, 5255, 3203, 1967], "orig_top_k_doc_id": [447, 448, 7534, 449, 6752, 1731, 6873, 927, 7307, 5255, 330, 3203, 502, 1967, 7752]}, {"qid": 378, "question": "What is the current SOTA for sentiment analysis on Twitter at the time of writing? in Semantic Sentiment Analysis of Twitter Data", "answer": ["deep convolutional networks BIBREF53 , BIBREF54"], "top_k_doc_id": [447, 448, 7746, 330, 1687, 3527, 5255, 7307, 4989, 7860, 502, 6172, 4991, 5057, 3731], "orig_top_k_doc_id": [5255, 447, 7746, 448, 3527, 330, 7307, 1687, 4989, 7860, 502, 6172, 4991, 5057, 3731]}, {"qid": 379, "question": "What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains? in Semantic Sentiment Analysis of Twitter Data", "answer": ["Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text"], "top_k_doc_id": [447, 448, 7746, 3528, 5059, 5060, 7307, 1052, 6206, 5421, 502, 5105, 3527, 606, 7752], "orig_top_k_doc_id": [448, 1052, 7307, 447, 6206, 7746, 3528, 5421, 502, 5105, 5059, 3527, 606, 5060, 7752]}, {"qid": 380, "question": "What are the metrics to evaluate sentiment analysis on Twitter? in Semantic Sentiment Analysis of Twitter Data", "answer": ["No"], "top_k_doc_id": [447, 448, 7746, 3528, 5059, 5060, 7307, 5058, 7860, 5422, 1957, 5255, 3612, 452, 462], "orig_top_k_doc_id": [447, 7746, 7307, 448, 5060, 5058, 7860, 3528, 5059, 5422, 1957, 5255, 3612, 452, 462]}, {"qid": 1673, "question": "What external sources of information are used? in Sentiment Analysis for Twitter : Going Beyond Tweet Text", "answer": ["landing pages of URLs"], "top_k_doc_id": [447, 448, 449, 6752, 6873, 7307, 330, 927, 762, 1499, 2385, 5973, 451, 2080, 3245], "orig_top_k_doc_id": [447, 448, 6752, 927, 6873, 449, 762, 1499, 2385, 5973, 7307, 451, 2080, 330, 3245]}, {"qid": 3229, "question": "What details are given about the Twitter dataset? in Generating Word and Document Embeddings for Sentiment Analysis", "answer": ["Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.", "one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels"], "top_k_doc_id": [447, 448, 7746, 330, 1687, 3527, 879, 5421, 6752, 5417, 5419, 2399, 1822, 6971, 7528], "orig_top_k_doc_id": [7746, 879, 448, 5421, 447, 6752, 5417, 1687, 5419, 330, 2399, 1822, 6971, 3527, 7528]}, {"qid": 1671, "question": "Do the authors report only on English language data? in Sentiment Analysis for Twitter : Going Beyond Tweet Text", "answer": ["Yes"], "top_k_doc_id": [447, 448, 449, 6752, 6873, 7307, 1839, 331, 762, 5255, 3731, 7752, 2385, 7308, 5256], "orig_top_k_doc_id": [1839, 6873, 6752, 447, 331, 7307, 448, 762, 5255, 3731, 7752, 2385, 7308, 5256, 449]}]}
{"group_id": 182, "group_size": 8, "items": [{"qid": 1828, "question": "What was their performance? in Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines", "answer": ["beneficial impact of word-representations and basic pre-processing"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 3542, 7131, 1039, 3623, 4799, 5737, 4798, 481], "orig_top_k_doc_id": [2660, 2659, 7263, 7265, 4800, 7131, 3542, 3623, 2698, 3730, 5737, 4799, 1039, 4798, 481]}, {"qid": 1830, "question": "What embeddings do they use? in Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines", "answer": ["GloVe"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 3542, 7131, 1039, 3623, 4799, 5737, 5007, 7484], "orig_top_k_doc_id": [2660, 2659, 7263, 7265, 7131, 3542, 4800, 1039, 3730, 5737, 3623, 2698, 4799, 5007, 7484]}, {"qid": 1826, "question": "How do they incorporate lexicon into the neural network? in Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines", "answer": ["concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation, cannot directly concatenate,  re-build the latter in token-based form"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 3542, 7131, 2696, 4798, 7009, 6640, 7008, 5008], "orig_top_k_doc_id": [2660, 2659, 7263, 7131, 7265, 3542, 2698, 3730, 4800, 2696, 7009, 4798, 6640, 7008, 5008]}, {"qid": 1827, "question": "What is the source of their lexicon? in Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines", "answer": ["DepecheMood"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 3542, 7131, 2696, 4798, 7009, 3623, 4799, 3624], "orig_top_k_doc_id": [2660, 2659, 7263, 3542, 7265, 4800, 7131, 7009, 3730, 2698, 3623, 4798, 4799, 2696, 3624]}, {"qid": 1829, "question": "How long is the dataset used for training? in Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines", "answer": ["No"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 3542, 7131, 1039, 3623, 4799, 4798, 4394, 5007], "orig_top_k_doc_id": [2660, 2659, 7263, 7131, 7265, 4800, 2698, 3542, 4799, 4798, 4394, 3623, 1039, 5007, 3730]}, {"qid": 4646, "question": "Do they use external financial knowledge in their approach? in Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines", "answer": ["Yes", "No"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 481, 747, 1039, 3203, 4798, 4799, 2696, 2079], "orig_top_k_doc_id": [7265, 7263, 2660, 3730, 4800, 4799, 4798, 2659, 2698, 481, 747, 2696, 1039, 3203, 2079]}, {"qid": 4648, "question": "Which finance specific word embedding model do they use? in Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines", "answer": ["word2vec", "a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 481, 747, 1039, 3203, 4798, 4799, 3542, 7009], "orig_top_k_doc_id": [7265, 7263, 2660, 2659, 2698, 3730, 4800, 4799, 747, 3542, 481, 7009, 3203, 1039, 4798]}, {"qid": 4647, "question": "Which evaluation metrics do they use? in Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines", "answer": [" Metric 1, Metric 2, Metric 3", "weighted cosine similarity, classification metric for sentences with one aspect"], "top_k_doc_id": [2659, 2660, 2698, 3730, 4800, 7263, 7265, 481, 1866, 110, 6619, 3336, 4799, 3542, 3623], "orig_top_k_doc_id": [7265, 2660, 7263, 481, 1866, 3730, 110, 2659, 4800, 6619, 3336, 4799, 3542, 2698, 3623]}]}
{"group_id": 183, "group_size": 8, "items": [{"qid": 1963, "question": "How they prove that multi-head self-attention is at least as powerful as convolution layer?  in On the Relationship between Self-Attention and Convolutional Layers", "answer": ["constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer"], "top_k_doc_id": [2943, 2940, 891, 890, 2049, 2944, 6114, 7169, 2942, 2941, 5284, 7251, 3559, 2917, 366], "orig_top_k_doc_id": [2943, 2942, 2940, 2941, 891, 5284, 2049, 7251, 3559, 2944, 2917, 7169, 6114, 890, 366]}, {"qid": 1964, "question": "Is there a way of converting existing convolution layers into self-attention to perform very same convolution? in On the Relationship between Self-Attention and Convolutional Layers", "answer": ["No"], "top_k_doc_id": [2943, 2940, 891, 890, 2049, 2944, 6114, 7169, 888, 889, 3358, 2495, 4305, 2494, 3821], "orig_top_k_doc_id": [891, 2943, 890, 888, 2944, 7169, 2940, 889, 3358, 2495, 6114, 2049, 4305, 2494, 3821]}, {"qid": 1967, "question": "What numerical experiments they perform? in On the Relationship between Self-Attention and Convolutional Layers", "answer": ["attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18"], "top_k_doc_id": [2943, 2940, 891, 890, 2049, 2917, 1357, 1358, 1360, 6065, 6066, 7251, 6068, 2083, 3559], "orig_top_k_doc_id": [2943, 891, 2917, 1357, 890, 1358, 2049, 1360, 6065, 6066, 2940, 7251, 6068, 2083, 3559]}, {"qid": 1966, "question": "Is there any nonnumerical experiment that also support author's claim, like analysis of attention layers in publicly available networks?  in On the Relationship between Self-Attention and Convolutional Layers", "answer": ["No"], "top_k_doc_id": [2943, 2940, 891, 1948, 2941, 2917, 2083, 2050, 7671, 1325, 3162, 401, 371, 2944, 2494], "orig_top_k_doc_id": [2940, 1948, 891, 2941, 2943, 2917, 2083, 2050, 7671, 1325, 3162, 401, 371, 2944, 2494]}, {"qid": 4594, "question": "How is the training time compared to the original position encoding?  in Self-Attention with Relative Position Representations", "answer": ["7% decrease in steps per second", "a modest 7% decrease in steps per second"], "top_k_doc_id": [2943, 2253, 2254, 2941, 5939, 6113, 6114, 7169, 7170, 7171, 2940, 2944, 651, 4035, 1024], "orig_top_k_doc_id": [7169, 2254, 2943, 2941, 651, 2944, 2940, 5939, 7170, 2253, 7171, 4035, 1024, 6114, 6113]}, {"qid": 4596, "question": "Can the new position representation be generalized to other tasks? in Self-Attention with Relative Position Representations", "answer": ["Not sure", "No"], "top_k_doc_id": [2943, 2253, 2254, 2941, 5939, 6113, 6114, 7169, 7170, 7171, 2940, 2944, 888, 5283, 2218], "orig_top_k_doc_id": [7169, 5939, 7170, 7171, 2253, 2254, 6114, 2940, 2943, 6113, 2944, 2941, 888, 5283, 2218]}, {"qid": 1965, "question": "What authors mean by sufficient number of heads? in On the Relationship between Self-Attention and Convolutional Layers", "answer": ["No"], "top_k_doc_id": [2943, 2940, 2941, 2944, 7169, 1561, 6068, 2942, 2945, 1560, 3559, 4034, 6069, 1562, 2050], "orig_top_k_doc_id": [2943, 2941, 2944, 7169, 1561, 6068, 2942, 2945, 1560, 2940, 3559, 4034, 6069, 1562, 2050]}, {"qid": 4595, "question": "Does the new relative position encoder require more parameters? in Self-Attention with Relative Position Representations", "answer": ["Yes", "No"], "top_k_doc_id": [2943, 2253, 2254, 2941, 5939, 6113, 6114, 7169, 7170, 7171, 5938, 3026, 653, 5940, 3027], "orig_top_k_doc_id": [7169, 2254, 7171, 5939, 7170, 6114, 5938, 3026, 2253, 2941, 653, 2943, 6113, 5940, 3027]}]}
{"group_id": 184, "group_size": 8, "items": [{"qid": 2325, "question": "What was the best performing baseline? in IndoSum: A New Benchmark Dataset for Indonesian Text Summarization", "answer": ["Lead-3"], "top_k_doc_id": [3715, 6144, 3716, 3717, 3718, 6036, 6146, 6147, 500, 4619, 6715, 6860, 5079, 5545, 4881], "orig_top_k_doc_id": [3718, 3717, 3715, 3716, 6144, 6147, 6146, 4619, 500, 6860, 5079, 6715, 5545, 4881, 6036]}, {"qid": 2327, "question": "What is the size of the dataset? in IndoSum: A New Benchmark Dataset for Indonesian Text Summarization", "answer": ["20K"], "top_k_doc_id": [3715, 6144, 3716, 3717, 3718, 6036, 6146, 6147, 500, 4619, 6715, 6716, 2226, 4425, 4763], "orig_top_k_doc_id": [3718, 3715, 3717, 3716, 6144, 6147, 6146, 6036, 500, 6715, 6716, 2226, 4619, 4425, 4763]}, {"qid": 3783, "question": "what is the size of the idn tagged corpus? in Toward a Standardized and More Accurate Indonesian Part-of-Speech Tagging", "answer": ["10K", "10K sentences, 250K tokens", "10K sentences and 250K tokens"], "top_k_doc_id": [3715, 6144, 3100, 4862, 6036, 6146, 6151, 3716, 6147, 6467, 7439, 6466, 4881, 4526, 1439], "orig_top_k_doc_id": [6144, 6147, 6146, 4862, 7439, 6466, 3715, 3716, 4881, 6151, 4526, 6467, 3100, 1439, 6036]}, {"qid": 3785, "question": "what rule based models were evaluated? in Toward a Standardized and More Accurate Indonesian Part-of-Speech Tagging", "answer": ["Rashel et al. BIBREF14", "rule-based tagger designed by Rashel et al. BIBREF14", "rule-based tagger designed by Rashel et al. BIBREF14"], "top_k_doc_id": [3715, 6144, 3100, 4862, 6036, 6146, 6151, 3716, 6147, 6467, 7287, 1577, 5879, 396, 3099], "orig_top_k_doc_id": [6144, 6147, 6146, 3715, 6151, 6467, 4862, 7287, 3716, 1577, 5879, 396, 6036, 3100, 3099]}, {"qid": 2326, "question": "Which approaches did they use? in IndoSum: A New Benchmark Dataset for Indonesian Text Summarization", "answer": ["SumBasic, Lsa, LexRank, TextRank, Bayes, Hmm, MaxEnt, NeuralSum, Lead-N"], "top_k_doc_id": [3715, 6144, 3716, 3717, 3718, 6036, 6146, 6147, 4478, 4481, 1132, 6716, 6573, 2797, 1217], "orig_top_k_doc_id": [3718, 3715, 3717, 3716, 6144, 6036, 6147, 4478, 6146, 4481, 1132, 6716, 6573, 2797, 1217]}, {"qid": 3784, "question": "what neural network models were explored? in Toward a Standardized and More Accurate Indonesian Part-of-Speech Tagging", "answer": ["Feedforward, biLSTM", "feedforward, bidirectional LSTM (biLSTM)", "feedforward network , bidirectional LSTM"], "top_k_doc_id": [3715, 6144, 3100, 4862, 6036, 6146, 6151, 3716, 6147, 930, 4526, 3087, 396, 2238, 1826], "orig_top_k_doc_id": [6144, 6151, 4862, 6036, 930, 4526, 3087, 3716, 3715, 396, 3100, 2238, 6146, 6147, 1826]}, {"qid": 2328, "question": "Did they use a crowdsourcing platform for the summaries? in IndoSum: A New Benchmark Dataset for Indonesian Text Summarization", "answer": ["No"], "top_k_doc_id": [3715, 6144, 3716, 3717, 3718, 5718, 5719, 2226, 6840, 6496, 5542, 7615, 7241, 6716, 4425], "orig_top_k_doc_id": [3718, 3715, 3717, 3716, 5718, 6144, 5719, 2226, 6840, 6496, 5542, 7615, 7241, 6716, 4425]}, {"qid": 3786, "question": "what datasets have been used for this task? in Toward a Standardized and More Accurate Indonesian Part-of-Speech Tagging", "answer": ["IDN Tagged Corpus ", "IDN Tagged Corpus", " IDN Tagged Corpus"], "top_k_doc_id": [3715, 6144, 3100, 4862, 6036, 6146, 6151, 3087, 7439, 6467, 3011, 3099, 5291, 449, 1578], "orig_top_k_doc_id": [6144, 4862, 6036, 3715, 3100, 3087, 6151, 7439, 6467, 3011, 3099, 5291, 449, 6146, 1578]}]}
{"group_id": 185, "group_size": 8, "items": [{"qid": 2895, "question": "What baseline model is used? in Attention-based method for categorizing different types of online harassment language", "answer": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 934, 7258, 7775, 4948, 3586, 7807], "orig_top_k_doc_id": [5085, 5086, 935, 7256, 5088, 6285, 3584, 3586, 7775, 3581, 7258, 6624, 4948, 934, 7807]}, {"qid": 2900, "question": "What was the baseline? in Attention-based method for categorizing different types of online harassment language", "answer": ["LastStateRNN, AvgRNN, AttentionRNN"], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 934, 7258, 7775, 4948, 3586, 936], "orig_top_k_doc_id": [5085, 5086, 935, 7256, 5088, 6285, 6624, 3584, 3581, 4948, 3586, 934, 7775, 7258, 936]}, {"qid": 2901, "question": "What were the datasets used in this paper? in Attention-based method for categorizing different types of online harassment language", "answer": ["The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. ", "Twitter dataset provided by organizers containing harassment and non-harassment tweets"], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 934, 7258, 7775, 4948, 7807, 4136], "orig_top_k_doc_id": [5085, 7256, 935, 5086, 6285, 3581, 3584, 7775, 6624, 4948, 934, 7807, 4136, 7258, 5088]}, {"qid": 2894, "question": "What language(s) is/are represented in the dataset? in Attention-based method for categorizing different types of online harassment language", "answer": ["english", "english"], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 934, 412, 3586, 4948, 7807, 6623], "orig_top_k_doc_id": [5085, 935, 5086, 7256, 3584, 6285, 3581, 934, 3586, 5088, 6624, 4948, 6623, 7807, 412]}, {"qid": 2898, "question": "What dataset is used for this work? in Attention-based method for categorizing different types of online harassment language", "answer": ["Twitter dataset provided by the organizers", "The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference."], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 934, 412, 3586, 4948, 7807, 7775], "orig_top_k_doc_id": [5085, 5086, 935, 7256, 6285, 3584, 3581, 4948, 5088, 7807, 412, 6624, 3586, 934, 7775]}, {"qid": 2899, "question": "What types of online harassment are studied? in Attention-based method for categorizing different types of online harassment language", "answer": ["indirect harassment, sexual and physical harassment", "indirect, physical, sexual"], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 934, 7258, 7775, 412, 4136, 3586], "orig_top_k_doc_id": [5085, 7256, 935, 5086, 6285, 934, 3584, 412, 7775, 6624, 3581, 5088, 7258, 4136, 3586]}, {"qid": 2897, "question": "What are the different variations of the attention-based approach which are examined? in Attention-based method for categorizing different types of online harassment language", "answer": ["classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer", " four attention mechanisms instead of one, a projection layer for the word embeddings"], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3581, 6624, 7807, 495, 5087, 4948, 3586, 7258], "orig_top_k_doc_id": [5085, 5086, 5088, 935, 7256, 6285, 7807, 495, 5087, 3581, 6624, 3584, 4948, 3586, 7258]}, {"qid": 2896, "question": "Which variation provides the best results on this dataset? in Attention-based method for categorizing different types of online harassment language", "answer": ["the model with multi-attention mechanism and a projected layer", "Projected Layer"], "top_k_doc_id": [935, 3584, 5085, 5086, 5088, 6285, 7256, 3989, 3586, 4948, 4136, 7258, 938, 412, 934], "orig_top_k_doc_id": [5085, 5086, 935, 7256, 3989, 3586, 4948, 5088, 4136, 6285, 7258, 938, 412, 3584, 934]}]}
{"group_id": 186, "group_size": 8, "items": [{"qid": 2905, "question": "How big is dataset of car-speak language? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["$3,209$ reviews ", "$3,209$ reviews about 553 different cars from 49 different car manufacturers"], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 3209, 6440, 7517, 1291, 2271, 6439, 2578, 1296, 2270], "orig_top_k_doc_id": [5090, 5089, 6440, 6776, 4143, 3167, 3209, 7517, 6439, 226, 2271, 2578, 1291, 1296, 2270]}, {"qid": 2908, "question": "How does car speak pertains to a car's physical attributes? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["we do not know exactly"], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 3209, 6440, 7517, 1291, 2271, 2723, 3789, 4216, 5257], "orig_top_k_doc_id": [5089, 5090, 4143, 3167, 1291, 3209, 6776, 2723, 6440, 2271, 226, 3789, 4216, 7517, 5257]}, {"qid": 2904, "question": "What are labels in car speak language dataset? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["car ", "the car"], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 3209, 6440, 7517, 3789, 1187, 790, 1296, 5944, 1185], "orig_top_k_doc_id": [5090, 5089, 6776, 4143, 3167, 3209, 3789, 1187, 790, 6440, 7517, 1296, 226, 5944, 1185]}, {"qid": 2903, "question": "Is order of \"words\" important in car speak language? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["No", "No"], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 3209, 6440, 2575, 5619, 5944, 538, 955, 1187, 6438], "orig_top_k_doc_id": [5090, 5089, 6776, 4143, 3167, 6440, 226, 2575, 5619, 3209, 5944, 538, 955, 1187, 6438]}, {"qid": 2906, "question": "What is the performance of classifiers? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject", "Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778."], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 3209, 227, 790, 1187, 2578, 3069, 3789, 7746, 2538], "orig_top_k_doc_id": [5090, 5089, 790, 6776, 3789, 4143, 226, 3209, 7746, 1187, 3069, 3167, 2578, 2538, 227]}, {"qid": 2907, "question": "What classifiers have been trained? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 3209, 227, 790, 1187, 2578, 3069, 3789, 7746, 7514], "orig_top_k_doc_id": [5090, 5089, 6776, 790, 3789, 4143, 226, 2578, 3069, 3209, 3167, 7514, 1187, 7746, 227]}, {"qid": 2902, "question": "Is car-speak language collection of abstract features that classifier is later trained on? in Understanding Car-Speak: Replacing Humans in Dealerships", "answer": ["No", "No"], "top_k_doc_id": [226, 6776, 3167, 4143, 5089, 5090, 2542, 6440, 1187, 1291, 7222, 7068, 2283, 2578, 1297], "orig_top_k_doc_id": [5090, 5089, 6776, 226, 4143, 2542, 6440, 3167, 1187, 1291, 7222, 7068, 2283, 2578, 1297]}, {"qid": 4498, "question": "Do the authors provide any benchmark tasks in this new environment? in HoME: a Household Multimodal Environment", "answer": ["No", "No"], "top_k_doc_id": [226, 6776, 7037, 7038, 7016, 3279, 7018, 2885, 6765, 6766, 406, 1541, 5680, 5667, 5685], "orig_top_k_doc_id": [7037, 7038, 7016, 3279, 7018, 2885, 6765, 6776, 226, 6766, 406, 1541, 5680, 5667, 5685]}]}
{"group_id": 187, "group_size": 8, "items": [{"qid": 2951, "question": "Do they report results only on English data? in Automated News Suggestions for Populating Wikipedia Entity Pages", "answer": ["Yes", "Yes"], "top_k_doc_id": [5128, 2023, 5121, 5122, 5123, 5130, 4865, 6227, 4573, 213, 2975, 3446, 4109, 6271, 3287], "orig_top_k_doc_id": [5122, 5121, 5130, 2023, 5128, 5123, 4109, 213, 3446, 6271, 2975, 4865, 3287, 6227, 4573]}, {"qid": 2952, "question": "What baseline model is used? in Automated News Suggestions for Populating Wikipedia Entity Pages", "answer": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "top_k_doc_id": [5128, 2023, 5121, 5122, 5123, 5130, 4865, 6227, 4573, 213, 2975, 3446, 5126, 5127, 3784], "orig_top_k_doc_id": [5122, 5130, 2023, 5121, 5128, 5123, 213, 3446, 5126, 4573, 6227, 5127, 3784, 4865, 2975]}, {"qid": 2953, "question": "What news article sources are used? in Automated News Suggestions for Populating Wikipedia Entity Pages", "answer": ["No", " the news external references in Wikipedia"], "top_k_doc_id": [5128, 2023, 5121, 5122, 5123, 5130, 4865, 6227, 3784, 5124, 5125, 5127, 6474, 3287, 7499], "orig_top_k_doc_id": [5122, 5121, 5130, 5128, 5123, 5127, 4865, 5124, 2023, 3287, 5125, 6227, 3784, 6474, 7499]}, {"qid": 2954, "question": "How do they determine the exact section to use the input article? in Automated News Suggestions for Populating Wikipedia Entity Pages", "answer": ["They use a multi-class classifier to determine the section it should be cited"], "top_k_doc_id": [5128, 2023, 5121, 5122, 5123, 5130, 4865, 6227, 4573, 5127, 7182, 5126, 6736, 3287, 7241], "orig_top_k_doc_id": [5122, 5123, 5121, 5130, 5128, 5127, 7182, 5126, 2023, 6736, 3287, 4573, 6227, 4865, 7241]}, {"qid": 2955, "question": "What features are used to represent the novelty of news articles to entity pages? in Automated News Suggestions for Populating Wikipedia Entity Pages", "answer": ["KL-divergences of language models for the news article and the already added news references", "KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6"], "top_k_doc_id": [5128, 2023, 5121, 5122, 5123, 5130, 4865, 6227, 3784, 5124, 5125, 5127, 6474, 5126, 213], "orig_top_k_doc_id": [5122, 5121, 5130, 5123, 5128, 5127, 5126, 5124, 213, 5125, 4865, 2023, 3784, 6474, 6227]}, {"qid": 2956, "question": "What features are used to represent the salience and relative authority of entities? in Automated News Suggestions for Populating Wikipedia Entity Pages", "answer": ["Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.\nThe relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.", "positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank "], "top_k_doc_id": [5128, 2023, 5121, 5122, 5123, 5130, 5124, 5126, 5125, 5127, 210, 2975, 3460, 4321, 213], "orig_top_k_doc_id": [5122, 5123, 5121, 5130, 5124, 5126, 5125, 5128, 5127, 2023, 210, 2975, 3460, 4321, 213]}, {"qid": 4604, "question": "How many tags are included in the ENE tag set? in Multi-class Multilingual Classification of Wikipedia Articles Using Extended Named Entity Tag Set", "answer": ["141 ", "200 fine-grained categories", "200"], "top_k_doc_id": [5128, 2824, 4668, 4866, 5127, 6151, 7101, 7182, 2206, 7183, 3938, 2731, 1774, 6144, 4881], "orig_top_k_doc_id": [7182, 5127, 4668, 6151, 2206, 7183, 3938, 7101, 2731, 4866, 1774, 5128, 6144, 2824, 4881]}, {"qid": 4605, "question": "Does the paper evaluate the dataset for smaller NE tag tests?  in Multi-class Multilingual Classification of Wikipedia Articles Using Extended Named Entity Tag Set", "answer": ["No"], "top_k_doc_id": [5128, 2824, 4668, 4866, 5127, 6151, 7101, 7182, 610, 358, 2348, 357, 6870, 7243, 1018], "orig_top_k_doc_id": [7182, 610, 4668, 6151, 7101, 358, 5128, 2824, 4866, 2348, 357, 5127, 6870, 7243, 1018]}]}
{"group_id": 188, "group_size": 8, "items": [{"qid": 2974, "question": "What is the performance of their method? in Text Summarization using Abstract Meaning Representation", "answer": ["No"], "top_k_doc_id": [4828, 5138, 5396, 5718, 6566, 2334, 2917, 3198, 4825, 4826, 6053, 6184, 3233, 5397, 1856], "orig_top_k_doc_id": [5138, 2917, 3198, 4825, 4826, 5718, 4828, 6184, 3233, 5396, 2334, 6566, 6053, 5397, 1856]}, {"qid": 2975, "question": "Which evaluation methods are used? in Text Summarization using Abstract Meaning Representation", "answer": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "top_k_doc_id": [4828, 5138, 5396, 5718, 6566, 3715, 4829, 6496, 6569, 2225, 3198, 4424, 1865, 6053, 2334], "orig_top_k_doc_id": [5138, 5396, 5718, 4828, 3715, 6566, 2225, 3198, 6569, 4829, 4424, 1865, 6053, 6496, 2334]}, {"qid": 2976, "question": "What dataset is used in this paper? in Text Summarization using Abstract Meaning Representation", "answer": ["AMR Bank, CNN-Dailymail", "AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )"], "top_k_doc_id": [4828, 5138, 5396, 5718, 6566, 2334, 2917, 3198, 4825, 4826, 6053, 6184, 3199, 3298, 5540], "orig_top_k_doc_id": [5138, 6184, 2917, 3198, 4826, 6566, 5718, 4825, 2334, 3199, 4828, 3298, 5396, 5540, 6053]}, {"qid": 2977, "question": "Which other methods do they compare with? in Text Summarization using Abstract Meaning Representation", "answer": ["Lead-3, Lead-1-AMR", "Lead-3 model,  Lead-1-AMR, BIBREF0 "], "top_k_doc_id": [4828, 5138, 5396, 5718, 6566, 3715, 4829, 6496, 6569, 679, 4825, 4826, 2917, 5139, 1695], "orig_top_k_doc_id": [5138, 6566, 3715, 679, 4825, 4828, 5396, 5718, 4826, 4829, 2917, 5139, 6496, 6569, 1695]}, {"qid": 1693, "question": "What is the problem with existing metrics that they are trying to address? in Facet-Aware Evaluation for Extractive Text Summarization", "answer": ["Answer with content missing: (whole introduction) However, recent\nstudies observe the limits of ROUGE and find in\nsome cases, it fails to reach consensus with human.\njudgment (Paulus et al., 2017; Schluter, 2017)."], "top_k_doc_id": [4828, 2419, 2420, 5557, 7281, 1132, 4478, 5142, 7280, 6955, 5138, 3194, 6957, 2334, 3936], "orig_top_k_doc_id": [2419, 2420, 1132, 5142, 5557, 7281, 4828, 6955, 5138, 7280, 3194, 6957, 2334, 4478, 3936]}, {"qid": 1694, "question": "How do they evaluate their proposed metric? in Facet-Aware Evaluation for Extractive Text Summarization", "answer": ["manually labeled and tell exactly if one sentence should be extracted (assuming our annotations are in agreement), to further verify that FAR correlates with human preference,"], "top_k_doc_id": [4828, 2419, 2420, 5557, 7281, 1132, 4478, 5142, 7280, 3715, 5554, 5556, 111, 4829, 7137], "orig_top_k_doc_id": [2419, 2420, 4828, 5557, 3715, 5554, 7281, 5142, 5556, 4478, 111, 4829, 7280, 7137, 1132]}, {"qid": 1695, "question": "What is a facet? in Facet-Aware Evaluation for Extractive Text Summarization", "answer": ["No"], "top_k_doc_id": [4828, 2419, 2420, 5557, 7281, 6435, 3936, 6789, 1077, 2334, 7283, 3715, 6924, 5701, 5138], "orig_top_k_doc_id": [2419, 2420, 6435, 5557, 3936, 6789, 4828, 7281, 1077, 2334, 7283, 3715, 6924, 5701, 5138]}, {"qid": 2978, "question": "How are sentences selected from the summary graph? in Text Summarization using Abstract Meaning Representation", "answer": [" finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs", " Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities)."], "top_k_doc_id": [4828, 5138, 5396, 5718, 6566, 5142, 5143, 545, 5997, 4380, 3198, 456, 5540, 3299, 734], "orig_top_k_doc_id": [5138, 5142, 5143, 6566, 5718, 545, 5396, 5997, 4380, 4828, 3198, 456, 5540, 3299, 734]}]}
{"group_id": 189, "group_size": 8, "items": [{"qid": 3180, "question": "What is a semicharacter architecture? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 4199, 6868, 6863, 4635, 6314, 6866, 5666, 6865, 5356], "orig_top_k_doc_id": [5351, 6864, 5354, 5353, 6868, 6865, 5667, 4635, 4199, 6314, 5352, 6866, 6863, 5666, 5356]}, {"qid": 3184, "question": "How do the backoff strategies work? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 4199, 6868, 6863, 4635, 6314, 6866, 5666, 6865, 5355], "orig_top_k_doc_id": [5351, 5354, 5352, 5353, 5355, 6864, 6868, 5667, 6863, 6866, 4199, 6865, 6314, 4635, 5666]}, {"qid": 3179, "question": "What end tasks do they evaluate on? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["Sentiment analysis and paraphrase detection under adversarial attacks"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 4199, 6868, 6863, 4635, 6314, 6866, 4202, 3861, 5157], "orig_top_k_doc_id": [5351, 6864, 5353, 5354, 6868, 5352, 4199, 6314, 4635, 6863, 5667, 4202, 3861, 6866, 5157]}, {"qid": 3182, "question": "Why is the adversarial setting appropriate for misspelling recognition? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["Adversarial misspellings are a real-world problem"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 4199, 6868, 6863, 4635, 4205, 5355, 5356, 5666, 4207], "orig_top_k_doc_id": [5351, 5354, 5353, 6864, 4199, 4205, 5352, 4635, 5355, 5667, 6868, 5356, 5666, 6863, 4207]}, {"qid": 3178, "question": "What does the \"sensitivity\" quantity denote? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["the number of distinct word recognition outputs that an attacker can induce", "The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations ", "the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is \u201cfooled\u201d"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 4199, 6868, 6863, 5355, 5356, 6866, 4512, 6865, 3565], "orig_top_k_doc_id": [5353, 5351, 5352, 6864, 5355, 5356, 5354, 6868, 6863, 6866, 4512, 5667, 4199, 6865, 3565]}, {"qid": 3183, "question": "Why do they experiment with RNNs instead of transformers for this task? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["No"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 4199, 6868, 7472, 1987, 5355, 4207, 6314, 7664, 6866], "orig_top_k_doc_id": [5354, 5351, 6864, 6868, 5352, 4199, 5353, 5667, 7472, 1987, 5355, 4207, 6314, 7664, 6866]}, {"qid": 3181, "question": "Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction? in Combating Adversarial Misspellings with Robust Word Recognition", "answer": ["No"], "top_k_doc_id": [5351, 5353, 5667, 5352, 5354, 6864, 1470, 1987, 6443, 5355, 5842, 1895, 6480, 5666, 1469], "orig_top_k_doc_id": [5351, 1470, 1987, 5352, 6443, 5354, 5355, 5353, 5842, 1895, 6864, 5667, 6480, 5666, 1469]}, {"qid": 3912, "question": "Are there experiments with real data? in Robust Speech Recognition Using Generative Adversarial Networks", "answer": ["Yes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech.", "No", "Yes"], "top_k_doc_id": [5351, 5353, 5667, 6314, 276, 1365, 1014, 3562, 1597, 3566, 2187, 1061, 6588, 3328, 95], "orig_top_k_doc_id": [6314, 276, 5667, 1365, 5353, 1014, 5351, 3562, 1597, 3566, 2187, 1061, 6588, 3328, 95]}]}
{"group_id": 190, "group_size": 8, "items": [{"qid": 3185, "question": "What baseline model is used? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["same baseline as used by lang2011unsupervised", "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head."], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 68, 2162, 5714, 2154, 5711, 5710, 2972, 6783], "orig_top_k_doc_id": [5357, 5361, 5360, 5358, 5359, 5780, 5711, 5714, 5715, 5710, 2972, 2162, 6783, 68, 2154]}, {"qid": 3190, "question": "What does an individual model consist of? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 68, 2162, 5714, 2154, 5711, 5710, 69, 7510], "orig_top_k_doc_id": [5357, 5361, 69, 5360, 5358, 5359, 68, 5714, 5715, 5780, 5711, 5710, 2162, 7510, 2154]}, {"qid": 3191, "question": "Do they improve on state-of-the-art semantic role induction? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["No", "Yes"], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 68, 2162, 5714, 2154, 5711, 5710, 3746, 73], "orig_top_k_doc_id": [5357, 5361, 5714, 2154, 5360, 68, 5780, 5715, 5711, 5358, 2162, 5710, 3746, 73, 5359]}, {"qid": 3188, "question": "Overall, does having parallel data improve semantic role induction across multiple languages? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["No", "No"], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 68, 2162, 5714, 2154, 5711, 5716, 80, 69], "orig_top_k_doc_id": [5357, 5361, 5360, 5358, 5715, 5359, 5714, 5711, 2154, 5716, 80, 2162, 5780, 68, 69]}, {"qid": 3186, "question": "Which additional latent variables are used in the model? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["CLV as a parent of the two corresponding role variables", "crosslingual latent variables"], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 5779, 6782, 4674, 5711, 4677, 4354, 5217, 5714], "orig_top_k_doc_id": [5357, 5361, 5358, 5360, 5359, 5780, 4674, 5779, 5711, 4677, 4354, 5715, 5217, 6782, 5714]}, {"qid": 3187, "question": "Which parallel corpora are used? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14", "the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 "], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 68, 2162, 5714, 3746, 5027, 1887, 5710, 5716], "orig_top_k_doc_id": [5357, 5361, 5360, 5715, 5358, 5359, 3746, 5027, 5780, 1887, 2162, 5714, 5710, 68, 5716]}, {"qid": 3189, "question": "Do they add one latent variable for each language pair in their Bayesian model? in A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "answer": ["Yes"], "top_k_doc_id": [5360, 5361, 5357, 5358, 5359, 5715, 5780, 5779, 6782, 69, 600, 599, 6959, 6761, 962], "orig_top_k_doc_id": [5357, 5361, 5358, 5780, 5360, 5359, 69, 5779, 600, 599, 6959, 6761, 6782, 5715, 962]}, {"qid": 1539, "question": "What clustering algorithm is used on top of the VerbNet-specialized representations? in Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "answer": ["MNCut spectral clustering algorithm BIBREF58"], "top_k_doc_id": [5360, 5361, 2156, 2155, 2154, 247, 5711, 6035, 5713, 4712, 2162, 5710, 4475, 4344, 5703], "orig_top_k_doc_id": [2156, 2155, 2154, 5361, 247, 5711, 6035, 5360, 5713, 4712, 2162, 5710, 4475, 4344, 5703]}]}
{"group_id": 191, "group_size": 8, "items": [{"qid": 3373, "question": "What were the baselines? in Cell-aware Stacked LSTMs for Modeling Sentences", "answer": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "top_k_doc_id": [2607, 2608, 2609, 4811, 1178, 2120, 4775, 5595, 5596, 5598, 6504, 1842, 4273, 3981, 6828], "orig_top_k_doc_id": [5595, 5598, 2607, 5596, 4811, 4775, 2608, 2609, 2120, 4273, 6504, 1178, 3981, 6828, 1842]}, {"qid": 3374, "question": "Which datasets were used? in Cell-aware Stacked LSTMs for Modeling Sentences", "answer": ["SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25", "SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25"], "top_k_doc_id": [2607, 2608, 2609, 4811, 1178, 2120, 4775, 5595, 5596, 5598, 6504, 1842, 4984, 5864, 2859], "orig_top_k_doc_id": [5595, 5598, 2607, 5596, 2608, 4775, 4811, 2609, 4984, 6504, 1178, 2120, 1842, 5864, 2859]}, {"qid": 3371, "question": "Which models did they experiment with? in Cell-aware Stacked LSTMs for Modeling Sentences", "answer": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "top_k_doc_id": [2607, 2608, 2609, 4811, 1178, 2120, 4775, 5595, 5596, 5598, 6504, 4984, 7497, 5228, 2610], "orig_top_k_doc_id": [5595, 2607, 5596, 5598, 4775, 2120, 4811, 2608, 6504, 1178, 4984, 7497, 5228, 2609, 2610]}, {"qid": 2750, "question": "What sentiment classification dataset is used? in Quasi-Recurrent Neural Networks", "answer": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "top_k_doc_id": [2607, 2608, 2609, 4811, 24, 2610, 3825, 4814, 5769, 1325, 3863, 5485, 1665, 1043, 5896], "orig_top_k_doc_id": [4811, 4814, 2607, 2610, 24, 2609, 5769, 1325, 3863, 5485, 1665, 1043, 3825, 2608, 5896]}, {"qid": 2751, "question": "What pooling function is used? in Quasi-Recurrent Neural Networks", "answer": ["dynamic average pooling", " f-pooling, fo-pooling, and ifo-pooling "], "top_k_doc_id": [2607, 2608, 2609, 4811, 24, 2610, 3825, 4814, 4812, 3088, 6016, 770, 5288, 5567, 7363], "orig_top_k_doc_id": [4811, 4814, 24, 2608, 2610, 2607, 2609, 4812, 3088, 6016, 770, 5288, 3825, 5567, 7363]}, {"qid": 3372, "question": "What were their best results on the benchmark datasets? in Cell-aware Stacked LSTMs for Modeling Sentences", "answer": ["In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5", "accuracy of 87.0%"], "top_k_doc_id": [2607, 2608, 2609, 4811, 1178, 2120, 4775, 5595, 5596, 5598, 3069, 4984, 2874, 3150, 2611], "orig_top_k_doc_id": [5595, 5598, 2607, 5596, 2608, 2609, 4775, 3069, 4984, 2120, 2874, 3150, 2611, 1178, 4811]}, {"qid": 1787, "question": "By how much do they outperform BiLSTMs in Sentiment Analysis? in Recurrently Controlled Recurrent Networks", "answer": ["Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets."], "top_k_doc_id": [2607, 1390, 2611, 3829, 6612, 7237, 7505, 5485, 105, 2610, 1578, 5486, 5417, 2608, 4813], "orig_top_k_doc_id": [2607, 2611, 5485, 105, 3829, 2610, 1578, 6612, 5486, 5417, 2608, 7505, 1390, 4813, 7237]}, {"qid": 1788, "question": "Does their model have more parameters than other models? in Recurrently Controlled Recurrent Networks", "answer": ["approximately equal parameterization"], "top_k_doc_id": [2607, 1390, 2611, 3829, 6612, 7237, 7505, 4814, 7183, 7238, 6247, 4811, 2374, 3103, 7504], "orig_top_k_doc_id": [2611, 1390, 2607, 7237, 6612, 4814, 7183, 7238, 7505, 6247, 3829, 4811, 2374, 3103, 7504]}]}
{"group_id": 192, "group_size": 8, "items": [{"qid": 3491, "question": "what were the evaluation metrics? in Compound Probabilistic Context-Free Grammars for Grammar Induction", "answer": ["INLINEFORM0 scores", "Unlabeled sentence-level F1, perplexity, grammatically judgment performance"], "top_k_doc_id": [2022, 3490, 3496, 5779, 7050, 1222, 2014, 2015, 2023, 4007, 5780, 1186, 2518, 5781, 4005], "orig_top_k_doc_id": [5779, 5780, 3490, 2015, 1222, 2022, 2023, 5781, 1186, 4005, 4007, 7050, 3496, 2518, 2014]}, {"qid": 3492, "question": "what are the state of the art methods? in Compound Probabilistic Context-Free Grammars for Grammar Induction", "answer": ["No", "No"], "top_k_doc_id": [2022, 3490, 3496, 5779, 7050, 1222, 2014, 2015, 2023, 4007, 5780, 1186, 2518, 5781, 3495], "orig_top_k_doc_id": [5779, 5780, 3490, 2015, 1222, 2023, 7050, 1186, 2022, 4007, 2014, 5781, 3496, 2518, 3495]}, {"qid": 3493, "question": "what english datasets were used? in Compound Probabilistic Context-Free Grammars for Grammar Induction", "answer": ["Answer with content missing: (Data section) Penn Treebank (PTB)"], "top_k_doc_id": [2022, 3490, 3496, 5779, 7050, 1222, 2014, 2015, 2023, 4007, 5780, 4005, 4006, 7053, 1186], "orig_top_k_doc_id": [5779, 5780, 3490, 2015, 1222, 4007, 2022, 4005, 2023, 4006, 1186, 3496, 2014, 7053, 7050]}, {"qid": 3494, "question": "which chinese datasets were used? in Compound Probabilistic Context-Free Grammars for Grammar Induction", "answer": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "top_k_doc_id": [2022, 3490, 3496, 5779, 7050, 1222, 2014, 2015, 2023, 4007, 5780, 4005, 4006, 7053, 7054], "orig_top_k_doc_id": [5779, 5780, 3490, 2015, 2022, 1222, 4005, 7053, 2023, 7054, 2014, 7050, 4006, 3496, 4007]}, {"qid": 2228, "question": "Do they evaluate the syntactic parses? in A Probabilistic Generative Grammar for Semantic Parsing", "answer": ["No"], "top_k_doc_id": [2022, 3490, 3496, 5779, 7050, 3489, 3495, 7054, 6937, 1591, 7053, 3916, 2735, 4678, 1586], "orig_top_k_doc_id": [3490, 3496, 3489, 2022, 7050, 6937, 1591, 7053, 3916, 2735, 3495, 4678, 5779, 7054, 1586]}, {"qid": 2229, "question": "What knowledge bases do they use? in A Probabilistic Generative Grammar for Semantic Parsing", "answer": ["NELL"], "top_k_doc_id": [2022, 3490, 3496, 5779, 7050, 3489, 3495, 7054, 2023, 4560, 5943, 1185, 7605, 1186, 1389], "orig_top_k_doc_id": [3490, 3489, 7050, 2022, 2023, 4560, 5779, 5943, 7054, 3496, 1185, 3495, 7605, 1186, 1389]}, {"qid": 1449, "question": "How did they induce the CFG? in Joint learning of ontology and semantic parser from text", "answer": ["the parser first learns to parse simple sentences, then proceeds to learn more complex ones. The induction method is iterative, semi-automatic and based on frequent patterns"], "top_k_doc_id": [2022, 3490, 2014, 2015, 2016, 2018, 2023, 3489, 4345, 5816, 3495, 7722, 3497, 80, 1389], "orig_top_k_doc_id": [2014, 2022, 2015, 2023, 3495, 7722, 4345, 3489, 3497, 2018, 3490, 2016, 80, 1389, 5816]}, {"qid": 1450, "question": "How big is their dataset? in Joint learning of ontology and semantic parser from text", "answer": ["1.1 million sentences, 119 different relation types (unique predicates)"], "top_k_doc_id": [2022, 3490, 2014, 2015, 2016, 2018, 2023, 3489, 4345, 5816, 683, 2019, 2021, 1737, 6908], "orig_top_k_doc_id": [2014, 2022, 2015, 3490, 683, 2023, 5816, 3489, 2019, 2016, 4345, 2018, 2021, 1737, 6908]}]}
{"group_id": 193, "group_size": 8, "items": [{"qid": 3593, "question": "What approaches do they use towards text analysis? in How we do things with words: Analyzing text as social and cultural data", "answer": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "top_k_doc_id": [242, 5905, 234, 5527, 5915, 6417, 1758, 6896, 1757, 2074, 5910, 6208, 5914, 5909, 447], "orig_top_k_doc_id": [5915, 5905, 5527, 242, 5910, 1758, 6208, 6417, 5914, 234, 1757, 5909, 447, 2074, 6896]}, {"qid": 3594, "question": "What dataset do they use for analysis? in How we do things with words: Analyzing text as social and cultural data", "answer": ["No"], "top_k_doc_id": [242, 5905, 234, 5527, 5915, 6417, 1758, 6896, 1757, 2074, 5910, 6208, 5908, 1419, 5907], "orig_top_k_doc_id": [6417, 5915, 5905, 1758, 1757, 234, 242, 5527, 6896, 6208, 5908, 5910, 2074, 1419, 5907]}, {"qid": 3596, "question": "What background do they have? in How we do things with words: Analyzing text as social and cultural data", "answer": ["No"], "top_k_doc_id": [242, 5905, 234, 5527, 5915, 6417, 1758, 6896, 1757, 5908, 5379, 5909, 4229, 5425, 3045], "orig_top_k_doc_id": [5908, 6417, 5915, 5379, 5909, 5905, 6896, 1758, 1757, 234, 4229, 242, 5527, 5425, 3045]}, {"qid": 3595, "question": "Do they demonstrate why interdisciplinary insights are important? in How we do things with words: Analyzing text as social and cultural data", "answer": ["No", "No"], "top_k_doc_id": [242, 5905, 234, 5527, 5915, 6417, 1758, 6896, 6209, 1955, 6210, 1419, 4112, 6207, 12], "orig_top_k_doc_id": [5905, 234, 6209, 1955, 6210, 6417, 1419, 5915, 4112, 5527, 1758, 6896, 6207, 242, 12]}, {"qid": 5041, "question": "What language model is trained? in Automatic Extraction of Personality from Text: Challenges and Opportunities", "answer": ["ULMFiT", "ULMFiT BIBREF21"], "top_k_doc_id": [242, 5905, 3145, 5406, 5409, 447, 2768, 6005, 7856, 2789, 7838, 7016, 5408, 4399, 7858], "orig_top_k_doc_id": [242, 5409, 5406, 3145, 5905, 6005, 2768, 7856, 2789, 7838, 7016, 5408, 4399, 447, 7858]}, {"qid": 5042, "question": "What machine learning models are considered? in Automatic Extraction of Personality from Text: Challenges and Opportunities", "answer": ["RandomForestRegressor, LinearSVR, KNeighborsRegressor, Support Vector Machine Regression", "RandomForestRegressor, LinearSVR, KNeighborsRegressor"], "top_k_doc_id": [242, 5905, 3145, 5406, 5409, 447, 2768, 6005, 7856, 3730, 643, 145, 2798, 5489, 6629], "orig_top_k_doc_id": [242, 5905, 6005, 7856, 3145, 3730, 5409, 643, 145, 2768, 2798, 5489, 6629, 5406, 447]}, {"qid": 3597, "question": "What kind of issues (that are not on the forefront of computational text analysis) do they tackle? in How we do things with words: Analyzing text as social and cultural data", "answer": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a \u201cbig question\u201d that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "top_k_doc_id": [242, 5905, 234, 5527, 5915, 6417, 6208, 6206, 995, 5373, 7743, 7114, 5910, 2329, 5914], "orig_top_k_doc_id": [5905, 5915, 242, 6417, 6208, 6206, 995, 234, 5373, 7743, 7114, 5910, 2329, 5527, 5914]}, {"qid": 5043, "question": "What is the agreement of the dataset? in Automatic Extraction of Personality from Text: Challenges and Opportunities", "answer": ["Answer with content missing: (Table 2): Krippednorff's alpha coefficient for dataset is: Stability -0.26, Extraversion 0.07, Openness 0.36, Agreeableness 0.51, Conscientiousness 0.31", "No"], "top_k_doc_id": [242, 5905, 3145, 5406, 5409, 3593, 7016, 2789, 5408, 3614, 7832, 4895, 5915, 9, 3318], "orig_top_k_doc_id": [242, 5409, 3593, 5406, 5905, 3145, 7016, 2789, 5408, 3614, 7832, 4895, 5915, 9, 3318]}]}
{"group_id": 194, "group_size": 8, "items": [{"qid": 3716, "question": "What baseline do they compare to? in Multitask Learning for Blackmarket Tweet Detection", "answer": [" spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.", "Wu et al. BIBREF4, Rajdev et. al. BIBREF11", "Word2Vec and Doc2Vec to encode the tweets, then MLP classifier; Random Forest classifier on a standard set of features"], "top_k_doc_id": [5485, 5486, 5976, 5977, 6056, 6057, 6058, 6059, 5255, 4561, 4878, 5488, 5979, 1499, 5487], "orig_top_k_doc_id": [6058, 6056, 6057, 6059, 5976, 5255, 5977, 5488, 4561, 4878, 5485, 5979, 1499, 5486, 5487]}, {"qid": 3717, "question": "What language is explored in this paper? in Multitask Learning for Blackmarket Tweet Detection", "answer": ["English", "English", "English"], "top_k_doc_id": [5485, 5486, 5976, 5977, 6056, 6057, 6058, 6059, 5255, 4561, 4878, 3277, 7599, 5180, 3543], "orig_top_k_doc_id": [6058, 6056, 6057, 6059, 5255, 5976, 3277, 5977, 5485, 7599, 4878, 5180, 4561, 5486, 3543]}, {"qid": 3271, "question": "By how much did they improve? in Multitask Learning for Fine-Grained Twitter Sentiment Analysis", "answer": ["They decrease MAE in 0.34"], "top_k_doc_id": [5485, 5486, 756, 1320, 2828, 5255, 5487, 5488, 5775, 4878, 5976, 6640, 7813, 7814, 5776], "orig_top_k_doc_id": [5485, 5486, 5488, 5775, 5487, 756, 1320, 6640, 7813, 5255, 5976, 7814, 4878, 5776, 2828]}, {"qid": 3272, "question": "What dataset did they use? in Multitask Learning for Fine-Grained Twitter Sentiment Analysis", "answer": [" high-quality datasets  from SemEval-2016 \u201cSentiment Analysis in Twitter\u201d task", " SemEval-2016 \u201cSentiment Analysis in Twitter\u201d"], "top_k_doc_id": [5485, 5486, 756, 1320, 2828, 5255, 5487, 5488, 5775, 4878, 5976, 6640, 7005, 3795, 447], "orig_top_k_doc_id": [5485, 5486, 5488, 6640, 1320, 5487, 5255, 5976, 756, 4878, 7005, 2828, 5775, 3795, 447]}, {"qid": 3715, "question": "How did they obtain the tweets? in Multitask Learning for Blackmarket Tweet Detection", "answer": ["crawled two blackmarket sites, used Twitter's REST API", "By crawling YouLikeHits and Like4Like sites and then using Twitter's REST API", "We used Twitter's REST API"], "top_k_doc_id": [5485, 5486, 5976, 5977, 6056, 6057, 6058, 6059, 5255, 414, 4895, 5978, 2080, 3575, 5979], "orig_top_k_doc_id": [6056, 6058, 6057, 6059, 5977, 5976, 5255, 5485, 5486, 414, 4895, 5978, 2080, 3575, 5979]}, {"qid": 3270, "question": "What was the baseline? in Multitask Learning for Fine-Grained Twitter Sentiment Analysis", "answer": ["SVMs, LR, BIBREF2", "SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt"], "top_k_doc_id": [5485, 5486, 756, 1320, 2828, 5255, 5487, 5488, 5775, 447, 606, 3795, 448, 7813, 1686], "orig_top_k_doc_id": [5485, 5486, 5488, 5487, 1320, 5255, 756, 5775, 447, 2828, 606, 3795, 448, 7813, 1686]}, {"qid": 3718, "question": "What blackmarket services do they look at? in Multitask Learning for Blackmarket Tweet Detection", "answer": ["Credit-based Freemium services", "Credit-based Freemium services", "YouLikeHits and Like4Like"], "top_k_doc_id": [5485, 5486, 5976, 5977, 6056, 6057, 6058, 6059, 449, 447, 4280, 4284, 4561, 7029, 4130], "orig_top_k_doc_id": [6056, 6058, 6057, 6059, 5976, 5977, 449, 447, 5485, 4280, 4284, 4561, 7029, 4130, 5486]}, {"qid": 4409, "question": "What parts of their multitask model are shared? in Sequence Labeling Parsing by Learning Across Representations", "answer": ["No", "stacked bilstms"], "top_k_doc_id": [5485, 5486, 1743, 4878, 5980, 7553, 4561, 6937, 6331, 7440, 1552, 6938, 7439, 438, 1365], "orig_top_k_doc_id": [5485, 1743, 4878, 5980, 7553, 4561, 6937, 6331, 7440, 1552, 6938, 7439, 5486, 438, 1365]}]}
{"group_id": 195, "group_size": 8, "items": [{"qid": 3825, "question": "What is the size of their published dataset? in MultiBooked: A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification", "answer": ["911", "The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343.", "910"], "top_k_doc_id": [1049, 1039, 1043, 1045, 1050, 1051, 1044, 3129, 6181, 6182, 6183, 6640, 1048, 5159, 2216], "orig_top_k_doc_id": [1043, 6183, 1044, 6181, 6182, 1050, 1039, 1045, 3129, 6640, 1049, 1051, 5159, 1048, 2216]}, {"qid": 3826, "question": "How many annotators do they have for their dataset? in MultiBooked: A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification", "answer": ["No", "No", "No"], "top_k_doc_id": [1049, 1039, 1043, 1045, 1050, 1051, 1044, 3129, 6181, 6182, 6183, 6640, 1048, 5159, 1040], "orig_top_k_doc_id": [6183, 1043, 6181, 1044, 6182, 6640, 1039, 1050, 1049, 1045, 3129, 1051, 1040, 1048, 5159]}, {"qid": 4180, "question": "What were the results of their experiment? in A Fine-Grained Sentiment Dataset for Norwegian", "answer": [".41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively", " .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$)"], "top_k_doc_id": [1049, 6640, 6643, 6644, 6645, 5486, 5777, 1051, 7472, 2062, 3578, 1050, 7789, 3883, 3795], "orig_top_k_doc_id": [6640, 6644, 6645, 6643, 1049, 1051, 7472, 2062, 3578, 1050, 5777, 7789, 3883, 5486, 3795]}, {"qid": 4181, "question": "How big is the dataset? in A Fine-Grained Sentiment Dataset for Norwegian", "answer": ["7451 sentences", "total of 7451 sentences ", "7451 sentences, 6949 polar expressions, 5289 targets, 635 holders"], "top_k_doc_id": [1049, 6640, 6643, 6644, 6645, 5486, 5777, 5775, 3845, 2306, 5485, 2973, 7856, 2291, 1052], "orig_top_k_doc_id": [6640, 6644, 6645, 1049, 5775, 3845, 6643, 2306, 5486, 5485, 2973, 7856, 5777, 2291, 1052]}, {"qid": 3824, "question": "Do any of their reviews contain translations for both Catalan and Basque? in MultiBooked: A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification", "answer": ["No", "No", "No"], "top_k_doc_id": [1049, 1039, 1043, 1045, 1050, 1051, 1044, 3129, 6181, 6182, 6183, 6640, 1839, 1046, 1040], "orig_top_k_doc_id": [6181, 6183, 1043, 1044, 1050, 6182, 1049, 1045, 1051, 1039, 6640, 1839, 1046, 3129, 1040]}, {"qid": 4182, "question": "What are all the domains the corpus came from? in A Fine-Grained Sentiment Dataset for Norwegian", "answer": ["No", " a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.", "professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc"], "top_k_doc_id": [1049, 6640, 6643, 6644, 6645, 2468, 2973, 1051, 2976, 554, 2977, 1625, 6548, 5485, 6181], "orig_top_k_doc_id": [6640, 6644, 6645, 2468, 1049, 2973, 1051, 2976, 6643, 554, 2977, 1625, 6548, 5485, 6181]}, {"qid": 833, "question": "what baseline do they compare to? in Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study", "answer": ["VecMap, Muse, Barista"], "top_k_doc_id": [1049, 1039, 1043, 1045, 1050, 1051, 1040, 1048, 1053, 1052, 1042, 1041, 1047, 247, 2601], "orig_top_k_doc_id": [1040, 1048, 1053, 1049, 1052, 1039, 1050, 1042, 1041, 1045, 1051, 1047, 247, 1043, 2601]}, {"qid": 4179, "question": "Was the entire annotation process done manually? in A Fine-Grained Sentiment Dataset for Norwegian", "answer": ["Yes", "Yes"], "top_k_doc_id": [1049, 6640, 6643, 6644, 6645, 3625, 5, 1320, 5960, 1689, 5332, 2976, 5037, 4610, 2982], "orig_top_k_doc_id": [6640, 6644, 6645, 1049, 3625, 6643, 5, 1320, 5960, 1689, 5332, 2976, 5037, 4610, 2982]}]}
{"group_id": 196, "group_size": 8, "items": [{"qid": 4417, "question": "What are their evaluation metrics? in Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering", "answer": ["average F1-score, accuracy", "average F1-scores"], "top_k_doc_id": [3216, 3220, 6648, 6947, 6948, 6949, 6950, 6951, 4557, 1120, 3851, 4559, 3221, 1637, 4320], "orig_top_k_doc_id": [6947, 6948, 6951, 6949, 6950, 3220, 6648, 3216, 1120, 3221, 4557, 1637, 4320, 4559, 3851]}, {"qid": 4421, "question": "What datasets do they evaluate on? in Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering", "answer": ["LC-QuAD, QALD-5", "(LC-QuAD) BIBREF8, (QALD-5) dataset BIBREF9"], "top_k_doc_id": [3216, 3220, 6648, 6947, 6948, 6949, 6950, 6951, 4557, 1120, 3851, 4559, 6876, 7664, 2661], "orig_top_k_doc_id": [6947, 6948, 6951, 6949, 6950, 3220, 6648, 1120, 4557, 6876, 3216, 7664, 4559, 3851, 2661]}, {"qid": 4418, "question": "Are their formal queries tree-structured? in Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering", "answer": ["No", "No"], "top_k_doc_id": [3216, 3220, 6648, 6947, 6948, 6949, 6950, 6951, 4557, 1120, 3218, 3221, 3219, 4555, 3223], "orig_top_k_doc_id": [6947, 6948, 6951, 6949, 3220, 6950, 3218, 3221, 3216, 4557, 6648, 3219, 1120, 4555, 3223]}, {"qid": 2586, "question": "What knowledge base do they use? in Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge", "answer": ["Freebase"], "top_k_doc_id": [3216, 3220, 2022, 4555, 4556, 4557, 4558, 4559, 4560, 6251, 3207, 7677, 4640, 1213, 5943], "orig_top_k_doc_id": [4555, 4559, 4558, 4560, 4557, 2022, 6251, 3216, 7677, 4556, 4640, 1213, 3207, 3220, 5943]}, {"qid": 2588, "question": "What task do they evaluate on? in Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge", "answer": ["Fill-in-the-blank natural language questions"], "top_k_doc_id": [3216, 3220, 2022, 4555, 4556, 4557, 4558, 4559, 4560, 6251, 3207, 4872, 2399, 6471, 247], "orig_top_k_doc_id": [4555, 4558, 4559, 4557, 4560, 6251, 2022, 4556, 3207, 3216, 3220, 4872, 2399, 6471, 247]}, {"qid": 4420, "question": "How do they recover from noisy entity linking? in Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering", "answer": ["by filtering errors in noisy entity linking by the empty query check or domain/range check in query structure ranking", "ranked query structures first and considered linking results in the last step, empty query check or domain/range check"], "top_k_doc_id": [3216, 3220, 6648, 6947, 6948, 6949, 6950, 6951, 4557, 7610, 4320, 1240, 4555, 4719, 4559], "orig_top_k_doc_id": [6948, 6951, 6947, 6949, 6950, 4557, 7610, 4320, 1240, 3216, 4555, 4719, 4559, 6648, 3220]}, {"qid": 2587, "question": "How big is their dataset? in Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge", "answer": ["3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing"], "top_k_doc_id": [3216, 3220, 2022, 4555, 4556, 4557, 4558, 4559, 4560, 6251, 3615, 1290, 2447, 7829, 3221], "orig_top_k_doc_id": [4555, 4558, 4559, 4557, 4560, 4556, 2022, 3216, 3615, 6251, 3220, 1290, 2447, 7829, 3221]}, {"qid": 4419, "question": "What knowledge base do they rely on? in Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering", "answer": ["DBpedia (2016-04), DBpedia (2015-10)", "DBpedia"], "top_k_doc_id": [3216, 3220, 6648, 6947, 6948, 6949, 6950, 6951, 6579, 1120, 7147, 4555, 720, 4900, 1637], "orig_top_k_doc_id": [6947, 6948, 6951, 6949, 6950, 3220, 6579, 1120, 3216, 7147, 4555, 720, 4900, 1637, 6648]}]}
{"group_id": 197, "group_size": 8, "items": [{"qid": 4622, "question": "what dataset were used? in Prediction Uncertainty Estimation for Hate Speech Classification", "answer": ["HatEval, YouToxic, OffensiveTweets", "HatEval, YouToxic, OffensiveTweets"], "top_k_doc_id": [3047, 5914, 7232, 7233, 7234, 7235, 1809, 3046, 3582, 3737, 6131, 6285, 770, 5292, 5913], "orig_top_k_doc_id": [7232, 7233, 7235, 7234, 3047, 3582, 5914, 6285, 1809, 770, 3737, 5292, 3046, 6131, 5913]}, {"qid": 4623, "question": "what was the baseline? in Prediction Uncertainty Estimation for Hate Speech Classification", "answer": ["logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ", " logistic regression (LR), Support Vector Machines (SVM)"], "top_k_doc_id": [3047, 5914, 7232, 7233, 7234, 7235, 1809, 3046, 3582, 3737, 6131, 5666, 1080, 5906, 771], "orig_top_k_doc_id": [7232, 7233, 7235, 7234, 3047, 1809, 5914, 5666, 3582, 1080, 3737, 5906, 771, 3046, 6131]}, {"qid": 2012, "question": "What languages are were included in the dataset of hateful content? in Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "answer": ["German"], "top_k_doc_id": [3047, 3045, 3046, 5976, 5977, 6133, 6286, 6375, 6376, 6378, 6176, 5914, 5641, 4137, 5913], "orig_top_k_doc_id": [3045, 3046, 3047, 6375, 6176, 6286, 5976, 6376, 5914, 5977, 5641, 4137, 5913, 6133, 6378]}, {"qid": 2014, "question": "How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition? in Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "answer": ["participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%)"], "top_k_doc_id": [3047, 3045, 3046, 5641, 5912, 5913, 5914, 6131, 6177, 3589, 5978, 414, 5976, 3588, 412], "orig_top_k_doc_id": [3046, 3045, 5913, 5912, 3047, 5914, 3589, 5978, 6177, 414, 5976, 6131, 3588, 5641, 412]}, {"qid": 2015, "question": "What definition was one of the groups was shown? in Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "answer": ["Twitter definition of hateful conduct"], "top_k_doc_id": [3047, 3045, 3046, 5641, 5912, 5913, 5914, 6131, 6177, 6176, 7857, 5908, 3011, 447, 1384], "orig_top_k_doc_id": [3046, 3045, 3047, 5914, 5913, 6177, 5912, 6131, 6176, 7857, 5908, 3011, 447, 5641, 1384]}, {"qid": 2017, "question": "How were potentially hateful messages identified? in Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "answer": ["10 hashtags that can be used in an insulting or offensive way"], "top_k_doc_id": [3047, 3045, 3046, 5976, 5977, 6133, 6286, 6375, 6376, 6378, 6134, 6379, 1079, 6377, 447], "orig_top_k_doc_id": [3045, 3046, 3047, 6375, 6133, 6376, 6134, 6378, 6379, 5976, 6286, 1079, 6377, 5977, 447]}, {"qid": 4624, "question": "what text embedding methods were used in their approach? in Prediction Uncertainty Estimation for Hate Speech Classification", "answer": ["Word2Vec, ELMo", "Word2Vec and ELMo embeddings."], "top_k_doc_id": [3047, 5914, 7232, 7233, 7234, 7235, 1809, 770, 771, 876, 5292, 5085, 5642, 5913, 1077], "orig_top_k_doc_id": [7232, 7233, 7235, 7234, 770, 3047, 5914, 1809, 771, 876, 5292, 5085, 5642, 5913, 1077]}, {"qid": 2013, "question": "How was reliability measured? in Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "answer": ["level of agreement (Krippendorff's INLINEFORM0 )"], "top_k_doc_id": [3047, 5914, 7232, 7233, 7234, 7235, 3045, 3046, 5913, 6177, 7857, 5911, 3512, 3960, 5041], "orig_top_k_doc_id": [3045, 3046, 5914, 5913, 7235, 6177, 3047, 7233, 7857, 7232, 7234, 5911, 3512, 3960, 5041]}]}
{"group_id": 198, "group_size": 8, "items": [{"qid": 4937, "question": "What aspects are used to judge question quality? in How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions", "answer": ["Is the question grammatically correct?, Is the spelling correct?, Is the question an explicit question, rather than a search query, a command, or a statement?", "Is the question grammatically correct?, Is the spelling correct?, Is the question an explicit question"], "top_k_doc_id": [7664, 7666, 2370, 3789, 5461, 5580, 7665, 7667, 7668, 490, 6823, 2264, 491, 3794, 495], "orig_top_k_doc_id": [7665, 7668, 7667, 7664, 7666, 5580, 490, 5461, 2370, 3789, 6823, 491, 2264, 3794, 495]}, {"qid": 4939, "question": "What characterizes the 303 domains? e.g. is this different subject tags? in How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions", "answer": ["sub areas from Stack Exchange data dumps", "The domains represent different subfields related to the topic of the questions.  "], "top_k_doc_id": [7664, 7666, 2370, 3789, 5461, 5580, 7665, 7667, 7668, 490, 6823, 2264, 5914, 2818, 6339], "orig_top_k_doc_id": [7665, 7664, 7666, 7668, 7667, 5580, 5461, 490, 6823, 2370, 3789, 2264, 5914, 2818, 6339]}, {"qid": 3677, "question": "Do any of the models use attention? in Can Neural Networks Learn Symbolic Rewriting?", "answer": ["Yes", "Yes", "Yes", "Yes"], "top_k_doc_id": [7664, 7666, 2426, 2705, 4620, 6022, 6023, 7514, 686, 4275, 7148, 7368, 3980, 7727, 5554], "orig_top_k_doc_id": [6022, 6023, 7664, 2705, 7666, 7368, 7148, 2426, 7514, 3980, 686, 4620, 4275, 7727, 5554]}, {"qid": 3679, "question": "What is symbolic rewriting? in Can Neural Networks Learn Symbolic Rewriting?", "answer": ["It is a process of translating a set of formal symbolic data to another set of formal symbolic data.", "No", "Symbolic rewriting is the method to rewrite ground and nonground data from one to another form using rules."], "top_k_doc_id": [7664, 7666, 2426, 2705, 4620, 6022, 6023, 7514, 686, 4275, 7148, 7665, 7668, 4621, 7667], "orig_top_k_doc_id": [6023, 6022, 7664, 2705, 2426, 4620, 7665, 7668, 7514, 7666, 4621, 7667, 7148, 686, 4275]}, {"qid": 4935, "question": "Do they report results only on English data? in How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions", "answer": ["No", "Yes"], "top_k_doc_id": [7664, 7666, 2370, 3789, 5461, 5580, 7665, 7667, 7668, 2368, 2369, 4621, 4622, 6823, 7089], "orig_top_k_doc_id": [7665, 7664, 7667, 7666, 7668, 5580, 2370, 2368, 4621, 5461, 3789, 2369, 6823, 4622, 7089]}, {"qid": 4936, "question": "What is the baseline method? in How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions", "answer": ["evaluate the original ill-formed question using the automatic metrics", "we also evaluate the original ill-formed question using the automatic metrics"], "top_k_doc_id": [7664, 7666, 2370, 3789, 5461, 5580, 7665, 7667, 7668, 2368, 2369, 4621, 4622, 490, 5044], "orig_top_k_doc_id": [7665, 7664, 7667, 7668, 7666, 5580, 2370, 4621, 3789, 2368, 5461, 490, 2369, 5044, 4622]}, {"qid": 4938, "question": "What did the human annotations consist of? in How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions", "answer": ["Is the question grammatically correct?, Is the spelling correct?, Is the question an explicit question,  annotators were asked to annotate each aspect with a binary (0/1) answer", "annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13"], "top_k_doc_id": [7664, 7666, 2370, 3789, 5461, 5580, 7665, 7667, 7668, 490, 6823, 5914, 5368, 6793, 352], "orig_top_k_doc_id": [7665, 7664, 7667, 7668, 7666, 5580, 3789, 2370, 6823, 490, 5914, 5461, 5368, 6793, 352]}, {"qid": 3678, "question": "What translation models are explored? in Can Neural Networks Learn Symbolic Rewriting?", "answer": ["NMT architecture BIBREF10", "architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism", "LSTM with attention"], "top_k_doc_id": [7664, 7666, 2426, 2705, 4620, 6022, 6023, 7514, 1743, 4619, 638, 7686, 6588, 1013, 490], "orig_top_k_doc_id": [6022, 6023, 2705, 7664, 1743, 4620, 4619, 638, 7686, 2426, 7666, 6588, 7514, 1013, 490]}]}
{"group_id": 199, "group_size": 7, "items": [{"qid": 42, "question": "How is embedding quality assessed? in Probabilistic Bias Mitigation in Word Embeddings", "answer": ["We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics.", "No"], "top_k_doc_id": [41, 42, 6560, 6964, 5155, 6738, 6965, 3941, 6739, 6962, 43, 6966, 6963, 2311, 7232], "orig_top_k_doc_id": [42, 41, 6964, 6965, 6962, 43, 6560, 6966, 3941, 5155, 6739, 6963, 2311, 7232, 6738]}, {"qid": 43, "question": "What are the three measures of bias which are reduced in experiments? in Probabilistic Bias Mitigation in Word Embeddings", "answer": ["RIPA, Neighborhood Metric, WEAT"], "top_k_doc_id": [41, 42, 6560, 6964, 5155, 6738, 6965, 3941, 6739, 6962, 43, 6966, 5814, 3943, 106], "orig_top_k_doc_id": [41, 42, 6560, 6962, 6965, 6964, 43, 5155, 6738, 3941, 6739, 6966, 5814, 3943, 106]}, {"qid": 4099, "question": "What other scenarios can the bias mitigation methods be applied to? in Reducing Gender Bias in Abusive Language Detection", "answer": ["sentiment analysis , other identity problems like racial", "other identity problems like racial, sentiment analysis", "developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time"], "top_k_doc_id": [41, 42, 6560, 6964, 5155, 6738, 6965, 3988, 5949, 6558, 7266, 7267, 6962, 7261, 3011], "orig_top_k_doc_id": [6560, 6558, 5949, 41, 6965, 5155, 42, 3988, 6738, 7266, 7261, 7267, 6964, 3011, 6962]}, {"qid": 4100, "question": "Are the three bias mitigation methods combined in any model? in Reducing Gender Bias in Abusive Language Detection", "answer": ["Yes", "Debiased Word Embeddings, Gender Swap, Bias fine-tuning"], "top_k_doc_id": [41, 42, 6560, 6964, 5155, 6738, 6965, 3988, 5949, 6558, 7266, 7267, 6962, 6739, 6736], "orig_top_k_doc_id": [6560, 6558, 5949, 5155, 3988, 41, 6965, 42, 7266, 6964, 7267, 6738, 6962, 6739, 6736]}, {"qid": 2997, "question": "how is mitigation of gender bias evaluated? in Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "answer": ["Using INLINEFORM0 and INLINEFORM1", "No"], "top_k_doc_id": [41, 42, 6560, 6964, 5155, 6738, 6965, 3941, 6739, 6962, 5153, 6737, 525, 5154, 7267], "orig_top_k_doc_id": [5153, 5155, 42, 3941, 6965, 6560, 41, 6964, 6962, 6737, 6738, 6739, 525, 5154, 7267]}, {"qid": 4101, "question": "Which of the three bias mitigation methods is most effective? in Reducing Gender Bias in Abusive Language Detection", "answer": ["Gender Swap", "most effective method was applying both debiased embedding and gender swap"], "top_k_doc_id": [41, 42, 6560, 6964, 5155, 6738, 6965, 3988, 5949, 6558, 7266, 7267, 6739, 7261, 6559], "orig_top_k_doc_id": [6560, 5949, 6558, 5155, 7266, 6738, 41, 3988, 6965, 6964, 42, 7267, 6739, 7261, 6559]}, {"qid": 44, "question": "What are the probabilistic observations which contribute to the more robust algorithm? in Probabilistic Bias Mitigation in Word Embeddings", "answer": ["No"], "top_k_doc_id": [41, 42, 6560, 6964, 6888, 43, 1034, 1938, 3538, 4259, 6966, 6962, 1934, 3539, 6126], "orig_top_k_doc_id": [42, 41, 6888, 43, 1034, 1938, 6964, 3538, 4259, 6966, 6962, 6560, 1934, 3539, 6126]}]}
{"group_id": 200, "group_size": 7, "items": [{"qid": 117, "question": "Did they propose other metrics? in Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "answer": ["Yes"], "top_k_doc_id": [3226, 3227, 135, 136, 137, 138, 139, 609, 612, 5636, 1671, 2342, 6590, 5324, 4744], "orig_top_k_doc_id": [135, 139, 136, 137, 138, 5636, 609, 6590, 5324, 612, 1671, 3227, 2342, 3226, 4744]}, {"qid": 119, "question": "How did they obtain human intuitions? in Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "answer": ["No"], "top_k_doc_id": [3226, 3227, 135, 136, 137, 138, 139, 609, 612, 5636, 1671, 2342, 6590, 4895, 3810], "orig_top_k_doc_id": [135, 137, 139, 136, 138, 6590, 4895, 3226, 609, 3227, 5636, 612, 3810, 2342, 1671]}, {"qid": 2114, "question": "What was the task given to workers? in Autocompletion interfaces make crowd workers slower, but their use promotes response diversity", "answer": ["conceptualization task"], "top_k_doc_id": [3226, 3227, 28, 899, 1646, 3225, 3228, 5042, 1244, 5038, 5043, 5731, 7222, 3679, 5727], "orig_top_k_doc_id": [3225, 3228, 3226, 3227, 899, 5042, 1646, 5038, 28, 5043, 1244, 7222, 3679, 5727, 5731]}, {"qid": 2115, "question": "How was lexical diversity measured? in Autocompletion interfaces make crowd workers slower, but their use promotes response diversity", "answer": ["By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions"], "top_k_doc_id": [3226, 3227, 28, 899, 1646, 3225, 3228, 5042, 5727, 6289, 7374, 5731, 5730, 5038, 1081], "orig_top_k_doc_id": [3225, 3228, 3226, 3227, 5731, 899, 5042, 28, 7374, 5730, 5727, 5038, 1646, 1081, 6289]}, {"qid": 2116, "question": "How many responses did they obtain? in Autocompletion interfaces make crowd workers slower, but their use promotes response diversity", "answer": ["1001"], "top_k_doc_id": [3226, 3227, 28, 899, 1646, 3225, 3228, 5042, 5727, 6289, 7374, 1669, 7222, 3450, 3679], "orig_top_k_doc_id": [3225, 3228, 3226, 3227, 899, 28, 5727, 1669, 1646, 7374, 6289, 7222, 3450, 5042, 3679]}, {"qid": 2117, "question": "What crowdsourcing platform was used? in Autocompletion interfaces make crowd workers slower, but their use promotes response diversity", "answer": ["AMT"], "top_k_doc_id": [3226, 3227, 28, 899, 1646, 3225, 3228, 5042, 1244, 5038, 5043, 5731, 7222, 3125, 4785], "orig_top_k_doc_id": [3225, 3228, 5038, 3227, 3226, 899, 5043, 1646, 7222, 5042, 28, 3125, 1244, 5731, 4785]}, {"qid": 118, "question": "Which real-world datasets did they use? in Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "answer": ["SST-2 (Stanford Sentiment Treebank, version 2), Snips", "SST-2, Snips"], "top_k_doc_id": [3226, 3227, 135, 136, 137, 138, 139, 609, 612, 5636, 4744, 6633, 1308, 4745, 5324], "orig_top_k_doc_id": [135, 139, 137, 138, 136, 5636, 4744, 6633, 3227, 1308, 612, 609, 3226, 4745, 5324]}]}
{"group_id": 201, "group_size": 7, "items": [{"qid": 196, "question": "What is the performance of their system? in CAiRE: An End-to-End Empathetic Chatbot", "answer": ["No"], "top_k_doc_id": [2967, 250, 251, 2888, 2892, 3357, 7222, 7760, 824, 3359, 3507, 3508, 2464, 5436, 3510], "orig_top_k_doc_id": [250, 251, 2967, 2892, 3357, 3507, 2888, 7222, 3359, 3508, 2464, 7760, 5436, 3510, 824]}, {"qid": 197, "question": "What evaluation metrics are used? in CAiRE: An End-to-End Empathetic Chatbot", "answer": ["No"], "top_k_doc_id": [2967, 250, 251, 2888, 2892, 3357, 7222, 7760, 824, 3359, 1817, 2891, 1771, 3361, 2890], "orig_top_k_doc_id": [250, 251, 2967, 7760, 3357, 3359, 2888, 2892, 1817, 2891, 824, 1771, 3361, 7222, 2890]}, {"qid": 199, "question": "What pretrained LM is used? in CAiRE: An End-to-End Empathetic Chatbot", "answer": ["Generative Pre-trained Transformer (GPT)", "Generative Pre-trained Transformer (GPT)"], "top_k_doc_id": [2967, 250, 251, 2888, 2892, 3357, 7222, 7760, 4690, 4691, 4689, 3074, 7299, 5013, 2488], "orig_top_k_doc_id": [250, 251, 2967, 4690, 4691, 4689, 3074, 7760, 7222, 2892, 7299, 5013, 3357, 2888, 2488]}, {"qid": 2220, "question": "How do they obtain human generated policies? in Ensemble-Based Deep Reinforcement Learning for Chatbots", "answer": ["derive rewards from human-human dialogues by assigning positive values to contextualised responses seen in the data, and negative values to randomly chosen responses due to lacking coherence"], "top_k_doc_id": [2967, 3357, 3475, 3476, 3477, 3478, 3479, 6588, 2867, 1641, 6589, 2868, 5744, 4536, 7479], "orig_top_k_doc_id": [3475, 3479, 2867, 3476, 2967, 1641, 6589, 2868, 3477, 3478, 5744, 4536, 6588, 3357, 7479]}, {"qid": 2221, "question": "How many agents do they ensemble over? in Ensemble-Based Deep Reinforcement Learning for Chatbots", "answer": ["100 "], "top_k_doc_id": [2967, 3357, 3475, 3476, 3477, 3478, 3479, 6588, 5428, 2888, 5425, 3358, 6143, 7758, 4445], "orig_top_k_doc_id": [3475, 3479, 3478, 3476, 3477, 3357, 5428, 2967, 2888, 6588, 5425, 3358, 6143, 7758, 4445]}, {"qid": 198, "question": "What is the source of the dialogues? in CAiRE: An End-to-End Empathetic Chatbot", "answer": ["No"], "top_k_doc_id": [2967, 250, 251, 822, 3508, 3510, 3476, 1770, 3475, 4863, 3507, 3359, 4862, 1771, 3509], "orig_top_k_doc_id": [250, 251, 2967, 822, 3508, 3510, 3476, 1770, 3475, 4863, 3507, 3359, 4862, 1771, 3509]}, {"qid": 2696, "question": "What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation?  in Towards a Continuous Knowledge Learning Engine for Chatbots", "answer": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "top_k_doc_id": [2967, 3357, 3475, 5432, 5429, 3359, 2970, 5425, 5428, 7758, 3358, 6651, 4719, 2969, 2888], "orig_top_k_doc_id": [3357, 5432, 5429, 3359, 2970, 5425, 5428, 7758, 3358, 6651, 2967, 4719, 2969, 3475, 2888]}]}
{"group_id": 202, "group_size": 7, "items": [{"qid": 277, "question": "What stylistic features are used to detect drunk texts? in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \n and Sentiment Ratio", "LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio."], "top_k_doc_id": [343, 344, 4138, 581, 4137, 6804, 580, 4410, 5701, 5900, 514, 3332, 5137, 5910, 4359], "orig_top_k_doc_id": [343, 344, 581, 6804, 5900, 580, 5701, 4138, 514, 3332, 4137, 5137, 5910, 4359, 4410]}, {"qid": 280, "question": "Do the authors equate drunk tweeting with drunk texting?  in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["Yes"], "top_k_doc_id": [343, 344, 4138, 581, 4137, 6804, 580, 4410, 5701, 5700, 7533, 7016, 56, 7017, 2989], "orig_top_k_doc_id": [343, 344, 581, 580, 4138, 4410, 6804, 4137, 5701, 5700, 7533, 7016, 56, 7017, 2989]}, {"qid": 276, "question": "What baseline model is used? in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["Human evaluators"], "top_k_doc_id": [343, 344, 4138, 581, 4137, 6804, 4444, 4382, 502, 1430, 1340, 1329, 5910, 5666, 5920], "orig_top_k_doc_id": [343, 344, 4138, 6804, 4444, 4382, 502, 1430, 581, 1340, 1329, 5910, 5666, 5920, 4137]}, {"qid": 274, "question": "Do they report results only on English data? in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["Yes"], "top_k_doc_id": [343, 344, 4138, 581, 6791, 56, 6041, 6042, 4049, 1066, 4653, 6201, 6324, 7533, 3685], "orig_top_k_doc_id": [343, 344, 6791, 56, 581, 6041, 6042, 4138, 4049, 1066, 4653, 6201, 6324, 7533, 3685]}, {"qid": 275, "question": "Do the authors mention any confounds to their study? in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["No"], "top_k_doc_id": [343, 344, 4138, 5700, 5701, 55, 3511, 3389, 7411, 4948, 308, 6833, 6791, 3893, 1348], "orig_top_k_doc_id": [343, 344, 5700, 5701, 55, 3511, 3389, 7411, 4948, 308, 4138, 6833, 6791, 3893, 1348]}, {"qid": 278, "question": "Is the data acquired under distant supervision verified by humans at any stage? in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["Yes"], "top_k_doc_id": [343, 344, 87, 502, 5256, 5546, 581, 5657, 1956, 2971, 3006, 1139, 6766, 2839, 7821], "orig_top_k_doc_id": [343, 344, 5546, 581, 5657, 87, 1956, 2971, 502, 5256, 3006, 1139, 6766, 2839, 7821]}, {"qid": 279, "question": "What hashtags are used for distant supervision? in A Computational Approach to Automatic Prediction of Drunk Texting", "answer": ["No"], "top_k_doc_id": [343, 344, 87, 502, 5256, 5546, 6821, 5314, 451, 331, 227, 2825, 6820, 5900, 1329], "orig_top_k_doc_id": [343, 344, 5256, 502, 5546, 6821, 5314, 87, 451, 331, 227, 2825, 6820, 5900, 1329]}]}
{"group_id": 203, "group_size": 7, "items": [{"qid": 350, "question": "What were the evaluation metrics used? in Self-Taught Convolutional Neural Networks for Short Text Clustering", "answer": ["accuracy, normalized mutual information"], "top_k_doc_id": [3300, 3305, 417, 418, 419, 420, 423, 2083, 2337, 6066, 931, 1325, 3283, 421, 3285], "orig_top_k_doc_id": [417, 418, 423, 419, 420, 2337, 931, 3305, 3300, 2083, 421, 3283, 6066, 3285, 1325]}, {"qid": 351, "question": "What were their performance results? in Self-Taught Convolutional Neural Networks for Short Text Clustering", "answer": ["On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%"], "top_k_doc_id": [3300, 3305, 417, 418, 419, 420, 423, 2083, 2337, 6066, 931, 1325, 3283, 3798, 6526], "orig_top_k_doc_id": [417, 418, 423, 419, 420, 3305, 3300, 2337, 2083, 1325, 6066, 3283, 3798, 931, 6526]}, {"qid": 352, "question": "By how much did they outperform the other methods? in Self-Taught Convolutional Neural Networks for Short Text Clustering", "answer": ["on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI"], "top_k_doc_id": [3300, 3305, 417, 418, 419, 420, 423, 3304, 6526, 4131, 3798, 3358, 7449, 2607, 3303], "orig_top_k_doc_id": [417, 418, 423, 419, 420, 3305, 6526, 3304, 3300, 4131, 3798, 3358, 7449, 2607, 3303]}, {"qid": 353, "question": "Which popular clustering methods did they experiment with? in Self-Taught Convolutional Neural Networks for Short Text Clustering", "answer": ["K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods"], "top_k_doc_id": [3300, 3305, 417, 418, 419, 420, 423, 3304, 6526, 3283, 421, 6656, 6462, 4049, 169], "orig_top_k_doc_id": [417, 418, 419, 420, 423, 3305, 3304, 3300, 3283, 421, 6656, 6526, 6462, 4049, 169]}, {"qid": 354, "question": "What datasets did they use? in Self-Taught Convolutional Neural Networks for Short Text Clustering", "answer": ["SearchSnippets, StackOverflow, Biomedical"], "top_k_doc_id": [3300, 3305, 417, 418, 419, 420, 423, 2083, 2337, 6066, 5896, 6526, 5288, 2917, 1668], "orig_top_k_doc_id": [417, 418, 423, 419, 420, 5896, 6526, 3305, 2083, 6066, 5288, 2337, 2917, 1668, 3300]}, {"qid": 2159, "question": "How do they evaluate their method? in Deep Representation Learning for Clustering of Health Tweets", "answer": ["Calinski-Harabasz score, t-SNE, UMAP"], "top_k_doc_id": [3300, 3305, 462, 2332, 3301, 3304, 4131, 165, 521, 3431, 3303, 463, 3076, 6418, 4687], "orig_top_k_doc_id": [3300, 3305, 3304, 4131, 3301, 165, 521, 462, 3431, 3303, 463, 3076, 2332, 6418, 4687]}, {"qid": 2160, "question": "What is an example of a health-related tweet? in Deep Representation Learning for Clustering of Health Tweets", "answer": ["The health benefits of alcohol consumption are more limited than previously thought, researchers say"], "top_k_doc_id": [3300, 3305, 462, 2332, 3301, 3304, 6156, 1757, 2402, 6155, 879, 3545, 6804, 6140, 6916], "orig_top_k_doc_id": [3305, 3300, 3304, 3301, 2332, 6156, 1757, 2402, 6155, 879, 3545, 462, 6804, 6140, 6916]}]}
{"group_id": 204, "group_size": 7, "items": [{"qid": 492, "question": "Do they release MED? in Can neural networks understand monotonicity reasoning?", "answer": ["Yes"], "top_k_doc_id": [583, 585, 586, 587, 2746, 4914, 588, 2405, 5767, 2461, 575, 3175, 6847, 3144, 6921], "orig_top_k_doc_id": [587, 585, 586, 583, 2746, 4914, 5767, 588, 2461, 575, 3175, 2405, 6847, 3144, 6921]}, {"qid": 495, "question": "What is monotonicity reasoning? in Can neural networks understand monotonicity reasoning?", "answer": ["a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures"], "top_k_doc_id": [583, 585, 586, 587, 2746, 4914, 588, 2405, 584, 3530, 4913, 4522, 3100, 4518, 690], "orig_top_k_doc_id": [583, 587, 585, 2746, 586, 588, 4914, 584, 2405, 3530, 4913, 4522, 3100, 4518, 690]}, {"qid": 494, "question": "How do they define upward and downward reasoning? in Can neural networks understand monotonicity reasoning?", "answer": ["Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific."], "top_k_doc_id": [583, 585, 586, 587, 2746, 4914, 588, 584, 6078, 6077, 4216, 2747, 1156, 2461, 7573], "orig_top_k_doc_id": [587, 583, 586, 585, 584, 6078, 4914, 6077, 2746, 4216, 2747, 1156, 588, 2461, 7573]}, {"qid": 1872, "question": "Do they beat current state-of-the-art on SICK? in MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity", "answer": ["No"], "top_k_doc_id": [583, 585, 586, 587, 2746, 584, 2747, 2748, 2749, 2750, 2751, 5992, 872, 875, 7274], "orig_top_k_doc_id": [2746, 2747, 2750, 2748, 2749, 2751, 872, 585, 583, 584, 587, 875, 7274, 586, 5992]}, {"qid": 1873, "question": "How do they combine MonaLog with BERT? in MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity", "answer": ["They use Monalog for data-augmentation to fine-tune BERT on this task"], "top_k_doc_id": [583, 585, 586, 587, 2746, 584, 2747, 2748, 2749, 2750, 2751, 5992, 1560, 4447, 7630], "orig_top_k_doc_id": [2746, 2747, 2750, 2749, 2748, 2751, 586, 584, 587, 583, 585, 1560, 4447, 5992, 7630]}, {"qid": 493, "question": "What NLI models do they analyze? in Can neural networks understand monotonicity reasoning?", "answer": ["BiMPM, ESIM, Decomposable Attention Model, KIM, BERT"], "top_k_doc_id": [583, 585, 586, 587, 2746, 4914, 5871, 256, 5870, 5019, 2661, 872, 4522, 875, 2064], "orig_top_k_doc_id": [583, 2746, 585, 587, 586, 5871, 256, 4914, 5870, 5019, 2661, 872, 4522, 875, 2064]}, {"qid": 1874, "question": "How do they select monotonicity facts? in MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity", "answer": ["They derive it from Wordnet"], "top_k_doc_id": [583, 585, 586, 587, 2746, 584, 2747, 2748, 2749, 2750, 4447, 2202, 2405, 6668, 3441], "orig_top_k_doc_id": [2746, 2747, 584, 583, 585, 4447, 586, 587, 2750, 2748, 2202, 2749, 2405, 6668, 3441]}]}
{"group_id": 205, "group_size": 7, "items": [{"qid": 506, "question": "Which unlabeled data do they pretrain with? in wav2vec: Unsupervised Pre-training for Speech Recognition", "answer": ["1000 hours of WSJ audio data"], "top_k_doc_id": [4209, 620, 2709, 2710, 2712, 2713, 4210, 4211, 1365, 2711, 622, 4674, 3274, 5656, 4922], "orig_top_k_doc_id": [2709, 2713, 4209, 4211, 4210, 2712, 2710, 620, 622, 4674, 2711, 3274, 1365, 5656, 4922]}, {"qid": 507, "question": "How many convolutional layers does their model have? in wav2vec: Unsupervised Pre-training for Speech Recognition", "answer": ["wav2vec has 12 convolutional layers"], "top_k_doc_id": [4209, 620, 2709, 2710, 2712, 2713, 4210, 4211, 1365, 2711, 418, 7138, 7688, 1175, 7002], "orig_top_k_doc_id": [2709, 2710, 2713, 2712, 4209, 4211, 4210, 620, 2711, 418, 7138, 7688, 1365, 1175, 7002]}, {"qid": 2494, "question": "What are baseline models on WSJ eval92 and LibriSpeech test-clean? in Deep Contextualized Acoustic Representations For Semi-Supervised Speech Recognition", "answer": ["Wav2vec BIBREF22, a fully-supervised system using all labeled data"], "top_k_doc_id": [4209, 5012, 5013, 5053, 5054, 620, 621, 622, 4211, 4210, 2709, 2713, 6315, 6970, 7537], "orig_top_k_doc_id": [4211, 4210, 622, 621, 5054, 4209, 5053, 2709, 5013, 2713, 620, 6315, 6970, 5012, 7537]}, {"qid": 2856, "question": "what were the baselines? in Jasper: An End-to-End Convolutional Neural Acoustic Model", "answer": ["No", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "top_k_doc_id": [4209, 5012, 5013, 5053, 5054, 381, 2484, 4972, 4974, 5014, 6313, 4543, 2485, 6540, 5987], "orig_top_k_doc_id": [5012, 5013, 5014, 5053, 4972, 5054, 381, 6313, 2484, 4543, 4209, 2485, 6540, 4974, 5987]}, {"qid": 2857, "question": "what competitive results did they obtain? in Jasper: An End-to-End Convolutional Neural Acoustic Model", "answer": ["In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.\nIn case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ", "On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.\nOn Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets."], "top_k_doc_id": [4209, 5012, 5013, 5053, 5054, 381, 2484, 4972, 4974, 5014, 6313, 2488, 2487, 6968, 259], "orig_top_k_doc_id": [5012, 5014, 5013, 5053, 4972, 4209, 381, 6313, 2488, 2487, 6968, 2484, 259, 5054, 4974]}, {"qid": 2875, "question": "what is the state of the art on WSJ? in Fully Convolutional Speech Recognition", "answer": ["CNN-DNN-BLSTM-HMM", "HMM-based system"], "top_k_doc_id": [4209, 5012, 5013, 5053, 5054, 620, 621, 622, 4211, 4973, 4972, 5055, 4974, 1161, 3423], "orig_top_k_doc_id": [622, 5054, 5053, 620, 621, 4973, 4972, 5055, 4974, 4209, 1161, 5013, 5012, 3423, 4211]}, {"qid": 508, "question": "Do they explore how much traning data is needed for which magnitude of improvement for WER?  in wav2vec: Unsupervised Pre-training for Speech Recognition", "answer": ["Yes"], "top_k_doc_id": [4209, 620, 2709, 2710, 2712, 2713, 4210, 4211, 622, 6310, 7152, 6517, 3407, 6518, 7539], "orig_top_k_doc_id": [2709, 2712, 2713, 4211, 2710, 622, 4210, 6310, 7152, 6517, 4209, 620, 3407, 6518, 7539]}]}
{"group_id": 206, "group_size": 7, "items": [{"qid": 509, "question": "How are character representations from various languages joint? in Cross-lingual, Character-Level Neural Morphological Tagging", "answer": ["shared character embeddings for taggers in both languages together through optimization of a joint loss function"], "top_k_doc_id": [398, 397, 626, 627, 628, 3903, 3904, 396, 624, 625, 7687, 7318, 1370, 1374, 7686], "orig_top_k_doc_id": [3904, 627, 624, 397, 625, 628, 3903, 7687, 398, 396, 7318, 1370, 1374, 7686, 626]}, {"qid": 510, "question": "On which dataset is the experiment conducted? in Cross-lingual, Character-Level Neural Morphological Tagging", "answer": ["We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . "], "top_k_doc_id": [398, 397, 626, 627, 628, 3903, 3904, 396, 624, 625, 221, 4147, 4146, 1773, 225], "orig_top_k_doc_id": [3904, 627, 397, 3903, 624, 628, 398, 625, 626, 221, 4147, 396, 4146, 1773, 225]}, {"qid": 2398, "question": "What other cross-lingual approaches is the model compared to? in Neural Factor Graph Models for Cross-lingual Morphological Tagging", "answer": ["The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data."], "top_k_doc_id": [398, 397, 626, 627, 628, 3903, 3904, 247, 2806, 3902, 6034, 6035, 2811, 1048, 633], "orig_top_k_doc_id": [3903, 3904, 627, 247, 397, 2806, 6035, 628, 2811, 6034, 398, 1048, 626, 3902, 633]}, {"qid": 2399, "question": "What languages are explored? in Neural Factor Graph Models for Cross-lingual Morphological Tagging", "answer": ["Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt)"], "top_k_doc_id": [398, 397, 626, 627, 628, 3903, 3904, 247, 2806, 3902, 6034, 6035, 624, 625, 3900], "orig_top_k_doc_id": [3903, 628, 627, 247, 3904, 3902, 6035, 2806, 626, 397, 398, 6034, 624, 625, 3900]}, {"qid": 1893, "question": "What low-resource languages were used in this work? in Cross-Lingual Adaptation Using Universal Dependencies", "answer": ["No"], "top_k_doc_id": [398, 67, 225, 2806, 5621, 222, 5716, 5622, 624, 5699, 6031, 221, 2808, 6033, 4568], "orig_top_k_doc_id": [2806, 398, 222, 5716, 5622, 624, 5699, 6031, 221, 5621, 2808, 225, 67, 6033, 4568]}, {"qid": 1894, "question": "What classification task was used to evaluate the cross-lingual adaptation method described in this work? in Cross-Lingual Adaptation Using Universal Dependencies", "answer": ["Paraphrase Identification"], "top_k_doc_id": [398, 67, 225, 2806, 5621, 1048, 2468, 632, 2807, 3903, 633, 4688, 2162, 6036, 6060], "orig_top_k_doc_id": [2806, 1048, 2468, 67, 632, 398, 2807, 3903, 5621, 225, 633, 4688, 2162, 6036, 6060]}, {"qid": 2260, "question": "What are the evaluation metrics used? in Bleaching Text: Abstract Features for Cross-lingual Gender Prediction", "answer": ["average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)"], "top_k_doc_id": [398, 3547, 3549, 3903, 3548, 6042, 6512, 3750, 221, 6041, 222, 1347, 3140, 6063, 5153], "orig_top_k_doc_id": [3547, 3549, 3903, 3548, 6042, 6512, 3750, 221, 6041, 222, 1347, 398, 3140, 6063, 5153]}]}
{"group_id": 207, "group_size": 7, "items": [{"qid": 553, "question": "What new model is proposed for binomial lists? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": ["null model "], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 3664, 5425, 5357, 5252, 4303], "orig_top_k_doc_id": [668, 677, 669, 673, 670, 671, 674, 676, 672, 675, 5425, 5357, 3664, 5252, 4303]}, {"qid": 556, "question": "What online text resources are used to test binomial lists? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": ["news publications, wine reviews, and Reddit"], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 3664, 5425, 2790, 2789, 4739], "orig_top_k_doc_id": [668, 677, 669, 673, 670, 674, 671, 676, 672, 675, 3664, 2790, 5425, 2789, 4739]}, {"qid": 550, "question": "How is order of binomials tracked across time? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": ["draw our data from news publications, wine reviews, and Reddit, develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time,  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time"], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 3664, 5425, 1001, 554, 898], "orig_top_k_doc_id": [668, 677, 669, 670, 673, 672, 674, 676, 671, 675, 3664, 1001, 554, 5425, 898]}, {"qid": 552, "question": "Are there any new finding in analasys of trinomials that was not present binomials? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": ["Trinomials are likely to appear in exactly one order"], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 3664, 5425, 7194, 5081, 951], "orig_top_k_doc_id": [677, 668, 676, 669, 670, 674, 673, 671, 672, 675, 5425, 3664, 7194, 5081, 951]}, {"qid": 551, "question": "What types of various community texts have been investigated for exploring global structure of binomials? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": ["news publications, wine reviews, and Reddit"], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 3664, 4739, 7256, 5386, 2157], "orig_top_k_doc_id": [668, 669, 677, 670, 674, 676, 673, 675, 672, 671, 4739, 7256, 5386, 2157, 3664]}, {"qid": 554, "question": "How was performance of previously proposed rules at very large scale? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": [" close to random,"], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 3664, 7456, 130, 2986, 2795], "orig_top_k_doc_id": [668, 677, 670, 669, 674, 673, 672, 676, 671, 675, 3664, 7456, 130, 2986, 2795]}, {"qid": 555, "question": "What previously proposed rules for predicting binoial ordering are used? in Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text", "answer": ["word length, number of phonemes, number of syllables, alphabetical order, and frequency"], "top_k_doc_id": [668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 4592, 4853, 160, 6184, 4738], "orig_top_k_doc_id": [668, 677, 669, 674, 673, 670, 672, 671, 676, 675, 4592, 4853, 160, 6184, 4738]}]}
{"group_id": 208, "group_size": 7, "items": [{"qid": 573, "question": "What is the size of the datasets employed? in Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study", "answer": ["(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark"], "top_k_doc_id": [5513, 5515, 699, 701, 703, 704, 5858, 4464, 5514, 700, 5780, 4799, 5428, 1373, 6344], "orig_top_k_doc_id": [704, 703, 699, 5515, 5858, 5513, 4799, 5780, 701, 5514, 4464, 5428, 700, 1373, 6344]}, {"qid": 574, "question": "What are the baseline models? in Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study", "answer": ["Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)"], "top_k_doc_id": [5513, 5515, 699, 701, 703, 704, 5858, 4464, 5514, 700, 5780, 5517, 5516, 686, 4779], "orig_top_k_doc_id": [704, 703, 699, 5515, 5513, 701, 5517, 5514, 5516, 686, 700, 5780, 4779, 5858, 4464]}, {"qid": 571, "question": "What is the performance achieved by the model described in the paper? in Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study", "answer": ["No"], "top_k_doc_id": [5513, 5515, 699, 701, 703, 704, 5858, 4464, 5514, 686, 7314, 491, 5428, 4779, 1393], "orig_top_k_doc_id": [704, 703, 699, 5513, 5514, 5515, 5858, 686, 4464, 7314, 701, 491, 5428, 4779, 1393]}, {"qid": 3298, "question": "How do they score phrasal compositionality? in Paraphrase-Supervised Models of Compositionality", "answer": ["Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators"], "top_k_doc_id": [5513, 5515, 2899, 5514, 5517, 5516, 3790, 1217, 3956, 95, 2893, 4221, 2705, 0, 1211], "orig_top_k_doc_id": [5517, 5515, 5514, 5516, 5513, 3790, 1217, 3956, 2899, 95, 2893, 4221, 2705, 0, 1211]}, {"qid": 3299, "question": "Which translation systems do they compare against? in Paraphrase-Supervised Models of Compositionality", "answer": ["hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)", " English-Spanish MT system "], "top_k_doc_id": [5513, 5515, 2899, 5514, 5517, 5516, 2691, 1188, 7272, 6295, 2897, 3562, 7664, 3563, 2894], "orig_top_k_doc_id": [5517, 5516, 5513, 2899, 2691, 1188, 5515, 5514, 7272, 6295, 2897, 3562, 7664, 3563, 2894]}, {"qid": 572, "question": "What is the best performance achieved by supervised models? in Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study", "answer": ["No"], "top_k_doc_id": [5513, 5515, 699, 701, 703, 704, 5858, 7272, 6050, 5780, 4302, 4779, 6485, 4303, 5737], "orig_top_k_doc_id": [703, 704, 699, 7272, 5513, 6050, 5780, 5515, 4302, 4779, 6485, 5858, 701, 4303, 5737]}, {"qid": 3297, "question": "Do they study numerical properties of their obtained vectors (such as orthogonality)? in Paraphrase-Supervised Models of Compositionality", "answer": ["No", "No"], "top_k_doc_id": [5513, 5515, 2899, 5514, 5517, 3746, 6532, 3208, 7275, 6534, 5314, 4221, 1695, 6053, 3701], "orig_top_k_doc_id": [5513, 3746, 6532, 3208, 5517, 5515, 7275, 5514, 6534, 5314, 4221, 1695, 2899, 6053, 3701]}]}
{"group_id": 209, "group_size": 7, "items": [{"qid": 585, "question": "What two large datasets are used for evaluation? in How Far are We from Effective Context Modeling ? An Exploratory Study on Semantic Parsing in Context", "answer": ["SParC BIBREF2 and CoSQL BIBREF6"], "top_k_doc_id": [720, 723, 724, 4561, 5651, 2850, 3957, 5650, 2257, 3467, 3956, 4986, 3370, 247, 6646], "orig_top_k_doc_id": [724, 720, 5651, 4561, 3957, 5650, 2850, 4986, 3956, 3467, 3370, 247, 723, 6646, 2257]}, {"qid": 587, "question": "What are two datasets models are tested on? in How Far are We from Effective Context Modeling ? An Exploratory Study on Semantic Parsing in Context", "answer": ["SParC BIBREF2 and CoSQL BIBREF6"], "top_k_doc_id": [720, 723, 724, 4561, 5651, 2850, 3957, 5650, 2257, 3467, 3956, 1486, 4825, 6279, 4826], "orig_top_k_doc_id": [724, 720, 5651, 5650, 1486, 4561, 4825, 6279, 3957, 2257, 4826, 3956, 723, 2850, 3467]}, {"qid": 586, "question": "What context modelling methods are evaluated? in How Far are We from Effective Context Modeling ? An Exploratory Study on Semantic Parsing in Context", "answer": ["Concat\nTurn\nGate\nAction Copy\nTree Copy\nSQL Attn\nConcat + Action Copy\nConcat + Tree Copy\nConcat + SQL Attn\nTurn + Action Copy\nTurn + Tree Copy\nTurn + SQL Attn\nTurn + SQL Attn + Action Copy"], "top_k_doc_id": [720, 723, 724, 4561, 5651, 2850, 3957, 5650, 3357, 3370, 3208, 1919, 234, 6208, 6251], "orig_top_k_doc_id": [724, 720, 5651, 4561, 3357, 5650, 723, 3370, 3208, 2850, 1919, 234, 3957, 6208, 6251]}, {"qid": 584, "question": "How big is improvement in performances of proposed model over state of the art? in How Far are We from Effective Context Modeling ? An Exploratory Study on Semantic Parsing in Context", "answer": ["Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively."], "top_k_doc_id": [720, 723, 724, 4561, 5651, 2257, 7066, 863, 3606, 2818, 7544, 4790, 6646, 3276, 1486], "orig_top_k_doc_id": [720, 724, 2257, 7066, 863, 5651, 3606, 2818, 4561, 723, 7544, 4790, 6646, 3276, 1486]}, {"qid": 4183, "question": "How big is benefit in experiments of this editing approach compared to generating entire SQL from scratch? in Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions", "answer": ["improvement of 7% question match accuracy and 11% interaction match accuracy", "our model delivers an improvement of 7% question match accuracy and 11% interaction match accuracy over the previous state-of-the-art"], "top_k_doc_id": [720, 723, 724, 721, 722, 2818, 6646, 6647, 6648, 6649, 6650, 2820, 7566, 7567, 7571], "orig_top_k_doc_id": [6646, 6648, 6649, 6650, 720, 6647, 722, 724, 2818, 723, 721, 2820, 7566, 7567, 7571]}, {"qid": 4184, "question": "What are state-of-the-art baselines? in Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions", "answer": ["guo2019towards who achieve state-of-the-art performance", "For SParC, context-dependent seq2seq and syntaxSQL-con. For Spider, a recursive decoding procedure, graph neural networks, and intermediate representation models.", "SQLNet, SyntaxSQLNet,\nSyntxSQLNet + data augmentation,\nRecursive Decodoing Procedure Lee(2019),\nGNN,\nIRNet and IRNet(BERT)"], "top_k_doc_id": [720, 723, 724, 721, 722, 2818, 6646, 6647, 6648, 6649, 6650, 494, 3220, 6039, 3216], "orig_top_k_doc_id": [6646, 6649, 6648, 6650, 720, 722, 6647, 724, 723, 721, 2818, 494, 3220, 6039, 3216]}, {"qid": 2506, "question": "How does this compare to contextual embedding methods? in Multimodal Word Distributions", "answer": [" represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word `bank' could overlap with distributions for words such as `finance' and `money', and another mode could overlap with the distributions for `river' and `creek'."], "top_k_doc_id": [720, 4264, 4262, 3171, 930, 4261, 412, 4263, 7141, 2119, 4329, 3412, 1685, 4259, 5761], "orig_top_k_doc_id": [4264, 4262, 3171, 930, 4261, 412, 4263, 7141, 2119, 4329, 3412, 1685, 4259, 5761, 720]}]}
{"group_id": 210, "group_size": 7, "items": [{"qid": 715, "question": "How do they determine the opinion summary? in Aspect Term Extraction with History Attention and Selective Transformation", "answer": ["the weighted sum of the new opinion representations, according to their associations with the current aspect representation"], "top_k_doc_id": [7472, 893, 894, 895, 896, 897, 2216, 7473, 598, 3102, 3130, 4826, 3152, 605, 3153], "orig_top_k_doc_id": [893, 897, 894, 896, 7472, 895, 7473, 2216, 3102, 598, 3152, 4826, 3130, 605, 3153]}, {"qid": 716, "question": "Do they explore how useful is the detection history and opinion summary? in Aspect Term Extraction with History Attention and Selective Transformation", "answer": ["Yes"], "top_k_doc_id": [7472, 893, 894, 895, 896, 897, 2216, 7473, 598, 3102, 3130, 4826, 3101, 6770, 4360], "orig_top_k_doc_id": [893, 894, 897, 896, 895, 7472, 3102, 2216, 7473, 598, 3101, 3130, 4826, 6770, 4360]}, {"qid": 717, "question": "Which dataset(s) do they use to train the model? in Aspect Term Extraction with History Attention and Selective Transformation", "answer": ["INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain."], "top_k_doc_id": [7472, 893, 894, 895, 896, 897, 2216, 7473, 725, 6113, 6116, 6117, 6115, 726, 2217], "orig_top_k_doc_id": [893, 897, 894, 896, 895, 7472, 6113, 7473, 6115, 2216, 6116, 725, 726, 2217, 6117]}, {"qid": 718, "question": "By how much do they outperform state-of-the-art methods? in Aspect Term Extraction with History Attention and Selective Transformation", "answer": ["Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively."], "top_k_doc_id": [7472, 893, 894, 895, 896, 897, 2216, 7473, 725, 6113, 6116, 6117, 605, 2173, 7579], "orig_top_k_doc_id": [893, 894, 7472, 897, 896, 895, 6116, 605, 725, 7473, 2173, 6113, 2216, 6117, 7579]}, {"qid": 3760, "question": "Do they show examples where only one sentence appears in a bag and their method works, as opposed to using selective attention? in Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [7472, 4218, 4219, 4305, 6113, 6114, 6115, 6116, 6117, 893, 894, 4307, 6737, 6736, 4308], "orig_top_k_doc_id": [6116, 6117, 6113, 6115, 6114, 4307, 7472, 4305, 4219, 6737, 894, 4218, 893, 6736, 4308]}, {"qid": 3761, "question": "By how much do they outperform previous state-of-the-art in terms of top-n precision? in Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction", "answer": ["Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%", "5.3 percent points", "Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3%"], "top_k_doc_id": [7472, 4218, 4219, 4305, 6113, 6114, 6115, 6116, 6117, 893, 894, 299, 5675, 7056, 631], "orig_top_k_doc_id": [6116, 6117, 6113, 6115, 6114, 4219, 4305, 299, 4218, 5675, 894, 893, 7472, 7056, 631]}, {"qid": 3759, "question": "Is their gating mechanism specially designed to handle one sentence bags? in Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction", "answer": ["Yes", "No", "No"], "top_k_doc_id": [7472, 4218, 4219, 4305, 6113, 6114, 6115, 6116, 6117, 5228, 7251, 725, 299, 4307, 5212], "orig_top_k_doc_id": [6117, 6116, 6115, 6113, 6114, 5228, 7251, 725, 4219, 4305, 299, 4218, 4307, 7472, 5212]}]}
{"group_id": 211, "group_size": 7, "items": [{"qid": 766, "question": "Are resolution mode variables hand crafted? in Unsupervised Ranking Model for Entity Coreference Resolution", "answer": ["No"], "top_k_doc_id": [7245, 3941, 3943, 962, 963, 964, 6939, 6942, 717, 3480, 3481, 3482, 1819, 2097, 4947], "orig_top_k_doc_id": [962, 964, 963, 1819, 6939, 7245, 3482, 3480, 3481, 3943, 717, 6942, 3941, 2097, 4947]}, {"qid": 767, "question": "What are resolution model variables? in Unsupervised Ranking Model for Entity Coreference Resolution", "answer": ["Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved."], "top_k_doc_id": [7245, 3941, 3943, 962, 963, 964, 6939, 6942, 717, 3480, 3481, 3482, 2847, 2848, 6940], "orig_top_k_doc_id": [962, 964, 963, 6939, 7245, 3482, 3480, 3481, 3943, 3941, 2847, 2848, 6940, 6942, 717]}, {"qid": 583, "question": "Which coreference resolution systems are tested? in Gender Bias in Coreference Resolution", "answer": ["the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."], "top_k_doc_id": [7245, 3941, 3943, 717, 718, 2844, 3942, 7266, 7267, 3480, 3482, 962, 2848, 3481, 1347], "orig_top_k_doc_id": [717, 3941, 3942, 718, 3943, 7266, 7267, 2844, 3480, 3482, 962, 2848, 7245, 3481, 1347]}, {"qid": 768, "question": "Is the model presented in the paper state of the art? in Unsupervised Ranking Model for Entity Coreference Resolution", "answer": ["No, supervised models perform better for this task."], "top_k_doc_id": [7245, 3941, 3943, 962, 963, 964, 6939, 6942, 164, 1819, 6941, 6940, 1821, 2847, 2848], "orig_top_k_doc_id": [962, 6939, 964, 963, 7245, 3941, 3943, 164, 1819, 6941, 6942, 6940, 1821, 2847, 2848]}, {"qid": 2411, "question": "What is the state-of-the-art neural coreference resolution model? in Gender Bias in Neural Natural Language Processing", "answer": ["BIBREF2 , BIBREF1 "], "top_k_doc_id": [7245, 3941, 3943, 717, 718, 2844, 3942, 7266, 7267, 1443, 6939, 1819, 533, 4915, 5153], "orig_top_k_doc_id": [3941, 3943, 7266, 717, 3942, 1443, 6939, 2844, 1819, 533, 4915, 718, 7267, 7245, 5153]}, {"qid": 1916, "question": "What dataset do they evaluate their model on? in Incorporating Context and External Knowledge for Pronoun Coreference Resolution", "answer": ["CoNLL-2012 shared task BIBREF21 corpus"], "top_k_doc_id": [7245, 1124, 1819, 2844, 2846, 2847, 2848, 3481, 6042, 7346, 717, 3943, 3942, 1347, 3941], "orig_top_k_doc_id": [2848, 2844, 2847, 717, 6042, 3943, 1819, 3481, 1124, 3942, 7245, 2846, 1347, 7346, 3941]}, {"qid": 1917, "question": "What is the source of external knowledge? in Incorporating Context and External Knowledge for Pronoun Coreference Resolution", "answer": ["counts of predicate-argument tuples from English Wikipedia"], "top_k_doc_id": [7245, 1124, 1819, 2844, 2846, 2847, 2848, 3481, 6042, 7346, 7349, 1255, 1154, 4278, 6043], "orig_top_k_doc_id": [2844, 2848, 2847, 7346, 3481, 7349, 1124, 1819, 7245, 6042, 2846, 1255, 1154, 4278, 6043]}]}
{"group_id": 212, "group_size": 7, "items": [{"qid": 941, "question": "What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks? in Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "answer": ["The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments"], "top_k_doc_id": [1211, 1214, 1216, 2707, 1215, 686, 2701, 3956, 5513, 687, 1212, 1217, 2733, 788, 689], "orig_top_k_doc_id": [1211, 1217, 1216, 686, 2707, 1215, 1212, 788, 689, 2701, 1214, 687, 5513, 3956, 2733]}, {"qid": 945, "question": "What are other approaches into creating compositional generalization benchmarks? in Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "answer": ["random , Output length, Input length, Output pattern, Input pattern"], "top_k_doc_id": [1211, 1214, 1216, 2707, 1215, 686, 2701, 3956, 5513, 687, 1212, 1217, 2733, 2704, 2703], "orig_top_k_doc_id": [1217, 1211, 1216, 1212, 2707, 686, 1215, 1214, 2733, 5513, 2704, 2703, 2701, 687, 3956]}, {"qid": 1857, "question": "How does the SCAN dataset evaluate compositional generalization? in Compositional generalization in a deep seq2seq model by separating syntax and semantics", "answer": ["it systematically holds out inputs in the training set containing basic primitive verb, \"jump\", and tests on sequences containing that verb."], "top_k_doc_id": [1211, 1214, 1216, 2707, 1215, 686, 2701, 3956, 5513, 2704, 2705, 2706, 2702, 689, 2703], "orig_top_k_doc_id": [2701, 1216, 2704, 2707, 2705, 1214, 2706, 2702, 689, 1211, 2703, 686, 1215, 3956, 5513]}, {"qid": 943, "question": "What three machine architectures are analyzed? in Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "answer": ["LSTM+attention, Transformer , Universal Transformer"], "top_k_doc_id": [1211, 1214, 1216, 2707, 1215, 686, 1217, 1212, 689, 3581, 4810, 2703, 4344, 3388, 276], "orig_top_k_doc_id": [1217, 1211, 1216, 1215, 1212, 686, 689, 1214, 2707, 3581, 4810, 2703, 4344, 3388, 276]}, {"qid": 940, "question": "How strong is negative correlation between compound divergence and accuracy in performed experiment? in Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "answer": [" between 0.81 and 0.88"], "top_k_doc_id": [1211, 1214, 1216, 2707, 1215, 1217, 1212, 1222, 1221, 3083, 4898, 4796, 2585, 2703, 2704], "orig_top_k_doc_id": [1215, 1217, 1216, 1211, 1212, 1214, 1222, 1221, 3083, 4898, 2707, 4796, 2585, 2703, 2704]}, {"qid": 942, "question": "How authors justify that question answering dataset presented is realistic? in Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "answer": ["CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets"], "top_k_doc_id": [1211, 1214, 1216, 2707, 1212, 1217, 2578, 2733, 7610, 410, 1555, 7146, 1550, 791, 6508], "orig_top_k_doc_id": [1217, 1211, 1212, 1216, 2733, 410, 1555, 7610, 7146, 1214, 2578, 1550, 2707, 791, 6508]}, {"qid": 944, "question": "How big is new question answering dataset? in Measuring Compositional Generalization: A Comprehensive Method on Realistic Data", "answer": ["239,357 English question-answer pairs"], "top_k_doc_id": [1211, 1214, 1216, 2707, 1212, 1217, 2578, 2733, 7610, 788, 2703, 2738, 4698, 1215, 3633], "orig_top_k_doc_id": [1217, 1211, 1212, 1216, 788, 2733, 2578, 1214, 2703, 2738, 2707, 7610, 4698, 1215, 3633]}]}
{"group_id": 213, "group_size": 7, "items": [{"qid": 960, "question": "What empricial investigations do they reference? in A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "answer": ["empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation"], "top_k_doc_id": [1244, 1245, 1250, 1161, 1246, 1247, 1248, 1249, 5669, 7266, 247, 2301, 2226, 2038, 4186], "orig_top_k_doc_id": [1244, 1250, 1249, 1245, 1247, 1248, 1246, 1161, 7266, 5669, 247, 2301, 2226, 2038, 4186]}, {"qid": 961, "question": "What languages do they investigate for machine translation? in A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "answer": ["English , Chinese "], "top_k_doc_id": [1244, 1245, 1250, 1161, 1246, 1247, 1248, 1249, 5669, 7266, 247, 2301, 5872, 1899, 34], "orig_top_k_doc_id": [1250, 1244, 1249, 1245, 1247, 1248, 1246, 7266, 2301, 247, 1161, 5872, 1899, 5669, 34]}, {"qid": 963, "question": "What percentage fewer errors did professional translations make? in A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "answer": ["36%"], "top_k_doc_id": [1244, 1245, 1250, 1161, 1246, 1247, 1248, 1249, 5669, 7266, 1164, 6616, 5564, 7661, 2038], "orig_top_k_doc_id": [1250, 1249, 1248, 1244, 1245, 1247, 1246, 7266, 1164, 6616, 1161, 5669, 5564, 7661, 2038]}, {"qid": 964, "question": "What was the weakness in Hassan et al's evaluation design? in A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "answer": ["MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set\n"], "top_k_doc_id": [1244, 1245, 1250, 1161, 1246, 1247, 1248, 1249, 5669, 2225, 2739, 2742, 2226, 6590, 34], "orig_top_k_doc_id": [1250, 1244, 1249, 1245, 1247, 1248, 2225, 2739, 1161, 2742, 1246, 2226, 5669, 6590, 34]}, {"qid": 962, "question": "What recommendations do they offer? in A Set of Recommendations for Assessing Human-Machine Parity in Language Translation", "answer": [" Choose professional translators as raters,  Evaluate documents, not sentences, Evaluate fluency in addition to adequacy, Do not heavily edit reference translations for fluency, Use original source texts"], "top_k_doc_id": [1244, 1245, 1250, 1161, 1246, 1247, 1248, 1249, 3772, 7266, 1899, 5734, 5365, 5362, 3075], "orig_top_k_doc_id": [1250, 1249, 1244, 1245, 1247, 1248, 3772, 1246, 7266, 1899, 5734, 5365, 5362, 1161, 3075]}, {"qid": 1101, "question": "Which language directions are machine translation systems of WMT evaluated on? in Microsoft Research Asia's Systems for WMT19", "answer": ["German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English, Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh"], "top_k_doc_id": [1244, 1245, 1250, 1456, 3260, 5455, 7344, 7436, 1458, 1455, 1453, 1477, 4027, 3723, 1410], "orig_top_k_doc_id": [1458, 1455, 1456, 1244, 1245, 1453, 5455, 7344, 1477, 4027, 3723, 1410, 7436, 1250, 3260]}, {"qid": 2829, "question": "what are the baseline systems? in Edinburgh Neural Machine Translation Systems for WMT 16", "answer": ["attentional encoder-decoder networks BIBREF0", " the dl4mt-tutorial"], "top_k_doc_id": [1244, 1245, 1250, 1456, 3260, 5455, 7344, 7436, 2762, 2761, 4178, 398, 397, 1246, 3652], "orig_top_k_doc_id": [1244, 2762, 2761, 4178, 1245, 7436, 398, 397, 1246, 5455, 3652, 1250, 1456, 3260, 7344]}]}
{"group_id": 214, "group_size": 7, "items": [{"qid": 983, "question": "How does the QuaSP+Zero model work? in QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships", "answer": ["does not just consider the question tokens, but also the relationship between those tokens and the properties"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1291, 1293, 1292, 3096, 361, 1295, 4074, 3097, 2052, 1109], "orig_top_k_doc_id": [1294, 1290, 1292, 1293, 7829, 1295, 1291, 7830, 7831, 3096, 4074, 3097, 2052, 1109, 361]}, {"qid": 984, "question": "Which off-the-shelf tools do they use on QuaRel? in QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships", "answer": ["information retrieval system, word-association method,  CCG-style rule-based semantic parser written specifically for friction questions, state-of-the-art neural semantic parser"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1291, 1293, 1292, 3096, 361, 1295, 2366, 4535, 6207, 2168], "orig_top_k_doc_id": [1290, 1294, 7829, 1292, 1295, 1291, 1293, 7830, 7831, 2366, 3096, 4535, 6207, 361, 2168]}, {"qid": 5030, "question": "How are properties being compared annotated? in QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions", "answer": ["qualitative relations were manually extracted by the authors from a large corpus, asked to annotate the two properties being compared, asked to author a situated, 2-way multiple-choice (MC) question that tested this relationship, asked to validate its answer and quality, asked to generate a new question by \u201cflipping\u201d the original so the answer switched", "crowdworkers were shown a seed sentence $K_i$, and asked to annotate the two properties being compared using the template"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1291, 1293, 1292, 3096, 5739, 4911, 4216, 1109, 5462, 7076], "orig_top_k_doc_id": [7829, 7831, 7830, 1294, 1291, 1293, 1290, 1292, 5739, 3096, 4911, 4216, 1109, 5462, 7076]}, {"qid": 985, "question": "How do they obtain the logical forms of their questions in their dataset? in QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships", "answer": [" workers were given a seed qualitative relation, asked to enter two objects, people, or situations to compare, created a question, guided by a large number of examples, LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1291, 1293, 1292, 1295, 2733, 7734, 2692, 4911, 2821, 7589], "orig_top_k_doc_id": [1290, 1292, 1291, 7829, 1294, 1295, 1293, 2733, 7734, 2692, 4911, 7830, 2821, 7831, 7589]}, {"qid": 5028, "question": "How many general qualitative statements are in dataset? in QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions", "answer": ["background corpus of 400 qualitative knowledge sentences", "400 qualitative knowledge sentences"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1291, 1293, 3096, 2889, 1109, 3097, 2386, 2910, 2387, 2890], "orig_top_k_doc_id": [7831, 7829, 7830, 3096, 1290, 2889, 1294, 1109, 3097, 1291, 1293, 2386, 2910, 2387, 2890]}, {"qid": 986, "question": "Do all questions in the dataset allow the answers to pick from 2 options? in QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships", "answer": ["Yes"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1291, 1295, 1292, 3097, 3099, 6876, 3855, 2910, 2227, 5159], "orig_top_k_doc_id": [7829, 1290, 1294, 1295, 1292, 7830, 1291, 3097, 3099, 6876, 3855, 7831, 2910, 2227, 5159]}, {"qid": 5029, "question": "What are state-of-the-art models on this dataset? in QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions", "answer": [" BERT (IR), BERT (IR upper bound), BERT-PFT (no knowledge), BERT-PFT (IR)", "BERT-PFT (IR)"], "top_k_doc_id": [1290, 1294, 7829, 7830, 7831, 1109, 3292, 7805, 3855, 2234, 7679, 4362, 822, 3291, 1292], "orig_top_k_doc_id": [7829, 7831, 7830, 1109, 3292, 1294, 1290, 7805, 3855, 2234, 7679, 4362, 822, 3291, 1292]}]}
{"group_id": 215, "group_size": 7, "items": [{"qid": 1496, "question": "What baseline did they compare Entity-GCN to? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"], "top_k_doc_id": [1154, 2096, 2097, 2100, 2101, 4273, 2099, 4274, 4278, 7546, 1155, 4276, 2752, 2461, 4216], "orig_top_k_doc_id": [2096, 2097, 2101, 2100, 2099, 1155, 4273, 1154, 4276, 4278, 2752, 2461, 7546, 4274, 4216]}, {"qid": 1497, "question": "How many documents at a time can Entity-GCN handle? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["No"], "top_k_doc_id": [1154, 2096, 2097, 2100, 2101, 4273, 2099, 4274, 4278, 7546, 5338, 457, 130, 5157, 5339], "orig_top_k_doc_id": [2096, 2101, 2097, 2100, 2099, 4273, 5338, 1154, 457, 130, 5157, 4278, 7546, 4274, 5339]}, {"qid": 1499, "question": "How did they get relations between mentions? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."], "top_k_doc_id": [1154, 2096, 2097, 2100, 1155, 2461, 4216, 2099, 2101, 7820, 4273, 4978, 7615, 3162, 2363], "orig_top_k_doc_id": [2096, 2097, 2101, 1154, 2100, 1155, 4216, 4273, 7820, 2099, 2461, 4978, 7615, 3162, 2363]}, {"qid": 1500, "question": "How did they detect entity mentions? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["Exact matches to the entity string and predictions from a coreference resolution system"], "top_k_doc_id": [1154, 2096, 2097, 2100, 1155, 2461, 4216, 2099, 2101, 7820, 130, 2234, 6951, 2733, 215], "orig_top_k_doc_id": [2096, 2097, 2101, 1154, 2100, 7820, 2461, 1155, 130, 2099, 2234, 4216, 6951, 2733, 215]}, {"qid": 1501, "question": "What is the metric used with WIKIHOP? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["Accuracy"], "top_k_doc_id": [1154, 2096, 2097, 2100, 2101, 4273, 326, 1155, 2363, 2364, 2461, 5157, 5158, 2362, 2365], "orig_top_k_doc_id": [2096, 2101, 2097, 2363, 5157, 5158, 2364, 326, 2461, 2362, 1155, 1154, 2100, 4273, 2365]}, {"qid": 1502, "question": "What performance does the Entity-GCN get on WIKIHOP? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models"], "top_k_doc_id": [1154, 2096, 2097, 2100, 2101, 4273, 326, 1155, 2363, 2364, 2461, 5157, 2099, 4278, 7546], "orig_top_k_doc_id": [2096, 2101, 2097, 2100, 2099, 4273, 2363, 2364, 1154, 1155, 326, 4278, 7546, 5157, 2461]}, {"qid": 1498, "question": "Did they use a relation extraction method to construct the edges in the graph? in Question Answering by Reasoning Across Documents with Graph Convolutional Networks", "answer": ["No"], "top_k_doc_id": [1154, 2096, 2097, 2100, 1155, 2461, 4216, 4274, 4273, 4464, 457, 1760, 5212, 3162, 4978], "orig_top_k_doc_id": [1154, 2096, 4216, 2097, 2461, 4274, 4273, 4464, 1155, 457, 1760, 5212, 3162, 4978, 2100]}]}
{"group_id": 216, "group_size": 7, "items": [{"qid": 1535, "question": "How strategy-based methods handle obstacles in NLG? in Unsupervised Pre-training for Natural Language Generation: A Literature Review", "answer": ["fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network"], "top_k_doc_id": [2146, 2150, 6064, 1471, 2149, 3719, 5850, 6060, 6063, 6410, 1473, 3431, 7138, 101, 2148], "orig_top_k_doc_id": [2146, 2150, 2149, 6060, 3719, 6064, 7138, 6410, 5850, 1471, 101, 1473, 2148, 6063, 3431]}, {"qid": 1536, "question": "How architecture-based method handle obstacles in NLG? in Unsupervised Pre-training for Natural Language Generation: A Literature Review", "answer": ["task-specific architecture during pre-training (task-specific methods), aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)"], "top_k_doc_id": [2146, 2150, 6064, 1471, 2149, 3719, 5850, 6060, 6063, 6410, 1473, 3431, 7138, 2435, 1921], "orig_top_k_doc_id": [2146, 2150, 2149, 6064, 6060, 3719, 5850, 3431, 1471, 2435, 6063, 1473, 6410, 7138, 1921]}, {"qid": 1533, "question": "Which future direction in NLG are discussed? in Unsupervised Pre-training for Natural Language Generation: A Literature Review", "answer": ["1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context?, 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks?, 3) How to reduce the computing resources required for large-scale pre-training?, 4) What aspect of knowledge do the pre-trained models provide for better language generation?"], "top_k_doc_id": [2146, 2150, 6064, 1471, 2149, 3719, 5850, 6060, 6063, 6410, 6184, 2147, 7725, 1255, 1921], "orig_top_k_doc_id": [2146, 2150, 2149, 6064, 3719, 6184, 6060, 6063, 5850, 6410, 2147, 7725, 1255, 1921, 1471]}, {"qid": 3115, "question": "What dataset do they use? in Pre-trained Language Model Representations for Language Generation", "answer": ["German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task", "German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task"], "top_k_doc_id": [2146, 425, 805, 2149, 5276, 6060, 1922, 2985, 5804, 1560, 2413, 1364, 52, 2414, 1256], "orig_top_k_doc_id": [2149, 6060, 1922, 5276, 2146, 425, 2985, 5804, 1560, 2413, 805, 1364, 52, 2414, 1256]}, {"qid": 3116, "question": "What other models do they compare to? in Pre-trained Language Model Representations for Language Generation", "answer": ["BIBREF11 , BIBREF26 "], "top_k_doc_id": [2146, 425, 805, 2149, 5276, 6060, 1922, 6448, 804, 5808, 3446, 2416, 424, 427, 7141], "orig_top_k_doc_id": [2149, 6448, 2146, 6060, 425, 804, 5808, 3446, 805, 2416, 5276, 1922, 424, 427, 7141]}, {"qid": 1534, "question": "What experimental phenomena are presented? in Unsupervised Pre-training for Natural Language Generation: A Literature Review", "answer": ["The advantage of pre-training gradually diminishes with the increase of labeled data, Fixed representations yield better results than fine-tuning in some cases, pre-training the Seq2Seq encoder outperforms pre-training the decoder"], "top_k_doc_id": [2146, 2150, 6064, 7115, 6448, 3069, 6411, 6676, 1921, 4642, 2268, 5106, 3328, 2265, 3374], "orig_top_k_doc_id": [2146, 2150, 7115, 6448, 3069, 6411, 6676, 6064, 1921, 4642, 2268, 5106, 3328, 2265, 3374]}, {"qid": 3117, "question": "What language model architectures are used? in Pre-trained Language Model Representations for Language Generation", "answer": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "top_k_doc_id": [2146, 425, 805, 2149, 5276, 6060, 490, 2150, 1560, 2186, 2147, 6067, 2187, 2148, 1005], "orig_top_k_doc_id": [2149, 5276, 490, 2150, 1560, 805, 2186, 2147, 6067, 2187, 2148, 425, 2146, 1005, 6060]}]}
{"group_id": 217, "group_size": 7, "items": [{"qid": 1544, "question": "what types of features were used? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": ["stylometric, lexical, grammatical, and semantic"], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 1517, 6667, 5786, 7047, 2157, 6101, 7048], "orig_top_k_doc_id": [2160, 2159, 2158, 4015, 6414, 4551, 6415, 1517, 6667, 2157, 5784, 6101, 7047, 5786, 7048]}, {"qid": 1547, "question": "what datasets were used? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": [" training dataset contains 2,815 examples, 761 testing examples"], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 1517, 6667, 5786, 7047, 5189, 6666, 7775], "orig_top_k_doc_id": [2160, 2159, 2158, 4015, 6414, 4551, 6415, 6667, 5784, 5189, 1517, 7047, 6666, 5786, 7775]}, {"qid": 1546, "question": "what is the size of the dataset? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": ["The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322."], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 1517, 6667, 5786, 5189, 3972, 2161, 2157], "orig_top_k_doc_id": [2160, 2159, 2158, 4015, 6414, 4551, 6415, 5784, 5786, 5189, 1517, 6667, 3972, 2161, 2157]}, {"qid": 1548, "question": "what are the three reasons everybody hates them? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": ["No"], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 1517, 6667, 5786, 6666, 4831, 5385, 6665], "orig_top_k_doc_id": [2160, 2159, 2158, 6415, 4015, 5784, 6414, 6667, 4551, 1517, 5786, 6666, 4831, 5385, 6665]}, {"qid": 1545, "question": "what lexical features did they experiment with? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": ["TF.IDF-based features"], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 1517, 6667, 2157, 2161, 7775, 5135, 4016], "orig_top_k_doc_id": [2160, 2159, 2158, 4015, 4551, 6414, 2157, 6415, 2161, 6667, 7775, 5135, 1517, 5784, 4016]}, {"qid": 1543, "question": "what are their evaluation metrics? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": ["F1, accuracy"], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 1517, 5920, 4442, 7047, 5189, 3807, 4320], "orig_top_k_doc_id": [2160, 2159, 2158, 6414, 4015, 4551, 6415, 5920, 4442, 1517, 7047, 5189, 3807, 4320, 5784]}, {"qid": 1542, "question": "what classifiers were used in this paper? in We Built a Fake News&Click-bait Filter: What Happened Next Will Blow Your Mind!", "answer": ["Support Vector Machines (SVM) classifier"], "top_k_doc_id": [2158, 2159, 2160, 4015, 4551, 5784, 6414, 6415, 6101, 3277, 6667, 7047, 6413, 31, 3593], "orig_top_k_doc_id": [2160, 2159, 6414, 2158, 4015, 4551, 6101, 6415, 3277, 6667, 5784, 7047, 6413, 31, 3593]}]}
{"group_id": 218, "group_size": 7, "items": [{"qid": 1558, "question": "How are graphs derived from a given text? in RaKUn: Rank-based Keyword extraction via Unsupervised learning and Meta vertex aggregation", "answer": ["The given corpus is traversed, and for each element INLINEFORM6 , its successor INLINEFORM7 , together with a given element, forms a directed edge INLINEFORM8 . Finally, such edges are weighted according to the number of times they appear in a given corpus. Thus the graph, constructed after traversing a given corpus, consists of all local neighborhoods (order one), merged into a single joint structure. Global contextual information is potentially kept intact (via weights), even though it needs to be detected via network analysis"], "top_k_doc_id": [2173, 2174, 2175, 2176, 5575, 4893, 6716, 7823, 561, 5902, 3694, 4496, 4160, 560, 3696], "orig_top_k_doc_id": [2173, 2176, 2175, 2174, 561, 5902, 6716, 4893, 7823, 3694, 4496, 5575, 4160, 560, 3696]}, {"qid": 1559, "question": "In what sense if the proposed method interpretable? in RaKUn: Rank-based Keyword extraction via Unsupervised learning and Meta vertex aggregation", "answer": ["No"], "top_k_doc_id": [2173, 2174, 2175, 2176, 5575, 4893, 6716, 7823, 3208, 4477, 559, 5306, 3207, 3996, 5305], "orig_top_k_doc_id": [2173, 2176, 2175, 2174, 7823, 3208, 5575, 6716, 4477, 559, 5306, 3207, 4893, 3996, 5305]}, {"qid": 4295, "question": "Which real world datasets do they experiment on? in MetaLDA: a Topic Model that Efficiently Incorporates Meta information", "answer": ["Reuters, 20 Newsgroup, New York Times, Web Snippet, Tag My News, ABC News", "Reuters, 20NG, 20 Newsgroup, NYT, New York Times, WS, Web Snippet, TMN, Tag My News, AN, ABC News", "Reuters-21578 dataset, 20 Newsgroup, New York Times, Tag My News, ABC News, Web Snippet"], "top_k_doc_id": [2173, 1768, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 3694, 5936, 2136, 3291, 4160], "orig_top_k_doc_id": [6796, 6803, 6797, 6802, 6798, 6801, 6800, 6799, 1768, 3694, 5936, 2136, 2173, 3291, 4160]}, {"qid": 4296, "question": "Which other models that incorporate meta information do they compare against? in MetaLDA: a Topic Model that Efficiently Incorporates Meta information", "answer": ["LLDA, PLLDA, DMR, WF-LDA, LF-LDA, GPU-DMM", "LDA BIBREF0, LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9, DMR, LDA with Dirichlet Multinomial Regression BIBREF8, WF-LDA, Word Feature LDA BIBREF16, LF-LDA, Latent Feature LDA BIBREF5, GPU-DMM, Generalized P\u00f3lya Urn DMM BIBREF7, PTM, Pseudo document based Topic Model BIBREF18"], "top_k_doc_id": [2173, 1768, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 3992, 3991, 2619, 6674, 3996], "orig_top_k_doc_id": [6803, 6796, 6797, 6802, 6798, 6801, 6799, 6800, 1768, 3992, 3991, 2619, 6674, 3996, 2173]}, {"qid": 4297, "question": "How do they measure topic quality? in MetaLDA: a Topic Model that Efficiently Incorporates Meta information", "answer": ["Normalised Pointwise Mutual Information", "NPMI scores"], "top_k_doc_id": [2173, 1768, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 2191, 1135, 5127, 1081, 121], "orig_top_k_doc_id": [6796, 6803, 6797, 6802, 6798, 6801, 6800, 6799, 2173, 1768, 2191, 1135, 5127, 1081, 121]}, {"qid": 4298, "question": "Which data augmentation techniques do they use? in MetaLDA: a Topic Model that Efficiently Incorporates Meta information", "answer": ["augmented with a set of Beta random variables, auxiliary variable", "the introduction of auxiliary variables"], "top_k_doc_id": [2173, 1768, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 3992, 5085, 6578, 7166, 868], "orig_top_k_doc_id": [6803, 6796, 6798, 6802, 6797, 6801, 6799, 6800, 5085, 1768, 6578, 2173, 7166, 868, 3992]}, {"qid": 1557, "question": "How are meta vertices computed? in RaKUn: Rank-based Keyword extraction via Unsupervised learning and Meta vertex aggregation", "answer": ["Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed)., The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex)."], "top_k_doc_id": [2173, 2174, 2175, 2176, 5575, 3694, 3696, 561, 3996, 3697, 6797, 559, 3698, 1314, 6817], "orig_top_k_doc_id": [2173, 2176, 2174, 2175, 3694, 3696, 561, 3996, 3697, 6797, 559, 3698, 1314, 6817, 5575]}]}
{"group_id": 219, "group_size": 7, "items": [{"qid": 1563, "question": "what previous systems were compared to? in Semi-supervised sequence tagging with bidirectional language models", "answer": ["Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), S\u00f8gaard and Goldberg (2016) "], "top_k_doc_id": [5656, 5659, 5660, 85, 2177, 2180, 6392, 2709, 5655, 7116, 1345, 5044, 2178, 2238, 6294], "orig_top_k_doc_id": [2177, 5659, 5660, 5656, 6392, 85, 2180, 2709, 5044, 1345, 5655, 2178, 2238, 6294, 7116]}, {"qid": 1564, "question": "what are the evaluation datasets? in Semi-supervised sequence tagging with bidirectional language models", "answer": ["CoNLL 2003, CoNLL 2000"], "top_k_doc_id": [5656, 5659, 5660, 85, 2177, 2180, 6392, 2709, 5655, 7116, 1345, 5044, 1, 3688, 2594], "orig_top_k_doc_id": [2180, 5660, 2177, 5659, 5655, 6392, 5656, 85, 1345, 5044, 1, 3688, 2709, 2594, 7116]}, {"qid": 1562, "question": "what results do they achieve? in Semi-supervised sequence tagging with bidirectional language models", "answer": ["91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task"], "top_k_doc_id": [5656, 5659, 5660, 85, 2177, 2180, 6392, 2709, 5655, 7116, 2594, 4211, 7420, 4538, 5657], "orig_top_k_doc_id": [2177, 5656, 5659, 2594, 4211, 5660, 7420, 5655, 2709, 2180, 4538, 85, 6392, 7116, 5657]}, {"qid": 1561, "question": "what metrics are used in evaluation? in Semi-supervised sequence tagging with bidirectional language models", "answer": ["micro-averaged F1"], "top_k_doc_id": [5656, 5659, 5660, 85, 2177, 2180, 6392, 2709, 5655, 2594, 1773, 2595, 5632, 3688, 120], "orig_top_k_doc_id": [5660, 2177, 5655, 2180, 5659, 2594, 5656, 1773, 6392, 2595, 5632, 3688, 120, 2709, 85]}, {"qid": 1560, "question": "how are the bidirectional lms obtained? in Semi-supervised sequence tagging with bidirectional language models", "answer": ["They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs."], "top_k_doc_id": [5656, 5659, 5660, 85, 2177, 2180, 6392, 2178, 1988, 3705, 1812, 3688, 5621, 2238, 1987], "orig_top_k_doc_id": [2180, 2178, 2177, 1988, 3705, 1812, 5659, 6392, 5656, 3688, 5621, 2238, 85, 5660, 1987]}, {"qid": 3419, "question": "Which deep learning architecture do they use for sentence segmentation? in Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations", "answer": ["Bi-LSTM-CRF", "Bi-LSTM-CRF"], "top_k_doc_id": [5656, 5659, 5660, 2433, 4851, 5655, 5658, 5661, 5662, 7420, 165, 5318, 2429, 4848, 6381], "orig_top_k_doc_id": [5655, 5662, 5659, 5661, 5660, 5656, 5658, 2433, 7420, 165, 5318, 4851, 2429, 4848, 6381]}, {"qid": 3420, "question": "How do they utilize unlabeled data to improve model representations? in Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations", "answer": ["During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data."], "top_k_doc_id": [5656, 5659, 5660, 2433, 4851, 5655, 5658, 5661, 5662, 5657, 4727, 2321, 4209, 2823, 4210], "orig_top_k_doc_id": [5662, 5655, 5659, 5661, 5660, 5656, 5658, 5657, 4727, 2321, 4209, 2823, 2433, 4210, 4851]}]}
{"group_id": 220, "group_size": 7, "items": [{"qid": 1679, "question": "What additional information is found in the dataset? in Large Arabic Twitter Dataset on COVID-19", "answer": ["the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet"], "top_k_doc_id": [2402, 4689, 4690, 5652, 65, 66, 5654, 5168, 5169, 1725, 4688, 5653, 1734, 6752, 1733], "orig_top_k_doc_id": [2402, 5652, 66, 65, 4690, 4689, 5654, 5169, 5653, 4688, 1734, 5168, 6752, 1733, 1725]}, {"qid": 1680, "question": "Is the dataset focused on a region? in Large Arabic Twitter Dataset on COVID-19", "answer": ["Yes"], "top_k_doc_id": [2402, 4689, 4690, 5652, 65, 66, 5654, 5168, 5169, 1727, 1734, 85, 86, 1375, 2788], "orig_top_k_doc_id": [2402, 5652, 66, 65, 5654, 5168, 4689, 85, 4690, 5169, 86, 1375, 1734, 2788, 1727]}, {"qid": 1682, "question": "Are the tweets location-specific? in Large Arabic Twitter Dataset on COVID-19", "answer": ["Yes"], "top_k_doc_id": [2402, 4689, 4690, 5652, 65, 66, 5654, 5168, 5169, 1727, 1734, 6752, 1728, 1733, 1318], "orig_top_k_doc_id": [2402, 5169, 5652, 5168, 66, 65, 5654, 4689, 1727, 1734, 4690, 6752, 1728, 1733, 1318]}, {"qid": 1683, "question": "How big is the dataset? in Large Arabic Twitter Dataset on COVID-19", "answer": ["more than 3,934,610 million tweets"], "top_k_doc_id": [2402, 4689, 4690, 5652, 65, 66, 5654, 5168, 5169, 1725, 4688, 5653, 6760, 1727, 6513], "orig_top_k_doc_id": [2402, 5652, 5654, 66, 65, 4689, 4690, 5169, 5168, 4688, 5653, 6760, 1725, 1727, 6513]}, {"qid": 3417, "question": "What is the CORD-19 dataset? in Identifying Radiological Findings Related to COVID-19 from Medical Literature", "answer": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "top_k_doc_id": [2402, 4689, 4690, 5652, 65, 66, 5654, 522, 4688, 5131, 5653, 7837, 1403, 5829, 5644], "orig_top_k_doc_id": [5652, 5654, 5653, 65, 66, 4689, 4690, 2402, 4688, 7837, 522, 1403, 5829, 5131, 5644]}, {"qid": 3418, "question": "How large is the collection of COVID-19 literature? in Identifying Radiological Findings Related to COVID-19 from Medical Literature", "answer": ["45,000 scholarly articles, including over 33,000 with full text"], "top_k_doc_id": [2402, 4689, 4690, 5652, 65, 66, 5654, 522, 4688, 5131, 5653, 7837, 5873, 5731, 5957], "orig_top_k_doc_id": [5652, 5654, 5653, 2402, 65, 66, 4689, 4690, 4688, 522, 5873, 5731, 7837, 5131, 5957]}, {"qid": 1681, "question": "Over what period of time were the tweets collected? in Large Arabic Twitter Dataset on COVID-19", "answer": ["from January 1, 2020 until April 15, 2020"], "top_k_doc_id": [2402, 4689, 4690, 5652, 5169, 1734, 4392, 4132, 1727, 1733, 4133, 6752, 1433, 5322, 4136], "orig_top_k_doc_id": [2402, 5169, 5652, 1734, 4392, 4132, 1727, 1733, 4133, 6752, 4690, 4689, 1433, 5322, 4136]}]}
{"group_id": 221, "group_size": 7, "items": [{"qid": 1707, "question": "How much better peformance is achieved in human evaluation when model is trained considering proposed metric? in Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models", "answer": ["Pearson correlation to human judgement - proposed vs next best metric\nSample level comparison:\n- Story generation: 0.387 vs 0.148\n- Dialogue: 0.472 vs 0.341\nModel level comparison:\n- Story generation:  0.631 vs 0.302\n- Dialogue: 0.783 vs 0.553"], "top_k_doc_id": [2438, 1921, 2435, 2436, 2440, 7300, 3805, 965, 4793, 6936, 7478, 7479, 1474, 2439, 3193], "orig_top_k_doc_id": [2435, 2436, 2438, 2440, 4793, 965, 7479, 1474, 7300, 1921, 7478, 3805, 6936, 2439, 3193]}, {"qid": 1708, "question": "Do the authors suggest that proposed metric replace human evaluation on this task? in Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models", "answer": ["No"], "top_k_doc_id": [2438, 1921, 2435, 2436, 2440, 7300, 3805, 965, 4793, 6936, 7478, 7479, 7664, 1139, 3719], "orig_top_k_doc_id": [6936, 2435, 1921, 2438, 2436, 4793, 3805, 7300, 2440, 7478, 7664, 965, 1139, 7479, 3719]}, {"qid": 1706, "question": "What previous automated evalution approaches authors mention? in Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models", "answer": ["Text Overlap Metrics, including BLEU, Perplexity, Parameterized Metrics"], "top_k_doc_id": [2438, 1921, 2435, 2436, 2440, 7300, 3805, 3332, 7351, 6320, 34, 7382, 7389, 5186, 1170], "orig_top_k_doc_id": [2435, 2438, 3805, 3332, 7351, 2436, 6320, 7300, 34, 1921, 2440, 7382, 7389, 5186, 1170]}, {"qid": 3917, "question": "Which metrics were considered? in Why We Need New Evaluation Metrics for NLG", "answer": ["ter, bleu , rouge , nist, lepor, cider, meteor, Semantic Text Similarity,  Flesch Reading Ease , characters per utterance (len) and per word (cpw), words per sentence, syllables per sentence (sps) and per word (spw), polysyllabic words per utterance (pol) and per word (ppw), the number of misspellings (msp) and the parsing score as returned by the Stanford parser (prs)", "ter, bleu, rouge, nist, lepor, cider, meteor, Semantic Similarity (sim), readability and grammaticality", "ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23, Semantic Similarity (sim), readability and grammaticality"], "top_k_doc_id": [2438, 111, 491, 4796, 492, 2001, 2435, 2440, 6320, 6324, 6673, 6674, 2439, 3719, 3181], "orig_top_k_doc_id": [2435, 2440, 6324, 111, 6320, 6674, 4796, 2001, 491, 492, 2439, 3719, 3181, 6673, 2438]}, {"qid": 3918, "question": "What NLG tasks were considered? in Why We Need New Evaluation Metrics for NLG", "answer": ["provide information about hotels and restaurants in San Francisco, provides information about restaurants in Cambridge", "utterance generation for spoken dialogue systems, provide information about hotels and restaurants in San Francisco, provides information about restaurants in Cambridge", "rnnlg, TGen,  lols"], "top_k_doc_id": [2438, 111, 491, 4796, 492, 2001, 2435, 2440, 6320, 6324, 6673, 6674, 6675, 2002, 6672], "orig_top_k_doc_id": [2435, 6674, 491, 111, 6673, 2440, 6320, 2001, 6324, 4796, 6675, 492, 2438, 2002, 6672]}, {"qid": 1006, "question": "What human evaluation metrics were used in the paper? in Evaluating Rewards for Question Generation Models", "answer": ["rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context"], "top_k_doc_id": [2438, 111, 491, 4796, 1323, 1322, 1969, 3805, 3800, 6590, 5445, 5178, 788, 5442, 2436], "orig_top_k_doc_id": [1323, 1322, 1969, 491, 3805, 2438, 4796, 3800, 6590, 5445, 111, 5178, 788, 5442, 2436]}, {"qid": 1705, "question": "How they add human prefference annotation to fine-tuning process? in Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models", "answer": ["human preference annotation is available, $Q(x_1, x_2) \\in \\lbrace >,<,\\approx \\rbrace $ is the true label for the pair"], "top_k_doc_id": [2438, 1921, 2435, 2436, 2440, 7300, 1474, 3719, 1322, 1473, 2268, 1136, 6671, 1922, 3857], "orig_top_k_doc_id": [2435, 1921, 1474, 2438, 2440, 3719, 1322, 1473, 2436, 2268, 7300, 1136, 6671, 1922, 3857]}]}
{"group_id": 222, "group_size": 7, "items": [{"qid": 1729, "question": "How long is the dataset? in Transductive Learning with String Kernels for Cross-Domain Text Classification", "answer": ["8000"], "top_k_doc_id": [2468, 2308, 2469, 2470, 2471, 2472, 2783, 2782, 2807, 6917, 2307, 415, 4730, 2811, 7139], "orig_top_k_doc_id": [2468, 2472, 2469, 2471, 2470, 2783, 2307, 2308, 415, 2782, 4730, 2811, 7139, 2807, 6917]}, {"qid": 1730, "question": "What machine learning algorithms are used? in Transductive Learning with String Kernels for Cross-Domain Text Classification", "answer": ["string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost, Transductive string kernels, transductive kernel classifier"], "top_k_doc_id": [2468, 2308, 2469, 2470, 2471, 2472, 2783, 2782, 2807, 6917, 2307, 7429, 6101, 855, 2590], "orig_top_k_doc_id": [2468, 2472, 2469, 2471, 2470, 6917, 2308, 7429, 2807, 2783, 2307, 6101, 2782, 855, 2590]}, {"qid": 1731, "question": "What is a string kernel? in Transductive Learning with String Kernels for Cross-Domain Text Classification", "answer": ["String kernel is a technique that uses character n-grams to measure the similarity of strings"], "top_k_doc_id": [2468, 2308, 2469, 2470, 2471, 2472, 2783, 2782, 2807, 6917, 2808, 2811, 5675, 2536, 2810], "orig_top_k_doc_id": [2472, 2469, 2471, 2468, 2470, 2782, 2783, 2807, 2808, 2811, 6917, 5675, 2536, 2810, 2308]}, {"qid": 3768, "question": "Do they reduce language variation of text by enhancing frequencies? in Combining Thesaurus Knowledge and Probabilistic Topic Models", "answer": ["No", "No", "Variation decreases when frequencies of synonyms is enhanced; variation increases when frequencies of synonyms, hyponyms, hypernyms are enhanced"], "top_k_doc_id": [2468, 6126, 6127, 6128, 6129, 6130, 5817, 6594, 4005, 6357, 2779, 5431, 5818, 4741, 2772], "orig_top_k_doc_id": [6128, 6126, 6127, 6129, 6130, 5817, 4005, 6594, 6357, 2779, 5431, 5818, 4741, 2772, 2468]}, {"qid": 3770, "question": "Which thesauri did they use? in Combining Thesaurus Knowledge and Probabilistic Topic Models", "answer": ["WordNet, European Union EuroVoc, RuThes", "WordNet, EuroVoc,  RuThes", "WordNet , EuroVoc , RuThes "], "top_k_doc_id": [2468, 6126, 6127, 6128, 6129, 6130, 5817, 6594, 2625, 6855, 5377, 3113, 2469, 6190, 5067], "orig_top_k_doc_id": [6126, 6129, 6130, 6127, 5817, 6128, 2625, 6855, 6594, 5377, 2468, 3113, 2469, 6190, 5067]}, {"qid": 1728, "question": "What domains are contained in the polarity classification dataset? in Transductive Learning with String Kernels for Cross-Domain Text Classification", "answer": ["Books, DVDs, Electronics, Kitchen appliances"], "top_k_doc_id": [2468, 2308, 2469, 2470, 2471, 2472, 2783, 2307, 2951, 1039, 5421, 5898, 2310, 2306, 1327], "orig_top_k_doc_id": [2468, 2472, 2469, 2471, 2470, 2307, 2308, 2951, 2783, 1039, 5421, 5898, 2310, 2306, 1327]}, {"qid": 3769, "question": "Which domains do they explore? in Combining Thesaurus Knowledge and Probabilistic Topic Models", "answer": ["economic, political", " news articles related to Islam and articles discussing Islam basics", "economic, political"], "top_k_doc_id": [2468, 6126, 6127, 6128, 6129, 6130, 2469, 2022, 6190, 7459, 7232, 4229, 7460, 5096, 6909], "orig_top_k_doc_id": [6126, 6129, 6128, 2469, 2468, 2022, 6190, 6127, 7459, 6130, 7232, 4229, 7460, 5096, 6909]}]}
{"group_id": 223, "group_size": 7, "items": [{"qid": 1869, "question": "What is in the model search space? in The Evolved Transformer", "answer": ["Our search space consists of two stackable cells, one for the model encoder and one for the decoder , Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs, Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one cell-level search field (number of cells)."], "top_k_doc_id": [2744, 2745, 890, 2739, 2741, 2742, 2743, 2740, 3781, 3782, 7813, 2933, 433, 6601, 1454], "orig_top_k_doc_id": [2744, 2742, 2743, 2739, 2745, 2741, 890, 2933, 2740, 7813, 3781, 433, 3782, 6601, 1454]}, {"qid": 1871, "question": "How does Progressive Dynamic Hurdles work? in The Evolved Transformer", "answer": ["It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached."], "top_k_doc_id": [2744, 2745, 890, 2739, 2741, 2742, 2743, 2740, 3781, 3782, 7813, 891, 1429, 1476, 1560], "orig_top_k_doc_id": [2744, 2741, 2739, 2745, 2742, 2743, 2740, 891, 890, 1429, 3781, 7813, 1476, 1560, 3782]}, {"qid": 1868, "question": "what is the proposed Progressive Dynamic Hurdles method? in The Evolved Transformer", "answer": ["allows models that are consistently performing well to train for more steps"], "top_k_doc_id": [2744, 2745, 890, 2739, 2741, 2742, 2743, 2740, 891, 4376, 4375, 4370, 2922, 2923, 7142], "orig_top_k_doc_id": [2744, 2742, 2741, 2745, 2739, 2743, 2740, 891, 890, 4376, 4375, 4370, 2922, 2923, 7142]}, {"qid": 4929, "question": "How much is training speeded up? in On Layer Normalization in the Transformer Architecture", "answer": ["40% speed-up rate", "40%"], "top_k_doc_id": [2744, 2745, 2740, 3027, 7646, 7647, 7648, 7649, 3781, 7651, 4310, 6840, 6292, 7652, 1865], "orig_top_k_doc_id": [7646, 7651, 7647, 7648, 4310, 2740, 6840, 2744, 3781, 2745, 3027, 6292, 7649, 7652, 1865]}, {"qid": 4930, "question": "What experiments do they perform? in On Layer Normalization in the Transformer Architecture", "answer": [" experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task, we train the Pre-LN BERT without the warm-up stage and compare it with the Post-LN BERT", "whether the learning rate warm-up stage is essential, whether the final model performance is sensitive to the value of Twarmup."], "top_k_doc_id": [2744, 2745, 2740, 3027, 7646, 7647, 7648, 7649, 3781, 7651, 6044, 2049, 3646, 4652, 7650], "orig_top_k_doc_id": [2745, 7646, 7647, 2740, 7648, 3027, 7649, 6044, 2744, 2049, 7651, 3646, 3781, 4652, 7650]}, {"qid": 1870, "question": "How much energy did the NAS consume? in The Evolved Transformer", "answer": ["No"], "top_k_doc_id": [2744, 2745, 890, 2739, 2741, 2742, 2743, 3081, 1174, 804, 6613, 1741, 6614, 4933, 7818], "orig_top_k_doc_id": [2743, 2741, 890, 2744, 2742, 3081, 2739, 1174, 804, 6613, 1741, 6614, 4933, 2745, 7818]}, {"qid": 4931, "question": "What is mean field theory? in On Layer Normalization in the Transformer Architecture", "answer": ["No", "No"], "top_k_doc_id": [2744, 2745, 2740, 3027, 7646, 7647, 7648, 7649, 7650, 4310, 5012, 1864, 1865, 651, 5013], "orig_top_k_doc_id": [7646, 2745, 2740, 7649, 7650, 7647, 3027, 2744, 4310, 5012, 1864, 7648, 1865, 651, 5013]}]}
{"group_id": 224, "group_size": 7, "items": [{"qid": 1899, "question": "Are the rules dataset specific? in Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning", "answer": ["Yes"], "top_k_doc_id": [2818, 1743, 2819, 2821, 2822, 6671, 7342, 7066, 2136, 2820, 5855, 6675, 6673, 1165, 231], "orig_top_k_doc_id": [2818, 2819, 6671, 2822, 2821, 2820, 2136, 1743, 6675, 6673, 1165, 7342, 7066, 5855, 231]}, {"qid": 1900, "question": "How many rules had to be defined? in Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning", "answer": ["WikiSQL - 2 rules (SELECT, WHERE)\nSimpleQuestions - 1 rule\nSequentialQA - 3 rules (SELECT, WHERE, COPY)"], "top_k_doc_id": [2818, 1743, 2819, 2821, 2822, 6671, 7342, 7066, 2136, 2820, 5855, 6672, 2806, 5026, 4344], "orig_top_k_doc_id": [2818, 2819, 6671, 2822, 2821, 2820, 1743, 7342, 7066, 2136, 5855, 6672, 2806, 5026, 4344]}, {"qid": 177, "question": "what were their experimental results in the low-resource dataset? in Revisiting Low-Resource Neural Machine Translation: A Case Study", "answer": ["10.37 BLEU"], "top_k_doc_id": [2818, 231, 627, 5026, 6943, 4615, 7342, 7847, 650, 2836, 626, 2906, 1781, 6944, 7066], "orig_top_k_doc_id": [2818, 6943, 650, 627, 4615, 2836, 231, 626, 5026, 7342, 2906, 1781, 6944, 7847, 7066]}, {"qid": 179, "question": "what pitfalls are mentioned in the paper? in Revisiting Low-Resource Neural Machine Translation: A Case Study", "answer": ["highly data-inefficient, underperform phrase-based statistical machine translation"], "top_k_doc_id": [2818, 231, 627, 5026, 6943, 4615, 7342, 7847, 1165, 1899, 3920, 5841, 453, 1044, 3625], "orig_top_k_doc_id": [2818, 6943, 7847, 7342, 5026, 627, 1165, 1899, 3920, 5841, 4615, 453, 1044, 3625, 231]}, {"qid": 1901, "question": "What datasets are used in this paper? in Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning", "answer": ["WikiSQL, SimpleQuestions, SequentialQA"], "top_k_doc_id": [2818, 1743, 2819, 2821, 2822, 6671, 7342, 7066, 231, 7070, 3416, 1782, 6943, 6672, 6673], "orig_top_k_doc_id": [2818, 2819, 6671, 2822, 1743, 2821, 231, 7070, 7066, 7342, 3416, 1782, 6943, 6672, 6673]}, {"qid": 178, "question": "what are the methods they compare with in the korean-english dataset? in Revisiting Low-Resource Neural Machine Translation: A Case Study", "answer": ["gu-EtAl:2018:EMNLP1"], "top_k_doc_id": [2818, 231, 627, 5026, 6943, 2906, 232, 4752, 2355, 2356, 1165, 7066, 2166, 650, 1111], "orig_top_k_doc_id": [231, 2906, 6943, 232, 4752, 2355, 2356, 1165, 7066, 627, 2166, 650, 2818, 1111, 5026]}, {"qid": 1898, "question": "How is the back-translation model trained? in Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning", "answer": [" applying the rule INLINEFORM4 to a set of natural language questions INLINEFORM5, both models are improved following the back-translation protocol that target sequences should follow the real data distribution"], "top_k_doc_id": [2818, 1743, 2819, 2821, 2822, 6671, 7342, 1722, 6267, 2820, 7825, 6943, 233, 231, 7023], "orig_top_k_doc_id": [2818, 2819, 2821, 6671, 1743, 2822, 1722, 6267, 2820, 7825, 6943, 233, 231, 7023, 7342]}]}
{"group_id": 225, "group_size": 7, "items": [{"qid": 2215, "question": "Is the origin of the dialogues in corpus some video game and what game is that? in ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation", "answer": ["No"], "top_k_doc_id": [3451, 3452, 3453, 4958, 7241, 558, 1073, 1074, 576, 7242, 5015, 575, 2689, 4189, 1283], "orig_top_k_doc_id": [3451, 3452, 3453, 7241, 1074, 1073, 576, 7242, 5015, 575, 2689, 558, 4189, 4958, 1283]}, {"qid": 2217, "question": "How the authors made sure that corpus is clean despite being crowdsourced? in ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation", "answer": ["manually cleaned human-produced utterances"], "top_k_doc_id": [3451, 3452, 3453, 4958, 7241, 558, 1073, 1074, 1070, 6808, 7299, 7403, 5017, 2498, 559], "orig_top_k_doc_id": [3451, 3453, 3452, 1070, 4958, 1073, 6808, 1074, 7241, 7299, 558, 7403, 5017, 2498, 559]}, {"qid": 3798, "question": "Do the QA tuples fall under a specific domain? in Time to Take Emoji Seriously: They Vastly Improve Casual Conversational Models", "answer": ["conversations, which consist of at least one question and one free-form answer", "No", "No"], "top_k_doc_id": [3451, 1169, 6158, 6159, 6160, 6161, 6616, 345, 346, 1422, 145, 7283, 3806, 1328, 3805], "orig_top_k_doc_id": [6159, 6161, 6160, 6616, 345, 6158, 346, 145, 1169, 7283, 3451, 1422, 3806, 1328, 3805]}, {"qid": 3799, "question": "What is the baseline model? in Time to Take Emoji Seriously: They Vastly Improve Casual Conversational Models", "answer": ["pre-trained version of BERT without special emoji tokens", "pre-trained version of BERT without special emoji tokens", "pre-trained version of BERT without special emoji tokens"], "top_k_doc_id": [3451, 1169, 6158, 6159, 6160, 6161, 6616, 103, 1433, 1968, 7371, 1770, 1687, 1686, 7283], "orig_top_k_doc_id": [6161, 6160, 6159, 6158, 6616, 1968, 1770, 1169, 1687, 103, 3451, 1686, 1433, 7283, 7371]}, {"qid": 3800, "question": "How large is the corpus of QA tuples? in Time to Take Emoji Seriously: They Vastly Improve Casual Conversational Models", "answer": ["2000 tuples", "2000 tuples", "2000 tuples"], "top_k_doc_id": [3451, 1169, 6158, 6159, 6160, 6161, 6616, 345, 346, 1422, 3096, 6793, 1855, 2344, 103], "orig_top_k_doc_id": [6159, 6161, 6160, 6158, 345, 346, 1422, 6616, 3451, 3096, 6793, 1855, 1169, 2344, 103]}, {"qid": 3801, "question": "What corpus did they use? in Time to Take Emoji Seriously: They Vastly Improve Casual Conversational Models", "answer": ["a customer support dataset", "2000 tuples collected by BIBREF24 that are sourced from Twitter", " customer support dataset with a relatively high usage of emoji"], "top_k_doc_id": [3451, 1169, 6158, 6159, 6160, 6161, 6616, 103, 1433, 1968, 7371, 2344, 6590, 6584, 1168], "orig_top_k_doc_id": [6161, 6159, 6158, 6160, 6616, 103, 3451, 1169, 1433, 7371, 1968, 2344, 6590, 6584, 1168]}, {"qid": 2216, "question": "Is any data-to-text generation model trained on this new corpus, what are the results? in ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation", "answer": ["Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%."], "top_k_doc_id": [3451, 3452, 3453, 4958, 7241, 4963, 3235, 560, 7443, 2435, 575, 5917, 6334, 3794, 4960], "orig_top_k_doc_id": [3451, 3453, 3452, 4958, 4963, 3235, 7241, 560, 7443, 2435, 575, 5917, 6334, 3794, 4960]}]}
{"group_id": 226, "group_size": 7, "items": [{"qid": 2252, "question": "What models does this overview cover? in An overview of embedding models of entities and relationships for knowledge base completion", "answer": ["This article presented a brief overview of embedding models of entity and relationships for KB completion. "], "top_k_doc_id": [6090, 1502, 3534, 4564, 1822, 3533, 6086, 6580, 4565, 3536, 4098, 629, 7678, 4318, 7677], "orig_top_k_doc_id": [4564, 1502, 3536, 4098, 629, 1822, 6090, 3533, 6086, 4565, 3534, 7678, 6580, 4318, 7677]}, {"qid": 2591, "question": "How many feature maps are generated for a given triple? in A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network", "answer": ["3 feature maps for a given tuple"], "top_k_doc_id": [6090, 1502, 3534, 4564, 1822, 3533, 6086, 6580, 4565, 5212, 3409, 6087, 7351, 3535, 267], "orig_top_k_doc_id": [6086, 4564, 6090, 6580, 4565, 5212, 3409, 3534, 6087, 7351, 3533, 3535, 1502, 267, 1822]}, {"qid": 1132, "question": "What scoring function does the model use to score triples? in STransE: a novel embedding model of entities and relationships in knowledge bases", "answer": ["$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $"], "top_k_doc_id": [6090, 1502, 3534, 4564, 1503, 1504, 3409, 3536, 3632, 4565, 4566, 6580, 3672, 182, 6581], "orig_top_k_doc_id": [1502, 4564, 1504, 3534, 4566, 4565, 1503, 3409, 6580, 3672, 182, 3632, 6581, 3536, 6090]}, {"qid": 1133, "question": "What datasets are used to evaluate the model? in STransE: a novel embedding model of entities and relationships in knowledge bases", "answer": ["WN18, FB15k"], "top_k_doc_id": [6090, 1502, 3534, 4564, 1503, 1504, 3409, 3536, 3632, 4565, 4566, 7801, 3628, 1290, 1154], "orig_top_k_doc_id": [1502, 4564, 1504, 3632, 3534, 4566, 7801, 3536, 1503, 4565, 3628, 6090, 1290, 1154, 3409]}, {"qid": 2592, "question": "How does the number of parameters compare to other knowledge base completion models? in A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network", "answer": ["No"], "top_k_doc_id": [6090, 1502, 3534, 4564, 1822, 3533, 6086, 6580, 1823, 4128, 4566, 4216, 3409, 145, 299], "orig_top_k_doc_id": [4564, 1823, 1822, 1502, 6086, 4128, 3533, 4566, 4216, 6090, 6580, 3534, 3409, 145, 299]}, {"qid": 3742, "question": "What size filters do they use in the convolution layer? in A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization", "answer": ["1x3 filter size is used in convolutional layers.", "No", "1x3"], "top_k_doc_id": [6090, 267, 6086, 6087, 6088, 6089, 6091, 4566, 4564, 2058, 2059, 4565, 1327, 2060, 178], "orig_top_k_doc_id": [6086, 6087, 6090, 6091, 6088, 6089, 4566, 4564, 2058, 2059, 267, 4565, 1327, 2060, 178]}, {"qid": 3743, "question": "By how much do they outperform state-of-the-art models on knowledge graph completion? in A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization", "answer": [" improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement), INLINEFORM1 % absolute improvement in Hits@10", "0.105 in MRR and 6.1 percent points in Hits@10 on FB15k-237", "On FB15k-237 dataset it outperforms 0.105 in MRR and  6.1% absolute improvement in Hits@10"], "top_k_doc_id": [6090, 267, 6086, 6087, 6088, 6089, 6091, 341, 4128, 3533, 1353, 4160, 2871, 4098, 1822], "orig_top_k_doc_id": [6086, 6091, 6090, 6087, 267, 6089, 341, 4128, 6088, 3533, 1353, 4160, 2871, 4098, 1822]}]}
{"group_id": 227, "group_size": 7, "items": [{"qid": 2257, "question": "Is this an English-language dataset? in Stance Classification for Rumour Analysis in Twitter: Exploiting Affective Information and Conversation Structure", "answer": ["Yes"], "top_k_doc_id": [3542, 7625, 3545, 4112, 4113, 4114, 4119, 2967, 3543, 3544, 7626, 7628, 4111, 4116, 4118], "orig_top_k_doc_id": [3542, 3543, 7625, 3544, 4114, 3545, 7626, 4111, 4119, 7628, 4112, 4113, 2967, 4118, 4116]}, {"qid": 2258, "question": "What affective-based features are used? in Stance Classification for Rumour Analysis in Twitter: Exploiting Affective Information and Conversation Structure", "answer": ["affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count"], "top_k_doc_id": [3542, 7625, 3545, 4112, 4113, 4114, 4119, 2967, 3543, 3544, 7626, 7628, 4111, 4116, 2970], "orig_top_k_doc_id": [3542, 3543, 7625, 3544, 7626, 3545, 7628, 2967, 2970, 4119, 4112, 4113, 4114, 4116, 4111]}, {"qid": 2259, "question": "What conversation-based features are used? in Stance Classification for Rumour Analysis in Twitter: Exploiting Affective Information and Conversation Structure", "answer": ["Text Similarity to Source Tweet, Text Similarity to Replied Tweet, Tweet Depth"], "top_k_doc_id": [3542, 7625, 3545, 4112, 4113, 4114, 4119, 2967, 3543, 3544, 7626, 7628, 7627, 2970, 7629], "orig_top_k_doc_id": [3542, 3543, 7625, 7626, 3544, 3545, 7628, 4114, 4119, 2967, 4112, 4113, 7627, 2970, 7629]}, {"qid": 4917, "question": "How do they split the dataset when training and evaluating their models? in Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity", "answer": ["SemEval-2017 task 8  dataset includes 325 rumorous conversation threads, and has been split into training, development and test sets. \nThe PHEME dataset provides 2,402 conversations covering nine events - in each fold, one event's conversations are used for testing, and all the rest events are used for training. ", "SemEval-2017 task 8 dataset is split into train, development and test sets. Two events go into test set and eight events go to train and development sets for every thread in the dataset. PHEME dataset is split as leave-one-event-out cross-validation. One event goes to test and the rest of events go to training set for each conversation. Nine folds are created"], "top_k_doc_id": [3542, 7625, 2157, 3287, 3486, 3487, 7626, 7627, 7628, 7629, 3289, 5646, 3545, 4942, 2116], "orig_top_k_doc_id": [7625, 7629, 7626, 7627, 7628, 3486, 3287, 3487, 3542, 2157, 5646, 3545, 4942, 2116, 3289]}, {"qid": 4918, "question": "Do they demonstrate the relationship between veracity and stance over time in the Twitter dataset? in Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity", "answer": ["No", "Yes"], "top_k_doc_id": [3542, 7625, 2157, 3287, 3486, 3487, 7626, 7627, 7628, 7629, 3289, 4111, 4112, 6741, 4119], "orig_top_k_doc_id": [7625, 7629, 7626, 7627, 7628, 3542, 3287, 3486, 4111, 2157, 4112, 3487, 6741, 4119, 3289]}, {"qid": 2461, "question": "Why is a Gaussian process an especially appropriate method for this classification problem? in Using Gaussian Processes for Rumour Stance Classification in Social Media", "answer": ["avoids the need for expensive cross-validation for hyperparameter selection"], "top_k_doc_id": [3542, 7625, 3545, 4112, 4113, 4114, 4119, 4111, 4115, 4116, 4118, 2157, 5927, 5928, 3860], "orig_top_k_doc_id": [4111, 4115, 4112, 4116, 4118, 4113, 4119, 3542, 4114, 2157, 3545, 5927, 7625, 5928, 3860]}, {"qid": 4919, "question": "How much improvement does their model yield over previous methods? in Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity", "answer": ["Their model improves macro-averaged F1 by 0.017 over previous best model in Rumor Stance Classification and improves macro-averaged F1 by 0.03 and 0.015 on Multi-task Rumor Veracity Prediction on SemEval and PHEME datasets respectively", "For single-task, proposed method show\noutperform by  0.031 and 0.053 Macro-F1 for SemEval and PHEME dataset respectively.\nFor multi-task, proposed method show\noutperform by 0.049 and 0.036 Macro-F1 for SemEval and PHEME dataset respectively."], "top_k_doc_id": [3542, 7625, 2157, 3287, 3486, 3487, 7626, 7627, 7628, 7629, 3485, 4111, 6666, 4533, 3545], "orig_top_k_doc_id": [7625, 7629, 7626, 7627, 7628, 3487, 3486, 3287, 3542, 3485, 4111, 2157, 6666, 4533, 3545]}]}
{"group_id": 228, "group_size": 7, "items": [{"qid": 2333, "question": "What experimental evaluation is used? in KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using Twitter Sentiments", "answer": ["root mean square error between the actual and the predicted price of Bitcoin for every minute"], "top_k_doc_id": [3732, 3733, 7307, 3730, 3731, 3734, 3735, 4798, 4989, 4991, 4799, 330, 2696, 1685, 883], "orig_top_k_doc_id": [3730, 3731, 3734, 3735, 3733, 3732, 4989, 4991, 4799, 7307, 4798, 2696, 1685, 330, 883]}, {"qid": 2334, "question": "How is the architecture fault-tolerant? in KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using Twitter Sentiments", "answer": ["By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault"], "top_k_doc_id": [3732, 3733, 7307, 3730, 3731, 3734, 3735, 4798, 4989, 4991, 4799, 330, 2696, 4283, 4722], "orig_top_k_doc_id": [3730, 3731, 3732, 3734, 3735, 3733, 4989, 4283, 4991, 4798, 4722, 2696, 7307, 4799, 330]}, {"qid": 2332, "question": "Do they evaluate only on English datasets? in KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using Twitter Sentiments", "answer": ["Yes"], "top_k_doc_id": [3732, 3733, 7307, 3730, 3731, 3734, 3735, 4798, 4989, 4991, 4799, 332, 331, 3306, 3204], "orig_top_k_doc_id": [3734, 3731, 3730, 3735, 3733, 3732, 4991, 4989, 7307, 4798, 4799, 332, 331, 3306, 3204]}, {"qid": 4687, "question": "What is the relationship between author and emotional valence? in Enhanced Twitter Sentiment Classification Using Contextual Information", "answer": ["people have different baseline emotional valences from one another", "Among those who wrote more than 50 tweets, 16% of the authors have average sentiment within [0.95, 1.00], while only 1.5% of the authors have average sentiment within [-1.00, -0.95]\n"], "top_k_doc_id": [3732, 3733, 7307, 1365, 1367, 1368, 3552, 5007, 5009, 7308, 7056, 7311, 3731, 6077, 5057], "orig_top_k_doc_id": [7307, 7308, 5007, 7311, 1368, 3731, 3733, 1365, 3552, 3732, 1367, 6077, 5009, 5057, 7056]}, {"qid": 4689, "question": "What is the relationship between location and emotional valence? in Enhanced Twitter Sentiment Classification Using Contextual Information", "answer": ["happier in certain states in the United States", "ranking of US states based on their Twitter sentiment correlates with the ranking of US states based on the well-being index"], "top_k_doc_id": [3732, 3733, 7307, 1365, 1367, 1368, 3552, 5007, 5009, 7308, 7056, 954, 5038, 953, 1366], "orig_top_k_doc_id": [7307, 7308, 954, 5007, 1368, 3733, 1365, 3552, 3732, 5038, 1367, 5009, 953, 7056, 1366]}, {"qid": 2335, "question": "Which elements of the platform are modular? in KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using Twitter Sentiments", "answer": ["handling large volume incoming data, sentiment analysis on tweets and predictive online learning"], "top_k_doc_id": [3732, 3733, 7307, 3730, 3731, 3734, 3735, 4798, 4989, 4991, 3554, 3859, 5118, 3135, 3136], "orig_top_k_doc_id": [3730, 3734, 3735, 3731, 3733, 3732, 4989, 4991, 7307, 3554, 3859, 4798, 5118, 3135, 3136]}, {"qid": 4688, "question": "What is the relationship between time and emotional valence? in Enhanced Twitter Sentiment Classification Using Contextual Information", "answer": ["people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays", "The closer the day of the week to Friday and Saturday, the more positive the sentiment; tweets made between 10 a.m. 12 noon are most positive, while those made around 3 a.m. and 20 p.m. are least positive; tweets made in April and May are most positive, while those made in August and September are least positive."], "top_k_doc_id": [3732, 3733, 7307, 1365, 1367, 1368, 3552, 5007, 5009, 7308, 1366, 1500, 7009, 7115, 3731], "orig_top_k_doc_id": [7307, 3733, 5007, 3732, 7308, 1368, 1367, 1366, 1365, 3552, 1500, 5009, 7009, 7115, 3731]}]}
{"group_id": 229, "group_size": 7, "items": [{"qid": 2623, "question": "What is the interannotator agreement for the human evaluation? in Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "answer": ["No", "No"], "top_k_doc_id": [4619, 4620, 4621, 4622, 4623, 3717, 6063, 3157, 6060, 5332, 1971, 6955, 4861, 2419, 6862], "orig_top_k_doc_id": [4619, 4622, 4621, 4623, 4620, 6063, 5332, 1971, 6955, 3717, 3157, 4861, 6060, 2419, 6862]}, {"qid": 2625, "question": "Is the template-based model realistic?   in Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "answer": ["Yes", "Yes"], "top_k_doc_id": [4619, 4620, 4621, 4622, 4623, 3717, 6063, 3157, 6060, 4304, 7284, 4307, 4308, 395, 7242], "orig_top_k_doc_id": [4619, 4623, 4621, 4622, 4620, 4304, 7284, 6060, 4307, 4308, 6063, 3157, 3717, 395, 7242]}, {"qid": 2624, "question": "Who were the human evaluators used? in Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "answer": ["20 evaluators were recruited from our institution and asked to each perform 20 annotations", "20 annotatos from author's institution"], "top_k_doc_id": [4619, 4620, 4621, 4622, 4623, 3717, 6063, 6862, 1135, 6955, 4307, 1134, 5844, 6861, 5142], "orig_top_k_doc_id": [4623, 4622, 4619, 4621, 4620, 6862, 1135, 6955, 4307, 1134, 3717, 6063, 5844, 6861, 5142]}, {"qid": 2627, "question": "What is the recent abstractive summarization method in this paper? in Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "answer": ["pointer networks with coverage mechanism (PG-net)", " pointer networks with coverage mechanism (PG-net)BIBREF0"], "top_k_doc_id": [4619, 4620, 4621, 4622, 4623, 3157, 1132, 4760, 6060, 6955, 2334, 6862, 7284, 4478, 6492], "orig_top_k_doc_id": [4619, 4623, 4621, 4620, 4622, 3157, 1132, 4760, 6060, 6955, 2334, 6862, 7284, 4478, 6492]}, {"qid": 2626, "question": "Is the student reflection data very different from the newspaper data?   in Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis", "answer": ["Yes", "Yes"], "top_k_doc_id": [4619, 4620, 4621, 4622, 4623, 2336, 2334, 2337, 6060, 2335, 6063, 7066, 4027, 4568, 3717], "orig_top_k_doc_id": [4619, 4623, 4621, 4622, 4620, 2336, 2334, 2337, 6060, 2335, 6063, 7066, 4027, 4568, 3717]}, {"qid": 2520, "question": "How are templates discovered from training data? in BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization", "answer": ["For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates."], "top_k_doc_id": [4619, 4620, 4621, 4622, 4308, 4307, 4304, 4305, 4306, 7762, 5259, 5277, 2148, 5260, 5276], "orig_top_k_doc_id": [4308, 4307, 4304, 4305, 4306, 4620, 7762, 4621, 5259, 4622, 5277, 4619, 2148, 5260, 5276]}, {"qid": 2539, "question": "What dataset they use for evaluation? in Unsupervised Text Summarization via Mixed Model Back-Translation", "answer": ["The same 2K set from Gigaword used in BIBREF7"], "top_k_doc_id": [4619, 4396, 4398, 2998, 6424, 5029, 1867, 2226, 6955, 117, 1973, 5165, 6060, 4397, 3160], "orig_top_k_doc_id": [4396, 4398, 2998, 4619, 6424, 5029, 1867, 2226, 6955, 117, 1973, 5165, 6060, 4397, 3160]}]}
{"group_id": 230, "group_size": 7, "items": [{"qid": 2699, "question": "What is the architecture of the model? in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification", "answer": ["one-layer CNN structure from previous works BIBREF22 , BIBREF4", " one-layer CNN"], "top_k_doc_id": [2030, 4727, 4732, 4728, 4729, 4731, 2953, 7422, 599, 5421, 7420, 2307, 2306, 7472, 309], "orig_top_k_doc_id": [4731, 4727, 4728, 4732, 4729, 7422, 7420, 2030, 2307, 2306, 2953, 5421, 7472, 599, 309]}, {"qid": 2700, "question": "What are the baseline methods? in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification", "answer": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "top_k_doc_id": [2030, 4727, 4732, 4728, 4729, 4731, 2953, 7422, 599, 5421, 7420, 2433, 4733, 5044, 3572], "orig_top_k_doc_id": [4731, 4732, 4727, 4728, 4729, 2030, 2953, 2433, 7422, 4733, 5044, 5421, 3572, 599, 7420]}, {"qid": 1455, "question": "How are different domains weighted in WDIRL? in Weighed Domain-Invariant Representation Learning for Cross-domain Sentiment Analysis", "answer": ["To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$"], "top_k_doc_id": [2030, 4727, 4732, 1048, 2031, 2032, 2033, 2035, 4733, 4728, 7282, 6910, 2468, 4583, 2659], "orig_top_k_doc_id": [2030, 2032, 2033, 2035, 4727, 2031, 4732, 6910, 7282, 4733, 1048, 4728, 2468, 4583, 2659]}, {"qid": 1456, "question": "How is DIRL evaluated? in Weighed Domain-Invariant Representation Learning for Cross-domain Sentiment Analysis", "answer": ["Through the experiments, we empirically studied our analysis on DIRL and the effectiveness of our proposed solution in dealing with the problem it suffered from."], "top_k_doc_id": [2030, 4727, 4732, 1048, 2031, 2032, 2033, 2035, 4733, 4728, 7282, 2034, 2307, 6398, 2308], "orig_top_k_doc_id": [2030, 2032, 2033, 2035, 2031, 2034, 4727, 4732, 2307, 4733, 4728, 6398, 2308, 7282, 1048]}, {"qid": 2698, "question": "How many labels do the datasets have? in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification", "answer": ["719313", "Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data."], "top_k_doc_id": [2030, 4727, 4732, 4728, 4729, 4731, 2953, 7422, 447, 390, 472, 450, 4916, 6181, 2433], "orig_top_k_doc_id": [4727, 4731, 4728, 4732, 4729, 2030, 2953, 447, 390, 472, 450, 7422, 4916, 6181, 2433]}, {"qid": 1457, "question": "Which sentiment analysis tasks are addressed? in Weighed Domain-Invariant Representation Learning for Cross-domain Sentiment Analysis", "answer": ["12 binary-class classification and multi-class classification of reviews based on rating"], "top_k_doc_id": [2030, 4727, 4732, 1048, 2031, 2032, 2033, 2035, 4733, 6910, 1039, 309, 1880, 1053, 7114], "orig_top_k_doc_id": [2030, 4727, 2033, 2032, 2035, 4732, 2031, 4733, 6910, 1039, 1048, 309, 1880, 1053, 7114]}, {"qid": 2701, "question": "What are the source and target domains? in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification", "answer": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "top_k_doc_id": [2030, 4727, 4732, 4728, 4729, 4731, 2308, 1048, 4733, 5044, 1049, 2306, 6910, 2468, 2307], "orig_top_k_doc_id": [4727, 4731, 4732, 4728, 2030, 4729, 2308, 1048, 4733, 5044, 1049, 2306, 6910, 2468, 2307]}]}
{"group_id": 231, "group_size": 7, "items": [{"qid": 2794, "question": "What summarization algorithms did the authors experiment with? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 6056, 2396, 2340, 265, 2339, 2334, 6195, 2397], "orig_top_k_doc_id": [4902, 4905, 4903, 2337, 4904, 2342, 2341, 2396, 2340, 6056, 2339, 265, 2334, 6195, 2397]}, {"qid": 2799, "question": "What is the average length of the sentences? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["15.5", "average:15.5"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 6056, 2396, 2340, 265, 2339, 739, 6057, 3306], "orig_top_k_doc_id": [4902, 4903, 4905, 2337, 4904, 2341, 2342, 2340, 6056, 265, 2339, 2396, 739, 6057, 3306]}, {"qid": 2798, "question": "What methods were used for sentence classification? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based", "Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 6056, 2396, 2340, 598, 5674, 6575, 2397, 6057], "orig_top_k_doc_id": [4902, 4903, 4905, 4904, 2337, 2396, 2341, 2342, 598, 2340, 6056, 5674, 6575, 2397, 6057]}, {"qid": 2800, "question": "What is the size of the real-life dataset? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["26972", "26972 sentences"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 6056, 2396, 265, 266, 825, 2397, 5479, 6575], "orig_top_k_doc_id": [4902, 4903, 4904, 4905, 2337, 6056, 265, 2341, 2342, 266, 2396, 825, 2397, 5479, 6575]}, {"qid": 2797, "question": "What evaluation metrics are looked at for classification tasks? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 6056, 6058, 2168, 336, 264, 265, 2340, 6575], "orig_top_k_doc_id": [4902, 4903, 4905, 4904, 2337, 2342, 2341, 6058, 2168, 336, 264, 6056, 265, 2340, 6575]}, {"qid": 2796, "question": "What clustering algorithms were used? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 6056, 5674, 6396, 3300, 2396, 5931, 2397, 6810], "orig_top_k_doc_id": [4902, 4903, 4905, 4904, 2337, 6056, 5674, 6396, 3300, 2341, 2342, 2396, 5931, 2397, 6810]}, {"qid": 2795, "question": "What evaluation metrics were used for the summarization task? in Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals", "answer": ["ROUGE BIBREF22 unigram score", "ROUGE"], "top_k_doc_id": [2337, 2341, 2342, 4902, 4903, 4904, 4905, 264, 111, 5152, 2340, 2339, 1134, 265, 3200], "orig_top_k_doc_id": [4902, 4905, 4903, 2337, 2342, 264, 4904, 2341, 111, 5152, 2340, 2339, 1134, 265, 3200]}]}
{"group_id": 232, "group_size": 7, "items": [{"qid": 2910, "question": "How many layers does the UTCNN model have? in UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "answer": ["eight layers"], "top_k_doc_id": [5092, 5094, 5095, 5096, 2828, 5091, 5093, 3542, 7625, 447, 3273, 4111, 4140, 3860, 3861], "orig_top_k_doc_id": [5094, 5095, 5096, 5091, 5092, 5093, 4140, 3273, 7625, 4111, 3542, 447, 3860, 3861, 2828]}, {"qid": 2914, "question": "What are the baselines? in UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "answer": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "top_k_doc_id": [5092, 5094, 5095, 5096, 2828, 5091, 5093, 3542, 7625, 447, 3273, 4111, 6666, 4119, 5180], "orig_top_k_doc_id": [5094, 5095, 5091, 5096, 5093, 5092, 6666, 3273, 4119, 7625, 447, 4111, 3542, 2828, 5180]}, {"qid": 2912, "question": "What is the size of the Chinese data? in UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "answer": ["32,595 posts", "32,595"], "top_k_doc_id": [5092, 5094, 5095, 5096, 2828, 5091, 5093, 3542, 7625, 447, 6740, 6666, 5180, 3861, 2873], "orig_top_k_doc_id": [5091, 5094, 5095, 5096, 5093, 5092, 447, 6740, 6666, 3542, 2828, 7625, 5180, 3861, 2873]}, {"qid": 2909, "question": "What topic is covered in the Chinese Facebook data?  in UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "answer": ["anti-nuclear-power", "anti-nuclear-power"], "top_k_doc_id": [5092, 5094, 5095, 5096, 2828, 5091, 5093, 447, 2827, 5377, 2077, 2076, 4948, 3542, 6456], "orig_top_k_doc_id": [5091, 5093, 5094, 5096, 5095, 5092, 447, 2827, 5377, 2077, 2076, 4948, 3542, 6456, 2828]}, {"qid": 2911, "question": "What topics are included in the debate data? in UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "answer": ["abortion, gay rights, Obama, marijuana", "abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)"], "top_k_doc_id": [5092, 5094, 5095, 5096, 2828, 5091, 5093, 447, 2827, 5377, 240, 241, 5180, 7625, 330], "orig_top_k_doc_id": [5094, 5091, 5095, 5093, 5096, 240, 5377, 5092, 2827, 241, 5180, 7625, 447, 330, 2828]}, {"qid": 2913, "question": "Did they collected the two datasets? in UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "answer": ["No", "No"], "top_k_doc_id": [5092, 5094, 5095, 5096, 2828, 5091, 5093, 3542, 7625, 6740, 4140, 4112, 2157, 4392, 3860], "orig_top_k_doc_id": [5091, 5094, 5095, 5096, 5093, 5092, 6740, 4140, 3542, 4112, 7625, 2828, 2157, 4392, 3860]}, {"qid": 4563, "question": "How much gain in performance was obtained with user embeddings? in Improved Abusive Comment Moderation with User Embeddings", "answer": ["On test set RNN that uses user embedding has AUC of 80.53 compared to base RNN 79.24.", "16.89 points on G-test from the baseline tBase"], "top_k_doc_id": [5092, 5094, 5095, 5096, 7125, 7124, 7123, 7807, 1876, 6558, 7260, 3585, 1079, 6519, 5144], "orig_top_k_doc_id": [7125, 7124, 7123, 7807, 1876, 6558, 5095, 7260, 5094, 3585, 1079, 5096, 5092, 6519, 5144]}]}
{"group_id": 233, "group_size": 7, "items": [{"qid": 3052, "question": "What evaluation metric is used? in Short Text Language Identification for Under Resourced Languages", "answer": ["average classification accuracy", "average classification accuracy, execution performance"], "top_k_doc_id": [2793, 5201, 5716, 4695, 6181, 45, 3637, 1039, 1043, 5835, 51, 1839, 2790, 986, 2785], "orig_top_k_doc_id": [5201, 2793, 4695, 45, 5716, 3637, 986, 2790, 51, 1039, 1043, 6181, 1839, 2785, 5835]}, {"qid": 3053, "question": "Which languages are similar to each other? in Short Text Language Identification for Under Resourced Languages", "answer": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "top_k_doc_id": [2793, 5201, 5716, 4695, 6181, 45, 3637, 1039, 1043, 5835, 51, 1839, 2790, 68, 4696], "orig_top_k_doc_id": [5201, 2793, 4695, 45, 3637, 5716, 1839, 51, 6181, 2790, 5835, 68, 1039, 4696, 1043]}, {"qid": 3049, "question": "What is the approach of previous work? in Short Text Language Identification for Under Resourced Languages", "answer": ["'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks", "BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24"], "top_k_doc_id": [2793, 5201, 5716, 4695, 6181, 45, 3637, 1039, 1043, 5835, 4692, 986, 6791, 6833, 661], "orig_top_k_doc_id": [5201, 3637, 2793, 4692, 5716, 4695, 1039, 5835, 1043, 45, 6181, 986, 6791, 6833, 661]}, {"qid": 3050, "question": "Is the lexicon the same for all languages? in Short Text Language Identification for Under Resourced Languages", "answer": ["Yes", "Yes"], "top_k_doc_id": [2793, 5201, 5716, 4695, 6181, 45, 3637, 1041, 661, 51, 2790, 2971, 1784, 68, 4696], "orig_top_k_doc_id": [5201, 6181, 2793, 1041, 661, 4695, 5716, 3637, 51, 45, 2790, 2971, 1784, 68, 4696]}, {"qid": 3051, "question": "How do they obtain the lexicon? in Short Text Language Identification for Under Resourced Languages", "answer": ["No", "built over all the data and therefore includes the vocabulary from both the training and testing sets"], "top_k_doc_id": [2793, 5201, 5716, 4695, 6181, 45, 1043, 4696, 1041, 50, 1042, 51, 5837, 5835, 4880], "orig_top_k_doc_id": [5201, 2793, 6181, 4695, 1043, 4696, 1041, 50, 45, 1042, 51, 5716, 5837, 5835, 4880]}, {"qid": 3057, "question": "Does the algorithm improve on the state-of-the-art methods? in Short Text Language Identification for Under Resourced Languages", "answer": ["Yes", "From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature."], "top_k_doc_id": [2793, 5201, 5716, 4695, 6181, 5835, 68, 2790, 4692, 1839, 1041, 7291, 3637, 7290, 5837], "orig_top_k_doc_id": [5201, 5835, 68, 2793, 6181, 5716, 2790, 4692, 1839, 1041, 7291, 3637, 7290, 4695, 5837]}, {"qid": 3056, "question": "What are the languages represented in the DSL datasets?  in Short Text Language Identification for Under Resourced Languages", "answer": ["No"], "top_k_doc_id": [2793, 5201, 5716, 2618, 2789, 2790, 5202, 2788, 2775, 2780, 2783, 1039, 2779, 3637, 68], "orig_top_k_doc_id": [5201, 2618, 2789, 2790, 5202, 2788, 2775, 2793, 2780, 2783, 5716, 1039, 2779, 3637, 68]}]}
{"group_id": 234, "group_size": 7, "items": [{"qid": 3063, "question": "Do the errors of the model reflect linguistic similarity between different L1s? in A Portuguese Native Language Identification Dataset", "answer": ["No"], "top_k_doc_id": [861, 2788, 5209, 5702, 6240, 7327, 2722, 862, 2721, 2723, 5210, 5211, 53, 1067, 2631], "orig_top_k_doc_id": [5209, 5210, 2722, 2721, 2723, 7327, 5211, 5702, 862, 53, 6240, 861, 1067, 2631, 2788]}, {"qid": 3064, "question": "Is the dataset balanced between speakers of different L1s? in A Portuguese Native Language Identification Dataset", "answer": ["No", "No"], "top_k_doc_id": [861, 2788, 5209, 5702, 6240, 7327, 2722, 862, 2721, 2723, 5210, 5211, 3548, 5201, 1287], "orig_top_k_doc_id": [5209, 5210, 2722, 5211, 2723, 2721, 5702, 2788, 3548, 861, 6240, 862, 7327, 5201, 1287]}, {"qid": 689, "question": "What is the baseline model for the agreement-based mode? in Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data", "answer": ["PCFGLA-based parser, viz. Berkeley parser BIBREF5, minimal span-based neural parser BIBREF6"], "top_k_doc_id": [861, 862, 863, 864, 865, 6096, 2917, 3039, 3043, 5209, 7327, 6224, 7331, 5624, 4345], "orig_top_k_doc_id": [861, 865, 863, 864, 862, 5209, 6096, 7327, 3043, 2917, 6224, 7331, 3039, 5624, 4345]}, {"qid": 690, "question": "Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages? in Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data", "answer": ["syntax-based system may generate correct syntactic analyses for partial grammatical fragments"], "top_k_doc_id": [861, 862, 863, 864, 865, 6096, 2917, 3039, 3043, 4344, 5816, 4343, 2957, 3121, 2956], "orig_top_k_doc_id": [865, 861, 863, 864, 862, 4344, 6096, 3039, 5816, 4343, 2917, 3043, 2957, 3121, 2956]}, {"qid": 3065, "question": "How long are the essays on average? in A Portuguese Native Language Identification Dataset", "answer": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "top_k_doc_id": [861, 2788, 5209, 5702, 6240, 7327, 2722, 6457, 6451, 5816, 5201, 6108, 3548, 4896, 2767], "orig_top_k_doc_id": [5209, 6240, 7327, 6457, 2788, 6451, 861, 5702, 5816, 5201, 6108, 3548, 4896, 2722, 2767]}, {"qid": 691, "question": "Who manually annotated the semantic roles for the set of learner texts? in Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data", "answer": ["Authors"], "top_k_doc_id": [861, 862, 863, 864, 865, 6096, 5209, 7332, 7327, 4343, 2957, 4344, 5210, 6098, 2958], "orig_top_k_doc_id": [861, 865, 862, 863, 864, 5209, 7332, 7327, 6096, 4343, 2957, 4344, 5210, 6098, 2958]}, {"qid": 3062, "question": "Are the annotations automatic or manually created? in A Portuguese Native Language Identification Dataset", "answer": ["Automatic", "We performed the annotation with freely available tools for the Portuguese language."], "top_k_doc_id": [861, 2788, 5209, 5702, 6240, 7327, 7332, 5816, 2694, 1287, 3588, 1839, 7103, 6006, 2796], "orig_top_k_doc_id": [7327, 5209, 7332, 5816, 6240, 2788, 2694, 5702, 1287, 3588, 861, 1839, 7103, 6006, 2796]}]}
{"group_id": 235, "group_size": 7, "items": [{"qid": 3232, "question": "What word-based and dictionary-based feature are used? in Generating Word and Document Embeddings for Sentiment Analysis", "answer": ["generate word embeddings specific to a domain, TDK (T\u00fcrk Dil Kurumu - \u201cTurkish Language Institution\u201d) dictionary to obtain word polarities"], "top_k_doc_id": [5417, 7743, 2139, 7174, 1687, 5418, 5420, 1040, 5105, 6855, 6398, 5419, 90, 2142, 5898], "orig_top_k_doc_id": [7174, 1687, 1040, 5417, 5418, 5105, 6855, 6398, 5419, 5420, 90, 7743, 2142, 5898, 2139]}, {"qid": 3233, "question": "How are the supervised scores of the words calculated? in Generating Word and Document Embeddings for Sentiment Analysis", "answer": ["(+1 or -1), words of opposite polarities (e.g. \u201chappy\" and \u201cunhappy\") get far away from each other"], "top_k_doc_id": [5417, 7743, 2139, 7174, 1687, 5418, 5420, 1703, 6053, 6971, 5912, 5421, 6927, 1625, 450], "orig_top_k_doc_id": [5417, 5418, 1703, 7174, 6053, 6971, 2139, 5912, 5421, 6927, 5420, 7743, 1687, 1625, 450]}, {"qid": 989, "question": "What kernels are used in the support vector machines? in Sentiment Analysis of Citations Using Word2vec", "answer": ["No"], "top_k_doc_id": [5417, 1301, 1302, 5407, 6007, 2468, 450, 2782, 2042, 6753, 2044, 1703, 2831, 6005, 7234], "orig_top_k_doc_id": [1302, 1301, 6007, 2468, 5417, 450, 2782, 2042, 6753, 5407, 2044, 1703, 2831, 6005, 7234]}, {"qid": 3228, "question": "What baseline method is used? in Generating Word and Document Embeddings for Sentiment Analysis", "answer": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "top_k_doc_id": [5417, 7743, 2139, 7174, 1567, 6471, 7417, 2649, 7418, 1040, 5421, 6472, 7333, 5896, 1039], "orig_top_k_doc_id": [5417, 7743, 1567, 6471, 7417, 2649, 7418, 1040, 5421, 6472, 7174, 7333, 5896, 1039, 2139]}, {"qid": 3231, "question": "Which hand-crafted features are combined with word2vec? in Generating Word and Document Embeddings for Sentiment Analysis", "answer": ["three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores", "polarity scores, which are minimum, mean, and maximum polarity scores, from each review"], "top_k_doc_id": [5417, 1301, 1302, 5407, 6053, 3195, 5420, 460, 2161, 5418, 5590, 2160, 1449, 3200, 3196], "orig_top_k_doc_id": [5417, 6053, 3195, 5420, 5407, 460, 1301, 1302, 2161, 5418, 5590, 2160, 1449, 3200, 3196]}, {"qid": 731, "question": "What is the dataset used as input to the Word2Vec algorithm? in An Analysis of Word2Vec for the Italian Language", "answer": ["Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words"], "top_k_doc_id": [5417, 1301, 3352, 6054, 52, 4931, 5947, 918, 1449, 2186, 2730, 6053, 3927, 6103, 916], "orig_top_k_doc_id": [3352, 6054, 52, 4931, 5947, 918, 5417, 1449, 2186, 2730, 6053, 3927, 1301, 6103, 916]}, {"qid": 3230, "question": "What details are given about the movie domain dataset? in Generating Word and Document Embeddings for Sentiment Analysis", "answer": ["there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score", "The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment."], "top_k_doc_id": [5417, 7743, 5421, 5419, 6971, 309, 5420, 3101, 7293, 2337, 5896, 6568, 5091, 6566, 1449], "orig_top_k_doc_id": [5421, 5419, 6971, 309, 5420, 5417, 3101, 7293, 2337, 5896, 6568, 7743, 5091, 6566, 1449]}]}
{"group_id": 236, "group_size": 7, "items": [{"qid": 3333, "question": "Which dataset do they train their models on? in Combining Recurrent and Convolutional Neural Networks for Relation Classification", "answer": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "top_k_doc_id": [7232, 1345, 3825, 4811, 5567, 5569, 7367, 4033, 631, 1665, 3081, 3162, 1668, 3535, 575], "orig_top_k_doc_id": [5567, 7367, 5569, 631, 3081, 1665, 3162, 4811, 1668, 3825, 7232, 1345, 4033, 3535, 575]}, {"qid": 3335, "question": "Which variant of the recurrent neural network do they use? in Combining Recurrent and Convolutional Neural Networks for Relation Classification", "answer": ["uni-directional RNN"], "top_k_doc_id": [7232, 1345, 3825, 4811, 5567, 5569, 7367, 4033, 631, 1665, 5288, 2127, 4814, 686, 281], "orig_top_k_doc_id": [3825, 7232, 4811, 5567, 631, 1665, 7367, 1345, 5569, 5288, 2127, 4033, 4814, 686, 281]}, {"qid": 3332, "question": "By how much does their best model outperform the state-of-the-art? in Combining Recurrent and Convolutional Neural Networks for Relation Classification", "answer": ["0.8% F1 better than the best state-of-the-art", "Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1."], "top_k_doc_id": [7232, 1345, 3825, 4811, 5567, 5569, 7367, 4033, 2607, 713, 4198, 686, 1578, 5485, 5215], "orig_top_k_doc_id": [5569, 3825, 2607, 713, 7232, 4811, 7367, 4198, 1345, 4033, 686, 5567, 1578, 5485, 5215]}, {"qid": 3334, "question": "How does their simple voting scheme work? in Combining Recurrent and Convolutional Neural Networks for Relation Classification", "answer": ["we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly", "Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly."], "top_k_doc_id": [7232, 1345, 3825, 4811, 5567, 5569, 7367, 4978, 5568, 6772, 2856, 6070, 4379, 4812, 4033], "orig_top_k_doc_id": [5569, 5567, 1345, 6772, 4811, 2856, 7367, 6070, 7232, 4379, 5568, 4812, 4033, 3825, 4978]}, {"qid": 3336, "question": "How do they obtain the new context represetation? in Combining Recurrent and Convolutional Neural Networks for Relation Classification", "answer": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "top_k_doc_id": [7232, 1345, 3825, 4811, 5567, 5569, 7367, 4978, 5568, 3162, 6119, 631, 112, 7233, 7251], "orig_top_k_doc_id": [5569, 5567, 7232, 3162, 3825, 7367, 6119, 4811, 631, 112, 7233, 5568, 7251, 1345, 4978]}, {"qid": 2492, "question": "What is binary variational dropout? in Bayesian Sparsification of Recurrent Neural Networks", "answer": ["the dropout technique of Gal & Ghahramani gal"], "top_k_doc_id": [7232, 4812, 7233, 4194, 4195, 4196, 4198, 3434, 3433, 4811, 6959, 24, 4813, 5780, 6666], "orig_top_k_doc_id": [4194, 4195, 4196, 4198, 7232, 7233, 3434, 3433, 4812, 4811, 6959, 24, 4813, 5780, 6666]}, {"qid": 4549, "question": "How many layers of recurrent neural networks do they use for encoding the global context? in Contextual Encoding for Translation Quality Estimation", "answer": ["8", "2"], "top_k_doc_id": [7232, 4812, 7233, 7108, 891, 2607, 2739, 4296, 4312, 2311, 2494, 2814, 4033, 2058, 2491], "orig_top_k_doc_id": [7108, 891, 7232, 2607, 2739, 4296, 4312, 2311, 2494, 2814, 4033, 2058, 7233, 4812, 2491]}]}
{"group_id": 237, "group_size": 7, "items": [{"qid": 3348, "question": "Are the answers double (and not triple) annotated? in Question Answering for Privacy Policies: Combining Computational and Legal Perspectives", "answer": ["Yes"], "top_k_doc_id": [6305, 242, 5577, 5578, 5579, 5580, 5581, 6209, 6711, 5910, 5257, 4641, 5518, 7352, 4656], "orig_top_k_doc_id": [5577, 5578, 5580, 6711, 5581, 5579, 4641, 6209, 242, 5518, 5910, 7352, 5257, 6305, 4656]}, {"qid": 3350, "question": "What type of neural model was used? in Question Answering for Privacy Policies: Combining Computational and Legal Perspectives", "answer": ["Bert + Unanswerable", "CNN, BERT"], "top_k_doc_id": [6305, 242, 5577, 5578, 5579, 5580, 5581, 6209, 6711, 5910, 5257, 4609, 1643, 4439, 56], "orig_top_k_doc_id": [5577, 5578, 5580, 6711, 5581, 242, 5579, 6209, 4609, 6305, 5910, 5257, 1643, 4439, 56]}, {"qid": 3351, "question": "Were other baselines tested to compare with the neural baseline? in Question Answering for Privacy Policies: Combining Computational and Legal Perspectives", "answer": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "top_k_doc_id": [6305, 242, 5577, 5578, 5579, 5580, 5581, 6209, 6711, 5910, 5522, 4609, 3775, 5518, 6956], "orig_top_k_doc_id": [5580, 5577, 5578, 5579, 5581, 6711, 6305, 6209, 5522, 4609, 3775, 242, 5910, 5518, 6956]}, {"qid": 3347, "question": "Are the experts comparable to real-world users? in Question Answering for Privacy Policies: Combining Computational and Legal Perspectives", "answer": ["No"], "top_k_doc_id": [6305, 242, 5577, 5578, 5579, 5580, 5581, 6209, 6711, 7459, 7728, 241, 4656, 3775, 5518], "orig_top_k_doc_id": [5577, 5578, 5580, 5581, 7459, 242, 6209, 5579, 6711, 7728, 6305, 241, 4656, 3775, 5518]}, {"qid": 3349, "question": "Who were the experts used for annotation? in Question Answering for Privacy Policies: Combining Computational and Legal Perspectives", "answer": ["Individuals with legal training", "Yes"], "top_k_doc_id": [6305, 242, 5577, 5578, 5579, 5580, 5581, 6209, 6711, 5910, 6010, 5957, 3582, 5907, 7459], "orig_top_k_doc_id": [5578, 5577, 5580, 6711, 5579, 242, 5910, 6209, 6305, 5581, 6010, 5957, 3582, 5907, 7459]}, {"qid": 3906, "question": "what is the baseline model in Legal Question Answering using Ranking SVM and Deep Convolutional Neural Network", "answer": ["two baseline models TF-IDF and LSI which only use Cosine similarity", "two baseline models TF-IDF and LSI", "The baseline models used for this paper are based on the TF-IDF and LSI features and cosine similarity as a retrieval method.", "For the first task they have two baseline models, TF-IDF and LSI which both use cosine similarity. For the QA task, they baseline models were the original CNN and CNN with separate TF-IDF, LSI features."], "top_k_doc_id": [6305, 2733, 2737, 5518, 5736, 6306, 6307, 6309, 6609, 7541, 606, 3175, 2752, 5288, 6532], "orig_top_k_doc_id": [6305, 6306, 6309, 6307, 5518, 5736, 6609, 2733, 7541, 606, 3175, 2737, 2752, 5288, 6532]}, {"qid": 3907, "question": "What contribute to improve the accuracy on legal question answering task? in Legal Question Answering using Ranking SVM and Deep Convolutional Neural Network", "answer": ["Adding more features to the traditional sets such as TF-IDF, BM25 and PL2F as well as using voting in a ranking system help to improve accuracy on a legal question answering task", "two additional statistic features: TF-IDF and LSI"], "top_k_doc_id": [6305, 2733, 2737, 5518, 5736, 6306, 6307, 6309, 5577, 3966, 6308, 3530, 1637, 7148, 3857], "orig_top_k_doc_id": [6305, 6309, 6306, 5518, 5577, 6307, 3966, 6308, 5736, 3530, 2737, 2733, 1637, 7148, 3857]}]}
{"group_id": 238, "group_size": 7, "items": [{"qid": 3379, "question": "what dataset statistics are provided? in MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge", "answer": ["More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).", "Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text"], "top_k_doc_id": [5605, 5610, 1512, 5608, 5609, 1147, 2910, 5607, 2268, 3972, 3973, 7727, 1405, 5019, 3487], "orig_top_k_doc_id": [5605, 5610, 5608, 5609, 3972, 1512, 3973, 5607, 2268, 1147, 2910, 7727, 1405, 5019, 3487]}, {"qid": 3380, "question": "what is the size of their dataset? in MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge", "answer": ["13,939"], "top_k_doc_id": [5605, 5610, 1512, 5608, 5609, 1147, 2910, 5607, 2268, 3972, 3973, 1822, 6888, 1515, 2840], "orig_top_k_doc_id": [5605, 5610, 5608, 5609, 3972, 3973, 1512, 2268, 5607, 1147, 1822, 6888, 2910, 1515, 2840]}, {"qid": 144, "question": "What are the key points in the role of script knowledge that can be studied? in InScript: Narrative texts annotated with script information", "answer": ["No"], "top_k_doc_id": [5605, 5610, 171, 174, 175, 176, 5606, 6888, 173, 5607, 4223, 2618, 172, 6249, 4612], "orig_top_k_doc_id": [176, 171, 5605, 5606, 6888, 5610, 174, 175, 5607, 4223, 2618, 172, 6249, 173, 4612]}, {"qid": 145, "question": "Did the annotators agreed and how much? in InScript: Narrative texts annotated with script information", "answer": ["For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.", "Moderate agreement of 0.64-0.68 Fleiss\u2019 Kappa over event type labels, 0.77 Fleiss\u2019 Kappa over participant labels, and good agreement of 90.5% over coreference information."], "top_k_doc_id": [5605, 5610, 171, 174, 175, 176, 5606, 6888, 173, 5607, 5609, 1702, 2830, 5707, 5272], "orig_top_k_doc_id": [176, 5606, 171, 174, 5605, 175, 173, 6888, 5609, 1702, 2830, 5707, 5607, 5272, 5610]}, {"qid": 3382, "question": "how was the data collected? in MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge", "answer": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "top_k_doc_id": [5605, 5610, 1512, 5608, 5609, 1147, 2910, 5607, 5606, 6916, 352, 2840, 3805, 2836, 1630], "orig_top_k_doc_id": [5605, 5610, 5608, 5609, 5606, 1512, 5607, 6916, 352, 1147, 2840, 3805, 2910, 2836, 1630]}, {"qid": 146, "question": "How many subjects have been used to create the annotations? in InScript: Narrative texts annotated with script information", "answer": [" four different annotators"], "top_k_doc_id": [5605, 5610, 171, 174, 175, 176, 5606, 6888, 370, 5386, 7857, 1363, 4355, 2975, 7289], "orig_top_k_doc_id": [176, 171, 5605, 5606, 174, 6888, 175, 5610, 370, 5386, 7857, 1363, 4355, 2975, 7289]}, {"qid": 3381, "question": "what crowdsourcing platform was used? in MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge", "answer": ["Amazon Mechanical Turk", "Amazon Mechanical Turk"], "top_k_doc_id": [5605, 5610, 1512, 5608, 5609, 171, 3125, 1070, 3972, 1075, 3973, 4075, 584, 2011, 6995], "orig_top_k_doc_id": [5605, 5610, 5609, 5608, 1512, 171, 3125, 1070, 3972, 1075, 3973, 4075, 584, 2011, 6995]}]}
{"group_id": 239, "group_size": 7, "items": [{"qid": 3628, "question": "What morphological typologies are considered? in Character-Level Models versus Morphology in Semantic Role Labeling", "answer": ["agglutinative and fusional languages", "agglutinative and fusional", "Turkish, Finnish, Czech, German, Spanish, Catalan and English", "agglutinative and fusional languages"], "top_k_doc_id": [6279, 6280, 1584, 3904, 4473, 5962, 5963, 5965, 919, 920, 922, 6251, 6255, 6216, 923], "orig_top_k_doc_id": [5962, 6280, 6279, 5965, 5963, 3904, 1584, 922, 6251, 6255, 919, 6216, 4473, 923, 920]}, {"qid": 3630, "question": "What type of morphological features are used? in Character-Level Models versus Morphology in Semantic Role Labeling", "answer": ["char3 slides a character window of width $n=3$ over the token, lemma of the token, additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units., characters, character sequences", "For all languages, morph outputs the lemma of the token followed by language specific morphological tags, additional information for some languages, such as parts-of-speech tags for Turkish", "language specific morphological tags", "morph outputs the lemma of the token followed by language specific morphological tags, semantic roles of verbal predicates"], "top_k_doc_id": [6279, 6280, 1584, 3904, 4473, 5962, 5963, 5965, 919, 920, 922, 6251, 6255, 6281, 6252], "orig_top_k_doc_id": [5962, 3904, 5965, 5963, 6251, 6255, 6280, 6279, 919, 6281, 922, 4473, 6252, 1584, 920]}, {"qid": 3889, "question": "Who made the stated claim (that \"this is because character-level models learn morphology\")? in What do character-level models learn about morphology? The case of dependency parsing", "answer": ["No", "Chung et al. (2016)", "No"], "top_k_doc_id": [6279, 6280, 1773, 6251, 6281, 6283, 5962, 5963, 6255, 5964, 923, 3598, 7318, 1584, 628], "orig_top_k_doc_id": [6279, 6280, 6281, 6283, 6251, 6255, 5963, 1773, 5962, 5964, 923, 3598, 7318, 1584, 628]}, {"qid": 3890, "question": "Which languages do they use? in What do character-level models learn about morphology? The case of dependency parsing", "answer": ["Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew", "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew", "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, Hebrew"], "top_k_doc_id": [6279, 6280, 1773, 6251, 6281, 6283, 627, 655, 1389, 1774, 3598, 6255, 5963, 7318, 4708], "orig_top_k_doc_id": [6280, 6279, 6283, 6281, 1773, 3598, 6251, 627, 6255, 1389, 5963, 7318, 655, 4708, 1774]}, {"qid": 3891, "question": "Do the character-level models perform better than models with access to morphological analyses only? in What do character-level models learn about morphology? The case of dependency parsing", "answer": ["No", "No", "No"], "top_k_doc_id": [6279, 6280, 1773, 6251, 6281, 6283, 5962, 5963, 6255, 6282, 6254, 1774, 5965, 627, 4147], "orig_top_k_doc_id": [6280, 6279, 6281, 6283, 6255, 6251, 5963, 5962, 1773, 6282, 6254, 1774, 5965, 627, 4147]}, {"qid": 3892, "question": "What is case syncretism? in What do character-level models learn about morphology? The case of dependency parsing", "answer": ["A situation in which a noun's syntactic function is ambiguous without context.", "The phenomena where words that have the same form express different morphological cases", "when noun case is ambiguous"], "top_k_doc_id": [6279, 6280, 1773, 6251, 6281, 6283, 627, 655, 1389, 1774, 3598, 7680, 7512, 5964, 4826], "orig_top_k_doc_id": [6279, 6281, 6283, 6280, 1773, 6251, 3598, 1774, 7680, 1389, 627, 7512, 655, 5964, 4826]}, {"qid": 3629, "question": "Does the model consider both derivational and inflectional morphology? in Character-Level Models versus Morphology in Semantic Role Labeling", "answer": ["Yes", "Yes", "Yes", "Yes"], "top_k_doc_id": [6279, 6280, 1584, 3904, 4473, 5962, 5963, 5965, 1430, 221, 648, 658, 4472, 222, 1431], "orig_top_k_doc_id": [5963, 5962, 4473, 1584, 6280, 1430, 221, 6279, 648, 658, 4472, 222, 5965, 1431, 3904]}]}
{"group_id": 240, "group_size": 7, "items": [{"qid": 3684, "question": "What is the state-of-the-art? in F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media", "answer": ["Peng and Dredze peng-dredze:2016:P16-2", "Peng and Dredze peng-dredze:2016:P16-2", "Peng and Dredze peng-dredze:2016:P16-2"], "top_k_doc_id": [2587, 4749, 4750, 6028, 21, 6029, 22, 23, 5189, 929, 1777, 4755, 5254, 1262, 3944], "orig_top_k_doc_id": [6028, 6029, 22, 4750, 21, 4749, 4755, 2587, 5254, 1777, 23, 929, 1262, 3944, 5189]}, {"qid": 3686, "question": "What dataset did they use? in F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media", "answer": ["Peng and Dredze peng-dredze:2016:P16-2, Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service", "Peng and Dredze peng-dredze:2016:P16-2", "a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2"], "top_k_doc_id": [2587, 4749, 4750, 6028, 21, 6029, 22, 23, 5189, 929, 1777, 4755, 7672, 4790, 4323], "orig_top_k_doc_id": [6028, 6029, 22, 21, 4750, 2587, 23, 4755, 4749, 5189, 1777, 929, 7672, 4790, 4323]}, {"qid": 2713, "question": "What state-of-the-art deep neural network is used? in Integrating Boundary Assembling into a DNN Framework for Named Entity Recognition in Chinese Social Media Text", "answer": ["LSTM model", "BIBREF15, BIBREF19, BIBREF20 "], "top_k_doc_id": [2587, 4749, 4750, 6028, 482, 929, 1262, 3422, 4751, 4755, 357, 3944, 6029, 6262, 4756], "orig_top_k_doc_id": [4749, 4750, 4751, 4755, 3422, 3944, 1262, 2587, 6029, 482, 929, 6028, 357, 6262, 4756]}, {"qid": 2715, "question": "What are previous state of the art results? in Integrating Boundary Assembling into a DNN Framework for Named Entity Recognition in Chinese Social Media Text", "answer": ["Overall F1 score:\n- He and Sun (2017) 58.23\n- Peng and Dredze (2017) 58.99\n- Xu et al. (2018) 59.11", "For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%"], "top_k_doc_id": [2587, 4749, 4750, 6028, 482, 929, 1262, 3422, 4751, 4755, 357, 3944, 6029, 2351, 1777], "orig_top_k_doc_id": [4750, 4749, 4751, 4755, 1262, 2587, 482, 3422, 6029, 929, 3944, 6028, 357, 2351, 1777]}, {"qid": 3683, "question": "What is F-score obtained? in F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media", "answer": ["For Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32", "50.60 on Named Entity and 59.32 on Nominal Mention", "Best proposed model achieves F1 score of 50.60, 59.32, 54.82, 20.96 on Named Entity, Nominam Mention, Overall, Out of vocabulary respectively.", "Best F1 score obtained is 54.82% overall"], "top_k_doc_id": [2587, 4749, 4750, 6028, 21, 6029, 22, 23, 5189, 5254, 4300, 4858, 1492, 3041, 3534], "orig_top_k_doc_id": [6028, 6029, 22, 21, 23, 5189, 4750, 2587, 4749, 5254, 4300, 4858, 1492, 3041, 3534]}, {"qid": 2714, "question": "What boundary assembling method is used? in Integrating Boundary Assembling into a DNN Framework for Named Entity Recognition in Chinese Social Media Text", "answer": ["This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.", "backward greedy search over each sentence's label sequence to identify word boundaries"], "top_k_doc_id": [2587, 4749, 4750, 6028, 482, 929, 1262, 3422, 4751, 4755, 3504, 4323, 3642, 5655, 5656], "orig_top_k_doc_id": [4750, 4751, 4749, 3422, 2587, 482, 3504, 1262, 6028, 4323, 929, 3642, 5655, 5656, 4755]}, {"qid": 3685, "question": "Which Chinese social media platform does the data come from? in F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media", "answer": ["No", "Sina Weibo service", "Sina Weibo"], "top_k_doc_id": [2587, 4749, 4750, 6028, 21, 6029, 4755, 3316, 4751, 1777, 929, 2079, 7172, 4287, 6534], "orig_top_k_doc_id": [6028, 6029, 4750, 4755, 3316, 4749, 4751, 1777, 929, 2587, 2079, 7172, 4287, 21, 6534]}]}
{"group_id": 241, "group_size": 7, "items": [{"qid": 3700, "question": "Which model architecture do they use? in Improving Fine-grained Entity Typing with Entity Linking", "answer": ["BiLSTMs , MLP ", "BiLSTM with a three-layer perceptron", "BiLSTM"], "top_k_doc_id": [3844, 3845, 6046, 7612, 7613, 3846, 3847, 3849, 3768, 3769, 6048, 6049, 7614, 4859, 7814], "orig_top_k_doc_id": [6046, 3844, 6048, 3845, 6049, 3849, 7612, 3846, 7613, 3847, 7614, 3769, 4859, 3768, 7814]}, {"qid": 3701, "question": "Which datasets do they evaluate on? in Improving Fine-grained Entity Typing with Entity Linking", "answer": ["FIGER (GOLD) BIBREF0, BBN BIBREF5", "FIGER (GOLD) , BBN", "FIGER (GOLD), BBN"], "top_k_doc_id": [3844, 3845, 6046, 7612, 7613, 3846, 3847, 3849, 3768, 3769, 6048, 6049, 2973, 4947, 2976], "orig_top_k_doc_id": [6046, 3844, 6048, 3845, 6049, 3849, 7612, 3846, 2973, 4947, 3769, 3768, 2976, 7613, 3847]}, {"qid": 4907, "question": "What is the architecture of the model? in Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds", "answer": ["logistic regression", "Document-level context encoder, entity and sentence-level context encoders with common attention, then logistic regression, followed by adaptive thresholds."], "top_k_doc_id": [3844, 3845, 6046, 7612, 7613, 6049, 7614, 1149, 7683, 3768, 981, 5651, 7789, 2422, 3769], "orig_top_k_doc_id": [7613, 7614, 7612, 3845, 6046, 1149, 3844, 7683, 3768, 981, 6049, 5651, 7789, 2422, 3769]}, {"qid": 4909, "question": "What hand-crafted features do other approaches use? in Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds", "answer": ["lexical and syntactic features", "e.g., lexical and syntactic features"], "top_k_doc_id": [3844, 3845, 6046, 7612, 7613, 6049, 7614, 1149, 7683, 459, 3847, 2861, 5327, 6014, 5329], "orig_top_k_doc_id": [7613, 7612, 7614, 6046, 3845, 459, 1149, 3847, 3844, 7683, 2861, 5327, 6014, 5329, 6049]}, {"qid": 2385, "question": "How do you find the entity descriptions? in Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities", "answer": ["Wikipedia"], "top_k_doc_id": [3844, 3845, 6046, 7612, 7613, 3846, 3847, 3849, 3850, 3848, 6297, 4858, 2975, 65, 4074], "orig_top_k_doc_id": [3844, 7612, 3845, 3849, 6046, 3850, 3846, 3848, 7613, 3847, 6297, 4858, 2975, 65, 4074]}, {"qid": 4908, "question": "What fine-grained semantic types are considered? in Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds", "answer": ["No", "/other/event/accident, /person/artist/music, /other/product/mobile phone, /other/event/sports event, /other/product/car"], "top_k_doc_id": [3844, 3845, 6046, 7612, 7613, 6049, 7614, 5327, 2977, 2973, 6014, 724, 3118, 238, 6923], "orig_top_k_doc_id": [7613, 7612, 7614, 6046, 3845, 3844, 5327, 2977, 2973, 6049, 6014, 724, 3118, 238, 6923]}, {"qid": 3699, "question": "How do they obtain the entity linking results in their model? in Improving Fine-grained Entity Typing with Entity Linking", "answer": ["They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.", "The mention is linked to the entity with the greatest commonness score.", "we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string."], "top_k_doc_id": [3844, 3845, 6046, 6048, 6049, 3849, 6047, 4158, 2973, 4859, 1240, 4947, 5923, 3770, 5924], "orig_top_k_doc_id": [6046, 3844, 6048, 6049, 3849, 6047, 4158, 2973, 4859, 3845, 1240, 4947, 5923, 3770, 5924]}]}
{"group_id": 242, "group_size": 7, "items": [{"qid": 3794, "question": "Do the authors report results on only English datasets? in Towards Automatic Bot Detection in Twitter for Health-related Tasks", "answer": ["Yes", "No", "No"], "top_k_doc_id": [1734, 1735, 2827, 6155, 6156, 6157, 6207, 1725, 1726, 1727, 3583, 6195, 6206, 3527, 6455], "orig_top_k_doc_id": [6155, 6157, 6156, 6207, 1735, 1726, 1734, 6195, 6206, 2827, 3583, 3527, 1727, 1725, 6455]}, {"qid": 3796, "question": "How can an existing bot detection system by customized for health-related research? in Towards Automatic Bot Detection in Twitter for Health-related Tasks", "answer": ["An existing bot detection score for each user  can be used as a feature in training", "Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. ", "simple derived features, we were able to significantly improve bot detection performance in health-related data"], "top_k_doc_id": [1734, 1735, 2827, 6155, 6156, 6157, 6207, 1725, 1726, 1727, 3583, 6195, 6206, 7774, 59], "orig_top_k_doc_id": [6157, 6155, 6156, 1735, 1726, 1727, 1725, 1734, 6207, 2827, 7774, 6206, 6195, 59, 3583]}, {"qid": 1263, "question": "Do they analyze what type of content Arabic bots spread in comparison to English? in Hateful People or Hateful Bots? Detection and Characterization of Bots Spreading Religious Hatred in Arabic Social Media", "answer": ["No"], "top_k_doc_id": [1734, 1735, 1725, 1726, 1727, 1728, 1733, 6056, 1729, 1730, 1732, 5976, 7773, 6155, 5979], "orig_top_k_doc_id": [1735, 1725, 1726, 1727, 1734, 1728, 1733, 1729, 5976, 1730, 1732, 6155, 5979, 6056, 7773]}, {"qid": 1264, "question": "Do they propose a new model to better detect Arabic bots specifically? in Hateful People or Hateful Bots? Detection and Characterization of Bots Spreading Religious Hatred in Arabic Social Media", "answer": ["Yes"], "top_k_doc_id": [1734, 1735, 1725, 1726, 1727, 1728, 1733, 6056, 1729, 1730, 1732, 5976, 7773, 5977, 5321], "orig_top_k_doc_id": [1735, 1725, 1727, 1726, 1734, 1728, 1733, 1729, 5976, 1732, 6056, 1730, 5977, 7773, 5321]}, {"qid": 3795, "question": "What are the characteristics of the dataset of Twitter users? in Towards Automatic Bot Detection in Twitter for Health-related Tasks", "answer": ["413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\"", "Tweet Diversity, URL score, Mean Daily Posts, Topics, Mean Post Length, Profile Picture", "a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter, Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites, Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information,  Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation"], "top_k_doc_id": [1734, 1735, 2827, 6155, 6156, 6157, 6207, 1725, 1726, 1727, 1728, 1190, 7016, 64, 1957], "orig_top_k_doc_id": [6155, 6157, 6156, 1735, 1734, 2827, 1725, 6207, 1727, 1728, 1190, 1726, 7016, 64, 1957]}, {"qid": 2538, "question": "How many tweets were manually labelled?  in Forex trading and Twitter: Spam, bots, and reputation manipulation", "answer": ["44,000 tweets"], "top_k_doc_id": [1734, 1735, 1725, 1726, 1727, 1728, 1733, 6056, 4395, 4392, 4393, 4394, 6817, 4112, 2157], "orig_top_k_doc_id": [4395, 4392, 4393, 4394, 1727, 6817, 1726, 6056, 1725, 1733, 1735, 1734, 4112, 2157, 1728]}, {"qid": 3797, "question": "What type of health-related research takes place in social media? in Towards Automatic Bot Detection in Twitter for Health-related Tasks", "answer": ["Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.,  Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. ", " drug reaction detection, syndromic surveillance, subject recruitment for cancer trials, characterizing drug abuse", "almost exclusively on population-level studies, very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women"], "top_k_doc_id": [1734, 1735, 2827, 6155, 6156, 6157, 6207, 6206, 6005, 6209, 1190, 6195, 59, 1193, 1192], "orig_top_k_doc_id": [6155, 6157, 2827, 6156, 6206, 6207, 6005, 1735, 6209, 1190, 6195, 59, 1193, 1734, 1192]}]}
{"group_id": 243, "group_size": 7, "items": [{"qid": 3899, "question": "What is the performance of large state-of-the-art models on these datasets? in Self-Attention Gazetteer Embeddings for Named-Entity Recognition", "answer": ["Average  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5", "Akbik et al. (2019) -  89.3 on Ontonotes 5\nBaevski et al. (2019) 93.5 on CoNLL-03", "93.5"], "top_k_doc_id": [6297, 2321, 6298, 6299, 2322, 4303, 4758, 5081, 7553, 22, 4755, 2320, 1256, 1781, 3784], "orig_top_k_doc_id": [6297, 6299, 2321, 5081, 6298, 7553, 4303, 22, 4755, 4758, 2320, 1256, 2322, 1781, 3784]}, {"qid": 3900, "question": "What is used as a baseline model? in Self-Attention Gazetteer Embeddings for Named-Entity Recognition", "answer": ["Neural CRF model with and without ELMo embeddings", "Neural CRF model with and without ELMo embeddings", "Neural CRF model with and without ELMo embeddings"], "top_k_doc_id": [6297, 2321, 6298, 6299, 2322, 4303, 4758, 5081, 7553, 930, 4573, 6928, 610, 7002, 3360], "orig_top_k_doc_id": [6297, 6299, 2321, 6298, 5081, 7553, 4303, 2322, 930, 4573, 4758, 6928, 610, 7002, 3360]}, {"qid": 2772, "question": "Which other approaches do they compare their model with? in Fine-Grained Named Entity Recognition using ELMo and Wikidata", "answer": ["Akbik et al. (2018), Link et al. (2012)", "They compare to Akbik et al. (2018) and Link et al. (2012)."], "top_k_doc_id": [6297, 4573, 4833, 4859, 4860, 5184, 5185, 6298, 65, 7553, 2973, 5923, 4576, 3845, 5051], "orig_top_k_doc_id": [6297, 4860, 4859, 5184, 6298, 2973, 4573, 5923, 7553, 5185, 4576, 4833, 3845, 65, 5051]}, {"qid": 2773, "question": "What results do they achieve using their proposed approach? in Fine-Grained Named Entity Recognition using ELMo and Wikidata", "answer": ["F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).", " total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%"], "top_k_doc_id": [6297, 4573, 4833, 4859, 4860, 5184, 5185, 6298, 65, 7553, 5775, 2096, 21, 5084, 4858], "orig_top_k_doc_id": [6297, 4860, 4859, 5184, 6298, 4573, 5775, 7553, 2096, 65, 21, 5185, 4833, 5084, 4858]}, {"qid": 3901, "question": "How do they build gazetter resources from Wikipedia knowlege base? in Self-Attention Gazetteer Embeddings for Named-Entity Recognition", "answer": ["process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long, we use the sitelink count to keep the six most popular types, To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure", "Extract entity type tuples at appropriate level of granularity depending on the NER task.", "To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State."], "top_k_doc_id": [6297, 2321, 6298, 6299, 1090, 1092, 1423, 4573, 7351, 1422, 497, 2974, 7002, 7100, 1098], "orig_top_k_doc_id": [6297, 2321, 1090, 1092, 1423, 4573, 7351, 6299, 1422, 497, 6298, 2974, 7002, 7100, 1098]}, {"qid": 1978, "question": "Did they experiment with the dataset on some tasks? in Automatically Annotated Turkish Corpus for Named Entity Recognition and Text Categorization using Large-Scale Gazetteers", "answer": ["Yes"], "top_k_doc_id": [6297, 2321, 2973, 2974, 2975, 7597, 4300, 4573, 4303, 2977, 7553, 1098, 7599, 2320, 7688], "orig_top_k_doc_id": [2973, 2974, 2975, 7597, 4300, 4573, 4303, 2977, 2321, 7553, 1098, 7599, 6297, 2320, 7688]}, {"qid": 2774, "question": "How do they combine a deep learning model with a knowledge base? in Fine-Grained Named Entity Recognition using ELMo and Wikidata", "answer": ["Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.", "ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token"], "top_k_doc_id": [6297, 4573, 4833, 4859, 4860, 5184, 5185, 6298, 7002, 4858, 1423, 6046, 7244, 2973, 5923], "orig_top_k_doc_id": [4859, 6297, 7002, 4860, 5184, 6298, 4858, 1423, 4573, 5185, 6046, 7244, 2973, 5923, 4833]}]}
{"group_id": 244, "group_size": 7, "items": [{"qid": 3902, "question": "What is the dataset that is used to train the embeddings? in Phonetic-and-Semantic Embedding of Spoken Words with Applications in Spoken Content Retrieval", "answer": [" LibriSpeech BIBREF46", "LibriSpeech", "LibriSpeech"], "top_k_doc_id": [6300, 6370, 2630, 2768, 4928, 6301, 6302, 6303, 6304, 6616, 6896, 2631, 4149, 3648, 46], "orig_top_k_doc_id": [6304, 6300, 6303, 6301, 6302, 6370, 4928, 6616, 2630, 6896, 4149, 2768, 2631, 3648, 46]}, {"qid": 3904, "question": "What language is used for the experiments? in Phonetic-and-Semantic Embedding of Spoken Words with Applications in Spoken Content Retrieval", "answer": ["English", "English", "English"], "top_k_doc_id": [6300, 6370, 2630, 2768, 4928, 6301, 6302, 6303, 6304, 6616, 6896, 2631, 4149, 2769, 3651], "orig_top_k_doc_id": [6304, 6300, 6303, 6301, 6302, 6370, 2630, 6896, 4149, 4928, 6616, 2768, 2631, 2769, 3651]}, {"qid": 3903, "question": "What speaker characteristics are used? in Phonetic-and-Semantic Embedding of Spoken Words with Applications in Spoken Content Retrieval", "answer": ["speaker characteristics, microphone characteristics, background noise", "No", "Acoustic factors such as speaker characteristics, microphone characteristics, background noise."], "top_k_doc_id": [6300, 6370, 2630, 2768, 4928, 6301, 6302, 6303, 6304, 6616, 6896, 3651, 7537, 3649, 46], "orig_top_k_doc_id": [6300, 6301, 6303, 6304, 6302, 6370, 6616, 2768, 3651, 2630, 7537, 6896, 4928, 3649, 46]}, {"qid": 3949, "question": "Which datasets do they use? in Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data", "answer": ["LibriSpeech corpus BIBREF46, GlobalPhone corpus BIBREF47", "LibriSpeech corpus, GlobalPhone corpus", "LibriSpeech, GlobalPhone"], "top_k_doc_id": [6300, 6370, 2713, 6310, 6371, 6372, 6373, 6374, 1622, 4875, 5825, 5217, 1785, 5218, 3843], "orig_top_k_doc_id": [6371, 6370, 6373, 6372, 5217, 6374, 6300, 2713, 5825, 1785, 5218, 4875, 6310, 3843, 1622]}, {"qid": 3951, "question": "Which pairs of languages do they consider similar enough to capture phonetic structure? in Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data", "answer": ["German and French", "English paired with any of the following: French, German, Czech, Spanish.", "English, German and French"], "top_k_doc_id": [6300, 6370, 2713, 6310, 6371, 6372, 6373, 6374, 1622, 4875, 5825, 6301, 6303, 4863, 6302], "orig_top_k_doc_id": [6370, 6373, 6371, 6372, 6374, 6300, 6301, 6303, 4863, 6302, 6310, 1622, 5825, 2713, 4875]}, {"qid": 3905, "question": "Is the embedding model test in any downstream task? in Phonetic-and-Semantic Embedding of Spoken Words with Applications in Spoken Content Retrieval", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6300, 6370, 2630, 2768, 4928, 6301, 6302, 6303, 6304, 6616, 4149, 2355, 2823, 7662, 986], "orig_top_k_doc_id": [6304, 6300, 6303, 6301, 6302, 6370, 6616, 4928, 2630, 2768, 4149, 2355, 2823, 7662, 986]}, {"qid": 3950, "question": "How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data? in Language Transfer of Audio Word2Vec: Learning Audio Segment Representations without Target Language Data", "answer": ["They compare retrieval performance in MAP.", "They compare MAP performance of query-by-example STD using representations obtained from naive encoder and their method", "MAP, MAP results on large testing database (250K segments)"], "top_k_doc_id": [6300, 6370, 2713, 6310, 6371, 6372, 6373, 6374, 2712, 2709, 5217, 6350, 4030, 5218, 4590], "orig_top_k_doc_id": [6371, 6373, 6370, 6372, 6310, 2712, 2709, 2713, 5217, 6350, 6300, 4030, 6374, 5218, 4590]}]}
{"group_id": 245, "group_size": 7, "items": [{"qid": 4012, "question": "Does programme plans gathering and open sourcing some large dataset for Icelandic language? in Language Technology Programme for Icelandic 2019-2023", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 7433, 875, 7432, 1439, 1441, 1440, 3711], "orig_top_k_doc_id": [6463, 6467, 6464, 6465, 6469, 6466, 6468, 6470, 1439, 1441, 1440, 875, 7432, 7433, 3711]}, {"qid": 4013, "question": "What concrete software is planned to be developed by the end of the programme? in Language Technology Programme for Icelandic 2019-2023", "answer": ["A lot of new software will be developed in all areas of the programme, some will be extensions of already available Greynir software.", "IceNLP, Greynir , Nefnir , ABLTagger, a flexible lexicon acquisition tool, A punctuation system for Icelandic ,  open source correction system, a statistical phrase-based MT system ,  a bidirectional LSTM model using the neural translation system OpenNMT, a system based on an attention-based neural network, An API and a web user interface"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 7433, 875, 7432, 275, 6814, 925, 7866], "orig_top_k_doc_id": [6464, 6463, 6465, 6467, 6466, 6469, 6468, 6470, 875, 275, 7433, 6814, 925, 7432, 7866]}, {"qid": 4014, "question": "What other national language technology programs are described in the paper? in Language Technology Programme for Icelandic 2019-2023", "answer": ["STEVIN programme in the Netherlands, Plan for the Advancement of Language Technology in Spain, Estonian LT programmes", "STEVIN programme in the Netherlands,  Plan for the Advancement of Language Technology in Spain, Estonian LT programmes", "Netherlands, Spain, Estonian"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 7433, 3127, 7149, 6591, 7148, 7162, 4215], "orig_top_k_doc_id": [6463, 6464, 6465, 6466, 6469, 6467, 6470, 6591, 6468, 7148, 7433, 7149, 3127, 7162, 4215]}, {"qid": 4015, "question": "When did language technology start in Iceland? in Language Technology Programme for Icelandic 2019-2023", "answer": ["Around year 2000", "in the year 2000", "in the year 2000, couple of LT resources and products were developed in the years leading up to that"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 7433, 3127, 7149, 875, 965, 7046, 3128], "orig_top_k_doc_id": [6463, 6465, 6464, 6469, 6470, 6467, 6466, 6468, 7433, 3127, 875, 7149, 965, 7046, 3128]}, {"qid": 1090, "question": "How are the substitution rules built? in Nefnir: A high accuracy lemmatizer for Icelandic", "answer": ["from the Database of Modern Icelandic Inflection (DMII) BIBREF1"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 1439, 1440, 1441, 1442, 7318, 3710, 3890, 1984], "orig_top_k_doc_id": [1441, 1440, 1439, 6466, 6467, 1442, 6469, 6465, 3710, 6468, 3890, 6463, 7318, 6464, 1984]}, {"qid": 1091, "question": "Which dataset do they use? in Nefnir: A high accuracy lemmatizer for Icelandic", "answer": ["a reference corpus of 21,093 tokens and their correct lemmas"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 1439, 1440, 1441, 1442, 7318, 397, 7320, 396], "orig_top_k_doc_id": [1441, 1439, 1440, 1442, 6467, 6466, 397, 6464, 6463, 6465, 6468, 7318, 6469, 7320, 396]}, {"qid": 4011, "question": "What private companies are members of consortium? in Language Technology Programme for Icelandic 2019-2023", "answer": ["Creditinfo, Grammatek, Mideind and Tiro", "The \u00c1rni Magn\u00fasson Instit. for Icelandic Studies, Reykjavik University (RU),  University of Iceland (UI),  R\u00daV, Creditinfo, The Association of the Visually Impaired, Grammatek, Mi\u00f0eind. Tiro", "Crediyinfo, Grammatek, \nMideind,\nTiro"], "top_k_doc_id": [6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 1441, 875, 3641, 4186, 3890, 6667, 6209], "orig_top_k_doc_id": [6465, 6463, 6464, 6470, 6469, 6466, 6467, 6468, 1441, 875, 3641, 4186, 3890, 6667, 6209]}]}
{"group_id": 246, "group_size": 7, "items": [{"qid": 4038, "question": "Which dataset do they use? in Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages", "answer": ["GlobalPhone corpus", "GlobalPhone\nCroatian\nHausa\nMandarin\nSpanish\nSwedish\nTurkish\nZRSC\nBuckeye\nXitsonga", "GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus"], "top_k_doc_id": [6485, 6487, 6491, 6484, 3617, 5709, 5710, 5712, 6060, 1061, 1064, 6486, 6782, 3619, 4568], "orig_top_k_doc_id": [6484, 6485, 6487, 6491, 5709, 5710, 3617, 6782, 1061, 5712, 3619, 4568, 6060, 6486, 1064]}, {"qid": 4039, "question": "Which intrisic measures do they use do evaluate obtained representations? in Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages", "answer": ["same-different, ABX evaluation measures", "same-different, ABX ", "Precision and recall at a given threshold"], "top_k_doc_id": [6485, 6487, 6491, 6484, 3617, 5709, 5710, 5712, 6060, 1061, 1064, 6486, 6488, 3618, 6035], "orig_top_k_doc_id": [6484, 6485, 6491, 5709, 5710, 3617, 5712, 6487, 6486, 6488, 6060, 1064, 1061, 3618, 6035]}, {"qid": 4036, "question": "With how many languages do they experiment in the multilingual setup? in Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages", "answer": ["ten languages", "16", "6"], "top_k_doc_id": [6485, 6487, 6491, 6484, 3617, 5709, 5710, 5712, 6060, 4510, 5360, 6782, 3297, 4030, 4568], "orig_top_k_doc_id": [6484, 6485, 3617, 5710, 5712, 6487, 5709, 4510, 6491, 5360, 6782, 3297, 4030, 6060, 4568]}, {"qid": 4037, "question": "How do they extract target language bottleneck features? in Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages", "answer": ["train a tdnn BIBREF36 with block softmax, tdnn has six 625-dimensional hidden layers followed by a 39-dimensional bottleneck layer", "Extracting cae features requires three steps, as illustrated in Figure FIGREF6 . First, an utd system is applied to the target language to extract pairs of speech segments that are likely to be instances of the same word or phrase"], "top_k_doc_id": [6485, 6487, 6491, 6484, 3617, 5709, 5710, 1061, 6486, 661, 4568, 4569, 1593, 7339, 7340], "orig_top_k_doc_id": [6484, 6485, 6487, 1061, 6486, 6491, 5709, 661, 5710, 4568, 3617, 4569, 1593, 7339, 7340]}, {"qid": 837, "question": "What is the performance difference in performance in unsupervised feature learning between adverserial training and FHVAE-based disentangled speech represenation learning? in Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling", "answer": ["No"], "top_k_doc_id": [6485, 6487, 6491, 6484, 1061, 1064, 1062, 2709, 1005, 1063, 276, 4754, 5822, 6300, 5712], "orig_top_k_doc_id": [1061, 1064, 1062, 2709, 6484, 1005, 6487, 1063, 6485, 276, 6491, 4754, 5822, 6300, 5712]}, {"qid": 2194, "question": "What are bottleneck features? in ASR-free CNN-DTW keyword spotting using multilingual bottleneck features for almost zero-resource languages", "answer": ["Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese, South African English, These features are typically obtained by training a deep neural network jointly on several languages for which labelled data is available., The final shared layer often has a lower dimensionality than the input layer, and is therefore referred to as a `bottleneck'."], "top_k_doc_id": [6485, 6487, 6491, 1112, 3401, 3402, 3403, 3404, 5822, 5824, 6486, 7339, 7340, 1061, 381], "orig_top_k_doc_id": [3401, 3402, 3403, 3404, 5822, 6487, 6486, 6485, 5824, 7339, 6491, 7340, 1061, 381, 1112]}, {"qid": 2195, "question": "What languages are considered? in ASR-free CNN-DTW keyword spotting using multilingual bottleneck features for almost zero-resource languages", "answer": ["Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese"], "top_k_doc_id": [6485, 6487, 6491, 1112, 3401, 3402, 3403, 3404, 5822, 5824, 6486, 5566, 1113, 1784, 247], "orig_top_k_doc_id": [3401, 3402, 3403, 3404, 5822, 6487, 6485, 6491, 6486, 1112, 5566, 1113, 1784, 247, 5824]}]}
{"group_id": 247, "group_size": 7, "items": [{"qid": 4045, "question": "Which languages are explored? in Neural Machine Translation with Imbalanced Classes", "answer": ["German (De) and English (En)", "German, English", "German (De) and English (En) languages"], "top_k_doc_id": [6498, 6501, 6054, 7600, 2329, 6007, 6014, 7042, 564, 628, 5868, 4184, 4590, 4712, 6917], "orig_top_k_doc_id": [6498, 6007, 6014, 7042, 2329, 564, 6054, 628, 6501, 5868, 4184, 4590, 4712, 7600, 6917]}, {"qid": 4047, "question": "What vocabulary sizes are explored? in Neural Machine Translation with Imbalanced Classes", "answer": ["Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "Word 64k, Word 32k, BPE 64k,  BPE 32k,   BPE 16k,   BPE 8k,   BPE 4k,   BPE 2k,   BPE 1k.", "Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176."], "top_k_doc_id": [6498, 6501, 6054, 7600, 2329, 6007, 6014, 7042, 1861, 6500, 4619, 6502, 7686, 6013, 6539], "orig_top_k_doc_id": [6498, 6501, 6014, 1861, 6500, 2329, 7600, 6007, 4619, 6502, 7686, 7042, 6013, 6054, 6539]}, {"qid": 4044, "question": "Which vocabulary size was the better performer? in Neural Machine Translation with Imbalanced Classes", "answer": ["Source BPE vocabulary size is 32000; target BPE vocabulary size is 8000.", "BPE 32k, 32k"], "top_k_doc_id": [6498, 6501, 4786, 6500, 6502, 6773, 7272, 7686, 2760, 6055, 648, 2186, 7595, 1668, 919], "orig_top_k_doc_id": [6498, 6501, 6773, 2760, 6055, 7272, 648, 2186, 7686, 6500, 6502, 7595, 4786, 1668, 919]}, {"qid": 4048, "question": "What vocabulary size was the best performer? in Neural Machine Translation with Imbalanced Classes", "answer": ["No", "Source BPE vocabulary size is 32000; target BPE vocabulary size is 8000."], "top_k_doc_id": [6498, 6501, 4786, 6500, 6502, 6773, 7272, 7686, 6013, 6554, 4315, 1115, 6614, 6014, 4116], "orig_top_k_doc_id": [6498, 6501, 6773, 7686, 6500, 7272, 6013, 4786, 6554, 4315, 1115, 6502, 6614, 6014, 4116]}, {"qid": 4050, "question": "Which vocab sizes did they analyze? in Neural Machine Translation with Imbalanced Classes", "answer": ["Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176.", "Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "Word 64k, Word 32k, BPE 64k,  BPE 32k,   BPE 16k,   BPE 8k,   BPE 4k,   BPE 2k,   BPE 1k."], "top_k_doc_id": [6498, 6501, 6054, 7600, 3652, 6500, 2761, 5498, 2762, 5278, 1861, 4316, 1843, 7595, 4786], "orig_top_k_doc_id": [6498, 3652, 6501, 6500, 2761, 5498, 2762, 5278, 1861, 4316, 6054, 1843, 7595, 7600, 4786]}, {"qid": 4046, "question": "What datasets are used in the paper? in Neural Machine Translation with Imbalanced Classes", "answer": ["Europarl v9 parallel data set, NewsTest2013, NewsTest2014", "Europarl v9 parallel data set, NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.", "Europarl v9, NewsTest2013, NewsTest2014"], "top_k_doc_id": [6498, 3446, 4786, 5498, 5499, 6013, 6014, 729, 6501, 7686, 7595, 5264, 5146, 6917, 4866], "orig_top_k_doc_id": [6498, 5498, 4786, 6014, 6013, 729, 6501, 7686, 5499, 3446, 7595, 5264, 5146, 6917, 4866]}, {"qid": 4049, "question": "What datasets do they look at? in Neural Machine Translation with Imbalanced Classes", "answer": ["Europarl v9, NewsTest2013 , NewsTest2014", "Europarl v9, NewsTest2013, NewsTest2014", "Europarl v9 parallel data set, NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track"], "top_k_doc_id": [6498, 3446, 4786, 5498, 5499, 6013, 6014, 575, 6591, 4118, 2538, 2547, 3260, 2041, 6943], "orig_top_k_doc_id": [6498, 6014, 3446, 5498, 4786, 6013, 575, 6591, 4118, 2538, 5499, 2547, 3260, 2041, 6943]}]}
{"group_id": 248, "group_size": 7, "items": [{"qid": 4102, "question": "What model architectures are used? in Reducing Gender Bias in Abusive Language Detection", "answer": ["Convolutional Neural Network, Gated Recurrent Unit, Bidirectional GRU with self-attention", "Convolutional Neural Network, Gated Recurrent Unit, Bidirectional GRU with self-attention", "Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU), Bidirectional GRU with self-attention ( INLINEFORM0 -GRU)"], "top_k_doc_id": [6558, 6560, 52, 7266, 3988, 5949, 6559, 7267, 5292, 6272, 7261, 3581, 3583, 5976, 3582], "orig_top_k_doc_id": [6558, 6560, 3988, 5949, 7267, 7261, 7266, 5292, 3581, 3583, 52, 6559, 6272, 5976, 3582]}, {"qid": 4103, "question": "What pre-trained word embeddings are used? in Reducing Gender Bias in Abusive Language Detection", "answer": ["word2vec, FastText, randomly initialized embeddings (random)", "word2vec train on Google News corpus; FastText train on Wikipedia corpus; randomly initialized embeddings", "word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus,"], "top_k_doc_id": [6558, 6560, 52, 7266, 3988, 5949, 6559, 7267, 5292, 6272, 7261, 5814, 5291, 7260, 3989], "orig_top_k_doc_id": [6558, 6560, 3988, 5949, 6559, 7266, 5292, 7267, 52, 7261, 5814, 5291, 7260, 6272, 3989]}, {"qid": 4104, "question": "What metrics are used to measure gender biases? in Reducing Gender Bias in Abusive Language Detection", "answer": ["False Positive Equality Difference, False Negative Equality Difference", "AUC scores on the original test set , AUC scores on the unbiased generated test set, the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate", "AUC scores on the original test set (Orig. AUC),  AUC scores on the unbiased generated test set (Gen. AUC), false positive/negative equality differences"], "top_k_doc_id": [6558, 6560, 52, 7266, 3988, 5949, 6559, 7267, 5155, 3011, 5153, 3010, 1443, 3007, 7064], "orig_top_k_doc_id": [6558, 6560, 7267, 3988, 6559, 7266, 5949, 5155, 52, 3011, 5153, 3010, 1443, 3007, 7064]}, {"qid": 2186, "question": "How are these biases found? in Measuring Social Bias in Knowledge Graph Embeddings", "answer": ["No"], "top_k_doc_id": [6558, 6560, 3011, 3581, 3988, 5525, 3354, 3356, 6735, 3355, 5006, 1799, 1355, 1800, 7266], "orig_top_k_doc_id": [3354, 3356, 6558, 3988, 6735, 3355, 3581, 5006, 6560, 3011, 1799, 1355, 1800, 7266, 5525]}, {"qid": 2436, "question": "Any other bias may be detected? in Racial Bias in Hate Speech and Abusive Language Detection Datasets", "answer": ["No"], "top_k_doc_id": [6558, 6560, 3011, 3581, 3988, 5525, 3989, 5292, 3007, 5976, 3587, 3582, 3045, 1788, 3309], "orig_top_k_doc_id": [3988, 3989, 6558, 5292, 3007, 5976, 3587, 5525, 3582, 3011, 6560, 3581, 3045, 1788, 3309]}, {"qid": 4785, "question": "What metrics of gender bias amplification are used to demonstrate the effectiveness of this approach? in On the Unintended Social Bias of Training Language Generation Models with Data from Local Media", "answer": ["the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman)", "bias amplification metric, bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators"], "top_k_doc_id": [6558, 6560, 52, 7266, 7455, 718, 3861, 5155, 7454, 5764, 717, 6272, 6735, 5153, 7064], "orig_top_k_doc_id": [7455, 6558, 718, 6560, 3861, 5155, 52, 7454, 7266, 5764, 717, 6272, 6735, 5153, 7064]}, {"qid": 4784, "question": "Do the authors evaluate only on English datasets? in On the Unintended Social Bias of Training Language Generation Models with Data from Local Media", "answer": ["No", "No"], "top_k_doc_id": [6558, 3861, 7455, 5472, 7523, 52, 3581, 5764, 6207, 4004, 3988, 6835, 3007, 5812, 4829], "orig_top_k_doc_id": [6558, 3861, 7455, 5472, 7523, 52, 3581, 5764, 6207, 4004, 3988, 6835, 3007, 5812, 4829]}]}
{"group_id": 249, "group_size": 7, "items": [{"qid": 4258, "question": "Which metrics are used for quantitative analysis? in Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "answer": ["perplexity, cross entropy", "Cross entropy between the trained model and models trained on different corpora.", "a measure that calculates the cross entropy between the word distribution of the model output and that of the target data"], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 624, 5508, 3102, 4414, 5506, 6070, 3559, 4124, 5507, 659], "orig_top_k_doc_id": [6747, 6751, 6749, 6748, 6750, 4414, 5506, 3102, 624, 3559, 4124, 5508, 6070, 5507, 659]}, {"qid": 4261, "question": "What metric did they use for qualitative evaluation? in Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "answer": ["Sample model output", "length and style of sample output"], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 624, 5508, 495, 627, 2225, 2970, 4878, 6590, 5933, 5507], "orig_top_k_doc_id": [6751, 6747, 6750, 6749, 6748, 5508, 6590, 5933, 2970, 5507, 627, 2225, 495, 624, 4878]}, {"qid": 4262, "question": "What metric did they use for quantitative evaluation? in Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "answer": ["perplexity", "Cross entropy between word distribution of model output and word distribution of target data."], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 624, 5508, 495, 627, 2225, 2970, 4878, 6590, 5506, 4124], "orig_top_k_doc_id": [6751, 6747, 6748, 6749, 6750, 6590, 2970, 5506, 627, 2225, 495, 624, 4878, 5508, 4124]}, {"qid": 4263, "question": "Which similarity metrics are used for quantitative analysis? in Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "answer": ["Cross entropy between word distribution of model output and word distribution of target data.", "cross entropy"], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 624, 5508, 3102, 4414, 5506, 6070, 6071, 2525, 2225, 627], "orig_top_k_doc_id": [6747, 6751, 6748, 6749, 6750, 6070, 4414, 5508, 5506, 3102, 6071, 2525, 2225, 624, 627]}, {"qid": 4259, "question": "Is their data open sourced? in Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "answer": ["No", "No", "No"], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 624, 495, 3102, 4124, 4414, 4811, 6588, 4415, 3274, 3559], "orig_top_k_doc_id": [6747, 6751, 6748, 6750, 6749, 4124, 4415, 3274, 6588, 495, 3559, 624, 4414, 4811, 3102]}, {"qid": 4260, "question": "What dataset did they use? in Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "answer": ["Workshop on Statistical Machine Translation (WMT) data, script of the drama, \u201cFriends,\", English bible data", "WMT'14, English bible corpus, Drama corpus, and main character corpora"], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 624, 495, 3102, 4124, 4414, 4811, 6588, 6943, 7227, 1034], "orig_top_k_doc_id": [6747, 6751, 6750, 6748, 6749, 4124, 624, 495, 4414, 3102, 6943, 7227, 4811, 6588, 1034]}, {"qid": 4062, "question": "What is a personalized language model? in Efficient Dynamic WFST Decoding for Personalized Language Models", "answer": ["A model that contains the expected user-specific entities.", "language model which contains user-specific entities", " contains the expected user-specific entities"], "top_k_doc_id": [6747, 6748, 6749, 6750, 6751, 6515, 6518, 6516, 6517, 4124, 495, 5506, 381, 5508, 4125], "orig_top_k_doc_id": [6515, 6518, 6516, 6517, 4124, 495, 6751, 5506, 6747, 6749, 381, 5508, 4125, 6750, 6748]}]}
{"group_id": 250, "group_size": 7, "items": [{"qid": 4339, "question": "What is the performance for the three languages tested? in A survey of cross-lingual features for zero-shot cross-lingual semantic parsing", "answer": ["Best authors achieved (different models) in terms of F1 score is:\nGerman - 0.6446\nItalian - 0.6999\nDutch - 0.6057", "Max-F Scores for German .6446, Italian .6999. Dutch .6057 compared to 0.8748 for English"], "top_k_doc_id": [247, 5621, 6854, 249, 6853, 4571, 5624, 4752, 6034, 4030, 6063, 5872, 2806, 6060, 6036], "orig_top_k_doc_id": [6854, 6853, 247, 4752, 249, 5621, 5624, 6034, 6063, 4030, 5872, 2806, 4571, 6060, 6036]}, {"qid": 4341, "question": "Do they evaluate any non-zero-shot parsers on the three languages? in A survey of cross-lingual features for zero-shot cross-lingual semantic parsing", "answer": ["No", "No"], "top_k_doc_id": [247, 5621, 6854, 249, 6853, 4571, 5624, 4752, 6034, 4030, 6063, 6870, 6062, 4031, 4568], "orig_top_k_doc_id": [6854, 6853, 247, 249, 5621, 5624, 6063, 4752, 4571, 4030, 6870, 6034, 6062, 4031, 4568]}, {"qid": 4343, "question": "What is the source of the crosslingual word embeddings? in A survey of cross-lingual features for zero-shot cross-lingual semantic parsing", "answer": ["MUSE BIBREF17", "MUSE BIBREF17"], "top_k_doc_id": [247, 5621, 6854, 249, 6853, 4571, 5624, 4752, 6034, 786, 5622, 4569, 4572, 6060, 4568], "orig_top_k_doc_id": [6854, 6853, 247, 249, 4752, 786, 6034, 5622, 5621, 4571, 4569, 5624, 4572, 6060, 4568]}, {"qid": 4338, "question": "How many lexical features are considered? in A survey of cross-lingual features for zero-shot cross-lingual semantic parsing", "answer": ["No", "3: In addition to word embedding, there is a POS tag embedding and a dependcy relation embedding. ", "No"], "top_k_doc_id": [247, 5621, 6854, 249, 6853, 2806, 2811, 4569, 4752, 5868, 2812, 6870, 5713, 4030, 6034], "orig_top_k_doc_id": [6854, 247, 6853, 249, 5621, 4752, 2806, 2811, 5868, 2812, 6870, 5713, 4030, 4569, 6034]}, {"qid": 4340, "question": "How many Universal Dependency features are considered? in A survey of cross-lingual features for zero-shot cross-lingual semantic parsing", "answer": ["No", "No"], "top_k_doc_id": [247, 5621, 6854, 249, 6853, 2806, 2811, 4569, 4752, 5622, 6852, 6060, 5624, 222, 5623], "orig_top_k_doc_id": [6854, 6853, 247, 2806, 5621, 249, 4569, 5622, 6852, 6060, 5624, 4752, 222, 5623, 2811]}, {"qid": 4342, "question": "How big is the Parallel Meaning Bank? in A survey of cross-lingual features for zero-shot cross-lingual semantic parsing", "answer": ["4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences", "6794 sentences"], "top_k_doc_id": [247, 5621, 6854, 249, 6853, 4571, 5624, 4030, 6852, 2811, 4570, 4569, 4568, 6031, 4572], "orig_top_k_doc_id": [247, 6854, 249, 4030, 5621, 4571, 6852, 6853, 5624, 2811, 4570, 4569, 4568, 6031, 4572]}, {"qid": 4709, "question": "What architecture is used in the encoder? in Improving Zero-shot Translation with Language-Independent Constraints", "answer": ["No", "Transformer"], "top_k_doc_id": [247, 5621, 6854, 7339, 7340, 4571, 6034, 7341, 4027, 4028, 4693, 1885, 786, 1639, 5624], "orig_top_k_doc_id": [7339, 7340, 6854, 4571, 6034, 5621, 7341, 4027, 247, 4028, 4693, 1885, 786, 1639, 5624]}]}
{"group_id": 251, "group_size": 7, "items": [{"qid": 4353, "question": "Are all generated examples semantics-preserving perturbations to the original text? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": ["While the models aim to generate examples which preserve the semantics of the text with minimal perturbations, the Random model randomly replaces a character, which may not preserve the semantics. ", "No"], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 6867, 6868, 4199, 1894, 6169, 6865, 4200, 4207, 3566], "orig_top_k_doc_id": [6864, 6863, 6868, 4202, 5558, 1894, 6169, 4203, 4204, 6865, 6867, 4200, 4199, 3566, 4207]}, {"qid": 4357, "question": "Do they use already trained model on some task in their reinforcement learning approach? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": ["Yes", "Yes"], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 6867, 6868, 4199, 1894, 6169, 6865, 4200, 4207, 5561], "orig_top_k_doc_id": [6863, 6864, 5558, 6867, 6868, 4204, 5561, 4203, 4202, 6865, 1894, 4199, 6169, 4207, 4200]}, {"qid": 4354, "question": "What is success rate of fooling tested models in experiments? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": ["Authors best attacking model resulted in dip in the accuracy of CNN-Word (IMDB) by 79.43% and CNN-Char (AG's News) model by 72.16%", "Our complete AEG model outperforms all the other models with significant drop in accuracy. For the CNN-Word, DeepWordBug decreases the accuracy from 89.95% to 28.13% while AEG model further reduces it to 18.5%."], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 6867, 6868, 4199, 5559, 5560, 5561, 4207, 1894, 3566], "orig_top_k_doc_id": [6863, 5558, 5561, 4199, 6868, 6864, 6867, 4207, 1894, 4203, 5559, 4202, 5560, 4204, 3566]}, {"qid": 4356, "question": "What models are able to be fooled for IMDB sentiment classification task by this approach? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": ["A word-based convolutional neural network (CNN-Word)", "word-based convolutional model (CNN-Word)"], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 6867, 6868, 4199, 5559, 5560, 5561, 5562, 3562, 7473], "orig_top_k_doc_id": [6863, 5558, 6864, 5561, 6867, 6868, 5559, 5560, 4203, 5562, 4199, 4202, 4204, 3562, 7473]}, {"qid": 4358, "question": "How does proposed reinforcement learning based approach generate adversarial examples in black-box settings? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": ["Training ::: Training with Reinforcement learning\nWe fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.\n\nTraining ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)\nIn SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\prime }$ sampled from the model's distribution $p(y^{\\prime }_j|y^{\\prime }_{", "able to generate both character and word level perturbations as necessary, modifying the standard decoder BIBREF29, BIBREF30 to have two-level decoder GRUs: word-GRU and character-GRU"], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 6867, 6868, 4199, 1894, 6169, 6865, 4206, 3566, 5561], "orig_top_k_doc_id": [6863, 6864, 5558, 4203, 4202, 1894, 4204, 4199, 4206, 3566, 6865, 6867, 5561, 6169, 6868]}, {"qid": 4355, "question": "What models are able to be fooled for AG's news corpus news categorization task by this approach? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": [" character-based convolutional model (CNN-Char)", "A word-based convolutional model (CNN-Word) and a character-based convolutional model (CNN-Char)"], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 6867, 6868, 3195, 5561, 3449, 6865, 3562, 203, 1894], "orig_top_k_doc_id": [6863, 6867, 6868, 6864, 5558, 3195, 5561, 4203, 4202, 4204, 3449, 6865, 3562, 203, 1894]}, {"qid": 4352, "question": "Do they manually check all adversarial examples that fooled some model for potential valid examples? in Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model", "answer": ["No", "Only 100 successfully adversarial examples were manually checked, not all of them."], "top_k_doc_id": [4202, 4203, 4204, 5558, 6863, 6864, 5561, 4199, 3563, 6169, 1894, 4206, 3562, 1893, 4205], "orig_top_k_doc_id": [6863, 6864, 5558, 5561, 4204, 4199, 3563, 6169, 1894, 4202, 4206, 4203, 3562, 1893, 4205]}]}
{"group_id": 252, "group_size": 7, "items": [{"qid": 4360, "question": "What languages do they experiment with? in How multilingual is Multilingual BERT?", "answer": ["Dutch, Spanish, English, German", "Answer with content missing: (subscripts 2 and 3)\nNER task: Arabic, Bengali, Czech, German, English, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Turkish, and Chinese.\nPOS task: Arabic, Bulgarian, Catalan, Czech, Danish, German, Greek, English, Spanish, Estonian, Basque, Persian, Finnish, French, Galician, Hebrew, Hindi, Croatian, Hungarian, Indonesian, Italian, Japanese, Korean, Latvian, Marathi, Dutch, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, Tamil, Telugu, Turkish, Urdu, and Chinese."], "top_k_doc_id": [4510, 6870, 5711, 5716, 6871, 247, 50, 5712, 3296, 5710, 3621, 4614, 1777, 2907, 2330], "orig_top_k_doc_id": [4510, 6870, 50, 6871, 5712, 3296, 5710, 3621, 247, 4614, 5711, 1777, 5716, 2907, 2330]}, {"qid": 4361, "question": "What language pairs are affected? in How multilingual is Multilingual BERT?", "answer": ["Language pairs that are typologically different", "No"], "top_k_doc_id": [4510, 6870, 5711, 5716, 6871, 247, 1639, 7339, 5715, 3141, 5702, 6208, 6872, 1789, 7290], "orig_top_k_doc_id": [5716, 6870, 4510, 6871, 1639, 7339, 5715, 3141, 247, 5711, 5702, 6208, 6872, 1789, 7290]}, {"qid": 4362, "question": "What evaluation metrics are used? in How multilingual is Multilingual BERT?", "answer": ["ner F1 score, pos zero-shot accuracy", "accuracy"], "top_k_doc_id": [4510, 6870, 50, 3010, 4289, 4668, 7285, 1886, 4290, 1773, 247, 5716, 6038, 6039, 4614], "orig_top_k_doc_id": [6870, 4510, 3010, 1886, 4668, 4290, 1773, 7285, 247, 50, 4289, 5716, 6038, 6039, 4614]}, {"qid": 4363, "question": "What datasets did they use? in How multilingual is Multilingual BERT?", "answer": ["CoNLL-2002 and -2003 , Universal Dependencies, WMT16 ", "CoNLL-2002 and -2003 sets, an in-house dataset with 16 languages, Universal Dependencies (UD) BIBREF7"], "top_k_doc_id": [4510, 6870, 5711, 5716, 6871, 5702, 534, 4668, 5570, 6312, 2906, 1149, 3130, 50, 3010], "orig_top_k_doc_id": [6870, 4510, 5702, 5716, 534, 4668, 5570, 5711, 6312, 6871, 2906, 1149, 3130, 50, 3010]}, {"qid": 4666, "question": "By how much did the new model outperform multilingual BERT? in Multilingual is not enough: BERT for Finnish", "answer": ["For POS,  improvements for cased BERT are 1.26 2.52  0.5 for TDT,  FTB and PUD datasets respectively.\nFor NER in-domain test set, improvement is  2.11 F1 and for NER out-of-domain test set, improvement is  5.32 F1.\nFor Dependency parsing, improvements are in range from 3.35 to 6.64 LAS for cased BERT.", "absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points, LAS results are 2.3\u20133.6% points above the previous state of the art, absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples"], "top_k_doc_id": [4510, 6870, 50, 3010, 4289, 4668, 7285, 7290, 7291, 7289, 5713, 3621, 5712, 533, 5572], "orig_top_k_doc_id": [7290, 7285, 7291, 7289, 5713, 3621, 4510, 5712, 50, 4289, 6870, 533, 5572, 3010, 4668]}, {"qid": 2515, "question": "What is the multilingual baseline? in Small and Practical BERT Models for Sequence Labeling", "answer": [" the Meta-LSTM BIBREF0"], "top_k_doc_id": [4510, 436, 3621, 4290, 4333, 4289, 3618, 1782, 1320, 6036, 439, 1991, 1149, 86, 438], "orig_top_k_doc_id": [4510, 4289, 436, 3621, 3618, 4290, 1782, 1320, 4333, 6036, 439, 1991, 1149, 86, 438]}, {"qid": 4359, "question": "Which languages with different script do they look at? in How multilingual is Multilingual BERT?", "answer": ["Urdu, Hindi, English, Japanese, Bulgarian", "Urdu, Hindi, English, Japanese, Bulgarian"], "top_k_doc_id": [4510, 436, 3621, 4290, 4333, 6871, 6870, 2906, 2907, 6190, 5715, 1773, 6038, 4618, 5712], "orig_top_k_doc_id": [6871, 6870, 2906, 4510, 4290, 4333, 2907, 6190, 5715, 1773, 6038, 4618, 436, 3621, 5712]}]}
{"group_id": 253, "group_size": 7, "items": [{"qid": 4426, "question": "By how much they outperform the baseline? in Bayesian Models for Unit Discovery on a Very Low Resource Language", "answer": ["18.08 percent points on F-score", "No"], "top_k_doc_id": [6490, 1061, 5822, 6960, 6961, 2948, 6959, 1064, 2712, 7066, 7852, 5516, 3750, 7045, 502], "orig_top_k_doc_id": [6961, 5822, 6960, 1064, 6959, 7066, 6490, 2948, 7852, 5516, 1061, 2712, 3750, 7045, 502]}, {"qid": 4427, "question": "How long are the datasets? in Bayesian Models for Unit Discovery on a Very Low Resource Language", "answer": ["5130", "5130 Mboshi speech utterances"], "top_k_doc_id": [6490, 1061, 5822, 6960, 6961, 2948, 6959, 1064, 2712, 7066, 2713, 4787, 1937, 231, 2189], "orig_top_k_doc_id": [6961, 5822, 6959, 6960, 7066, 2948, 1061, 2712, 2713, 1064, 4787, 6490, 1937, 231, 2189]}, {"qid": 4428, "question": "What bayesian model is trained? in Bayesian Models for Unit Discovery on a Very Low Resource Language", "answer": ["Structured Variational AutoEncoder (SVAE) AUD, Bayesian Hidden Markov Model (HMM)", "non-parametric Bayesian Hidden Markov Model"], "top_k_doc_id": [6490, 1061, 5822, 6960, 6961, 2948, 6959, 1937, 602, 5360, 6758, 5357, 6666, 7232, 2189], "orig_top_k_doc_id": [6961, 6959, 5822, 6960, 1937, 602, 2948, 6490, 5360, 6758, 5357, 1061, 6666, 7232, 2189]}, {"qid": 4429, "question": "What low resource languages are considered? in Bayesian Models for Unit Discovery on a Very Low Resource Language", "answer": ["Mboshi ", "Mboshi (Bantu C25)"], "top_k_doc_id": [6490, 1061, 5822, 6960, 6961, 4615, 7066, 6943, 5566, 6491, 6310, 3126, 4592, 4590, 4568], "orig_top_k_doc_id": [6961, 5822, 4615, 1061, 6490, 7066, 6943, 6960, 5566, 6491, 6310, 3126, 4592, 4590, 4568]}, {"qid": 1968, "question": "What dataset is used? in Learning to Discover, Ground and Use Words with Segmental Neural Language Models", "answer": ["Brent corpus, PTB , Beijing University Corpus, Penn Chinese Treebank"], "top_k_doc_id": [6490, 373, 374, 2946, 2948, 3941, 6943, 7100, 5822, 375, 1822, 1175, 690, 2315, 3795], "orig_top_k_doc_id": [2948, 6490, 2946, 374, 3941, 373, 7100, 5822, 375, 6943, 1822, 1175, 690, 2315, 3795]}, {"qid": 1969, "question": "What language do they look at? in Learning to Discover, Ground and Use Words with Segmental Neural Language Models", "answer": ["English, Chinese"], "top_k_doc_id": [6490, 373, 374, 2946, 2948, 3941, 6943, 1577, 6190, 6489, 4184, 2730, 6882, 5908, 7502], "orig_top_k_doc_id": [2948, 2946, 6490, 1577, 6943, 6190, 6489, 373, 4184, 2730, 3941, 374, 6882, 5908, 7502]}, {"qid": 3528, "question": "How is the vocabulary of word-like or phoneme-like units automatically discovered? in Topic Identification for Speech without ASR", "answer": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "top_k_doc_id": [6490, 1061, 5822, 4875, 5823, 661, 4972, 6485, 1889, 1268, 2451, 2452, 6489, 4863, 1336], "orig_top_k_doc_id": [5822, 4875, 5823, 661, 4972, 6485, 1889, 6490, 1268, 2451, 1061, 2452, 6489, 4863, 1336]}]}
{"group_id": 254, "group_size": 7, "items": [{"qid": 4439, "question": "what language pairs are explored? in Phrase Table as Recommendation Memory for Neural Machine Translation", "answer": ["Chinese-English, English-Japanese", "Chinese-English , English-Japanese"], "top_k_doc_id": [6975, 6977, 6978, 6979, 3686, 6976, 2053, 2135, 3102, 564, 5029, 6597, 7344, 1185, 302], "orig_top_k_doc_id": [6975, 6979, 6976, 6977, 6978, 564, 2053, 3102, 5029, 2135, 6597, 3686, 7344, 1185, 302]}, {"qid": 4440, "question": "what datasets did they use? in Phrase Table as Recommendation Memory for Neural Machine Translation", "answer": ["NIST 2003 (MT03), NIST2004-2006 (MT04-06), NIST 2008 (MT08), KFTT ", "NIST 2003, NIST2004-2006, NIST 2008, KFTT"], "top_k_doc_id": [6975, 6977, 6978, 6979, 3686, 6976, 2053, 2135, 3102, 3416, 1124, 2761, 510, 6879, 3019], "orig_top_k_doc_id": [6975, 6979, 6977, 6976, 6978, 2135, 3416, 2053, 3686, 1124, 3102, 2761, 510, 6879, 3019]}, {"qid": 4438, "question": "what were the evaluation metrics? in Phrase Table as Recommendation Memory for Neural Machine Translation", "answer": ["BLEU ", "BLEU"], "top_k_doc_id": [6975, 6977, 6978, 6979, 3686, 6976, 2763, 4389, 4849, 566, 3020, 6600, 2053, 7352, 2375], "orig_top_k_doc_id": [6975, 6979, 6978, 6977, 4849, 566, 6976, 3020, 3686, 6600, 2763, 4389, 2053, 7352, 2375]}, {"qid": 4441, "question": "which attention based nmt method did they compare with? in Phrase Table as Recommendation Memory for Neural Machine Translation", "answer": ["attention-based NMT system BIBREF23 , BIBREF24", " BIBREF23 , BIBREF24"], "top_k_doc_id": [6975, 6977, 6978, 6979, 3686, 6976, 2053, 3920, 774, 2761, 4387, 2760, 4388, 3817, 6943], "orig_top_k_doc_id": [6975, 6979, 6977, 6978, 3686, 3920, 774, 6976, 2761, 4387, 2760, 2053, 4388, 3817, 6943]}, {"qid": 4442, "question": "by how much did their system improve? in Phrase Table as Recommendation Memory for Neural Machine Translation", "answer": ["The average improvement for CH-EN is 3.99 BLEU points, for EN-JA it is 3.59 BLEU points.", "In CH-EN translation, the average improvement is up to 2.23 BLEU points, in EN-JA translation, the improvement can reach 1.96 BLEU point."], "top_k_doc_id": [6975, 6977, 6978, 6979, 3686, 6976, 2763, 4389, 3102, 2760, 7847, 2762, 7581, 4692, 4312], "orig_top_k_doc_id": [6975, 6979, 6978, 6977, 6976, 3102, 3686, 2760, 2763, 7847, 2762, 7581, 4692, 4312, 4389]}, {"qid": 636, "question": "What datasets were used? in Incorporating Discrete Translation Lexicons into Neural Machine Translation", "answer": ["KFTT BIBREF12 and BTEC BIBREF13"], "top_k_doc_id": [6975, 6977, 6978, 6979, 774, 777, 778, 2238, 5105, 5106, 5696, 775, 2705, 1768, 5107], "orig_top_k_doc_id": [778, 5105, 774, 5106, 6977, 5696, 6975, 6979, 775, 2705, 2238, 777, 1768, 6978, 5107]}, {"qid": 637, "question": "What language pairs did they experiment with? in Incorporating Discrete Translation Lexicons into Neural Machine Translation", "answer": ["English-Japanese"], "top_k_doc_id": [6975, 6977, 6978, 6979, 774, 777, 778, 2238, 5105, 5106, 7158, 6856, 6855, 3817, 5026], "orig_top_k_doc_id": [778, 6979, 774, 6977, 6978, 6975, 5105, 2238, 5106, 7158, 6856, 6855, 3817, 5026, 777]}]}
{"group_id": 255, "group_size": 7, "items": [{"qid": 4476, "question": "What were the performance results of their network? in Unfolding and Shrinking Neural Machine Translation Ensembles", "answer": ["For the test set a BLEU score of 25.7 on Ja-En and 20.7 (2014 test set), 23.1 (2015 test set), and 26.1 (2016 test set) on En-De", "gain of 2.2 BLEU compared to the original single NMT network"], "top_k_doc_id": [4309, 1044, 4953, 7011, 7012, 7014, 7015, 5698, 7013, 7260, 4841, 6992, 7785, 5697, 4766], "orig_top_k_doc_id": [7011, 7015, 7014, 7012, 4309, 5698, 7260, 7785, 7013, 4953, 5697, 4841, 6992, 4766, 1044]}, {"qid": 4478, "question": "What dataset is used? in Unfolding and Shrinking Neural Machine Translation Ensembles", "answer": [" Japanese-English (Ja-En) ASPEC data set BIBREF26, WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. ", "Japanese-English (Ja-En) ASPEC data set BIBREF26, WMT data set for English-German (En-De), news-test2014, news-test2015 and news-test2016"], "top_k_doc_id": [4309, 1044, 4953, 7011, 7012, 7014, 7015, 5698, 7013, 7260, 4841, 6992, 5201, 6993, 6184], "orig_top_k_doc_id": [7011, 7015, 7014, 7012, 4953, 4309, 6992, 1044, 7260, 5698, 5201, 4841, 6993, 7013, 6184]}, {"qid": 1067, "question": "Which algorithm is used in the UDS-DFKI system? in UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task", "answer": ["Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. "], "top_k_doc_id": [4309, 397, 2618, 2619, 6267, 1410, 1412, 1458, 3542, 3543, 5956, 7437, 398, 1839, 6918], "orig_top_k_doc_id": [1412, 1410, 3542, 3543, 2618, 2619, 6267, 5956, 397, 7437, 1458, 398, 4309, 1839, 6918]}, {"qid": 1068, "question": "Does the use of out-of-domain data improve the performance of the method? in UDS--DFKI Submission to the WMT2019 Similar Language Translation Shared Task", "answer": ["No"], "top_k_doc_id": [4309, 397, 2618, 2619, 6267, 1410, 1412, 1458, 3542, 2180, 2073, 6920, 4568, 3604, 1456], "orig_top_k_doc_id": [1412, 1410, 6267, 1458, 2180, 2073, 397, 4309, 2619, 6920, 4568, 2618, 3542, 3604, 1456]}, {"qid": 4477, "question": "What were the baselines? in Unfolding and Shrinking Neural Machine Translation Ensembles", "answer": ["simple ensembling method (prediction averaging)", "a widely used, simple ensembling method (prediction averaging) "], "top_k_doc_id": [4309, 1044, 4953, 7011, 7012, 7014, 7015, 5698, 7013, 7260, 6391, 7669, 1045, 3416, 4907], "orig_top_k_doc_id": [7011, 7015, 7014, 7012, 7260, 1044, 4953, 5698, 6391, 7669, 1045, 4309, 3416, 7013, 4907]}, {"qid": 2521, "question": "What is WNGT 2019 shared task? in Efficiency through Auto-Sizing: Notre Dame NLP's Submission to the WNGT 2019 Efficiency Task", "answer": ["efficiency task aimed  at reducing the number of parameters while minimizing drop in performance"], "top_k_doc_id": [4309, 397, 2618, 2619, 6267, 4310, 4311, 2887, 6265, 2717, 1788, 2003, 2544, 2714, 2042], "orig_top_k_doc_id": [4309, 4310, 4311, 2887, 6267, 2618, 2619, 6265, 2717, 1788, 2003, 2544, 2714, 397, 2042]}, {"qid": 4479, "question": "Do they explore other language pairs? in Unfolding and Shrinking Neural Machine Translation Ensembles", "answer": ["Yes", "English-German (En-De)"], "top_k_doc_id": [4309, 1044, 4953, 7011, 7012, 7014, 7015, 2135, 4841, 2136, 6389, 6265, 6190, 7429, 2356], "orig_top_k_doc_id": [7011, 7015, 7014, 7012, 4953, 2135, 1044, 4309, 4841, 2136, 6389, 6265, 6190, 7429, 2356]}]}
{"group_id": 256, "group_size": 7, "items": [{"qid": 4515, "question": "Which of the 12 languages showed the strongest tendency towards male defaults? in Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate", "answer": ["Malay", "in general, exception of the Basque idiom"], "top_k_doc_id": [7064, 7267, 377, 7060, 7061, 7062, 7063, 7065, 379, 7059, 7266, 7268, 5153, 7269, 5764], "orig_top_k_doc_id": [7064, 7063, 7065, 7062, 7060, 7061, 7059, 379, 7267, 377, 5153, 7268, 7266, 7269, 5764]}, {"qid": 4516, "question": "How many different sentence constructions are translated in gender neutral languages? in Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate", "answer": ["17", "No"], "top_k_doc_id": [7064, 7267, 377, 7060, 7061, 7062, 7063, 7065, 379, 7059, 7266, 7268, 378, 1350, 5773], "orig_top_k_doc_id": [7060, 7064, 7065, 7063, 7062, 7061, 379, 378, 377, 7268, 7267, 1350, 7266, 5773, 7059]}, {"qid": 4514, "question": "Do the authors examine the real-world distribution of female workers in the country/countries where the gender neutral languages are spoken? in Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate", "answer": ["No", "No"], "top_k_doc_id": [7064, 7267, 377, 7060, 7061, 7062, 7063, 7065, 718, 717, 378, 3355, 1443, 5153, 3008], "orig_top_k_doc_id": [7065, 7063, 7060, 7064, 7061, 7062, 718, 717, 378, 7267, 377, 3355, 1443, 5153, 3008]}, {"qid": 4650, "question": "What three languages are used in the translation experiments? in Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem", "answer": ["German, Spanish, Hebrew", "German, Spanish, Hebrew"], "top_k_doc_id": [7064, 7267, 7266, 7268, 7269, 7271, 379, 7063, 7060, 5026, 2056, 5841, 7065, 5455, 4951], "orig_top_k_doc_id": [7266, 7268, 7267, 7271, 7064, 7269, 7063, 7060, 5026, 2056, 5841, 7065, 379, 5455, 4951]}, {"qid": 4652, "question": "How is the set of trusted, gender-balanced examples selected? in Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem", "answer": [" create a tiny, handcrafted profession-based dataset", "They select professions from the list collected by BIBREF4 from US labour statistics and manually translate masculine and feminine examples"], "top_k_doc_id": [7064, 7267, 7266, 7268, 7269, 7271, 379, 7063, 5155, 1444, 7270, 5768, 6700, 2723, 6272], "orig_top_k_doc_id": [7266, 7268, 7267, 7271, 7064, 7269, 5155, 7063, 1444, 7270, 379, 5768, 6700, 2723, 6272]}, {"qid": 570, "question": "Which model do they use to convert between masculine-inflected and feminine-inflected sentences? in Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology", "answer": ["Markov random field with an optional neural parameterization"], "top_k_doc_id": [7064, 7267, 377, 697, 7268, 694, 379, 378, 7269, 7266, 655, 221, 5773, 6737, 696], "orig_top_k_doc_id": [697, 7268, 694, 7267, 379, 378, 7269, 7266, 655, 221, 5773, 7064, 6737, 377, 696]}, {"qid": 4649, "question": "How does lattice rescoring improve inference? in Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem", "answer": ["By transducing initial hypotheses produced by the biased baseline system to create gender-inflected search spaces which can\nbe rescored by the adapted model", "initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored"], "top_k_doc_id": [7064, 7267, 7266, 7268, 7269, 7271, 7270, 6780, 6781, 3705, 3708, 3709, 6943, 2311, 2056], "orig_top_k_doc_id": [7266, 7271, 7269, 7268, 7270, 7267, 6780, 6781, 3705, 7064, 3708, 3709, 6943, 2311, 2056]}]}
{"group_id": 257, "group_size": 7, "items": [{"qid": 4614, "question": "What was their best performing model? in A Focus on Neural Machine Translation for African Languages", "answer": ["Transformer", "Transformer"], "top_k_doc_id": [5201, 7190, 7669, 3641, 7191, 4766, 7193, 4615, 5983, 1244, 5841, 5599, 4526, 3547, 661], "orig_top_k_doc_id": [7669, 7190, 3641, 7193, 5201, 7191, 4766, 5983, 1244, 4615, 5841, 5599, 4526, 3547, 661]}, {"qid": 4615, "question": "What datasets did they use? in A Focus on Neural Machine Translation for African Languages", "answer": ["English to Afrikaans, isiZulu, N. Sotho,\nSetswana, and Xitsonga parallel corpora from the Autshumato project", "Autshumato"], "top_k_doc_id": [5201, 7190, 7669, 3641, 7191, 4766, 7193, 4615, 5983, 6190, 3416, 2917, 2775, 5010, 6595], "orig_top_k_doc_id": [7190, 7669, 3641, 5201, 7193, 7191, 6190, 3416, 4766, 2917, 2775, 5010, 5983, 4615, 6595]}, {"qid": 4612, "question": "What evaluation metrics did they use? in A Focus on Neural Machine Translation for African Languages", "answer": ["BLEU", "BLEU"], "top_k_doc_id": [5201, 7190, 7669, 3641, 7191, 4766, 7193, 4615, 6190, 111, 1122, 1244, 6590, 3181, 3604], "orig_top_k_doc_id": [3641, 7669, 7190, 5201, 6190, 7193, 111, 1122, 4615, 7191, 1244, 4766, 6590, 3181, 3604]}, {"qid": 4613, "question": "What NMT techniques did they explore? in A Focus on Neural Machine Translation for African Languages", "answer": ["ConvS2S, Transformer", "ConvS2S, Transformer"], "top_k_doc_id": [5201, 7190, 7669, 3641, 7191, 4766, 7193, 6190, 5983, 6035, 7339, 4184, 5026, 5455, 6943], "orig_top_k_doc_id": [7190, 7669, 3641, 7193, 6190, 5983, 6035, 7191, 4766, 7339, 4184, 5026, 5455, 6943, 5201]}, {"qid": 2294, "question": "Which languages do they focus on? in Masakhane -- Machine Translation For Africa", "answer": ["No"], "top_k_doc_id": [5201, 7190, 7669, 3641, 7191, 2823, 7670, 7042, 1347, 6837, 986, 1040, 6836, 5203, 5977], "orig_top_k_doc_id": [3641, 7190, 7669, 5201, 2823, 7670, 7042, 1347, 6837, 986, 7191, 1040, 6836, 5203, 5977]}, {"qid": 3054, "question": "Which datasets are employed for South African languages LID? in Short Text Language Identification for Under Resourced Languages", "answer": ["DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora"], "top_k_doc_id": [5201, 7190, 7669, 45, 51, 3244, 3401, 3402, 4004, 5202, 5203, 6834, 7193, 986, 6836], "orig_top_k_doc_id": [5201, 7190, 5202, 3402, 7193, 4004, 3401, 3244, 7669, 5203, 6834, 986, 51, 45, 6836]}, {"qid": 3055, "question": "Does the paper report the performance of a baseline model on South African languages LID? in Short Text Language Identification for Under Resourced Languages", "answer": ["Yes", "Yes"], "top_k_doc_id": [5201, 7190, 7669, 45, 51, 3244, 3401, 3402, 4004, 5202, 5203, 6834, 7193, 2292, 3406], "orig_top_k_doc_id": [5201, 7190, 5202, 7193, 3401, 3402, 4004, 45, 7669, 3244, 6834, 51, 5203, 2292, 3406]}]}
{"group_id": 258, "group_size": 7, "items": [{"qid": 4659, "question": "What methods to they compare to? in Shallow Discourse Parsing with Maximum Entropy Model", "answer": ["(1) Baseline_1, which applies the probability information, (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model", " Baseline_1, which applies the probability information, Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature"], "top_k_doc_id": [7762, 7763, 7277, 4296, 7764, 7765, 4781, 6471, 7023, 7279, 4345, 2422, 1284, 4780, 7479], "orig_top_k_doc_id": [7277, 7763, 7762, 7764, 6471, 7279, 7023, 4296, 4345, 2422, 4781, 1284, 4780, 7479, 7765]}, {"qid": 4660, "question": "Which dataset to they train and evaluate on? in Shallow Discourse Parsing with Maximum Entropy Model", "answer": ["PDTB as training set, Section 22 as testing set", "Penn Discourse Treebank"], "top_k_doc_id": [7762, 7763, 7277, 4296, 7764, 7765, 4781, 6471, 7023, 7279, 1600, 7278, 5184, 1392, 1743], "orig_top_k_doc_id": [7277, 7762, 7763, 7765, 7764, 1600, 7278, 7279, 7023, 5184, 6471, 4781, 1392, 4296, 1743]}, {"qid": 4657, "question": "Do they manage to consistenly outperform the best performing methods? in Shallow Discourse Parsing with Maximum Entropy Model", "answer": ["Yes", "No"], "top_k_doc_id": [7762, 7763, 7277, 4296, 7764, 7765, 2424, 896, 6430, 4825, 1392, 6853, 2321, 7755, 7723], "orig_top_k_doc_id": [7277, 7763, 7764, 7765, 4296, 2424, 896, 7762, 6430, 4825, 1392, 6853, 2321, 7755, 7723]}, {"qid": 4987, "question": "Is proposed approach compared to some baselines? in Improved Document Modelling with a Neural Discourse Parser", "answer": ["Yes", "Yes"], "top_k_doc_id": [7762, 7763, 4380, 4635, 4781, 7761, 7765, 4826, 6472, 7277, 7279, 4636, 6473, 5613, 7023], "orig_top_k_doc_id": [7761, 7762, 4380, 4781, 7763, 7279, 4635, 4636, 6473, 7277, 4826, 6472, 7765, 5613, 7023]}, {"qid": 4988, "question": "What datasets are used for this tasks? in Improved Document Modelling with a Neural Discourse Parser", "answer": ["CNN/DailyMail corpus, US Petition dataset", " US Petition dataset, CNN/DailyMail corpus, 385 documents from the Wall Street Journal"], "top_k_doc_id": [7762, 7763, 4380, 4635, 4781, 7761, 7765, 4826, 6472, 7277, 6852, 4780, 2038, 460, 2873], "orig_top_k_doc_id": [7761, 4781, 7762, 7763, 6472, 6852, 4826, 7765, 4635, 4780, 4380, 2038, 460, 7277, 2873]}, {"qid": 4658, "question": "Do they try to use other models aside from Maximum Entropy? in Shallow Discourse Parsing with Maximum Entropy Model", "answer": ["No", "No"], "top_k_doc_id": [7762, 7763, 7277, 7279, 7278, 1600, 1225, 4047, 7755, 5567, 897, 5206, 4345, 7342, 1284], "orig_top_k_doc_id": [7277, 7279, 7763, 7278, 1600, 1225, 4047, 7755, 5567, 897, 5206, 4345, 7342, 7762, 1284]}, {"qid": 4989, "question": "How big are improvements on these tasks? in Improved Document Modelling with a Neural Discourse Parser", "answer": ["No", "Observing that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20),  We are able to reproduce the performance of the baseline model (\u201cCNN w/ GloVe\u201d), and find that once again, adding the shallow discourse features improves results."], "top_k_doc_id": [7762, 7763, 4380, 4635, 4781, 7761, 7765, 2306, 6852, 6294, 5673, 7023, 6053, 6473, 7027], "orig_top_k_doc_id": [7761, 7763, 7762, 4781, 7765, 2306, 6852, 4380, 6294, 5673, 4635, 7023, 6053, 6473, 7027]}]}
{"group_id": 259, "group_size": 7, "items": [{"qid": 4670, "question": "How long is the dataset? in A Surrogate-based Generic Classifier for Chinese TV Series Reviews", "answer": ["Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series.", "No"], "top_k_doc_id": [7292, 6640, 7293, 7294, 7295, 7296, 7678, 7679, 2968, 2861, 6386, 7241, 4122, 5105, 5106], "orig_top_k_doc_id": [7296, 7293, 7292, 7294, 7295, 7678, 7679, 2968, 6386, 7241, 2861, 6640, 4122, 5105, 5106]}, {"qid": 4672, "question": "What are the eight predefined categories? in A Surrogate-based Generic Classifier for Chinese TV Series Reviews", "answer": ["Plot of the TV series, Actor/actress, Role, Dialogue, Analysis, Platform, Thumb up or down, Noise or others", "Eight categories are: Plot of the TV series, Actor/actress actors, Role, Dialogue discussion, Analysis, Platform, Thumb up or down and Noise or others."], "top_k_doc_id": [7292, 6640, 7293, 7294, 7295, 7296, 7678, 7679, 2968, 2861, 7408, 6401, 4156, 6548, 1152], "orig_top_k_doc_id": [7296, 7293, 7295, 7294, 7292, 7408, 7678, 7679, 6401, 2968, 6640, 4156, 6548, 2861, 1152]}, {"qid": 1344, "question": "what was the baseline? in Classifying movie genres by analyzing text reviews", "answer": ["There is no baseline."], "top_k_doc_id": [7292, 1845, 1846, 1847, 2335, 2337, 7293, 7295, 194, 606, 193, 190, 518, 191, 483], "orig_top_k_doc_id": [1845, 1846, 1847, 194, 606, 193, 7293, 2337, 7292, 2335, 190, 7295, 518, 191, 483]}, {"qid": 1346, "question": "what evaluation metrics are discussed? in Classifying movie genres by analyzing text reviews", "answer": ["precision , recall , Hamming loss, micro averaged precision and recall "], "top_k_doc_id": [7292, 1845, 1846, 1847, 2335, 2337, 7293, 7295, 6569, 5090, 5373, 2342, 6570, 3879, 5562], "orig_top_k_doc_id": [1845, 1847, 1846, 7295, 7293, 7292, 6569, 2335, 5090, 5373, 2342, 2337, 6570, 3879, 5562]}, {"qid": 4671, "question": "Is manual annotation performed? in A Surrogate-based Generic Classifier for Chinese TV Series Reviews", "answer": ["Yes", "Yes"], "top_k_doc_id": [7292, 6640, 7293, 7294, 7295, 7296, 7678, 7679, 2968, 6098, 6097, 7241, 309, 6401, 4583], "orig_top_k_doc_id": [7296, 7293, 7292, 7295, 7294, 6098, 6097, 7679, 6640, 7678, 7241, 309, 2968, 6401, 4583]}, {"qid": 1345, "question": "how many movie genres do they explore? in Classifying movie genres by analyzing text reviews", "answer": ["27 "], "top_k_doc_id": [7292, 1845, 1846, 1847, 482, 5900, 483, 1168, 3879, 7399, 1405, 3413, 7008, 6696, 2342], "orig_top_k_doc_id": [1845, 1846, 1847, 482, 5900, 483, 1168, 3879, 7399, 7292, 1405, 3413, 7008, 6696, 2342]}, {"qid": 4669, "question": "How many TV series are considered? in A Surrogate-based Generic Classifier for Chinese TV Series Reviews", "answer": ["3", "Three tv series are considered."], "top_k_doc_id": [7292, 6640, 7293, 7294, 7295, 7296, 7678, 7679, 5881, 3988, 7241, 5793, 1933, 6177, 4156], "orig_top_k_doc_id": [7296, 7292, 7293, 7295, 7294, 7678, 7679, 5881, 3988, 7241, 6640, 5793, 1933, 6177, 4156]}]}
{"group_id": 260, "group_size": 7, "items": [{"qid": 4675, "question": "What baseline method was used? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["LinearSVM, LASSO, Weninger at al. (SVM)", "LinearSVM, LASSO, Weninger et al."], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 4653, 6541, 5564, 7404, 4780, 1112, 3162, 3264], "orig_top_k_doc_id": [7297, 7298, 6540, 3266, 4921, 739, 4780, 4784, 6541, 1112, 5564, 7404, 4653, 3162, 3264]}, {"qid": 4679, "question": "How do the speakers' reputations bias the dataset? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["No", "No"], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 4653, 6541, 5564, 7404, 5061, 5483, 6793, 3161], "orig_top_k_doc_id": [7297, 7298, 6540, 3266, 739, 6541, 4921, 5061, 5564, 4784, 7404, 4653, 5483, 6793, 3161]}, {"qid": 4674, "question": "When the authors say their method largely outperforms the baseline, does this mean that the baseline performed better in some cases? If so, which ones? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["Baseline performed better in \"Fascinating\" and \"Jaw-dropping\" categories.", "Weninger et al. (SVM) model outperforms on the Fascinating category."], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 3161, 4780, 4783, 6970, 737, 3502, 3145, 738], "orig_top_k_doc_id": [7297, 7298, 4921, 6540, 6970, 3266, 3161, 4784, 4780, 4783, 737, 3502, 3145, 738, 739]}, {"qid": 4677, "question": "How was a causal diagram used to carefully remove this bias? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["By confining to transcripts only and normalizing ratings to remove the effects of speaker's reputations, popularity gained by publicity, contemporary hot topics, etc.", "No"], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 3161, 4780, 4783, 951, 3162, 3163, 7404, 4782], "orig_top_k_doc_id": [7297, 7298, 739, 4784, 951, 4780, 6540, 3161, 3266, 4783, 4921, 3162, 3163, 7404, 4782]}, {"qid": 4678, "question": "How does publicity bias the dataset? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["No", "No"], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 4653, 6541, 3264, 5660, 3502, 3161, 4780, 3143], "orig_top_k_doc_id": [7297, 7298, 6540, 3266, 4921, 4784, 739, 3264, 5660, 6541, 4653, 3502, 3161, 4780, 3143]}, {"qid": 4673, "question": "Do they report results only on English data? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["No", "No"], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 4653, 1111, 5660, 5564, 1112, 3264, 6110, 5659], "orig_top_k_doc_id": [7297, 7298, 4921, 6540, 739, 3266, 1111, 5660, 4653, 4784, 5564, 1112, 3264, 6110, 5659]}, {"qid": 4676, "question": "What was the motivation for using a dependency tree based recursive architecture? in A Causality-Guided Prediction of the TED Talk Ratings from the Speech-Transcripts using Neural Networks", "answer": ["No", "It performs better than other models predicting TED talk ratings."], "top_k_doc_id": [739, 3266, 4784, 4921, 6540, 7297, 7298, 3161, 4780, 4774, 6122, 6123, 3162, 1389, 6541], "orig_top_k_doc_id": [7297, 7298, 6540, 3266, 4921, 4774, 739, 6122, 4780, 6123, 3162, 1389, 3161, 4784, 6541]}]}
{"group_id": 261, "group_size": 7, "items": [{"qid": 4698, "question": "What are state of the art results on OSA and PD corpora used for testing? in Pathological speech detection using x-vector embeddings", "answer": ["PD : i-vectors had segment level F1 score 66.6 and for speaker level had 75.6 F1 score\n\nOSA: For the same levels it had F1 scores of 65.5 and 75.0", "State of the art F1 scores are:\nPPD: Seg 66.7, Spk 75.6\nOSA: Seg 73.3, Spk 81.7\nSPD: Seg 79.0, Spk 87.0"], "top_k_doc_id": [7323, 5061, 7325, 7326, 7324, 1372, 1554, 1555, 6026, 7621, 7622, 2720, 629, 2330, 5410], "orig_top_k_doc_id": [7323, 7325, 7324, 7326, 7621, 7622, 5061, 1555, 2720, 1554, 6026, 629, 2330, 5410, 1372]}, {"qid": 4699, "question": "How better does x-vectors perform than knowlege-based features in same-language corpora? in Pathological speech detection using x-vector embeddings", "answer": ["For OSA detection x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by 8%.", "For Portuguese PD corpus, x-vector outperform KB for segment and speaker level  for 2.2 and 2.2 F1 respectively.\nFor Portuguese OSA corpus, x-vector outperform KB for segment and speaker level  for 8.5 and 0.1 F1 respectively.", "Portuguese PD Corpus: for segment level i-vectors had better F1 score comparing to KB by 2.1% and for speaker level by 3.5%\nIn case of Spanish PD corpus, KB had higher F1 scores in terms of Segment level and Speaker level by 3.3% and 2.0%.  "], "top_k_doc_id": [7323, 5061, 7325, 7326, 7324, 1578, 2720, 2722, 7743, 2351, 629, 1301, 484, 5710, 1047], "orig_top_k_doc_id": [7325, 7323, 7326, 2351, 2720, 7324, 629, 7743, 1578, 1301, 2722, 5061, 484, 5710, 1047]}, {"qid": 4700, "question": "What is meant by domain missmatch occuring? in Pathological speech detection using x-vector embeddings", "answer": ["tasks whose domain does not match that of the training data"], "top_k_doc_id": [7323, 5061, 7325, 7326, 7324, 1578, 2720, 2722, 7743, 1329, 7256, 2103, 5292, 3291, 7534], "orig_top_k_doc_id": [5061, 7325, 7326, 7743, 1329, 7323, 7324, 2720, 7256, 2103, 2722, 5292, 1578, 3291, 7534]}, {"qid": 4701, "question": "How big are OSA and PD corporas used for testing? in Pathological speech detection using x-vector embeddings", "answer": ["For Portuguese PD have  for patient 1.24h and for control 1.07 h.\nFor Portuguese OSA have  for patient 1.10h and for control 1.05 h.\nFor Spanish PD have for patient 0.49h  and for control 0.50h.", "15 percent of the corpora is used for testing. OSA contains 60 speakers, 3495 segments and PD 140 speakers and 3365 segments."], "top_k_doc_id": [7323, 5061, 7325, 7326, 7324, 1372, 1554, 1555, 6026, 7621, 7622, 7233, 7798, 2857, 2311], "orig_top_k_doc_id": [7323, 7325, 7324, 7326, 7621, 5061, 7622, 7233, 1372, 6026, 1555, 1554, 7798, 2857, 2311]}, {"qid": 2878, "question": "what is the size of the augmented dataset? in The Effect of Heterogeneous Data for Alzheimer's Disease Detection from Speech", "answer": ["No", "609"], "top_k_doc_id": [7323, 5061, 7325, 7326, 1192, 1193, 4868, 821, 4866, 2063, 4886, 4485, 6365, 5739, 7622], "orig_top_k_doc_id": [1192, 7323, 7325, 7326, 1193, 4868, 5061, 821, 4866, 2063, 4886, 4485, 6365, 5739, 7622]}, {"qid": 4915, "question": "Is dataset balanced in terms of available data per language? in Convolutional Neural Networks and a Transfer Learning Strategy to Classify Parkinson's Disease from Speech in Three Different Languages", "answer": ["Yes", "No"], "top_k_doc_id": [7323, 75, 1777, 2137, 5291, 7324, 7621, 7622, 7623, 7624, 3437, 6031, 627, 624, 5201], "orig_top_k_doc_id": [7622, 7621, 7623, 7323, 3437, 6031, 1777, 7324, 627, 5291, 624, 7624, 2137, 75, 5201]}, {"qid": 4916, "question": "What datasets are used? in Convolutional Neural Networks and a Transfer Learning Strategy to Classify Parkinson's Disease from Speech in Three Different Languages", "answer": ["PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) ", "the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18"], "top_k_doc_id": [7323, 75, 1777, 2137, 5291, 7324, 7621, 7622, 7623, 7624, 770, 5288, 4974, 4671, 1228], "orig_top_k_doc_id": [7621, 7622, 7623, 7323, 7324, 7624, 5291, 1777, 770, 75, 5288, 4974, 2137, 4671, 1228]}]}
{"group_id": 262, "group_size": 7, "items": [{"qid": 4855, "question": "what are the baselines? in Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus", "answer": ["ESIM", "ESIM"], "top_k_doc_id": [7371, 7375, 7376, 7541, 7542, 7543, 7544, 885, 4188, 7374, 886, 4293, 1671, 3190, 1669], "orig_top_k_doc_id": [7541, 7543, 7544, 7542, 7371, 7375, 4188, 886, 885, 1671, 7374, 4293, 3190, 1669, 7376]}, {"qid": 4857, "question": "what pretrained word embeddings are used? in Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus", "answer": ["GloVe, FastText ", "300-dimensional GloVe vectors"], "top_k_doc_id": [7371, 7375, 7376, 7541, 7542, 7543, 7544, 885, 4188, 7374, 886, 4293, 6546, 2063, 2058], "orig_top_k_doc_id": [7543, 7541, 7544, 7542, 885, 7375, 4293, 4188, 7371, 6546, 886, 7376, 2063, 7374, 2058]}, {"qid": 4736, "question": "What is possible future improvement for proposed method/s? in Memory-Augmented Recurrent Networks for Dialogue Coherence", "answer": ["memory module could be applied to other domains such as summary generation, future approach might combine memory module architectures with pointer softmax networks", "Strategies to reduce number of parameters, space out calls over larger time intervals and use context dependent embeddings."], "top_k_doc_id": [7371, 7375, 7376, 400, 3357, 4297, 7372, 7374, 164, 1154, 1155, 3475, 5937, 3507, 7584], "orig_top_k_doc_id": [7375, 7371, 7374, 7376, 7372, 4297, 5937, 1155, 3507, 3357, 3475, 400, 164, 7584, 1154]}, {"qid": 4737, "question": "What is percentage change in performance for better model when compared to baseline? in Memory-Augmented Recurrent Networks for Dialogue Coherence", "answer": ["9.2% reduction in perplexity", "This is a 0.68 perplexity improvement over the vanilla language model without the NTM augmentation."], "top_k_doc_id": [7371, 7375, 7376, 400, 3357, 4297, 7372, 7374, 164, 1154, 1155, 3475, 4296, 7300, 5936], "orig_top_k_doc_id": [7375, 7374, 7371, 1155, 7376, 4297, 7372, 4296, 400, 7300, 1154, 3475, 164, 5936, 3357]}, {"qid": 4854, "question": "how does end of utterance and token tags affect the performance in Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus", "answer": ["Performance degrades if the tags are not used.", "The performance is significantly degraded without two special tags (0,025 in MRR)"], "top_k_doc_id": [7371, 7375, 7376, 7541, 7542, 7543, 7544, 885, 4188, 7374, 2255, 5881, 4295, 4189, 3190], "orig_top_k_doc_id": [7544, 7543, 7541, 7542, 7371, 4188, 2255, 5881, 7375, 7374, 885, 7376, 4295, 4189, 3190]}, {"qid": 4738, "question": "Which of two design architectures have better performance? in Memory-Augmented Recurrent Networks for Dialogue Coherence", "answer": ["NTM-LM", " NTM-LM"], "top_k_doc_id": [7371, 7375, 7376, 400, 3357, 4297, 7372, 7374, 7367, 4774, 4546, 4298, 1768, 7373, 4296], "orig_top_k_doc_id": [7375, 7371, 7374, 7376, 3357, 400, 7372, 4297, 7367, 4774, 4546, 4298, 1768, 7373, 4296]}, {"qid": 4856, "question": "what kind of conversations are in the douban conversation corpus? in Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus", "answer": ["Conversations that are typical for a social networking service.", "Conversations from popular social networking service in China"], "top_k_doc_id": [7371, 7375, 7376, 7541, 7542, 7543, 7544, 3365, 3190, 3362, 4293, 6334, 7508, 1804, 6336], "orig_top_k_doc_id": [7543, 7541, 7544, 7542, 7375, 7371, 3365, 3190, 3362, 7376, 4293, 6334, 7508, 1804, 6336]}]}
{"group_id": 263, "group_size": 7, "items": [{"qid": 4902, "question": "Are experiments conducted on multiple datasets? in Question Answering on Freebase via Relation Extraction and Textual Evidence", "answer": ["No", "No"], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7608, 4463, 3915, 4216, 2019, 2022, 2896, 1961, 2898, 4278], "orig_top_k_doc_id": [7605, 7610, 2019, 7606, 7609, 3915, 7608, 2022, 2896, 4216, 4557, 4463, 1961, 2898, 4278]}, {"qid": 4903, "question": "What baselines is the neural relation extractor compared to? in Question Answering on Freebase via Relation Extraction and Textual Evidence", "answer": ["BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10, BIBREF11 , BIBREF12, BIBREF7 , BIBREF13 , BIBREF14,  BIBREF16", "Berant et al. (2013), Yao and Van Durme (2014), Xu et al. (2014), Berant and Liang (2014), Bao et al. (2014), Border et al. (2014), Dong et al. (2015), Yao (2015), Bast and Haussmann (2015), Berant and Liang (2015), Reddy et al. (2016), Yih et al. (2015)"], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7608, 4463, 3915, 4216, 2023, 4858, 4464, 559, 1760, 4075], "orig_top_k_doc_id": [7605, 7610, 7606, 7609, 3915, 7608, 4216, 2023, 4858, 4464, 559, 4463, 1760, 4075, 4557]}, {"qid": 4905, "question": "How much improvement they get from the previous state-of-the-art? in Question Answering on Freebase via Relation Extraction and Textual Evidence", "answer": ["0.8 point improvement", "0.8 point on average (question-wise) F1 measure "], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7608, 4463, 3915, 1540, 3916, 4074, 4841, 4982, 7820, 4558], "orig_top_k_doc_id": [7605, 7610, 7609, 7608, 4841, 3915, 4463, 4074, 4982, 7820, 4557, 1540, 7606, 3916, 4558]}, {"qid": 4906, "question": "What is the previous state-of-the-art? in Question Answering on Freebase via Relation Extraction and Textual Evidence", "answer": ["F1 score of 39.9 for semantic-based parsing methods. For information extraction methods, 49.4 using relation extraction, 40.8 using distributed representations, and 52.5 using neural networks models", "yih-EtAl:2015:ACL-IJCNLP"], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7608, 4463, 3915, 1540, 3916, 4074, 4841, 4982, 4075, 4845], "orig_top_k_doc_id": [7605, 7610, 7609, 7608, 3915, 4841, 4463, 7606, 4075, 4845, 4074, 4557, 3916, 1540, 4982]}, {"qid": 4904, "question": "What additional evidence they use? in Question Answering on Freebase via Relation Extraction and Textual Evidence", "answer": ["Wikipedia sentences that validate or support KB facts", "by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones"], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7608, 4463, 3409, 1961, 1240, 7082, 883, 2362, 5579, 1965], "orig_top_k_doc_id": [7605, 7610, 7609, 7608, 4463, 3409, 1961, 1240, 7082, 883, 7606, 2362, 5579, 4557, 1965]}, {"qid": 1875, "question": "How does the model recognize entities and their relation to answers at inference time when answers are not accessible? in Question Dependent Recurrent Entity Network for Question Answering", "answer": ["gating function, Dynamic Memory"], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7608, 4858, 2096, 5735, 7806, 7607, 1154, 1090, 7803, 7677], "orig_top_k_doc_id": [7605, 4858, 2096, 5735, 7609, 7806, 7606, 7607, 1154, 7610, 1090, 7608, 7803, 7677, 4557]}, {"qid": 283, "question": "Is an entity linking process used? in Answering Complex Questions Using Open Information Extraction", "answer": ["No"], "top_k_doc_id": [4557, 7605, 7606, 7609, 7610, 7351, 1293, 4158, 1090, 4597, 4719, 4720, 1240, 1422, 6948], "orig_top_k_doc_id": [7610, 4557, 7351, 7605, 1293, 4158, 1090, 4597, 7609, 4719, 4720, 7606, 1240, 1422, 6948]}]}
{"group_id": 264, "group_size": 7, "items": [{"qid": 5045, "question": "Which three variants of sequential validation are examined? in How to evaluate sentiment classifiers for Twitter time-ordered data?", "answer": ["seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,\n\n, seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,\n\n, seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points.\n\n", "9:1 training:test ratio, 20 equidistant samples, 9:1 training:test ratio, 10 equidistant samples, 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points"], "top_k_doc_id": [7747, 7862, 7863, 7756, 7860, 7861, 4392, 7865, 7746, 7751, 7754, 2831, 7864, 7866, 7753], "orig_top_k_doc_id": [7860, 7862, 7861, 7863, 7864, 7756, 7865, 7866, 7746, 7747, 4392, 7754, 2831, 7751, 7753]}, {"qid": 5046, "question": "Which three variants of cross-validation are examined? in How to evaluate sentiment classifiers for Twitter time-ordered data?", "answer": ["10-fold, stratified, blocked;, 10-fold, not stratified, blocked;, 10-fold, stratified, random selection of examples.", "xval(9:1, strat, block) - 10-fold, stratified, blocked;\n\n, xval(9:1, no-strat, block) - 10-fold, not stratified, blocked;\n\n, xval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.\n\n"], "top_k_doc_id": [7747, 7862, 7863, 7756, 7860, 7861, 4392, 7865, 7746, 7751, 7754, 2831, 7864, 7866, 2285], "orig_top_k_doc_id": [7860, 7862, 7861, 7863, 7756, 7747, 7864, 7746, 7865, 2831, 4392, 7866, 7754, 7751, 2285]}, {"qid": 5048, "question": "In what way are sentiment classes ordered? in How to evaluate sentiment classifiers for Twitter time-ordered data?", "answer": ["time-ordered", "negative, neutral, positive"], "top_k_doc_id": [7747, 7862, 7863, 7756, 7860, 7861, 4392, 7865, 7746, 7751, 7754, 7753, 5485, 7757, 7755], "orig_top_k_doc_id": [7860, 7746, 7863, 7756, 7861, 7862, 7751, 7754, 7753, 7747, 5485, 4392, 7757, 7755, 7865]}, {"qid": 5044, "question": "Do the authors offer any potential reasons why cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it? in How to evaluate sentiment classifiers for Twitter time-ordered data?", "answer": ["No", "Yes"], "top_k_doc_id": [7747, 7862, 7863, 7756, 7860, 7861, 4392, 7865, 7864, 7866, 6133, 4986, 6535, 7311, 5561], "orig_top_k_doc_id": [7860, 7864, 7861, 7866, 7862, 7865, 7863, 7747, 7756, 6133, 4392, 4986, 6535, 7311, 5561]}, {"qid": 5047, "question": "Which European languages are targeted? in How to evaluate sentiment classifiers for Twitter time-ordered data?", "answer": ["Albanian, Bulgarian, English, German, Hungarian, Polish, Portughese, Russian, Serbian, Slovak, Slovenian, Spanish, Swedish", "Albanian\nBulgarian\nEnglish\nGerman\nHungarian\nPolish\nPortuguese\nRussian\nSer/Cro/Bos\nSlovak\nSlovenian\nSpanish\nSwedish"], "top_k_doc_id": [7747, 7862, 7863, 7756, 7860, 7861, 7746, 1049, 6176, 7866, 7753, 7752, 1052, 7751, 1051], "orig_top_k_doc_id": [7860, 7862, 7746, 1049, 7861, 7863, 7747, 6176, 7866, 7753, 7752, 7756, 1052, 7751, 1051]}, {"qid": 4977, "question": "What evidence is presented that humans perceive the sentiment classes as ordered? in Multilingual Twitter Sentiment Classification: The Role of Human Annotators", "answer": ["average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8", "the average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8"], "top_k_doc_id": [7747, 7862, 7863, 7756, 7860, 7746, 7751, 7754, 7753, 2282, 7308, 6770, 2105, 448, 87], "orig_top_k_doc_id": [7746, 7747, 7751, 7754, 7753, 7863, 7862, 7860, 2282, 7308, 6770, 7756, 2105, 448, 87]}, {"qid": 4980, "question": "What statistical test(s) is used to compare the top classification models? in Multilingual Twitter Sentiment Classification: The Role of Human Annotators", "answer": [" Friedman-Nemenyi test", " Friedman-Nemenyi test BIBREF14 , BIBREF15"], "top_k_doc_id": [7747, 7862, 7863, 7746, 86, 1329, 7751, 7752, 6770, 6036, 3637, 5979, 3949, 7308, 1500], "orig_top_k_doc_id": [7746, 7747, 86, 1329, 7751, 7752, 7862, 7863, 6770, 6036, 3637, 5979, 3949, 7308, 1500]}]}
{"group_id": 265, "group_size": 6, "items": [{"qid": 16, "question": "How do the authors measure how temporally dynamic a community is? in Community Identity and User Engagement in a Multi-Community Landscape", "answer": ["the average volatility of all utterances"], "top_k_doc_id": [12, 13, 14, 15, 16, 17, 18, 235, 4580, 521, 3641, 4739, 241, 3584, 5793], "orig_top_k_doc_id": [12, 15, 18, 16, 17, 13, 14, 235, 4580, 3641, 4739, 241, 521, 3584, 5793]}, {"qid": 17, "question": "How do the authors measure how distinctive a community is? in Community Identity and User Engagement in a Multi-Community Landscape", "answer": [" the average specificity of all utterances"], "top_k_doc_id": [12, 13, 14, 15, 16, 17, 18, 235, 4580, 521, 3641, 4739, 241, 3584, 7449], "orig_top_k_doc_id": [12, 15, 18, 16, 17, 13, 14, 235, 4739, 4580, 3641, 241, 521, 3584, 7449]}, {"qid": 14, "question": "What patterns do they observe about how user engagement varies with the characteristics of a community? in Community Identity and User Engagement in a Multi-Community Landscape", "answer": ["communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members, within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers "], "top_k_doc_id": [12, 13, 14, 15, 16, 17, 18, 235, 4580, 521, 3641, 4739, 241, 5793, 2967], "orig_top_k_doc_id": [12, 18, 15, 16, 17, 13, 235, 521, 241, 14, 5793, 3641, 4739, 4580, 2967]}, {"qid": 12, "question": "Do they report results only on English data? in Community Identity and User Engagement in a Multi-Community Landscape", "answer": ["No", "No"], "top_k_doc_id": [12, 13, 14, 15, 16, 17, 18, 235, 4580, 521, 3641, 4739, 5793, 5976, 7260], "orig_top_k_doc_id": [12, 18, 15, 17, 16, 13, 235, 3641, 14, 4739, 521, 5793, 5976, 7260, 4580]}, {"qid": 13, "question": "How do the various social phenomena examined manifest in different types of communities? in Community Identity and User Engagement in a Multi-Community Landscape", "answer": ["Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.\n"], "top_k_doc_id": [12, 13, 14, 15, 16, 17, 18, 235, 4580, 1397, 4581, 4739, 3584, 6417, 7451], "orig_top_k_doc_id": [18, 12, 17, 16, 15, 13, 14, 235, 4581, 4739, 3584, 6417, 7451, 4580, 1397]}, {"qid": 15, "question": "How did the select the 300 Reddit communities for comparison? in Community Identity and User Engagement in a Multi-Community Landscape", "answer": ["They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.", "They collect subreddits from January 2013 to December 2014,2 for which there are at\nleast 500 words in the vocabulary used to estimate the measures,\nin at least 4 months of the subreddit\u2019s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language."], "top_k_doc_id": [12, 13, 14, 15, 16, 17, 18, 235, 4580, 1397, 4581, 7260, 3641, 441, 3127], "orig_top_k_doc_id": [12, 18, 13, 17, 15, 16, 14, 4580, 235, 1397, 7260, 3641, 441, 3127, 4581]}]}
{"group_id": 266, "group_size": 6, "items": [{"qid": 56, "question": "How is the intensity of the PTSD established? in LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "answer": ["Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.", "defined into four categories from high risk, moderate risk, to low risk"], "top_k_doc_id": [59, 60, 61, 63, 64, 643, 1701, 1702, 6725, 5057, 62, 1480, 5058, 1703, 5056], "orig_top_k_doc_id": [64, 60, 59, 61, 63, 1701, 1702, 6725, 62, 643, 1480, 1703, 5056, 5057, 5058]}, {"qid": 58, "question": "How many twitter users are surveyed using the clinically validated survey? in LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "answer": ["210"], "top_k_doc_id": [59, 60, 61, 63, 64, 643, 1701, 1702, 6725, 5057, 62, 1480, 5058, 521, 522], "orig_top_k_doc_id": [64, 60, 59, 61, 63, 1701, 643, 1702, 62, 1480, 5057, 5058, 521, 6725, 522]}, {"qid": 59, "question": "Which clinically validated survey tools are used? in LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "answer": ["DOSPERT, BSSS and VIAS"], "top_k_doc_id": [59, 60, 61, 63, 64, 643, 1701, 1702, 6725, 5057, 62, 1480, 2343, 6206, 647], "orig_top_k_doc_id": [64, 60, 59, 61, 63, 643, 1701, 1702, 6725, 62, 1480, 2343, 6206, 647, 5057]}, {"qid": 54, "question": "Do they evaluate only on English datasets? in LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "answer": ["No"], "top_k_doc_id": [59, 60, 61, 63, 64, 643, 1701, 1702, 6725, 5057, 62, 5058, 3574, 1704, 6616], "orig_top_k_doc_id": [64, 60, 59, 61, 1701, 1702, 6725, 63, 643, 5057, 5058, 3574, 62, 1704, 6616]}, {"qid": 55, "question": "Do the authors mention any possible confounds in this study? in LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "answer": ["No"], "top_k_doc_id": [59, 60, 61, 63, 64, 643, 1701, 1702, 6725, 5057, 521, 4139, 4948, 5466, 5058], "orig_top_k_doc_id": [64, 60, 59, 61, 1701, 1702, 643, 63, 6725, 5057, 521, 4139, 4948, 5466, 5058]}, {"qid": 57, "question": "How is LIWC incorporated into this system? in LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "answer": [" For each user, we calculate the proportion of tweets scored positively by each LIWC category.", "to calculate the possible scores of each survey question using PTSD Linguistic Dictionary "], "top_k_doc_id": [59, 60, 61, 63, 64, 643, 1701, 1702, 6725, 62, 3320, 1704, 4964, 521, 1703], "orig_top_k_doc_id": [64, 60, 59, 61, 1702, 1701, 63, 6725, 62, 3320, 1704, 643, 4964, 521, 1703]}]}
{"group_id": 267, "group_size": 6, "items": [{"qid": 94, "question": "What is the baseline for the experiments? in Towards Detection of Subjective Bias using Contextualized Word Embeddings", "answer": ["FastText, BiLSTM, BERT", "FastText, BERT , two-layer BiLSTM architecture with GloVe word embeddings"], "top_k_doc_id": [105, 5525, 6268, 6269, 6271, 6272, 5812, 5814, 106, 606, 6270, 5711, 1329, 3627, 5710], "orig_top_k_doc_id": [105, 6268, 6269, 106, 6272, 5525, 6271, 5711, 6270, 5812, 1329, 5814, 3627, 606, 5710]}, {"qid": 95, "question": "Which experiments are perfomed? in Towards Detection of Subjective Bias using Contextualized Word Embeddings", "answer": ["They used BERT-based models to detect subjective language in the WNC corpus"], "top_k_doc_id": [105, 5525, 6268, 6269, 6271, 6272, 5812, 5814, 106, 606, 6270, 6560, 6770, 5172, 203], "orig_top_k_doc_id": [105, 6268, 6269, 6272, 106, 5525, 6271, 6560, 5812, 6270, 5814, 6770, 606, 5172, 203]}, {"qid": 3883, "question": "Which works better according to human evaluation, the concurrent or the modular system? in Automatically Neutralizing Subjective Bias in Text", "answer": ["Modular", "Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text.", "They are equal"], "top_k_doc_id": [105, 5525, 6268, 6269, 6271, 6272, 6270, 6273, 5949, 7478, 778, 4829, 5428, 6978, 6558], "orig_top_k_doc_id": [6271, 6268, 6272, 6269, 6270, 6273, 5525, 7478, 778, 5949, 4829, 5428, 6978, 6558, 105]}, {"qid": 3885, "question": "How is subjective text automatically neutralized? in Automatically Neutralizing Subjective Bias in Text", "answer": [" Identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy and suggest edits that would make it more neutral.", "The text is modified to remove the subjective bias while preserve the meaning as much as possible", "algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed"], "top_k_doc_id": [105, 5525, 6268, 6269, 6271, 6272, 6270, 6273, 5949, 106, 606, 4959, 503, 6399, 5524], "orig_top_k_doc_id": [6268, 6269, 6271, 6270, 6272, 106, 105, 6273, 5525, 5949, 606, 4959, 503, 6399, 5524]}, {"qid": 93, "question": "Do the authors report only on English? in Towards Detection of Subjective Bias using Contextualized Word Embeddings", "answer": ["No"], "top_k_doc_id": [105, 5525, 6268, 6269, 6271, 6272, 5812, 5814, 5815, 5976, 5977, 2473, 7267, 5710, 1910], "orig_top_k_doc_id": [105, 6268, 6271, 6272, 6269, 5815, 5976, 5977, 2473, 5814, 5812, 7267, 5525, 5710, 1910]}, {"qid": 3884, "question": "Were the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it? in Automatically Neutralizing Subjective Bias in Text", "answer": ["Wiki community effort", "Wikipedia editors", " Wikipedia edits"], "top_k_doc_id": [105, 5525, 6268, 6269, 6271, 6272, 6270, 6273, 4107, 5581, 1443, 5580, 4106, 2171, 6966], "orig_top_k_doc_id": [6268, 6269, 6271, 6272, 105, 6270, 6273, 4107, 5581, 1443, 5580, 5525, 4106, 2171, 6966]}]}
{"group_id": 268, "group_size": 6, "items": [{"qid": 97, "question": "what language models do they use? in Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!", "answer": ["LSTM LMs"], "top_k_doc_id": [4796, 107, 110, 111, 1247, 2439, 6324, 518, 109, 108, 1642, 4963, 6321, 1924, 5844], "orig_top_k_doc_id": [107, 111, 110, 4796, 518, 1247, 6324, 109, 108, 1642, 2439, 4963, 6321, 1924, 5844]}, {"qid": 98, "question": "what questions do they ask human judges? in Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!", "answer": ["No"], "top_k_doc_id": [4796, 107, 110, 111, 1247, 2439, 6324, 6323, 787, 491, 6068, 5562, 1866, 6339, 5471], "orig_top_k_doc_id": [6323, 111, 107, 787, 6324, 491, 6068, 110, 4796, 2439, 5562, 1866, 6339, 1247, 5471]}, {"qid": 2746, "question": "How many people participated in their evaluation study of table-to-text models? in Handling Divergent Reference Texts when Evaluating Table-to-Text Generation", "answer": ["about 500", "No"], "top_k_doc_id": [4796, 1969, 4793, 4794, 4795, 4797, 1921, 2436, 1929, 6916, 6837, 5844, 1889, 2261, 1795], "orig_top_k_doc_id": [4796, 4793, 4795, 4794, 4797, 1969, 1929, 6916, 1921, 6837, 2436, 5844, 1889, 2261, 1795]}, {"qid": 2747, "question": "By how much more does PARENT correlate with human judgements in comparison to other text generation metrics? in Handling Divergent Reference Texts when Evaluating Table-to-Text Generation", "answer": ["Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.", "Their average correlation tops the best other model by 0.155 on WikiBio."], "top_k_doc_id": [4796, 1969, 4793, 4794, 4795, 4797, 1921, 2436, 111, 2435, 110, 1924, 107, 1923, 4744], "orig_top_k_doc_id": [4796, 4793, 4795, 111, 4794, 2435, 2436, 1969, 110, 4797, 1924, 107, 1923, 1921, 4744]}, {"qid": 96, "question": "Is ROUGE their only baseline? in Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!", "answer": ["No", "No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU."], "top_k_doc_id": [4796, 107, 110, 111, 109, 108, 264, 6323, 518, 4478, 4764, 1867, 5892, 4828, 2444], "orig_top_k_doc_id": [111, 110, 107, 109, 108, 264, 6323, 518, 4478, 4764, 1867, 5892, 4828, 2444, 4796]}, {"qid": 2745, "question": "Ngrams of which length are aligned using PARENT? in Handling Divergent Reference Texts when Evaluating Table-to-Text Generation", "answer": ["No", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "top_k_doc_id": [4796, 1969, 4793, 4794, 4795, 4797, 4961, 4407, 7176, 6320, 6126, 3196, 5504, 6127, 1929], "orig_top_k_doc_id": [4796, 4793, 4795, 4797, 4794, 1969, 4961, 4407, 7176, 6320, 6126, 3196, 5504, 6127, 1929]}]}
{"group_id": 269, "group_size": 6, "items": [{"qid": 138, "question": "What multimodality is available in the dataset? in Procedural Reasoning Networks for Understanding Multimodal Procedures", "answer": ["context is a procedural text, the question and the multiple choice answers are composed of images", "images and text"], "top_k_doc_id": [7138, 276, 6010, 160, 163, 164, 1256, 3175, 4554, 5429, 7518, 5015, 3655, 3657, 589], "orig_top_k_doc_id": [160, 164, 5015, 6010, 1256, 3655, 3657, 163, 589, 3175, 7518, 5429, 7138, 4554, 276]}, {"qid": 139, "question": "What are previously reported models? in Procedural Reasoning Networks for Understanding Multimodal Procedures", "answer": ["Hasty Student, Impatient Reader, BiDAF, BiDAF w/ static memory"], "top_k_doc_id": [7138, 276, 6010, 160, 163, 164, 1256, 3175, 4554, 5429, 7518, 2900, 7147, 112, 2116], "orig_top_k_doc_id": [160, 164, 163, 1256, 2900, 6010, 3175, 7138, 7147, 276, 112, 5429, 4554, 7518, 2116]}, {"qid": 140, "question": "How better is accuracy of new model compared to previously reported models? in Procedural Reasoning Networks for Understanding Multimodal Procedures", "answer": ["Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59"], "top_k_doc_id": [7138, 276, 6010, 160, 163, 164, 1256, 3175, 2900, 112, 2120, 7148, 427, 4277, 2899], "orig_top_k_doc_id": [160, 164, 163, 6010, 7138, 2900, 112, 1256, 2120, 7148, 3175, 276, 427, 4277, 2899]}, {"qid": 2585, "question": "Does this paper propose a new task that others can try to improve performance on? in Leveraging Recurrent Neural Networks for Multimodal Recognition of Social Norm Violation in Dialog", "answer": ["No, there has been previous work on recognizing social norm violation."], "top_k_doc_id": [7138, 226, 929, 930, 2900, 3175, 5481, 4552, 4553, 4554, 4755, 4124, 848, 4120, 2253], "orig_top_k_doc_id": [4552, 4553, 4554, 5481, 2900, 226, 4755, 7138, 4124, 929, 848, 3175, 4120, 930, 2253]}, {"qid": 2802, "question": "Do they experiment with the toolkits? in GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing", "answer": ["Yes", "Yes"], "top_k_doc_id": [7138, 276, 6010, 4908, 4909, 7149, 6595, 7854, 286, 7148, 7420, 7163, 5655, 2058, 4983], "orig_top_k_doc_id": [4908, 4909, 7149, 276, 6595, 7854, 6010, 286, 7148, 7420, 7163, 5655, 7138, 2058, 4983]}, {"qid": 3888, "question": "What are the deep learning architectures used? in Sign Language Recognition Analysis using Multimodal Data", "answer": ["Axis Independent Architecture (AI-LSTM), Spatial AI-LSTM,  Max CNN-LSTM, 3D CNN", "Recurrent Neural Networks (RNN), 3D Convolutional Neural Network, Axis Independent LSTM, Spatial AI-LSTM, Max CNN-LSTM network", "3D CNN, Axis independent LSTM,  spatial axis independent LSTM, and combined network "], "top_k_doc_id": [7138, 226, 929, 930, 2900, 3175, 5481, 6275, 6274, 6278, 4756, 931, 7149, 7148, 6277], "orig_top_k_doc_id": [6275, 6274, 6278, 4756, 931, 5481, 7138, 929, 930, 2900, 7149, 7148, 6277, 3175, 226]}]}
{"group_id": 270, "group_size": 6, "items": [{"qid": 356, "question": "What neural configurations are explored? in Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations", "answer": ["tried many configurations of our network models, but report results with only three configurations, Transformer Type 1, Transformer Type 2, Transformer Type 3"], "top_k_doc_id": [426, 424, 425, 428, 427, 1145, 1182, 1357, 2739, 4914, 790, 4288, 6972, 2238, 7140], "orig_top_k_doc_id": [424, 425, 426, 428, 1357, 427, 1145, 1182, 4914, 2739, 6972, 2238, 4288, 7140, 790]}, {"qid": 359, "question": "What datasets do they use? in Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations", "answer": ["AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20"], "top_k_doc_id": [426, 424, 425, 428, 427, 1145, 1182, 1357, 2739, 4914, 790, 4288, 411, 1683, 3807], "orig_top_k_doc_id": [424, 425, 428, 1145, 1357, 426, 427, 411, 1182, 1683, 4914, 4288, 3807, 790, 2739]}, {"qid": 358, "question": "How is this problem evaluated? in Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations", "answer": ["BLEU-2, average accuracies over 3 test trials on different randomly sampled test sets"], "top_k_doc_id": [426, 424, 425, 428, 427, 1145, 1182, 1357, 2739, 4914, 2190, 1361, 453, 118, 1819], "orig_top_k_doc_id": [424, 428, 425, 426, 1357, 427, 1145, 2190, 4914, 1182, 1361, 453, 2739, 118, 1819]}, {"qid": 355, "question": "Does pre-training on general text corpus improve performance? in Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations", "answer": ["No"], "top_k_doc_id": [426, 424, 425, 428, 427, 1145, 1182, 1357, 2739, 7140, 1146, 533, 4287, 6002, 118], "orig_top_k_doc_id": [424, 425, 426, 1145, 428, 427, 7140, 1357, 1146, 2739, 533, 1182, 4287, 6002, 118]}, {"qid": 357, "question": "Are the Transformers masked? in Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations", "answer": ["Yes"], "top_k_doc_id": [426, 424, 425, 428, 7140, 5724, 5540, 7117, 4414, 2190, 731, 4760, 4652, 533, 1560], "orig_top_k_doc_id": [424, 426, 425, 7140, 5724, 5540, 7117, 428, 4414, 2190, 731, 4760, 4652, 533, 1560]}, {"qid": 2572, "question": "What NLP tasks do the authors evaluate feed-forward networks on? in Natural Language Processing with Small Feed-Forward Networks", "answer": ["language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation"], "top_k_doc_id": [426, 4526, 785, 784, 7109, 1592, 3559, 2739, 4309, 1578, 4310, 6937, 7117, 3273, 4812], "orig_top_k_doc_id": [4526, 785, 784, 7109, 426, 1592, 3559, 2739, 4309, 1578, 4310, 6937, 7117, 3273, 4812]}]}
{"group_id": 271, "group_size": 6, "items": [{"qid": 393, "question": "What were their results on the three datasets? in A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "answer": ["accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR"], "top_k_doc_id": [462, 463, 1329, 5291, 6401, 6770, 331, 1876, 5091, 5813, 5422, 447, 6176, 770, 6684], "orig_top_k_doc_id": [6770, 5813, 6401, 463, 331, 5291, 462, 1329, 770, 6176, 5091, 5422, 6684, 447, 1876]}, {"qid": 394, "question": "What was the baseline? in A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "answer": ["We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN., we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. "], "top_k_doc_id": [462, 463, 1329, 5291, 6401, 6770, 331, 1876, 5091, 5813, 5422, 447, 6176, 756, 521], "orig_top_k_doc_id": [6770, 463, 1329, 5422, 5813, 6401, 5291, 447, 331, 462, 1876, 756, 6176, 5091, 521]}, {"qid": 397, "question": "Which three Twitter sentiment classification datasets are used for experiments? in A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "answer": ["Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)"], "top_k_doc_id": [462, 463, 1329, 5291, 6401, 6770, 331, 1876, 5091, 5813, 5422, 6684, 7131, 3300, 756], "orig_top_k_doc_id": [6770, 463, 462, 6401, 331, 5291, 6684, 7131, 5091, 5422, 3300, 5813, 1329, 1876, 756]}, {"qid": 396, "question": "Are results reported only on English datasets? in A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "answer": ["Yes"], "top_k_doc_id": [462, 463, 1329, 5291, 6401, 6770, 331, 1876, 5091, 5813, 6176, 7285, 309, 756, 1489], "orig_top_k_doc_id": [6770, 5291, 463, 5813, 331, 6401, 1329, 6176, 462, 1876, 7285, 309, 756, 1489, 5091]}, {"qid": 395, "question": "Which datasets did they use? in A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "answer": ["Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)"], "top_k_doc_id": [462, 463, 1329, 5291, 6401, 6770, 331, 1876, 5091, 5085, 5498, 7522, 6176, 5422, 756], "orig_top_k_doc_id": [6770, 463, 331, 6401, 5291, 5091, 462, 5085, 1876, 5498, 7522, 1329, 6176, 5422, 756]}, {"qid": 398, "question": "What semantic rules are proposed? in A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "answer": ["rules that compute polarity of words after POS tagging or parsing steps"], "top_k_doc_id": [462, 463, 1329, 5291, 6401, 6770, 447, 599, 3300, 756, 2216, 598, 521, 465, 770], "orig_top_k_doc_id": [463, 462, 6770, 447, 599, 3300, 5291, 756, 2216, 1329, 598, 521, 465, 6401, 770]}]}
{"group_id": 272, "group_size": 6, "items": [{"qid": 413, "question": "Which of the two speech recognition models works better overall on CN-Celeb? in CN-CELEB: a challenging Chinese speaker recognition dataset", "answer": ["x-vector"], "top_k_doc_id": [482, 483, 484, 2011, 2313, 2314, 403, 1372, 5369, 7032, 7033, 6036, 7034, 2351, 3828], "orig_top_k_doc_id": [482, 484, 483, 2313, 2314, 403, 2011, 5369, 7032, 7033, 6036, 7034, 1372, 2351, 3828]}, {"qid": 414, "question": "By how much is performance on CN-Celeb inferior to performance on VoxCeleb? in CN-CELEB: a challenging Chinese speaker recognition dataset", "answer": ["For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb"], "top_k_doc_id": [482, 483, 484, 2011, 2313, 2314, 403, 1372, 5369, 7032, 7033, 2722, 5371, 2312, 5370], "orig_top_k_doc_id": [484, 482, 483, 2314, 2313, 5369, 2722, 2011, 5371, 403, 2312, 7032, 5370, 1372, 7033]}, {"qid": 409, "question": "What was the performance of both approaches on their dataset? in CN-CELEB: a challenging Chinese speaker recognition dataset", "answer": ["ERR of 19.05 with i-vectors and 15.52 with x-vectors"], "top_k_doc_id": [482, 483, 484, 2011, 2313, 2314, 403, 1372, 5369, 6036, 3422, 1266, 2351, 2722, 5371], "orig_top_k_doc_id": [482, 484, 483, 2314, 6036, 403, 3422, 1266, 2011, 1372, 2313, 2351, 5369, 2722, 5371]}, {"qid": 411, "question": "What genres are covered? in CN-CELEB: a challenging Chinese speaker recognition dataset", "answer": ["genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement"], "top_k_doc_id": [482, 483, 484, 2011, 2313, 2314, 403, 608, 7794, 1266, 3422, 6036, 5565, 4231, 3743], "orig_top_k_doc_id": [482, 484, 483, 2011, 608, 2314, 7794, 1266, 3422, 403, 6036, 2313, 5565, 4231, 3743]}, {"qid": 412, "question": "Do they experiment with cross-genre setups? in CN-CELEB: a challenging Chinese speaker recognition dataset", "answer": ["No"], "top_k_doc_id": [482, 483, 484, 2011, 2313, 2314, 5393, 5391, 6036, 1266, 6262, 1365, 7799, 3548, 3828], "orig_top_k_doc_id": [482, 484, 483, 5393, 2314, 2313, 5391, 6036, 1266, 6262, 2011, 1365, 7799, 3548, 3828]}, {"qid": 410, "question": "What kind of settings do the utterances come from? in CN-CELEB: a challenging Chinese speaker recognition dataset", "answer": ["entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement"], "top_k_doc_id": [482, 483, 484, 2011, 5394, 7793, 7794, 5220, 2253, 3998, 7159, 7795, 5428, 5393, 7156], "orig_top_k_doc_id": [482, 484, 483, 5394, 7793, 7794, 5220, 2011, 2253, 3998, 7159, 7795, 5428, 5393, 7156]}]}
{"group_id": 273, "group_size": 6, "items": [{"qid": 430, "question": "How is effective word score calculated? in Efficient Twitter Sentiment Classification using Subjective Distant Supervision", "answer": ["We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x."], "top_k_doc_id": [7307, 7308, 7311, 87, 502, 1049, 5486, 6752, 331, 503, 5008, 1687, 450, 7309, 5417], "orig_top_k_doc_id": [502, 7308, 7311, 87, 1049, 5008, 1687, 5486, 503, 450, 7307, 6752, 331, 7309, 5417]}, {"qid": 431, "question": "How is tweet subjectivity measured? in Efficient Twitter Sentiment Classification using Subjective Distant Supervision", "answer": ["No"], "top_k_doc_id": [7307, 7308, 7311, 87, 502, 1049, 5486, 6752, 331, 503, 506, 606, 447, 330, 598], "orig_top_k_doc_id": [502, 7308, 503, 7307, 1049, 5486, 506, 87, 7311, 331, 606, 447, 6752, 330, 598]}, {"qid": 4684, "question": "Do the authors mention any possible confounds in this study? in Enhanced Twitter Sentiment Classification Using Contextual Information", "answer": ["No", "Yes"], "top_k_doc_id": [7307, 7308, 7311, 447, 4948, 7056, 6752, 7118, 5180, 4499, 7310, 7862, 4949, 7752, 6629], "orig_top_k_doc_id": [4948, 7307, 7308, 7118, 447, 6752, 5180, 7311, 4499, 7056, 7310, 7862, 4949, 7752, 6629]}, {"qid": 4685, "question": "Do they report results only on English data? in Enhanced Twitter Sentiment Classification Using Contextual Information", "answer": ["Yes", "Yes"], "top_k_doc_id": [7307, 7308, 7311, 447, 4948, 7056, 6752, 7118, 2385, 1330, 5417, 3826, 7755, 5813, 6101], "orig_top_k_doc_id": [7308, 7307, 6752, 2385, 7311, 1330, 5417, 7056, 3826, 7755, 7118, 5813, 447, 4948, 6101]}, {"qid": 429, "question": "What previously proposed methods is this method compared against? in Efficient Twitter Sentiment Classification using Subjective Distant Supervision", "answer": ["Naive Bayes, SVM, Maximum Entropy classifiers"], "top_k_doc_id": [7307, 7308, 7311, 87, 502, 1049, 5486, 6752, 5900, 450, 447, 7861, 6821, 6816, 6989], "orig_top_k_doc_id": [502, 7308, 1049, 5486, 5900, 450, 87, 447, 7311, 7861, 6821, 7307, 6752, 6816, 6989]}, {"qid": 4686, "question": "Are there any other standard linguistic features used, other than ngrams? in Enhanced Twitter Sentiment Classification Using Contextual Information", "answer": ["No", "No"], "top_k_doc_id": [7307, 7308, 7311, 447, 4948, 7056, 5168, 1489, 4404, 7309, 1329, 6753, 7310, 3825, 7115], "orig_top_k_doc_id": [7311, 7307, 447, 7308, 5168, 1489, 4404, 7056, 7309, 4948, 1329, 6753, 7310, 3825, 7115]}]}
{"group_id": 274, "group_size": 6, "items": [{"qid": 477, "question": "What were the evaluation metrics? in Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever", "answer": ["BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5"], "top_k_doc_id": [570, 571, 569, 572, 573, 574, 2550, 3191, 3192, 3357, 3509, 965, 6588, 3359, 2551], "orig_top_k_doc_id": [569, 573, 572, 571, 574, 570, 2550, 3191, 3192, 965, 3357, 6588, 3509, 3359, 2551]}, {"qid": 478, "question": "What were the baseline systems? in Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever", "answer": ["Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR"], "top_k_doc_id": [570, 571, 569, 572, 573, 574, 2550, 3191, 3192, 3357, 3509, 4669, 4128, 3507, 7840], "orig_top_k_doc_id": [569, 573, 572, 571, 574, 570, 2550, 3191, 3192, 3509, 3357, 4669, 4128, 3507, 7840]}, {"qid": 479, "question": "Which dialog datasets did they experiment with? in Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever", "answer": ["Camrest, InCar Assistant"], "top_k_doc_id": [570, 571, 569, 572, 573, 574, 2550, 3191, 4128, 6588, 7840, 4129, 7839, 6587, 3509], "orig_top_k_doc_id": [569, 573, 572, 571, 574, 4128, 7840, 570, 6588, 4129, 7839, 2550, 3191, 6587, 3509]}, {"qid": 480, "question": "What KB is used? in Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever", "answer": ["No"], "top_k_doc_id": [570, 571, 569, 572, 573, 574, 2550, 3191, 4128, 6588, 7840, 2551, 4720, 4597, 2555], "orig_top_k_doc_id": [569, 573, 572, 571, 574, 570, 2550, 2551, 7840, 6588, 4128, 3191, 4720, 4597, 2555]}, {"qid": 2820, "question": "What is the state-of-the-art model for the task? in Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)", "answer": ["OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4", "No"], "top_k_doc_id": [570, 571, 785, 4934, 4935, 4937, 4939, 5896, 6646, 6690, 7443, 7444, 6551, 1867, 6552], "orig_top_k_doc_id": [4939, 4934, 4935, 4937, 570, 7443, 7444, 785, 6690, 6646, 571, 6551, 1867, 6552, 5896]}, {"qid": 2821, "question": "What is the strong baseline? in Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)", "answer": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "top_k_doc_id": [570, 571, 785, 4934, 4935, 4937, 4939, 5896, 6646, 6690, 7443, 7444, 4936, 7348, 7551], "orig_top_k_doc_id": [4939, 4934, 4937, 4935, 7444, 570, 785, 7443, 571, 4936, 5896, 7348, 6690, 7551, 6646]}]}
{"group_id": 275, "group_size": 6, "items": [{"qid": 563, "question": "How does the automatic theorem prover infer the relation? in Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization", "answer": ["No"], "top_k_doc_id": [686, 687, 689, 2276, 4216, 688, 1216, 1217, 2705, 2515, 2518, 2517, 786, 2704, 6022], "orig_top_k_doc_id": [689, 686, 687, 2515, 2518, 2517, 4216, 786, 2704, 6022, 688, 1216, 2705, 1217, 2276]}, {"qid": 565, "question": "How many samples did they generate for the artificial language? in Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization", "answer": ["70,000"], "top_k_doc_id": [686, 687, 689, 2276, 4216, 688, 1216, 1217, 2705, 7138, 5855, 2706, 7340, 1874, 2578], "orig_top_k_doc_id": [686, 689, 687, 7138, 1216, 1217, 5855, 2706, 2276, 4216, 2705, 7340, 688, 1874, 2578]}, {"qid": 3545, "question": "What tasks do they experiment with? in Dynamic Compositional Neural Networks over Tree Structure", "answer": ["text classification and text semantic matching", "text classification and text semantic matching"], "top_k_doc_id": [686, 1217, 2718, 4774, 5855, 5857, 689, 2733, 2737, 2738, 5856, 5858, 5859, 6122, 1216], "orig_top_k_doc_id": [5855, 5858, 5859, 4774, 5856, 686, 689, 2733, 2718, 1217, 5857, 2738, 2737, 6122, 1216]}, {"qid": 3546, "question": "What is the meta knowledge specifically? in Dynamic Compositional Neural Networks over Tree Structure", "answer": ["No"], "top_k_doc_id": [686, 1217, 2718, 4774, 5855, 5857, 689, 2733, 2737, 2738, 5856, 5858, 5859, 7227, 1389], "orig_top_k_doc_id": [5855, 5856, 5858, 4774, 5859, 5857, 7227, 2733, 1217, 689, 2737, 2718, 1389, 686, 2738]}, {"qid": 564, "question": "If these model can learn the first-order logic on artificial language, why can't it lear for natural language? in Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization", "answer": ["No"], "top_k_doc_id": [686, 687, 689, 2276, 4216, 2746, 786, 2733, 5429, 7138, 2426, 3535, 470, 540, 7141], "orig_top_k_doc_id": [686, 689, 687, 2276, 2746, 4216, 786, 2733, 5429, 7138, 2426, 3535, 470, 540, 7141]}, {"qid": 2734, "question": "Which baselines did they compare against? in Dynamic Compositionality in Recursive Neural Networks with Structure-aware Tag Representations", "answer": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "top_k_doc_id": [686, 1217, 2718, 4774, 5855, 5857, 4775, 259, 2706, 804, 3912, 95, 1389, 4779, 2917], "orig_top_k_doc_id": [4774, 4775, 1217, 2718, 5855, 5857, 259, 2706, 804, 686, 3912, 95, 1389, 4779, 2917]}]}
{"group_id": 276, "group_size": 6, "items": [{"qid": 599, "question": "How big is dataset used for training/testing? in Textual Data for Time Series Forecasting", "answer": ["4,261  days for France and 4,748 for the UK"], "top_k_doc_id": [4798, 747, 748, 751, 752, 754, 999, 4799, 7411, 7861, 3731, 5651, 7858, 5650, 7862], "orig_top_k_doc_id": [747, 7861, 751, 748, 754, 4799, 3731, 5651, 4798, 7858, 7411, 999, 752, 5650, 7862]}, {"qid": 600, "question": "Is there any example where geometric property is visible for context similarity between words? in Textual Data for Time Series Forecasting", "answer": ["Yes"], "top_k_doc_id": [4798, 747, 748, 751, 752, 754, 749, 753, 5646, 5647, 7514, 4801, 7502, 7560, 7861], "orig_top_k_doc_id": [749, 748, 754, 747, 5647, 752, 4801, 7514, 751, 7502, 7560, 7861, 4798, 5646, 753]}, {"qid": 601, "question": "What geometric properties do embeddings display? in Textual Data for Time Series Forecasting", "answer": ["Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters."], "top_k_doc_id": [4798, 747, 748, 751, 752, 754, 749, 753, 5646, 5647, 7514, 5651, 4799, 5481, 3372], "orig_top_k_doc_id": [749, 754, 748, 747, 752, 5651, 4799, 753, 7514, 4798, 751, 5481, 5646, 3372, 5647]}, {"qid": 602, "question": "How accurate is model trained on text exclusively? in Textual Data for Time Series Forecasting", "answer": ["Relative error is less than 5%"], "top_k_doc_id": [4798, 747, 748, 751, 752, 754, 999, 4799, 7411, 7861, 7026, 4810, 749, 2700, 4807], "orig_top_k_doc_id": [748, 751, 747, 754, 4798, 752, 7411, 7026, 4799, 999, 7861, 4810, 749, 2700, 4807]}, {"qid": 3415, "question": "What labels for antisocial events are available in datasets? in Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop", "answer": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "top_k_doc_id": [4798, 3362, 3731, 5646, 5647, 5649, 5650, 5651, 6285, 7382, 7411, 5648, 2689, 3521, 7628], "orig_top_k_doc_id": [5646, 5647, 5651, 5650, 5649, 6285, 7382, 3731, 7411, 5648, 2689, 3521, 3362, 7628, 4798]}, {"qid": 3416, "question": "What are two datasets model is applied to? in Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop", "answer": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "top_k_doc_id": [4798, 3362, 3731, 5646, 5647, 5649, 5650, 5651, 6285, 4799, 3367, 3667, 3583, 3537, 6624], "orig_top_k_doc_id": [5646, 5651, 5647, 5650, 5649, 4799, 3731, 3367, 3667, 4798, 6285, 3362, 3583, 3537, 6624]}]}
{"group_id": 277, "group_size": 6, "items": [{"qid": 683, "question": "How better is proposed method than baselines perpexity wise? in A Multi-Turn Emotionally Engaging Dialog Model", "answer": ["Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set."], "top_k_doc_id": [848, 850, 851, 852, 4444, 5920, 2967, 6651, 7299, 495, 899, 5918, 4127, 1940, 1677], "orig_top_k_doc_id": [852, 850, 848, 851, 4127, 5920, 4444, 2967, 6651, 1940, 7299, 495, 1677, 5918, 899]}, {"qid": 685, "question": "How is human evaluation performed? in A Multi-Turn Emotionally Engaging Dialog Model", "answer": ["(1) grammatical correctness, (2) contextual coherence, (3) emotional appropriateness"], "top_k_doc_id": [848, 850, 851, 852, 4444, 5920, 2967, 6651, 7299, 495, 899, 5918, 101, 6655, 6585], "orig_top_k_doc_id": [852, 851, 848, 850, 2967, 4444, 5920, 6651, 899, 7299, 101, 495, 6655, 6585, 5918]}, {"qid": 686, "question": "Is some other metrics other then perplexity measured? in A Multi-Turn Emotionally Engaging Dialog Model", "answer": ["No"], "top_k_doc_id": [848, 850, 851, 852, 4444, 5920, 2967, 6651, 7299, 3193, 4926, 4441, 1817, 4120, 4445], "orig_top_k_doc_id": [852, 851, 7299, 850, 848, 6651, 4444, 4441, 2967, 5920, 4926, 3193, 1817, 4120, 4445]}, {"qid": 687, "question": "What two baseline models are used? in A Multi-Turn Emotionally Engaging Dialog Model", "answer": [" sequence-to-sequence model (denoted as S2S), HRAN"], "top_k_doc_id": [848, 850, 851, 852, 4444, 5920, 2967, 6651, 7299, 3193, 4926, 495, 4443, 7504, 7842], "orig_top_k_doc_id": [852, 850, 851, 848, 4444, 5920, 7299, 495, 6651, 2967, 4443, 3193, 7504, 7842, 4926]}, {"qid": 684, "question": "How does the multi-turn dialog system learns? in A Multi-Turn Emotionally Engaging Dialog Model", "answer": ["we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution"], "top_k_doc_id": [848, 850, 851, 852, 4444, 5920, 7841, 7840, 849, 5918, 101, 6793, 899, 7504, 898], "orig_top_k_doc_id": [852, 848, 850, 851, 7841, 4444, 7840, 849, 5918, 101, 6793, 899, 7504, 5920, 898]}, {"qid": 4680, "question": "What is the state-of-the-art approach? in Emotional Neural Language Generation Grounded in Situational Contexts", "answer": ["Rashkin et al. BIBREF3 ", "For particular Empathetic-Dialogues corpus released Raskin et al. is state of the art (as well as the baseline) approach. Two terms are used interchangeably in the paper."], "top_k_doc_id": [848, 7299, 494, 6005, 6723, 6345, 558, 495, 2967, 7567, 2970, 5954, 7261, 2414, 5481], "orig_top_k_doc_id": [7299, 494, 6005, 6723, 6345, 848, 558, 495, 2967, 7567, 2970, 5954, 7261, 2414, 5481]}]}
{"group_id": 278, "group_size": 6, "items": [{"qid": 707, "question": "what were the baselines? in Joint Learning of Sentence Embeddings for Relevance and Entailment", "answer": ["RNN model, CNN model , RNN-CNN model, attn1511 model, Deep Averaging Network model, avg mean of word embeddings in the sentence with projection matrix"], "top_k_doc_id": [883, 884, 3198, 3201, 873, 885, 5737, 322, 5735, 6449, 6261, 3096, 3200, 5592, 4806], "orig_top_k_doc_id": [883, 3201, 5735, 3198, 5737, 885, 322, 6261, 873, 3096, 3200, 884, 5592, 6449, 4806]}, {"qid": 710, "question": "what datasets did they use? in Joint Learning of Sentence Embeddings for Relevance and Entailment", "answer": ["Argus Dataset, AI2-8grade/CK12 Dataset, MCTest Dataset"], "top_k_doc_id": [883, 884, 3198, 3201, 873, 885, 5737, 322, 5735, 6449, 2661, 2664, 2899, 4901, 3202], "orig_top_k_doc_id": [883, 873, 3201, 3198, 2661, 884, 2664, 322, 5735, 2899, 5737, 4901, 6449, 885, 3202]}, {"qid": 709, "question": "what is the size of the introduced dataset? in Joint Learning of Sentence Embeddings for Relevance and Entailment", "answer": ["No"], "top_k_doc_id": [883, 884, 3198, 3201, 873, 885, 5737, 4806, 2666, 910, 7652, 912, 6922, 7632, 4264], "orig_top_k_doc_id": [884, 883, 5737, 4806, 885, 873, 2666, 3198, 910, 3201, 7652, 912, 6922, 7632, 4264]}, {"qid": 708, "question": "what is the state of the art for ranking mc test answers? in Joint Learning of Sentence Embeddings for Relevance and Entailment", "answer": ["ensemble of hand-crafted syntactic and frame-semantic features BIBREF16"], "top_k_doc_id": [883, 884, 3198, 3201, 971, 6449, 361, 5590, 5970, 6259, 3116, 5735, 972, 606, 3202], "orig_top_k_doc_id": [884, 883, 3198, 971, 6449, 3201, 361, 5590, 5970, 6259, 3116, 5735, 972, 606, 3202]}, {"qid": 3463, "question": "Do they report results only on English data? in A Question-Entailment Approach to Question Answering", "answer": ["No", "Yes"], "top_k_doc_id": [883, 2661, 5735, 5739, 2662, 5869, 5236, 5022, 5237, 6135, 3096, 5871, 5235, 2584, 4901], "orig_top_k_doc_id": [5735, 2662, 883, 5869, 2661, 5236, 5022, 5237, 6135, 3096, 5871, 5235, 2584, 5739, 4901]}, {"qid": 3464, "question": "What machine learning and deep learning methods are used for RQE? in A Question-Entailment Approach to Question Answering", "answer": ["Logistic Regression, neural networks"], "top_k_doc_id": [883, 2661, 5735, 5739, 5737, 5743, 5741, 5736, 5738, 6256, 5742, 490, 5740, 1547, 6257], "orig_top_k_doc_id": [5735, 5737, 5739, 5743, 5741, 5736, 5738, 6256, 5742, 490, 883, 5740, 1547, 6257, 2661]}]}
{"group_id": 279, "group_size": 6, "items": [{"qid": 720, "question": "What baseline models are offered? in Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset", "answer": ["3-gram and 4-gram conditional language model, Convolution, LSTM models BIBREF27 with and without attention BIBREF28, Transformer, GPT-2"], "top_k_doc_id": [898, 899, 900, 901, 902, 6795, 852, 1473, 3568, 3571, 411, 7842, 4127, 4553, 4444], "orig_top_k_doc_id": [898, 902, 900, 899, 6795, 901, 3571, 411, 7842, 3568, 1473, 852, 4127, 4553, 4444]}, {"qid": 721, "question": "Which six domains are covered in the dataset? in Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset", "answer": ["ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations"], "top_k_doc_id": [898, 899, 900, 901, 902, 6795, 852, 1473, 3568, 3571, 1472, 1135, 2276, 5917, 1134], "orig_top_k_doc_id": [898, 899, 900, 902, 1472, 1135, 3571, 6795, 1473, 901, 3568, 2276, 5917, 852, 1134]}, {"qid": 1112, "question": "What was the criteria for human evaluation? in Few-shot Natural Language Generation for Task-Oriented Dialog", "answer": ["to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness"], "top_k_doc_id": [898, 1471, 1472, 1473, 1474, 1475, 3193, 3679, 5917, 6793, 6063, 3571, 6794, 4545, 2276], "orig_top_k_doc_id": [1473, 1471, 898, 1472, 1474, 1475, 5917, 3193, 6793, 3571, 6794, 6063, 3679, 4545, 2276]}, {"qid": 1113, "question": "What automatic metrics are used to measure performance of the system? in Few-shot Natural Language Generation for Task-Oriented Dialog", "answer": ["BLEU scores and the slot error rate (ERR)"], "top_k_doc_id": [898, 1471, 1472, 1473, 1474, 1475, 3193, 3679, 5917, 6793, 6063, 7476, 7479, 902, 4550], "orig_top_k_doc_id": [1471, 1473, 898, 1475, 1474, 3193, 6793, 5917, 7476, 1472, 7479, 902, 3679, 6063, 4550]}, {"qid": 719, "question": "What is the average number of turns per dialog? in Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset", "answer": ["The average number of utterances per dialog is about 23 "], "top_k_doc_id": [898, 899, 900, 901, 902, 6795, 3773, 6793, 4485, 7842, 102, 4486, 1472, 5918, 5920], "orig_top_k_doc_id": [898, 900, 899, 3773, 6793, 902, 6795, 4485, 7842, 102, 4486, 1472, 901, 5918, 5920]}, {"qid": 1114, "question": "What existing methods is SC-GPT compared to? in Few-shot Natural Language Generation for Task-Oriented Dialog", "answer": ["$({1})$ SC-LSTM BIBREF3, $({2})$ GPT-2 BIBREF6 , $({3})$ HDSA BIBREF7"], "top_k_doc_id": [898, 1471, 1472, 1473, 1474, 1475, 3193, 3679, 5917, 6793, 2146, 4545, 7166, 3273, 6671], "orig_top_k_doc_id": [1471, 1473, 1472, 1475, 1474, 6793, 898, 3193, 2146, 3679, 4545, 7166, 5917, 3273, 6671]}]}
{"group_id": 280, "group_size": 6, "items": [{"qid": 727, "question": "How many annotators are used to write natural language explanations to SNLI-VE-2.0? in e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations", "answer": ["2,060 workers"], "top_k_doc_id": [912, 913, 914, 910, 911, 915, 1144, 6768, 7441, 7442, 2662, 5246, 2661, 871, 2664], "orig_top_k_doc_id": [912, 910, 914, 911, 913, 915, 1144, 7442, 2662, 7441, 6768, 5246, 2661, 871, 2664]}, {"qid": 730, "question": "What is the class with highest error rate in SNLI-VE? in e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations", "answer": ["neutral class"], "top_k_doc_id": [912, 913, 914, 910, 911, 915, 1144, 6768, 7441, 7442, 873, 3325, 5738, 874, 4725], "orig_top_k_doc_id": [910, 911, 912, 914, 915, 913, 7442, 6768, 873, 7441, 3325, 5738, 1144, 874, 4725]}, {"qid": 726, "question": "Is model explanation output evaluated, what metric was used? in e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations", "answer": ["balanced accuracy, i.e., the average of the three accuracies on each class"], "top_k_doc_id": [912, 913, 914, 910, 911, 915, 871, 2065, 3530, 3531, 3532, 290, 7442, 252, 253], "orig_top_k_doc_id": [910, 912, 914, 911, 913, 915, 290, 2065, 7442, 3532, 871, 3531, 3530, 252, 253]}, {"qid": 728, "question": "How many natural language explanations are human-written? in e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations", "answer": ["Totally 6980 validation and test image-sentence pairs have been corrected."], "top_k_doc_id": [912, 913, 914, 910, 911, 915, 871, 2065, 3530, 3531, 3532, 4994, 807, 2662, 329], "orig_top_k_doc_id": [910, 912, 911, 914, 913, 915, 4994, 3530, 871, 807, 2065, 2662, 3532, 329, 3531]}, {"qid": 729, "question": "How much is performance difference of existing model between original and corrected corpus? in e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations", "answer": ["73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set"], "top_k_doc_id": [912, 913, 914, 910, 911, 915, 1144, 6768, 2749, 7331, 5738, 2750, 2748, 5842, 2065], "orig_top_k_doc_id": [912, 910, 911, 914, 913, 915, 2749, 6768, 7331, 5738, 2750, 2748, 5842, 2065, 1144]}, {"qid": 2250, "question": "How do they measure correlation between the prediction and explanation quality? in e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations", "answer": ["They look at the performance accuracy of explanation and the prediction performance"], "top_k_doc_id": [912, 913, 914, 3530, 3531, 3532, 325, 253, 5365, 252, 4497, 809, 808, 4496, 811], "orig_top_k_doc_id": [3530, 3531, 3532, 325, 253, 5365, 252, 4497, 913, 914, 809, 912, 808, 4496, 811]}]}
{"group_id": 281, "group_size": 6, "items": [{"qid": 738, "question": "How are the auxiliary signals from the morphology table incorporated in the decoder? in Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation", "answer": ["an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language."], "top_k_doc_id": [6215, 923, 6216, 398, 649, 650, 919, 920, 921, 922, 1584, 2491, 924, 2494, 925], "orig_top_k_doc_id": [920, 919, 6216, 922, 921, 923, 6215, 924, 2491, 2494, 650, 649, 1584, 925, 398]}, {"qid": 739, "question": "What type of morphological information is contained in the \"morphology table\"? in Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation", "answer": ["target-side affixes"], "top_k_doc_id": [6215, 923, 6216, 398, 649, 650, 919, 920, 921, 922, 1584, 2491, 648, 377, 3904], "orig_top_k_doc_id": [6216, 920, 919, 922, 650, 649, 921, 648, 6215, 398, 1584, 923, 2491, 377, 3904]}, {"qid": 864, "question": "Where does the vocabulary come from? in Word, Subword or Character? An Empirical Study of Granularity in Chinese-English NMT", "answer": ["LDC corpus"], "top_k_doc_id": [6215, 648, 831, 1115, 1116, 1117, 1118, 1119, 1370, 1373, 1584, 649, 650, 4387, 1583], "orig_top_k_doc_id": [1119, 1115, 1117, 1584, 1118, 1116, 6215, 1370, 648, 1373, 650, 4387, 649, 831, 1583]}, {"qid": 866, "question": "What dataset did they use? in Word, Subword or Character? An Empirical Study of Granularity in Chinese-English NMT", "answer": ["LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)"], "top_k_doc_id": [6215, 648, 831, 1115, 1116, 1117, 1118, 1119, 1370, 1373, 1584, 649, 650, 4387, 5837], "orig_top_k_doc_id": [1115, 1119, 1117, 1116, 1118, 1584, 6215, 1370, 648, 1373, 4387, 650, 5837, 649, 831]}, {"qid": 865, "question": "What is the worst performing translation granularity? in Word, Subword or Character? An Empirical Study of Granularity in Chinese-English NMT", "answer": ["No"], "top_k_doc_id": [6215, 648, 831, 1115, 1116, 1117, 1118, 1119, 1370, 1373, 1584, 6730, 829, 830, 7655], "orig_top_k_doc_id": [1115, 1119, 1117, 1118, 1116, 1584, 6215, 831, 1370, 1373, 648, 6730, 829, 830, 7655]}, {"qid": 3843, "question": "what is the previous work they are comparing to? in Improved English to Russian Translation by Neural Suffix Prediction", "answer": ["RNN and Transformer baseline systems utilize BPE BIBREF3, fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work", "Subword based NMT, Character-based NMT", "RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides"], "top_k_doc_id": [6215, 923, 6216, 6218, 6219, 2494, 4714, 6536, 6591, 6592, 6828, 6217, 648, 5702, 6283], "orig_top_k_doc_id": [6215, 6218, 6216, 6219, 923, 2494, 4714, 6536, 6591, 6592, 6828, 6217, 648, 5702, 6283]}]}
{"group_id": 282, "group_size": 6, "items": [{"qid": 762, "question": "What languages do they experiment with? in Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering", "answer": ["Chinese"], "top_k_doc_id": [2910, 957, 1106, 7351, 1091, 1109, 2911, 4841, 6256, 1090, 2234, 6847, 2210, 510, 2661], "orig_top_k_doc_id": [2910, 7351, 1091, 957, 4841, 2911, 6256, 1090, 1109, 2234, 1106, 2210, 6847, 510, 2661]}, {"qid": 763, "question": "What are the baselines? in Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering", "answer": ["MemN2N BIBREF12, Attentive and Impatient Readers BIBREF6"], "top_k_doc_id": [2910, 957, 1106, 7351, 1091, 1109, 2911, 4841, 6256, 1090, 2234, 6847, 352, 958, 1110], "orig_top_k_doc_id": [957, 2910, 7351, 6847, 4841, 1106, 1109, 352, 2911, 6256, 1091, 2234, 958, 1110, 1090]}, {"qid": 765, "question": "Did they use a crowdsourcing platform? in Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering", "answer": ["No"], "top_k_doc_id": [2910, 957, 1106, 7351, 1091, 1109, 2911, 4841, 6256, 1290, 510, 5231, 958, 3451, 883], "orig_top_k_doc_id": [2910, 7351, 957, 4841, 2911, 1290, 1109, 1106, 510, 5231, 6256, 958, 3451, 883, 1091]}, {"qid": 1947, "question": "How much more accurate is the model than the baseline? in Conclusion-Supplement Answer Generation for Non-Factoid Questions", "answer": ["For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. "], "top_k_doc_id": [2910, 2911, 2913, 2914, 2915, 2916, 2912, 2210, 3809, 7351, 6449, 7161, 1091, 491, 4463], "orig_top_k_doc_id": [2910, 2915, 2914, 2913, 2911, 2916, 2912, 2210, 3809, 7351, 6449, 7161, 1091, 491, 4463]}, {"qid": 2219, "question": "Does CLSTM have any benefits over BERT? in Contextual LSTM (CLSTM) models for Large scale NLP tasks", "answer": ["No"], "top_k_doc_id": [2910, 2911, 2913, 2914, 2915, 3473, 3467, 3468, 3470, 3471, 3469, 3472, 7318, 4590, 436], "orig_top_k_doc_id": [3473, 3467, 3468, 3470, 3471, 3469, 3472, 2914, 2913, 2915, 7318, 2911, 2910, 4590, 436]}, {"qid": 2504, "question": "How much does HAS-QA improve over baselines? in HAS-QA: Hierarchical Answer Spans Model for Open-domain Question Answering", "answer": ["For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. , For example, in QuasarT, it improves 4.6% in EM score and 3.5% in F1 score."], "top_k_doc_id": [2910, 957, 1106, 7351, 4250, 4245, 352, 4698, 1145, 5472, 2268, 1141, 2519, 5735, 1146], "orig_top_k_doc_id": [4250, 1106, 4245, 7351, 352, 2910, 4698, 1145, 5472, 2268, 1141, 2519, 5735, 1146, 957]}]}
{"group_id": 283, "group_size": 6, "items": [{"qid": 901, "question": "How do they measure the diversity of inferences? in Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "answer": ["by number of distinct n-grams"], "top_k_doc_id": [1159, 4273, 1156, 1157, 1158, 1160, 4278, 1136, 1512, 1819, 1673, 285, 5646, 1768, 3973], "orig_top_k_doc_id": [1156, 1159, 1157, 1160, 4273, 1158, 4278, 1673, 285, 1136, 1819, 1512, 5646, 1768, 3973]}, {"qid": 902, "question": "By how much do they improve the accuracy of inferences over state-of-the-art methods? in Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "answer": ["ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively."], "top_k_doc_id": [1159, 4273, 1156, 1157, 1158, 1160, 4278, 1136, 1512, 1819, 4277, 945, 4994, 4276, 2746], "orig_top_k_doc_id": [1156, 1159, 4273, 4278, 1157, 1160, 1158, 4277, 1819, 945, 4994, 1136, 4276, 1512, 2746]}, {"qid": 903, "question": "Which models do they use as baselines on the Atomic dataset? in Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "answer": ["RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained"], "top_k_doc_id": [1159, 4273, 1156, 1157, 1158, 1160, 4278, 945, 4277, 7518, 7519, 7520, 5646, 4276, 1819], "orig_top_k_doc_id": [1156, 1159, 1157, 1158, 1160, 4273, 7518, 4278, 4277, 7519, 5646, 4276, 945, 1819, 7520]}, {"qid": 905, "question": "What is the size of the Atomic dataset? in Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "answer": ["No"], "top_k_doc_id": [1159, 4273, 1156, 1157, 1158, 1160, 4278, 945, 4277, 7518, 7519, 7520, 3973, 3972, 1822], "orig_top_k_doc_id": [1156, 1159, 1157, 1158, 1160, 7518, 4273, 3973, 7519, 3972, 4278, 7520, 4277, 1822, 945]}, {"qid": 904, "question": "How does the context-aware variational autoencoder learn event background information? in Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder", "answer": [" CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target."], "top_k_doc_id": [1159, 4273, 1156, 1157, 1158, 1160, 6433, 1768, 3739, 6959, 285, 5282, 6427, 3740, 1940], "orig_top_k_doc_id": [1156, 1159, 1157, 1160, 1158, 6433, 1768, 3739, 6959, 285, 5282, 6427, 3740, 1940, 4273]}, {"qid": 2509, "question": "Do they consider other tasks? in KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning", "answer": ["No"], "top_k_doc_id": [1159, 4273, 4277, 4278, 4274, 4276, 2101, 7518, 945, 1819, 7514, 160, 2194, 4994, 690], "orig_top_k_doc_id": [4277, 4273, 4278, 4274, 4276, 2101, 1159, 7518, 945, 1819, 7514, 160, 2194, 4994, 690]}]}
{"group_id": 284, "group_size": 6, "items": [{"qid": 953, "question": "What is result of their attention distribution analysis? in On Leveraging the Visual Modality for Neural Machine Translation", "answer": ["visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings"], "top_k_doc_id": [1237, 1238, 112, 4755, 4756, 5966, 7138, 1239, 2927, 3655, 3658, 4758, 4759, 116, 7143], "orig_top_k_doc_id": [1237, 4756, 1238, 7138, 112, 4755, 3655, 4759, 4758, 3658, 5966, 2927, 1239, 116, 7143]}, {"qid": 954, "question": "What is result of their Principal Component Analysis? in On Leveraging the Visual Modality for Neural Machine Translation", "answer": ["existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT"], "top_k_doc_id": [1237, 1238, 112, 4755, 4756, 5966, 7138, 1239, 2927, 3655, 3658, 4758, 4759, 5708, 3175], "orig_top_k_doc_id": [1238, 1237, 4756, 7138, 3655, 4758, 5966, 112, 1239, 4755, 5708, 3175, 4759, 3658, 2927]}, {"qid": 1144, "question": "What metrics are used in challenge? in Modality-Balanced Models for Visual Dialogue", "answer": ["NDCG, MRR, recall@k, mean rank"], "top_k_doc_id": [1237, 1238, 525, 1527, 1531, 4759, 575, 576, 788, 4756, 4758, 578, 2803, 2417, 1529], "orig_top_k_doc_id": [576, 1527, 1531, 788, 4756, 578, 2803, 2417, 1237, 4758, 1238, 575, 525, 1529, 4759]}, {"qid": 1148, "question": "How big is dataset for this challenge? in Modality-Balanced Models for Visual Dialogue", "answer": ["133,287 images"], "top_k_doc_id": [1237, 1238, 525, 1527, 1531, 4759, 575, 576, 788, 4756, 4758, 5793, 4755, 1239, 2413], "orig_top_k_doc_id": [576, 1527, 1531, 525, 1238, 788, 4756, 5793, 575, 1237, 4758, 4755, 4759, 1239, 2413]}, {"qid": 955, "question": "What are 3 novel fusion techniques that are proposed? in On Leveraging the Visual Modality for Neural Machine Translation", "answer": ["Step-Wise Decoder Fusion, Multimodal Attention Modulation, Visual-Semantic (VS) Regularizer"], "top_k_doc_id": [1237, 1238, 112, 4755, 4756, 5966, 7138, 7141, 7142, 3175, 2116, 2805, 6686, 2121, 493], "orig_top_k_doc_id": [1237, 7138, 1238, 4755, 4756, 7141, 7142, 3175, 2116, 112, 2805, 5966, 6686, 2121, 493]}, {"qid": 1147, "question": "Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters? in Modality-Balanced Models for Visual Dialogue", "answer": ["ensemble model"], "top_k_doc_id": [1237, 1238, 525, 1527, 1531, 4759, 1529, 1530, 7141, 7142, 7143, 7138, 1239, 6020, 2116], "orig_top_k_doc_id": [1531, 1529, 1527, 1530, 525, 7141, 7142, 1238, 7143, 7138, 1239, 4759, 6020, 1237, 2116]}]}
{"group_id": 285, "group_size": 6, "items": [{"qid": 1057, "question": "Does the analysis find that coalitions are formed in the same way for different policy areas? in Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities", "answer": ["No"], "top_k_doc_id": [1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 2074, 2076, 2667], "orig_top_k_doc_id": [1377, 1388, 1387, 1382, 1379, 1385, 1384, 1378, 1386, 1381, 1383, 1380, 2074, 2667, 2076]}, {"qid": 1058, "question": "What insights does the analysis give about the cohesion of political groups in the European parliament? in Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities", "answer": ["Greens-EFA, S&D, and EPP exhibit the highest cohesion, non-aligned members NI have the lowest cohesion, followed by EFDD and ENL, two methods disagree is the level of cohesion of GUE-NGL"], "top_k_doc_id": [1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 2074, 2076, 3335], "orig_top_k_doc_id": [1377, 1388, 1382, 1379, 1384, 1387, 1386, 1378, 1383, 1381, 1385, 1380, 2076, 2074, 3335]}, {"qid": 1055, "question": "Do the authors mention any possible confounds in their study? in Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities", "answer": ["Yes"], "top_k_doc_id": [1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 2074, 4884, 2075], "orig_top_k_doc_id": [1379, 1377, 1388, 1382, 1384, 1378, 1387, 1381, 1380, 1385, 1386, 1383, 2074, 4884, 2075]}, {"qid": 1056, "question": "What is the relationship between the co-voting and retweeting patterns? in Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities", "answer": ["we observe a positive correlation between retweeting and co-voting, strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets, Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union, significantly negative coefficient, is the area Economic and monetary system"], "top_k_doc_id": [1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 2074, 4884, 4885], "orig_top_k_doc_id": [1377, 1388, 1379, 1382, 1387, 1378, 1386, 1384, 1380, 1381, 1385, 1383, 2074, 4885, 4884]}, {"qid": 1059, "question": "Do they authors account for differences in usage of Twitter amongst MPs into their model? in Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities", "answer": ["No"], "top_k_doc_id": [1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 2074, 2076, 4885], "orig_top_k_doc_id": [1379, 1377, 1388, 1382, 1384, 1378, 1387, 1385, 1381, 1380, 1386, 1383, 2074, 2076, 4885]}, {"qid": 1060, "question": "Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile? in Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities", "answer": ["No"], "top_k_doc_id": [1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1482, 2076, 5468], "orig_top_k_doc_id": [1377, 1379, 1388, 1382, 1378, 1387, 1384, 1380, 1386, 1381, 1385, 1383, 1482, 2076, 5468]}]}
{"group_id": 286, "group_size": 6, "items": [{"qid": 1071, "question": "How many languages are included in the tweets? in Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter", "answer": ["No"], "top_k_doc_id": [1419, 1420, 1421, 2402, 6804, 2535, 3795, 982, 3527, 5273, 4989, 6558, 5973, 4990, 7456], "orig_top_k_doc_id": [1419, 1421, 1420, 2402, 982, 3527, 2535, 5273, 4989, 6558, 6804, 5973, 4990, 7456, 3795]}, {"qid": 1073, "question": "Which countries did they look at? in Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter", "answer": ["No"], "top_k_doc_id": [1419, 1420, 1421, 2402, 6804, 2535, 3795, 333, 5323, 2539, 4003, 3964, 3965, 2080, 520], "orig_top_k_doc_id": [1419, 1420, 1421, 2535, 2402, 333, 5323, 6804, 3795, 2539, 4003, 3964, 3965, 2080, 520]}, {"qid": 1760, "question": "What model do they train? in Towards Real-Time, Country-Level Location Classification of Worldwide Tweets", "answer": ["Support Vector Machines (SVM), Gaussian Naive Bayes, Multinomial Naive Bayes, Decision Trees, Random Forests and a Maximum Entropy classifier"], "top_k_doc_id": [1419, 2533, 2534, 2535, 2536, 2537, 2541, 2542, 2543, 3007, 2538, 2827, 7030, 2402, 6155], "orig_top_k_doc_id": [2535, 2533, 2534, 2536, 1419, 2542, 2827, 3007, 2541, 2543, 2537, 7030, 2402, 2538, 6155]}, {"qid": 1761, "question": "What are the eight features mentioned? in Towards Real-Time, Country-Level Location Classification of Worldwide Tweets", "answer": ["User location (uloc), User language (ulang), Timezone (tz), Tweet language (tlang), Offset (offset), User name (name), User description (description), Tweet content (content)"], "top_k_doc_id": [1419, 2533, 2534, 2535, 2536, 2537, 2541, 2542, 2543, 3007, 2538, 2827, 7030, 2540, 2539], "orig_top_k_doc_id": [2533, 2535, 2536, 2537, 2534, 2540, 2542, 2827, 2538, 2541, 1419, 2539, 7030, 3007, 2543]}, {"qid": 1072, "question": "What languages are explored? in Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter", "answer": ["No"], "top_k_doc_id": [1419, 1420, 1421, 2402, 6804, 3527, 3964, 4184, 1187, 3007, 3963, 982, 7307, 82, 6344], "orig_top_k_doc_id": [1419, 1421, 1420, 3527, 3964, 4184, 1187, 3007, 3963, 6804, 982, 2402, 7307, 82, 6344]}, {"qid": 1762, "question": "How many languages are considered in the experiments? in Towards Real-Time, Country-Level Location Classification of Worldwide Tweets", "answer": ["No"], "top_k_doc_id": [1419, 2533, 2534, 2535, 2536, 2537, 2541, 2542, 2543, 3007, 5291, 2402, 3008, 2540, 6833], "orig_top_k_doc_id": [2535, 2533, 2534, 1419, 2536, 2542, 3007, 2537, 2541, 2543, 5291, 2402, 3008, 2540, 6833]}]}
{"group_id": 287, "group_size": 6, "items": [{"qid": 1162, "question": "What DCGs are used? in AandP: Utilizing Prolog for converting between active sentence and passive sentence with three-steps conversion", "answer": ["Author's own DCG rules are defined from scratch."], "top_k_doc_id": [1556, 1557, 1558, 1559, 2807, 5429, 1748, 2748, 2749, 1747, 2962, 7707, 1187, 7197, 20], "orig_top_k_doc_id": [1556, 1559, 1558, 1557, 5429, 1187, 1747, 2807, 1748, 2748, 7707, 2962, 2749, 7197, 20]}, {"qid": 1164, "question": "What is used for evaluation of this approach? in AandP: Utilizing Prolog for converting between active sentence and passive sentence with three-steps conversion", "answer": ["No"], "top_k_doc_id": [1556, 1557, 1558, 1559, 2807, 5429, 1748, 2748, 2749, 1747, 2962, 7707, 1187, 5862, 3799], "orig_top_k_doc_id": [1556, 1559, 1558, 1557, 5429, 1747, 1187, 2748, 2962, 2807, 1748, 7707, 2749, 5862, 3799]}, {"qid": 1166, "question": "Are there some experiments performed in the paper? in AandP: Utilizing Prolog for converting between active sentence and passive sentence with three-steps conversion", "answer": ["No"], "top_k_doc_id": [1556, 1557, 1558, 1559, 2807, 5429, 1748, 2748, 2749, 1747, 2962, 7707, 1355, 1059, 1354], "orig_top_k_doc_id": [1556, 1559, 1558, 1557, 2748, 5429, 1355, 1059, 1354, 1748, 2807, 2749, 7707, 1747, 2962]}, {"qid": 1161, "question": "Is there a machine learning approach that tries to solve same problem? in AandP: Utilizing Prolog for converting between active sentence and passive sentence with three-steps conversion", "answer": ["No"], "top_k_doc_id": [1556, 1557, 1558, 1559, 2807, 5429, 1748, 2748, 2749, 273, 6397, 7449, 1355, 4615, 3821], "orig_top_k_doc_id": [1556, 1559, 1558, 1557, 2748, 5429, 273, 6397, 2807, 1748, 2749, 7449, 1355, 4615, 3821]}, {"qid": 1163, "question": "What else is tried to be solved other than 12 tenses, model verbs and negative form? in AandP: Utilizing Prolog for converting between active sentence and passive sentence with three-steps conversion", "answer": ["cases of singular/plural, subject pronoun/object pronoun, etc."], "top_k_doc_id": [1556, 1557, 1558, 1559, 2807, 5429, 223, 1187, 7707, 1213, 4335, 273, 3307, 4337, 1854], "orig_top_k_doc_id": [1556, 1559, 1558, 1557, 7707, 1213, 2807, 4335, 223, 273, 3307, 1187, 4337, 5429, 1854]}, {"qid": 1165, "question": "Is there information about performance of these conversion methods? in AandP: Utilizing Prolog for converting between active sentence and passive sentence with three-steps conversion", "answer": ["No"], "top_k_doc_id": [1556, 1557, 1558, 1559, 2807, 5429, 223, 1187, 7707, 225, 5862, 4058, 6318, 1354, 20], "orig_top_k_doc_id": [1556, 1559, 1558, 1557, 223, 7707, 225, 5862, 1187, 4058, 6318, 1354, 20, 5429, 2807]}]}
{"group_id": 288, "group_size": 6, "items": [{"qid": 1171, "question": "Do they predict the sentiment of the review summary? in Exploring Hierarchical Interaction Between Review and Summary for Better Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [7465, 7466, 1565, 1566, 1567, 1568, 1569, 2337, 3101, 2119, 6181, 6472, 893, 7761, 6570], "orig_top_k_doc_id": [1565, 1568, 1569, 3101, 1566, 1567, 7465, 893, 2119, 6472, 7466, 2337, 7761, 6181, 6570]}, {"qid": 1173, "question": "Which review dataset do they use? in Exploring Hierarchical Interaction Between Review and Summary for Better Sentiment Analysis", "answer": ["SNAP (Stanford Network Analysis Project)"], "top_k_doc_id": [7465, 7466, 1565, 1566, 1567, 1568, 1569, 2337, 3101, 2119, 6181, 6472, 3752, 7336, 5898], "orig_top_k_doc_id": [1565, 1568, 1569, 1567, 3101, 2119, 7466, 1566, 7465, 6472, 3752, 7336, 2337, 6181, 5898]}, {"qid": 4795, "question": "What kind of baseline model do they compare against? in Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation", "answer": ["Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16", "rating-based collaborative filtering methods, state-of-the-art methods that leverage the semantic information in reviews"], "top_k_doc_id": [7465, 7466, 6547, 6549, 7468, 7469, 7470, 7471, 1948, 1956, 2850, 4292, 3020, 3019, 4199], "orig_top_k_doc_id": [7465, 7466, 7469, 1948, 3020, 3019, 6549, 7471, 2850, 1956, 7468, 4199, 7470, 6547, 4292]}, {"qid": 4797, "question": "Which set of datasets do they use? in Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation", "answer": ["9 Amazon product review datasets for 9 different domains, large-scale Yelp challenge dataset on restaurant reviews", "9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews"], "top_k_doc_id": [7465, 7466, 6547, 6549, 7468, 7469, 7470, 7471, 1948, 1956, 2850, 4292, 1949, 5506, 730], "orig_top_k_doc_id": [7465, 7466, 7469, 7471, 7470, 6547, 1948, 1949, 2850, 7468, 6549, 5506, 4292, 1956, 730]}, {"qid": 1172, "question": "What is the performance difference of using a generated summary vs. a user-written one? in Exploring Hierarchical Interaction Between Review and Summary for Better Sentiment Analysis", "answer": ["2.7 accuracy points"], "top_k_doc_id": [7465, 7466, 1565, 1566, 1567, 1568, 1569, 2337, 3101, 6570, 5405, 6568, 2340, 6692, 5508], "orig_top_k_doc_id": [1565, 1568, 1567, 3101, 1569, 7465, 6570, 5405, 6568, 1566, 2340, 7466, 6692, 2337, 5508]}, {"qid": 4796, "question": "Do they analyze which types of sentences/reviews are useful or not? in Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation", "answer": ["Yes", "No"], "top_k_doc_id": [7465, 7466, 6547, 6549, 7468, 7469, 7470, 7471, 3102, 3101, 7467, 5089, 3105, 6692, 597], "orig_top_k_doc_id": [7465, 7466, 7470, 6547, 7468, 7469, 7471, 3102, 6549, 3101, 7467, 5089, 3105, 6692, 597]}]}
{"group_id": 289, "group_size": 6, "items": [{"qid": 1183, "question": "Do they test their approach on large-resource tasks? in Multilingual Speech Recognition with Corpus Relatedness Sampling", "answer": ["Yes"], "top_k_doc_id": [5564, 6031, 45, 1593, 1594, 1595, 1596, 6310, 3617, 3618, 3691, 994, 381, 5702, 3439], "orig_top_k_doc_id": [1593, 1596, 1594, 1595, 45, 3617, 6031, 3618, 994, 381, 6310, 3691, 5702, 3439, 5564]}, {"qid": 1184, "question": "By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks? in Multilingual Speech Recognition with Corpus Relatedness Sampling", "answer": ["1.6% lower phone error rate on average"], "top_k_doc_id": [5564, 6031, 45, 1593, 1594, 1595, 1596, 6310, 3617, 3618, 3691, 3621, 50, 5566, 6032], "orig_top_k_doc_id": [1593, 1596, 1594, 1595, 3617, 6031, 3618, 3691, 3621, 5564, 50, 45, 5566, 6310, 6032]}, {"qid": 1198, "question": "How is validation of the data performed? in Common Voice: A Massively-Multilingual Speech Corpus", "answer": ["A maximum of three contributors will listen to any audio clip. If an $<$audio,transcript$>$ pair first receives two up-votes, then the clip is marked as valid. If instead the clip first receives two down-votes, then it is marked as invalid."], "top_k_doc_id": [5564, 6031, 48, 1622, 1623, 1624, 3402, 5715, 7537, 7325, 50, 6190, 5712, 2627, 7172], "orig_top_k_doc_id": [1624, 1622, 1623, 5564, 6031, 7325, 5715, 50, 6190, 3402, 5712, 2627, 48, 7172, 7537]}, {"qid": 1199, "question": "Is audio data per language balanced in dataset? in Common Voice: A Massively-Multilingual Speech Corpus", "answer": ["No"], "top_k_doc_id": [5564, 6031, 48, 1622, 1623, 1624, 3402, 5715, 7537, 3264, 595, 3266, 3265, 6032, 3438], "orig_top_k_doc_id": [1622, 1623, 6031, 1624, 5564, 5715, 3264, 595, 3266, 48, 7537, 3265, 3402, 6032, 3438]}, {"qid": 1185, "question": "How do they compute corpus-level embeddings? in Multilingual Speech Recognition with Corpus Relatedness Sampling", "answer": ["First, the embedding matrix INLINEFORM4 for all corpora is initialized, during the training phase, INLINEFORM9 can be used to bias the input feature, Next, we apply the language specific softmax to compute logits INLINEFORM4 and optimize them with the CTC objective"], "top_k_doc_id": [5564, 6031, 45, 1593, 1594, 1595, 1596, 6310, 7688, 994, 247, 381, 1773, 1622, 1624], "orig_top_k_doc_id": [1593, 1596, 1595, 1594, 7688, 994, 247, 5564, 381, 1773, 45, 1622, 1624, 6310, 6031]}, {"qid": 1197, "question": "What crowdsourcing platform is used for data collection and data validation? in Common Voice: A Massively-Multilingual Speech Corpus", "answer": ["the Common Voice website,  iPhone app"], "top_k_doc_id": [5564, 6031, 48, 1622, 1623, 1624, 3125, 6190, 7222, 5911, 3587, 1075, 6468, 5391, 6995], "orig_top_k_doc_id": [1622, 1623, 5564, 3125, 1624, 6190, 7222, 5911, 6031, 3587, 1075, 6468, 48, 5391, 6995]}]}
{"group_id": 290, "group_size": 6, "items": [{"qid": 1200, "question": "What is the performance of their model? in Weakly Supervised Domain Detection", "answer": ["No"], "top_k_doc_id": [4130, 4135, 1625, 2165, 2166, 5116, 5117, 65, 2162, 6378, 1539, 6379, 3912, 1658, 3916], "orig_top_k_doc_id": [5117, 1625, 4135, 2166, 4130, 5116, 2165, 2162, 6378, 1539, 6379, 3912, 1658, 3916, 65]}, {"qid": 1202, "question": "What domains are detected in this paper? in Weakly Supervised Domain Detection", "answer": ["Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: \u201cBusiness and Commerce\u201d (BUS), \u201cGovernment and Politics\u201d (GOV), \u201cPhysical and Mental Health\u201d (HEA), \u201cLaw and Order\u201d (LAW),\n\u201cLifestyle\u201d (LIF), \u201cMilitary\u201d (MIL), and \u201cGeneral Purpose\u201d (GEN). Exceptionally, GEN does\nnot have a natural root category."], "top_k_doc_id": [4130, 4135, 1625, 2165, 2166, 5116, 5117, 65, 6172, 4131, 2306, 6881, 571, 6584, 1626], "orig_top_k_doc_id": [1625, 4130, 5117, 6172, 4135, 2166, 4131, 2306, 2165, 6881, 571, 65, 5116, 6584, 1626]}, {"qid": 2465, "question": "What is the training and test data used? in Determining the Scale of Impact from Denial-of-Service Attacks in Real Time Using Twitter", "answer": ["Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo."], "top_k_doc_id": [4130, 4135, 4131, 4132, 447, 2076, 4113, 441, 7772, 5930, 6140, 448, 7256, 4205, 1711], "orig_top_k_doc_id": [4130, 4131, 4135, 4113, 2076, 441, 7772, 5930, 4132, 6140, 448, 447, 7256, 4205, 1711]}, {"qid": 2466, "question": "Was performance of the weakly-supervised model compared to the performance of a supervised model? in Determining the Scale of Impact from Denial-of-Service Attacks in Real Time Using Twitter", "answer": ["Yes"], "top_k_doc_id": [4130, 4135, 4131, 4132, 447, 2076, 6378, 3916, 3566, 7862, 4133, 6376, 2165, 6379, 413], "orig_top_k_doc_id": [4130, 4131, 4135, 4132, 2076, 6378, 3916, 3566, 7862, 4133, 6376, 2165, 6379, 447, 413]}, {"qid": 1201, "question": "Which text genres did they experiment with? in Weakly Supervised Domain Detection", "answer": ["No"], "top_k_doc_id": [4130, 4135, 1625, 2165, 2166, 5116, 5117, 1847, 873, 1628, 6376, 4046, 4131, 6656, 617], "orig_top_k_doc_id": [1625, 4135, 5117, 5116, 1847, 873, 2166, 4130, 1628, 2165, 6376, 4046, 4131, 6656, 617]}, {"qid": 2464, "question": "Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption? in Determining the Scale of Impact from Denial-of-Service Attacks in Real Time Using Twitter", "answer": ["The dataset contains about 590 tweets about DDos attacks."], "top_k_doc_id": [4130, 4135, 4131, 4132, 4133, 7772, 7029, 413, 2533, 4111, 6632, 6804, 330, 331, 4112], "orig_top_k_doc_id": [4130, 4131, 4132, 4135, 4133, 7772, 7029, 413, 2533, 4111, 6632, 6804, 330, 331, 4112]}]}
{"group_id": 291, "group_size": 6, "items": [{"qid": 1205, "question": "Do they literally just treat this as \"predict the next spell that appears in the text\"? in Harry Potter and the Action Prediction Challenge from Natural Language", "answer": ["Yes"], "top_k_doc_id": [5849, 101, 1630, 1631, 3443, 4878, 6068, 319, 6250, 5609, 29, 4075, 745, 575, 4074], "orig_top_k_doc_id": [1630, 101, 1631, 4878, 6068, 3443, 5609, 29, 4075, 6250, 745, 575, 5849, 4074, 319]}, {"qid": 1206, "question": "How well does a simple bag-of-words baseline do? in Harry Potter and the Action Prediction Challenge from Natural Language", "answer": ["No"], "top_k_doc_id": [5849, 101, 1630, 1631, 3443, 4878, 6068, 319, 6250, 316, 537, 5683, 6246, 5579, 315], "orig_top_k_doc_id": [1630, 3443, 6068, 5849, 4878, 101, 1631, 316, 319, 6250, 537, 5683, 6246, 5579, 315]}, {"qid": 1204, "question": "Isn't simple word association enough to predict the next spell? in Harry Potter and the Action Prediction Challenge from Natural Language", "answer": ["No"], "top_k_doc_id": [5849, 101, 1630, 1631, 3443, 4878, 6068, 7640, 2276, 7641, 1674, 315, 4667, 2623, 316], "orig_top_k_doc_id": [1630, 101, 4878, 1631, 6068, 3443, 7640, 2276, 7641, 5849, 1674, 315, 4667, 2623, 316]}, {"qid": 1203, "question": "Why do they think this task is hard?  What is the baseline performance? in Harry Potter and the Action Prediction Challenge from Natural Language", "answer": ["1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.\n2. Macro F1 = 14.6 (MLR, length 96 snippet)\nWeighted F1 = 31.1 (LSTM, length 128 snippet)"], "top_k_doc_id": [5849, 101, 1630, 1631, 3443, 4878, 6068, 1170, 5906, 4550, 5157, 5246, 2276, 3972, 3530], "orig_top_k_doc_id": [1630, 4878, 6068, 1631, 5849, 101, 1170, 3443, 5906, 4550, 5157, 5246, 2276, 3972, 3530]}, {"qid": 3541, "question": "How do they split text to obtain sentence levels? in Query-based Attention CNN for Text Similarity Map", "answer": ["No"], "top_k_doc_id": [5849, 3034, 3199, 3840, 5847, 5848, 6257, 6258, 3198, 884, 7335, 38, 6334, 5333, 6119], "orig_top_k_doc_id": [5847, 5848, 6258, 6257, 5849, 3199, 3840, 3198, 884, 3034, 7335, 38, 6334, 5333, 6119]}, {"qid": 3542, "question": "Do they experiment with their proposed model on any other dataset other than MovieQA? in Query-based Attention CNN for Text Similarity Map", "answer": ["Yes", "Yes"], "top_k_doc_id": [5849, 3034, 3199, 3840, 5847, 5848, 6257, 6258, 3841, 3842, 2210, 2852, 5810, 7126, 120], "orig_top_k_doc_id": [5848, 5849, 5847, 3840, 3841, 3842, 6257, 2210, 3199, 6258, 3034, 2852, 5810, 7126, 120]}]}
{"group_id": 292, "group_size": 6, "items": [{"qid": 1230, "question": "How much better were results of the proposed models than base LSTM-RNN model? in AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses", "answer": ["on diversity 6.87 and on relevance 4.6 points higher"], "top_k_doc_id": [1669, 1673, 1670, 1671, 1672, 2440, 7299, 7300, 1768, 6141, 7372, 6750, 6543, 488, 7543], "orig_top_k_doc_id": [1673, 1669, 1670, 1672, 1671, 6750, 6543, 7300, 488, 7299, 6141, 1768, 2440, 7372, 7543]}, {"qid": 1231, "question": "Which one of the four proposed models performed best? in AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses", "answer": ["the hybrid model MinAvgOut + RL"], "top_k_doc_id": [1669, 1673, 1670, 1671, 1672, 2440, 7299, 7300, 1768, 6141, 7372, 5748, 6794, 4731, 1471], "orig_top_k_doc_id": [1673, 1669, 1670, 1672, 1671, 7300, 7299, 5748, 2440, 7372, 6141, 1768, 6794, 4731, 1471]}, {"qid": 1228, "question": "To what other competitive baselines is this approach compared? in AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses", "answer": ["LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL"], "top_k_doc_id": [1669, 1673, 1670, 1671, 1672, 2440, 7299, 7300, 6610, 5487, 7371, 4731, 3193, 6391, 6705], "orig_top_k_doc_id": [1669, 1673, 1670, 1672, 1671, 7300, 7299, 6610, 2440, 5487, 7371, 4731, 3193, 6391, 6705]}, {"qid": 1229, "question": "How is human evaluation performed, what was the criteria? in AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses", "answer": ["Through Amazon MTurk annotators to determine plausibility and content richness of the response"], "top_k_doc_id": [1669, 1673, 1670, 1671, 1672, 2440, 7299, 7300, 2969, 2438, 1866, 2970, 1729, 823, 5748], "orig_top_k_doc_id": [1673, 1669, 1672, 1670, 1671, 7300, 2969, 2438, 1866, 2970, 1729, 2440, 823, 7299, 5748]}, {"qid": 2298, "question": "Do they only test on one dataset? in Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU", "answer": ["Yes"], "top_k_doc_id": [1669, 1673, 28, 3652, 3687, 5166, 7011, 7600, 7057, 6582, 4908, 2275, 6991, 5283, 2887], "orig_top_k_doc_id": [3652, 28, 1669, 3687, 7600, 7057, 5166, 6582, 4908, 2275, 1673, 6991, 5283, 2887, 7011]}, {"qid": 2299, "question": "What baseline decoder do they use? in Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU", "answer": ["a standard beam search decoder BIBREF5 with several straightforward performance optimizations"], "top_k_doc_id": [1669, 1673, 28, 3652, 3687, 5166, 7011, 7600, 6975, 4212, 3137, 1768, 3781, 7014, 2494], "orig_top_k_doc_id": [3652, 1669, 28, 5166, 1673, 6975, 4212, 7011, 3137, 1768, 3687, 7600, 3781, 7014, 2494]}]}
{"group_id": 293, "group_size": 6, "items": [{"qid": 1268, "question": "Does model uses pretrained Transformer encoders? in Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss", "answer": ["No"], "top_k_doc_id": [1741, 1742, 1739, 1740, 1769, 1768, 4178, 5938, 5941, 6044, 5678, 6045, 5710, 5184, 2529], "orig_top_k_doc_id": [1739, 1741, 1742, 1740, 1769, 5678, 4178, 6044, 6045, 5710, 5184, 1768, 5941, 5938, 2529]}, {"qid": 1269, "question": "What was previous state of the art model? in Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss", "answer": ["LSTM-based RNN-T"], "top_k_doc_id": [1741, 1742, 1739, 1740, 1769, 1768, 4178, 5938, 5941, 6044, 2485, 5724, 3405, 6968, 2484], "orig_top_k_doc_id": [1739, 1742, 1741, 1740, 1769, 2485, 5724, 3405, 6968, 1768, 5938, 4178, 5941, 2484, 6044]}, {"qid": 1270, "question": "What was previous state of the art accuracy on LibriSpeech benchmark? in Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss", "answer": ["No"], "top_k_doc_id": [1741, 1742, 1739, 1740, 1769, 651, 653, 2485, 2709, 6968, 6969, 6970, 622, 2238, 1618], "orig_top_k_doc_id": [1742, 1741, 1739, 1740, 6968, 6970, 653, 2709, 6969, 1769, 622, 651, 2485, 2238, 1618]}, {"qid": 1271, "question": "How big is LibriSpeech dataset? in Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss", "answer": ["970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset"], "top_k_doc_id": [1741, 1742, 1739, 1740, 1769, 651, 653, 2485, 2709, 6968, 6969, 6970, 652, 2710, 6294], "orig_top_k_doc_id": [1741, 1742, 1739, 1740, 6968, 653, 2709, 6970, 651, 6969, 1769, 652, 2710, 6294, 2485]}, {"qid": 4435, "question": "How do they define their tokens (words, word-piece)? in Semantic Mask for Transformer based End-to-End Speech Recognition", "answer": ["No", "a word or a word-piece"], "top_k_doc_id": [1741, 1742, 4918, 5987, 6968, 2451, 7687, 5724, 477, 2415, 7680, 5185, 6970, 1777, 436], "orig_top_k_doc_id": [6968, 4918, 2451, 7687, 5724, 1741, 477, 2415, 1742, 7680, 5185, 6970, 1777, 5987, 436]}, {"qid": 4436, "question": "By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s  in Semantic Mask for Transformer based End-to-End Speech Recognition", "answer": ["relative 4.5$\\%$ gain, built upon the code base of ESPnet, and achieves relative $10\\%$ gains due to the better architecture and masking strategy", "10%"], "top_k_doc_id": [1741, 1742, 4918, 5987, 6968, 1987, 373, 4972, 5564, 4369, 4863, 3838, 5012, 4771, 1812], "orig_top_k_doc_id": [6968, 1742, 1987, 373, 5987, 4972, 5564, 4918, 4369, 4863, 3838, 5012, 1741, 4771, 1812]}]}
{"group_id": 294, "group_size": 6, "items": [{"qid": 1272, "question": "Which language(s) do they work with? in Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "answer": ["No"], "top_k_doc_id": [1743, 1744, 4826, 6922, 7272, 155, 510, 5540, 4278, 1237, 1325, 4317, 7664, 4563, 1872], "orig_top_k_doc_id": [1743, 1744, 6922, 5540, 4278, 1237, 7272, 1325, 4826, 155, 4317, 510, 7664, 4563, 1872]}, {"qid": 1273, "question": "How do they evaluate their sentence representations? in Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "answer": ["standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, transfer learning evaluation in an artificially constructed low-resource setting"], "top_k_doc_id": [1743, 1744, 4826, 6922, 7272, 155, 510, 5540, 1746, 1745, 6053, 6656, 291, 2058, 1779], "orig_top_k_doc_id": [1743, 1744, 6922, 4826, 5540, 510, 7272, 1746, 1745, 155, 6053, 6656, 291, 2058, 1779]}, {"qid": 1277, "question": "Which data sources do they use? in Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "answer": ["- En-Fr (WMT14)\n- En-De (WMT15)\n- Skipthought (BookCorpus)\n- AllNLI (SNLI + MultiNLI)\n- Parsing (PTB + 1-billion word)"], "top_k_doc_id": [1743, 1744, 4826, 6922, 7272, 155, 7038, 7664, 6656, 1934, 3860, 4456, 4278, 291, 2372], "orig_top_k_doc_id": [1744, 1743, 6922, 7038, 7664, 6656, 1934, 3860, 4826, 4456, 7272, 155, 4278, 291, 2372]}, {"qid": 1275, "question": "How many tokens can sentences in their model at most contain? in Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "answer": ["No"], "top_k_doc_id": [1743, 1744, 4826, 6922, 7272, 5540, 2956, 6053, 2058, 7630, 6658, 6657, 1872, 5276, 456], "orig_top_k_doc_id": [1743, 1744, 6922, 5540, 2956, 6053, 4826, 2058, 7272, 7630, 6658, 6657, 1872, 5276, 456]}, {"qid": 1276, "question": "Which training objectives do they combine? in Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "answer": ["multi-lingual NMT, natural language inference, constituency parsing, skip-thought vectors"], "top_k_doc_id": [1743, 1744, 4826, 6922, 7272, 5540, 4415, 7275, 7139, 4456, 2149, 7276, 6368, 3860, 3633], "orig_top_k_doc_id": [1743, 1744, 7272, 5540, 4415, 7275, 7139, 4826, 6922, 4456, 2149, 7276, 6368, 3860, 3633]}, {"qid": 1274, "question": "Which model architecture do they for sentence encoding? in Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning", "answer": ["Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs\n- RNN"], "top_k_doc_id": [1743, 1744, 4826, 6922, 5540, 4561, 7139, 4278, 1411, 1880, 1237, 891, 1779, 6119, 2607], "orig_top_k_doc_id": [1743, 1744, 4826, 5540, 4561, 7139, 4278, 6922, 1411, 1880, 1237, 891, 1779, 6119, 2607]}]}
{"group_id": 295, "group_size": 6, "items": [{"qid": 1293, "question": "What data is the Prague Dependency Treebank built on? in Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER", "answer": ["No"], "top_k_doc_id": [1774, 247, 438, 1439, 1773, 1775, 1776, 7287, 397, 398, 1991, 4146, 437, 7439, 1994], "orig_top_k_doc_id": [1773, 1776, 1774, 1775, 438, 7287, 1439, 397, 398, 1991, 247, 4146, 437, 7439, 1994]}, {"qid": 1294, "question": "What data is used to build the embeddings? in Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER", "answer": ["large raw Czech corpora available from the LINDAT/CLARIN repository, Czech Wikipedia"], "top_k_doc_id": [1774, 247, 438, 1439, 1773, 1775, 1776, 7287, 397, 398, 2348, 436, 3844, 6284, 6283], "orig_top_k_doc_id": [1773, 1776, 1774, 1775, 1439, 247, 438, 2348, 397, 436, 398, 3844, 6284, 7287, 6283]}, {"qid": 1291, "question": "What previous approaches did this method outperform? in Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER", "answer": ["Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47"], "top_k_doc_id": [1774, 247, 438, 1439, 1773, 1775, 1776, 7287, 397, 930, 439, 7285, 4790, 436, 2321], "orig_top_k_doc_id": [1774, 1773, 1776, 1775, 1439, 7287, 438, 930, 439, 7285, 4790, 436, 2321, 247, 397]}, {"qid": 1292, "question": "How big is the Universal Dependencies corpus? in Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER", "answer": ["No"], "top_k_doc_id": [1774, 247, 438, 1439, 1773, 1775, 1776, 7287, 4790, 4881, 4146, 7318, 7332, 2806, 3845], "orig_top_k_doc_id": [1773, 1774, 1776, 1775, 4790, 7287, 4881, 1439, 4146, 7318, 438, 7332, 2806, 3845, 247]}, {"qid": 4696, "question": "what previous work do they also look at? in A Simple Joint Model for Improved Contextual Neural Lemmatization", "answer": ["N18-1126, UDPipe, D15-1272, Morfette", "N18-1126, UDPipe system of K17-3009, D15-1272, Morfette"], "top_k_doc_id": [1774, 397, 398, 904, 2174, 7318, 7320, 7322, 1776, 1439, 6824, 7319, 5221, 6283, 4256], "orig_top_k_doc_id": [7320, 7318, 397, 1774, 1776, 1439, 2174, 904, 6824, 7319, 7322, 5221, 398, 6283, 4256]}, {"qid": 4697, "question": "what languages did they experiment with? in A Simple Joint Model for Improved Contextual Neural Lemmatization", "answer": ["They experiment with: arabic, basque, croatian, dutch, estonian, finnish, german, greek, hindi, hungarian, italian, latvian, polish, portuguese, romanian, russian, slovak, slovenian, turkish and urdu.", "Arabic, Basque, Croatian, Dutch, Estonian, Finnish, German, Greek, Hindi, Hungarian, Italian, Latvian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Turkish, Urdu"], "top_k_doc_id": [1774, 397, 398, 904, 2174, 7318, 7320, 7322, 5341, 6595, 5238, 7321, 4471, 3904, 1434], "orig_top_k_doc_id": [7320, 7318, 397, 5341, 6595, 5238, 398, 1774, 7322, 7321, 4471, 3904, 904, 1434, 2174]}]}
{"group_id": 296, "group_size": 6, "items": [{"qid": 1347, "question": "How big is dataset used? in News-Driven Stock Prediction With Attention-Based Noisy Recurrent State Transition", "answer": ["553,451 documents"], "top_k_doc_id": [3730, 3731, 6249, 6250, 1851, 1852, 3204, 4810, 1848, 1849, 1850, 2700, 3203, 2697, 2698], "orig_top_k_doc_id": [1848, 1849, 1850, 1852, 1851, 6249, 2700, 3204, 3730, 3731, 4810, 2697, 6250, 2698, 3203]}, {"qid": 1348, "question": "What is dataset used for news-driven stock movement prediction? in News-Driven Stock Prediction With Attention-Based Noisy Recurrent State Transition", "answer": ["the public financial news dataset released by BIBREF4"], "top_k_doc_id": [3730, 3731, 6249, 6250, 1851, 1852, 3204, 4810, 1848, 1849, 1850, 2700, 3203, 747, 4800], "orig_top_k_doc_id": [1848, 1852, 1849, 1851, 6250, 6249, 2700, 1850, 3731, 3204, 3730, 747, 4810, 4800, 3203]}, {"qid": 2850, "question": "What is the dimension of the embeddings? in Sentiment Analysis of Twitter Data for Predicting Stock Market Movements", "answer": ["300", "300"], "top_k_doc_id": [3730, 3731, 6249, 6250, 3203, 3206, 4798, 4799, 4989, 4991, 1851, 3204, 4990, 1848, 4392], "orig_top_k_doc_id": [4989, 4991, 6250, 3203, 1851, 3206, 3731, 6249, 3730, 4798, 4799, 1848, 3204, 4990, 4392]}, {"qid": 2851, "question": "What dataset is used to train the model? in Sentiment Analysis of Twitter Data for Predicting Stock Market Movements", "answer": ["2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016", "Collected tweets and opening and closing stock prices of Microsoft."], "top_k_doc_id": [3730, 3731, 6249, 6250, 3203, 3206, 4798, 4799, 4989, 4991, 1851, 3204, 4990, 2696, 1852], "orig_top_k_doc_id": [4989, 4991, 3203, 3206, 3731, 3730, 6250, 4798, 6249, 3204, 1851, 4799, 2696, 4990, 1852]}, {"qid": 2748, "question": "Which stock market sector achieved the best performance? in Multimodal deep learning for short-term stock volatility prediction", "answer": ["Energy with accuracy of 0.538", "Energy"], "top_k_doc_id": [3730, 3731, 6249, 6250, 1851, 1852, 3204, 4810, 4805, 4799, 4800, 4798, 4809, 4807, 3205], "orig_top_k_doc_id": [4810, 4805, 4799, 4800, 3730, 1851, 4798, 3731, 3204, 6249, 1852, 4809, 4807, 3205, 6250]}, {"qid": 2849, "question": "Do they remove seasonality from the time series? in Sentiment Analysis of Twitter Data for Predicting Stock Market Movements", "answer": ["No", "No"], "top_k_doc_id": [3730, 3731, 6249, 6250, 3203, 3206, 4798, 4799, 4989, 4991, 7860, 1848, 2696, 747, 2081], "orig_top_k_doc_id": [4989, 4991, 3730, 3203, 3731, 6250, 4798, 4799, 7860, 1848, 6249, 3206, 2696, 747, 2081]}]}
{"group_id": 297, "group_size": 6, "items": [{"qid": 1406, "question": "How/where are the natural question generated? in Multimodal Differential Network for Visual Question Generation", "answer": ["Decoder that generates question using an LSTM-based language model"], "top_k_doc_id": [1940, 1941, 1942, 1943, 1944, 7148, 7164, 2900, 3175, 1945, 7138, 7147, 7144, 3798, 4744], "orig_top_k_doc_id": [1940, 1941, 1942, 1944, 7148, 1943, 1945, 7164, 3175, 7147, 2900, 7138, 7144, 3798, 4744]}, {"qid": 1409, "question": "How do the authors define exemplars? in Multimodal Differential Network for Visual Question Generation", "answer": ["Exemplars aim to provide appropriate context., joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption"], "top_k_doc_id": [1940, 1941, 1942, 1943, 1944, 7148, 7164, 2900, 3175, 1945, 7138, 7147, 7144, 1946, 413], "orig_top_k_doc_id": [1940, 1941, 1942, 1945, 1943, 1944, 1946, 7148, 7144, 3175, 7164, 7138, 2900, 413, 7147]}, {"qid": 1405, "question": "What were the previous state of the art benchmarks? in Multimodal Differential Network for Visual Question Generation", "answer": ["BIBREF35 for VQA dataset, BIBREF5, BIBREF36"], "top_k_doc_id": [1940, 1941, 1942, 1943, 1944, 7148, 7164, 2900, 3175, 1945, 7138, 7147, 867, 276, 4756], "orig_top_k_doc_id": [1940, 1942, 1944, 1941, 1943, 867, 2900, 3175, 276, 7164, 7147, 7138, 1945, 4756, 7148]}, {"qid": 1404, "question": "Do they report results only on English datasets? in Multimodal Differential Network for Visual Question Generation", "answer": ["No"], "top_k_doc_id": [1940, 1941, 1942, 1943, 1944, 7148, 7164, 2900, 3175, 413, 2899, 7805, 160, 4756, 3657], "orig_top_k_doc_id": [1940, 1942, 1941, 1944, 7164, 1943, 2900, 413, 2899, 7805, 160, 7148, 3175, 4756, 3657]}, {"qid": 1407, "question": "What is the input to the differential network? in Multimodal Differential Network for Visual Question Generation", "answer": ["image"], "top_k_doc_id": [1940, 1941, 1942, 1943, 1944, 7148, 7164, 1945, 1946, 2697, 2698, 2699, 2700, 3175, 7138], "orig_top_k_doc_id": [1940, 1942, 1941, 1944, 1945, 7164, 1946, 1943, 2700, 2698, 2697, 7148, 3175, 2699, 7138]}, {"qid": 1408, "question": "How do the authors define a differential network? in Multimodal Differential Network for Visual Question Generation", "answer": ["The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module."], "top_k_doc_id": [1940, 1941, 1942, 1943, 1944, 7148, 7164, 1945, 1946, 2697, 2698, 2699, 2700, 6479, 2696], "orig_top_k_doc_id": [1940, 1941, 1942, 1944, 1945, 7164, 1946, 1943, 2700, 2698, 2699, 6479, 2697, 2696, 7148]}]}
{"group_id": 298, "group_size": 6, "items": [{"qid": 1514, "question": "How big is the evaluated dataset? in Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers", "answer": ["contains thousands of XML files, each of which are constructed by several records"], "top_k_doc_id": [2127, 2129, 2855, 2856, 5674, 5675, 5739, 5740, 2128, 2130, 3742, 1242, 3344, 2189, 2181], "orig_top_k_doc_id": [2856, 2127, 2855, 2128, 2130, 5675, 5739, 2129, 2189, 5674, 3742, 2181, 3344, 1242, 5740]}, {"qid": 1515, "question": "By how much does their model outperform existing methods? in Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers", "answer": ["Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result."], "top_k_doc_id": [2127, 2129, 2855, 2856, 5674, 5675, 5739, 5740, 2128, 2130, 3742, 1242, 3344, 3781, 3198], "orig_top_k_doc_id": [2127, 2856, 5675, 2855, 2128, 5739, 2130, 5674, 2129, 3781, 3344, 3198, 3742, 5740, 1242]}, {"qid": 1516, "question": "What is the performance of their model? in Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers", "answer": ["Answer with content missing: (Table II) Proposed model has F1 score of  0.7220."], "top_k_doc_id": [2127, 2129, 2855, 2856, 5674, 5675, 5739, 5740, 2128, 2130, 3742, 3198, 2189, 2184, 2186], "orig_top_k_doc_id": [2127, 2856, 2855, 2130, 2128, 5675, 5739, 2129, 5674, 3742, 3198, 5740, 2189, 2184, 2186]}, {"qid": 1517, "question": "What are the existing methods mentioned in the paper? in Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers", "answer": ["Chowdhury BIBREF14 and Thomas et al. BIBREF11, FBK-irst BIBREF10, Liu et al. BIBREF9, Sahu et al. BIBREF12"], "top_k_doc_id": [2127, 2129, 2855, 2856, 5674, 5675, 5739, 5740, 2128, 2130, 3742, 3198, 5827, 1152, 854], "orig_top_k_doc_id": [2127, 2856, 2855, 2128, 2130, 5675, 5739, 5674, 3742, 5827, 2129, 3198, 5740, 1152, 854]}, {"qid": 1922, "question": "What were the sizes of the test sets? in A Multi-Task Learning Framework for Extracting Drugs and Their Interactions from Drug Labels", "answer": ["Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences"], "top_k_doc_id": [2127, 2129, 2855, 2856, 5674, 5675, 5739, 5740, 5743, 6195, 6916, 7833, 7834, 2858, 3744], "orig_top_k_doc_id": [2855, 2856, 2127, 6916, 5739, 5740, 5674, 5675, 5743, 7834, 7833, 2858, 3744, 6195, 2129]}, {"qid": 1923, "question": "What training data did they use? in A Multi-Task Learning Framework for Extracting Drugs and Their Interactions from Drug Labels", "answer": ["Training-22, NLM-180"], "top_k_doc_id": [2127, 2129, 2855, 2856, 5674, 5675, 5739, 5740, 5743, 6195, 6916, 7833, 7834, 2128, 7742], "orig_top_k_doc_id": [2855, 2856, 5674, 5675, 2127, 5739, 6916, 5740, 5743, 7834, 2129, 2128, 7742, 7833, 6195]}]}
{"group_id": 299, "group_size": 6, "items": [{"qid": 1528, "question": "What is the agreement value for each dataset? in $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [2139, 2140, 2141, 2142, 2143, 2144, 2145, 3615, 5058, 7263, 7746, 1729, 5910, 5136, 7174], "orig_top_k_doc_id": [2144, 2142, 2141, 2143, 2145, 2139, 3615, 7746, 1729, 5058, 7263, 2140, 5910, 5136, 7174]}, {"qid": 1529, "question": "How many annotators participated? in $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [2139, 2140, 2141, 2142, 2143, 2144, 2145, 3615, 5058, 7263, 7746, 4833, 462, 6181, 7752], "orig_top_k_doc_id": [2142, 2144, 2141, 2139, 2140, 2143, 2145, 7746, 4833, 3615, 462, 7263, 6181, 7752, 5058]}, {"qid": 1527, "question": "What are the other models they compare to? in $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "answer": ["CNN-C, CNN-W, CNN-Lex-C, CNN-Lex-W, Bi-LSTM-C , Bi-LSTM-W, Lex-rule, BOW"], "top_k_doc_id": [2139, 2140, 2141, 2142, 2143, 2144, 2145, 3615, 462, 5962, 7174, 1327, 0, 7746, 1040], "orig_top_k_doc_id": [2142, 2144, 2141, 2143, 2145, 2139, 3615, 5962, 2140, 1327, 462, 0, 7174, 7746, 1040]}, {"qid": 1530, "question": "How long are the datasets? in $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "answer": ["Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses"], "top_k_doc_id": [2139, 2140, 2141, 2142, 2143, 2144, 2145, 3615, 462, 5962, 7174, 756, 7263, 5106, 1687], "orig_top_k_doc_id": [2142, 2144, 2141, 2143, 2145, 2139, 462, 3615, 2140, 756, 7263, 5962, 7174, 5106, 1687]}, {"qid": 1531, "question": "What are the sources of the data? in $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "answer": ["User reviews written in Chinese collected online for hotel, mobile phone, and travel domains"], "top_k_doc_id": [2139, 2140, 2141, 2142, 2143, 2144, 2145, 3615, 5058, 7263, 6752, 7672, 1329, 0, 462], "orig_top_k_doc_id": [2142, 2144, 2141, 2143, 2145, 2139, 7263, 2140, 6752, 7672, 1329, 5058, 0, 3615, 462]}, {"qid": 1532, "question": "What is the new labeling strategy? in $\\rho$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis", "answer": ["They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations"], "top_k_doc_id": [2139, 2140, 2141, 2142, 2143, 2144, 2145, 3615, 4503, 0, 7746, 5962, 6503, 1688, 2697], "orig_top_k_doc_id": [2142, 2145, 2144, 2143, 2141, 2140, 2139, 4503, 0, 7746, 3615, 5962, 6503, 1688, 2697]}]}
{"group_id": 300, "group_size": 6, "items": [{"qid": 1675, "question": "What are the key issues around whether the gold standard data produced in such an annotation is reliable?  in Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?", "answer": [" only 1 in 9 qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics, low-effort responses from crowdworkers"], "top_k_doc_id": [3581, 2386, 2388, 2393, 2394, 2395, 2396, 3593, 2389, 2390, 7860, 2398, 2391, 2074, 5910], "orig_top_k_doc_id": [2386, 3581, 2393, 2388, 3593, 2395, 7860, 2394, 2390, 2389, 2398, 2391, 2396, 2074, 5910]}, {"qid": 1676, "question": "How were the machine learning papers from ArXiv sampled? in Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?", "answer": ["sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph), filtered for papers in which the title or abstract included at least one of the words \u201cmachine learning\u201d, \u201cclassif*\u201d, or \u201csupervi*\u201d (case insensitive), filtered to papers in which the title or abstract included at least \u201ctwitter\u201d or \u201ctweet\u201d (case insensitive)"], "top_k_doc_id": [3581, 2386, 2388, 2393, 2394, 2395, 2396, 3593, 2389, 2390, 2397, 2547, 3582, 5591, 2399], "orig_top_k_doc_id": [2386, 2388, 2389, 2393, 2397, 2394, 2396, 3581, 2390, 2395, 2547, 3582, 5591, 3593, 2399]}, {"qid": 1677, "question": "What are the core best practices of structured content analysis? in Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?", "answer": ["\u201ccoding scheme\u201d is defined, coders are trained with the coding scheme, Training sometimes results in changes to the coding scheme, calculation of \u201cinter-annotator agreement\u201d or \u201cinter-rater reliability.\u201d, there is a process of \u201creconciliation\u201d for disagreements"], "top_k_doc_id": [3581, 2386, 2388, 2393, 2394, 2395, 2396, 3593, 2387, 2397, 2398, 4629, 7449, 5905, 3582], "orig_top_k_doc_id": [2386, 3593, 3581, 2387, 2388, 2394, 2393, 5905, 3582, 2396, 2395, 2398, 4629, 2397, 7449]}, {"qid": 1678, "question": "In what sense is data annotation similar to structured content analysis?  in Garbage In, Garbage Out? Do Machine Learning Application Papers in Social Computing Report Where Human-Labeled Training Data Comes From?", "answer": ["structured content analysis (also called \u201cclosed coding\u201d) is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data, Projects usually involve teams of \u201ccoders\u201d (also called \u201cannotators\u201d, \u201clabelers\u201d, or \u201creviewers\u201d), with human labor required to \u201ccode\u201d, \u201cannotate\u201d, or \u201clabel\u201d a corpus of items."], "top_k_doc_id": [3581, 2386, 2388, 2393, 2394, 2395, 2396, 3593, 2387, 2397, 2398, 4629, 7449, 5910, 2392], "orig_top_k_doc_id": [2386, 3581, 3593, 2387, 2396, 2398, 2388, 2397, 2393, 2394, 4629, 5910, 2395, 2392, 7449]}, {"qid": 4975, "question": "Do they release their code? in Measuring Issue Ownership using Word Embeddings", "answer": ["No", "No"], "top_k_doc_id": [3581, 5006, 6558, 7743, 7744, 7745, 102, 1194, 2388, 3071, 2137, 3596, 1014, 103, 5498], "orig_top_k_doc_id": [7743, 7745, 7744, 6558, 102, 1194, 2388, 3071, 5006, 2137, 3596, 1014, 103, 5498, 3581]}, {"qid": 4976, "question": "What media sources do they use? in Measuring Issue Ownership using Word Embeddings", "answer": ["Swedish online data from 2018 crawled by Trendiction, manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative)", "party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative)"], "top_k_doc_id": [3581, 5006, 6558, 7743, 7744, 7745, 3135, 1494, 6005, 1172, 5928, 1517, 6833, 5909, 1757], "orig_top_k_doc_id": [7743, 7745, 7744, 6558, 3581, 3135, 1494, 6005, 1172, 5006, 5928, 1517, 6833, 5909, 1757]}]}
{"group_id": 301, "group_size": 6, "items": [{"qid": 1710, "question": "How do they ensure the generated questions are unanswerable? in Learning to Ask Unanswerable Questions for Machine Reading Comprehension", "answer": ["learn to ask unanswerable questions by editing answerable ones with word exchanges, negations, etc"], "top_k_doc_id": [2442, 2444, 4709, 2443, 2445, 2446, 4189, 5579, 4710, 4911, 5471, 5580, 5577, 4915, 4188], "orig_top_k_doc_id": [2442, 2445, 2446, 2443, 2444, 4189, 4709, 5580, 5577, 5471, 5579, 4911, 4915, 4188, 4710]}, {"qid": 1711, "question": "Does their approach require a dataset of unanswerable questions mapped to similar answerable questions? in Learning to Ask Unanswerable Questions for Machine Reading Comprehension", "answer": ["Yes"], "top_k_doc_id": [2442, 2444, 4709, 2443, 2445, 2446, 4189, 5579, 4710, 4911, 5471, 5580, 5368, 1972, 4074], "orig_top_k_doc_id": [2442, 2445, 2446, 2443, 2444, 4709, 5580, 4189, 5579, 5471, 4710, 4911, 5368, 1972, 4074]}, {"qid": 2685, "question": "Do they use attention? in Stochastic Answer Networks for SQuAD 2.0", "answer": ["Yes", "Yes"], "top_k_doc_id": [2442, 2444, 4709, 2050, 2051, 4518, 4522, 5236, 5237, 2445, 4256, 5235, 2052, 4710, 4255], "orig_top_k_doc_id": [2050, 2051, 5236, 4709, 2442, 2052, 4522, 4710, 2444, 4255, 4518, 4256, 2445, 5235, 5237]}, {"qid": 2686, "question": "What other models do they compare to? in Stochastic Answer Networks for SQuAD 2.0", "answer": ["SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo", "BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo"], "top_k_doc_id": [2442, 2444, 4709, 2050, 2051, 4518, 4522, 5236, 5237, 2445, 4256, 5235, 2664, 3420, 4520], "orig_top_k_doc_id": [4522, 2050, 2051, 5236, 2444, 2445, 5237, 4518, 2442, 4256, 2664, 4709, 5235, 3420, 4520]}, {"qid": 1709, "question": "What is the training objective of their pair-to-sequence model? in Learning to Ask Unanswerable Questions for Machine Reading Comprehension", "answer": ["is to minimize the negative likelihood of the aligned unanswerable question $\\tilde{q}$ given the answerable question $q$ and its corresponding paragraph $p$ that contains the answer "], "top_k_doc_id": [2442, 2444, 4709, 2443, 2445, 2446, 4189, 5579, 1822, 490, 5577, 2519, 4915, 4188, 3805], "orig_top_k_doc_id": [2442, 2446, 2443, 2445, 2444, 4189, 4709, 1822, 490, 5577, 2519, 4915, 4188, 3805, 5579]}, {"qid": 2687, "question": "What is the architecture of the span detector? in Stochastic Answer Networks for SQuAD 2.0", "answer": ["adopt a multi-turn answer module for the span detector BIBREF1", "No"], "top_k_doc_id": [2442, 2444, 4709, 2050, 2051, 4518, 4522, 5236, 5237, 4710, 4711, 1144, 2048, 2234, 4520], "orig_top_k_doc_id": [4709, 4710, 2051, 4711, 5236, 2050, 4518, 4522, 2442, 2444, 1144, 2048, 5237, 2234, 4520]}]}
{"group_id": 302, "group_size": 6, "items": [{"qid": 1749, "question": "Does Grail accept Prolog inputs? in The Grail theorem prover: Type theory for syntax and semantics", "answer": ["No"], "top_k_doc_id": [2513, 2515, 2518, 2516, 689, 2506, 2509, 2512, 2517, 4333, 2132, 1459, 1222, 5429, 1282], "orig_top_k_doc_id": [2518, 2515, 2517, 2506, 2516, 689, 2132, 1459, 2509, 1222, 5429, 2512, 1282, 2513, 4333]}, {"qid": 1750, "question": "What formalism does Grail use? in The Grail theorem prover: Type theory for syntax and semantics", "answer": ["a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors)."], "top_k_doc_id": [2513, 2515, 2518, 2516, 689, 2506, 2509, 2512, 2517, 4333, 2511, 3490, 4339, 2933, 4331], "orig_top_k_doc_id": [2518, 2515, 2506, 2517, 2516, 689, 2511, 3490, 4339, 2509, 2933, 4331, 2512, 2513, 4333]}, {"qid": 5003, "question": "What logic rules can be learned using ELMo? in Revisiting the Importance of Encoding Logic Rules in Sentiment Classification", "answer": ["1).But   2).Eng  3). A-But-B", "A-but-B and negation"], "top_k_doc_id": [2513, 2515, 2518, 2516, 470, 686, 2426, 2510, 2746, 3539, 5429, 7788, 7789, 5944, 2004], "orig_top_k_doc_id": [7788, 7789, 2746, 470, 5429, 3539, 686, 2516, 2515, 5944, 2513, 2518, 2510, 2426, 2004]}, {"qid": 5004, "question": "Does Elmo learn all possible logic rules? in Revisiting the Importance of Encoding Logic Rules in Sentiment Classification", "answer": ["Yes", "No"], "top_k_doc_id": [2513, 2515, 2518, 2516, 470, 686, 2426, 2510, 2746, 3539, 5429, 7788, 7789, 2509, 471], "orig_top_k_doc_id": [7789, 7788, 470, 3539, 2515, 2518, 5429, 2746, 2516, 2509, 471, 2426, 2510, 2513, 686]}, {"qid": 1700, "question": "How are proof scores calculated? in Towards Neural Theorem Proving at Scale", "answer": ["'= ( , { ll k(h:, g:) if hV, gV\n\n1 otherwise } )\n\nwhere $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively."], "top_k_doc_id": [2513, 2515, 2518, 2426, 2427, 2506, 2507, 2517, 7394, 7395, 7397, 7398, 7649, 2941, 3370], "orig_top_k_doc_id": [2506, 2515, 2517, 7394, 2426, 2513, 7395, 2507, 2427, 7398, 7649, 2941, 7397, 2518, 3370]}, {"qid": 1701, "question": "What are proof paths? in Towards Neural Theorem Proving at Scale", "answer": ["A sequence of logical statements represented in a computational graph"], "top_k_doc_id": [2513, 2515, 2518, 2426, 2427, 2506, 2507, 2517, 7394, 7395, 7397, 7398, 7649, 2942, 3372], "orig_top_k_doc_id": [2426, 2506, 2517, 2515, 7394, 2513, 2427, 7395, 7649, 7398, 2507, 7397, 2518, 2942, 3372]}]}
{"group_id": 303, "group_size": 6, "items": [{"qid": 1751, "question": "Which components of QA and QG models are shared during training? in A Joint Model for Question Answering and Question Generation", "answer": ["parameter sharing"], "top_k_doc_id": [491, 3805, 3806, 4252, 4253, 4257, 4258, 490, 494, 2519, 3807, 4256, 6932, 6936, 4641], "orig_top_k_doc_id": [4252, 490, 3805, 494, 2519, 491, 4253, 6932, 4257, 4258, 3806, 4641, 6936, 4256, 3807]}, {"qid": 1752, "question": "How much improvement does jointly learning QA and QG give, compared to only training QA? in A Joint Model for Question Answering and Question Generation", "answer": ["We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. "], "top_k_doc_id": [491, 3805, 3806, 4252, 4253, 4257, 4258, 490, 494, 2519, 3807, 4256, 6932, 6936, 4637], "orig_top_k_doc_id": [4256, 4252, 4257, 490, 494, 6932, 4258, 4253, 3805, 3806, 491, 2519, 3807, 6936, 4637]}, {"qid": 2368, "question": "How they evaluate quality of generated output? in Ask to Learn: A Study on Curiosity-driven Question Generation", "answer": ["Through human evaluation where they are asked to evaluate the generated output on a likert scale."], "top_k_doc_id": [491, 3805, 490, 495, 3789, 3806, 3807, 3808, 3809, 3810, 6320, 823, 4267, 494, 2910], "orig_top_k_doc_id": [3810, 3805, 3807, 3808, 3809, 3806, 3789, 490, 491, 823, 495, 4267, 6320, 494, 2910]}, {"qid": 2369, "question": "What automated metrics authors investigate? in Ask to Learn: A Study on Curiosity-driven Question Generation", "answer": ["BLEU, Self-BLEU, n-gram based score, probability score"], "top_k_doc_id": [491, 3805, 490, 495, 3789, 3806, 3807, 3808, 3809, 3810, 6320, 1322, 898, 2439, 7300], "orig_top_k_doc_id": [3805, 3808, 3810, 3807, 3806, 3809, 491, 495, 490, 1322, 6320, 898, 3789, 2439, 7300]}, {"qid": 2364, "question": "Is it a neural model? How is it trained? in Question Asking as Program Generation", "answer": ["No, it is a probabilistic model trained by finding feature weights through gradient ascent"], "top_k_doc_id": [491, 3805, 490, 495, 3789, 3790, 3794, 2519, 559, 3793, 5229, 7147, 2818, 3791, 7377], "orig_top_k_doc_id": [3789, 3790, 3805, 3794, 2519, 491, 559, 3793, 5229, 490, 495, 7147, 2818, 3791, 7377]}, {"qid": 2505, "question": "What does \"explicitly leverages their probabilistic correlation to guide the training process of both models\" mean? in Question Answering and Question Generation as Dual Tasks", "answer": ["The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization."], "top_k_doc_id": [491, 3805, 3806, 4252, 4253, 4257, 4258, 5226, 559, 1357, 1453, 3490, 4719, 1139, 2900], "orig_top_k_doc_id": [4257, 3805, 4253, 4252, 5226, 4258, 491, 559, 3806, 1357, 1453, 3490, 4719, 1139, 2900]}]}
{"group_id": 304, "group_size": 6, "items": [{"qid": 1772, "question": "How successful are they at matching names of authors in Japanese and English? in Integration of Japanese Papers Into the DBLP Data Set", "answer": ["180221 of 231162 author names could be matched successfully"], "top_k_doc_id": [2565, 2558, 2562, 2563, 2564, 2566, 2570, 2571, 2572, 2573, 2567, 2568, 2569, 2560, 1777], "orig_top_k_doc_id": [2572, 2570, 2571, 2564, 2558, 2573, 2566, 2569, 2565, 2563, 2562, 2567, 2568, 2560, 1777]}, {"qid": 1774, "question": "Do they translate metadata from Japanese papers to English? in Integration of Japanese Papers Into the DBLP Data Set", "answer": ["No"], "top_k_doc_id": [2565, 2558, 2562, 2563, 2564, 2566, 2570, 2571, 2572, 2573, 2567, 2568, 2569, 5837, 2039], "orig_top_k_doc_id": [2558, 2572, 2571, 2563, 2564, 2570, 2573, 2566, 2569, 2562, 2565, 2568, 2567, 5837, 2039]}, {"qid": 4822, "question": "Do they compare to other methods? in Similarity measure for Public Persons", "answer": ["No", "No"], "top_k_doc_id": [2565, 7502, 7503, 3512, 6418, 7525, 7743, 6462, 6079, 4835, 5000, 305, 608, 6360, 7307], "orig_top_k_doc_id": [7502, 7503, 2565, 6462, 6418, 7525, 3512, 6079, 4835, 5000, 305, 608, 6360, 7307, 7743]}, {"qid": 4823, "question": "How large is the dataset? in Similarity measure for Public Persons", "answer": ["70287", "English corpus has a dictionary of length 106.848, German version has a dictionary of length 163.788"], "top_k_doc_id": [2565, 7502, 7503, 3512, 6418, 7525, 7743, 6455, 455, 6563, 3880, 5641, 3945, 6793, 5468], "orig_top_k_doc_id": [7502, 7503, 6455, 7743, 2565, 6418, 7525, 455, 3512, 6563, 3880, 5641, 3945, 6793, 5468]}, {"qid": 1773, "question": "Is their approach applicable to papers outside computer science? in Integration of Japanese Papers Into the DBLP Data Set", "answer": ["No"], "top_k_doc_id": [2565, 2558, 2562, 2563, 2564, 2566, 2570, 2571, 2572, 2573, 2574, 2388, 2389, 2037, 2387], "orig_top_k_doc_id": [2558, 2572, 2571, 2563, 2564, 2573, 2570, 2566, 2574, 2562, 2388, 2389, 2037, 2387, 2565]}, {"qid": 4821, "question": "Did they build a dataset? in Similarity measure for Public Persons", "answer": ["Yes", "Yes"], "top_k_doc_id": [2565, 7502, 7503, 4839, 6462, 6079, 5641, 6805, 4837, 2080, 6572, 6584, 3945, 2081, 2386], "orig_top_k_doc_id": [7502, 7503, 4839, 6462, 6079, 5641, 6805, 4837, 2080, 2565, 6572, 6584, 3945, 2081, 2386]}]}
{"group_id": 305, "group_size": 6, "items": [{"qid": 1879, "question": "How is the PBMT system trained? in Pre-Translation for Neural Machine Translation", "answer": ["systems were optimized on the tst2014 using Minimum error rate training BIBREF20"], "top_k_doc_id": [2762, 2760, 2761, 2763, 774, 776, 777, 2764, 5165, 5166, 5167, 6599, 7342, 4396, 2839], "orig_top_k_doc_id": [2763, 2762, 2761, 2760, 2764, 5167, 774, 5166, 5165, 777, 6599, 776, 7342, 4396, 2839]}, {"qid": 1881, "question": "Do they train the NMT model on PBMT outputs? in Pre-Translation for Neural Machine Translation", "answer": ["Yes"], "top_k_doc_id": [2762, 2760, 2761, 2763, 774, 776, 777, 2764, 5165, 5166, 5167, 4822, 4824, 4027, 7827], "orig_top_k_doc_id": [2760, 2761, 2763, 2762, 5167, 2764, 774, 5166, 5165, 4822, 776, 777, 4824, 4027, 7827]}, {"qid": 1878, "question": "Which dataset do they use? in Pre-Translation for Neural Machine Translation", "answer": ["parallel data available for the WMT 2016"], "top_k_doc_id": [2762, 2760, 2761, 2763, 7190, 6295, 2836, 7669, 5175, 7827, 2839, 7342, 3655, 7660, 1458], "orig_top_k_doc_id": [2760, 7190, 6295, 2836, 7669, 2762, 5175, 2763, 7827, 2839, 7342, 2761, 3655, 7660, 1458]}, {"qid": 1880, "question": "Which NMT architecture do they use? in Pre-Translation for Neural Machine Translation", "answer": ["trained using Nematus, default configuration"], "top_k_doc_id": [2762, 2760, 2761, 2763, 2188, 5835, 5165, 6041, 7847, 3652, 3817, 1347, 3920, 1476, 7345], "orig_top_k_doc_id": [2760, 2761, 2188, 5835, 2762, 5165, 2763, 6041, 7847, 3652, 3817, 1347, 3920, 1476, 7345]}, {"qid": 4715, "question": "What is the architecture of the model? in Nematus: a Toolkit for Neural Machine Translation", "answer": ["attentional encoder\u2013decoder", "attentional encoder\u2013decoder"], "top_k_doc_id": [2762, 1044, 4695, 4816, 5455, 6043, 7345, 7669, 115, 34, 4561, 4424, 4863, 4906, 5482], "orig_top_k_doc_id": [7345, 6043, 115, 4816, 4695, 34, 2762, 7669, 4561, 5455, 4424, 4863, 4906, 5482, 1044]}, {"qid": 4716, "question": "How many translation pairs are used for training? in Nematus: a Toolkit for Neural Machine Translation", "answer": ["No", "No"], "top_k_doc_id": [2762, 1044, 4695, 4816, 5455, 6043, 7345, 7669, 3563, 7342, 4875, 2810, 4389, 7191, 3564], "orig_top_k_doc_id": [7345, 4695, 5455, 7669, 2762, 1044, 3563, 6043, 7342, 4875, 2810, 4816, 4389, 7191, 3564]}]}
{"group_id": 306, "group_size": 6, "items": [{"qid": 1919, "question": "Which dataset of texts do they use? in Improving Textual Network Embedding with Global Attention via Optimal Transport", "answer": ["Cora, Hepth, Zhihu"], "top_k_doc_id": [2850, 2853, 1237, 1609, 2851, 7335, 2048, 908, 5199, 5336, 3102, 5335, 2173, 3017, 4916], "orig_top_k_doc_id": [2851, 2853, 2850, 1609, 1237, 2048, 5336, 7335, 908, 3102, 5335, 2173, 3017, 4916, 5199]}, {"qid": 1921, "question": "Which other embeddings do they compare against? in Improving Textual Network Embedding with Global Attention via Optimal Transport", "answer": ["MMB, DeepWalk, LINE,  Node2vec, TADW, CENE, CANE, WANE, DMTE"], "top_k_doc_id": [2850, 2853, 1237, 1609, 2851, 7335, 2048, 908, 5199, 5336, 6073, 1198, 6072, 3177, 1238], "orig_top_k_doc_id": [2853, 2851, 2850, 1609, 1237, 6073, 2048, 7335, 1198, 6072, 5336, 908, 3177, 5199, 1238]}, {"qid": 1918, "question": "Which of their proposed attention methods works better overall? in Improving Textual Network Embedding with Global Attention via Optimal Transport", "answer": ["attention parsing"], "top_k_doc_id": [2850, 2853, 1237, 1609, 2851, 7335, 2048, 3102, 709, 1768, 2216, 3021, 470, 3017, 3020], "orig_top_k_doc_id": [2851, 2853, 2850, 1609, 1237, 3102, 2048, 709, 1768, 2216, 3021, 470, 3017, 3020, 7335]}, {"qid": 1920, "question": "Do they measure how well they perform on longer sequences specifically? in Improving Textual Network Embedding with Global Attention via Optimal Transport", "answer": ["Yes"], "top_k_doc_id": [2850, 2853, 1237, 1609, 2851, 7335, 7770, 3696, 4312, 5199, 2191, 5336, 3697, 3694, 891], "orig_top_k_doc_id": [2851, 2850, 2853, 1609, 7770, 1237, 3696, 4312, 7335, 5199, 2191, 5336, 3697, 3694, 891]}, {"qid": 2317, "question": "What text sequences are associated with each vertex? in Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment", "answer": ["abstracts, sentences"], "top_k_doc_id": [2850, 2853, 259, 2854, 3694, 3695, 3696, 3697, 3698, 1314, 4457, 3370, 829, 7612, 1627], "orig_top_k_doc_id": [3694, 3697, 3695, 3696, 3698, 2853, 2850, 2854, 1314, 259, 4457, 3370, 829, 7612, 1627]}, {"qid": 2318, "question": "How long does it take for the model to run? in Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment", "answer": ["No"], "top_k_doc_id": [2850, 2853, 259, 2854, 3694, 3695, 3696, 3697, 4293, 6070, 4530, 365, 2124, 4680, 248], "orig_top_k_doc_id": [2850, 3694, 2854, 2853, 259, 4293, 6070, 4530, 3697, 3695, 365, 2124, 4680, 248, 3696]}]}
{"group_id": 307, "group_size": 6, "items": [{"qid": 2001, "question": "Did they use other evaluation metrics? in Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling", "answer": ["Yes"], "top_k_doc_id": [3023, 3024, 3938, 1133, 6648, 1322, 3357, 2445, 1132, 1488, 1138, 2442, 5495, 292, 245], "orig_top_k_doc_id": [3024, 3938, 6648, 3357, 3023, 2445, 1133, 1132, 1322, 1488, 1138, 2442, 5495, 292, 245]}, {"qid": 2002, "question": "What was their perplexity score? in Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling", "answer": ["Perplexity score 142.84 on dev and 138.91 on test"], "top_k_doc_id": [3023, 3024, 3938, 1133, 6648, 1322, 3357, 3025, 3667, 3939, 4484, 2007, 7785, 5228, 901], "orig_top_k_doc_id": [3024, 3938, 3025, 3023, 6648, 1322, 1133, 3667, 3939, 4484, 2007, 3357, 7785, 5228, 901]}, {"qid": 2003, "question": "What languages are explored in this paper? in Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling", "answer": ["Mandarin, English"], "top_k_doc_id": [3023, 3024, 3938, 1133, 6648, 243, 1488, 3025, 6854, 6853, 1000, 1389, 7818, 4934, 1784], "orig_top_k_doc_id": [3938, 3023, 3024, 1488, 6854, 6648, 6853, 1000, 1389, 243, 1133, 7818, 3025, 4934, 1784]}, {"qid": 2004, "question": "What parallel corpus did they use? in Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling", "answer": ["Parallel monolingual corpus in English and Mandarin"], "top_k_doc_id": [3023, 3024, 3938, 1133, 6648, 243, 1488, 3025, 6310, 3357, 493, 5838, 274, 5789, 6190], "orig_top_k_doc_id": [3024, 3023, 3938, 6310, 3025, 6648, 3357, 243, 1133, 493, 5838, 274, 5789, 6190, 1488]}, {"qid": 2409, "question": "What is the architecture of the model? in Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning", "answer": ["LSTM"], "top_k_doc_id": [3023, 3024, 3938, 1000, 1067, 2917, 3025, 3598, 3637, 3939, 3940, 4754, 5429, 5185, 3640], "orig_top_k_doc_id": [3940, 3939, 3938, 3023, 3024, 3637, 3025, 3598, 2917, 1000, 4754, 5429, 5185, 1067, 3640]}, {"qid": 2410, "question": "What languages are explored in the work? in Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning", "answer": ["Mandarin, English"], "top_k_doc_id": [3023, 3024, 3938, 1000, 1067, 2917, 3025, 3598, 3637, 3939, 3940, 4754, 6333, 1389, 4753], "orig_top_k_doc_id": [3023, 3938, 1000, 3940, 3939, 3024, 3025, 2917, 3598, 3637, 1067, 6333, 1389, 4753, 4754]}]}
{"group_id": 308, "group_size": 6, "items": [{"qid": 2040, "question": "What environment is used for self-critical sequence training? in A Better Variant of Self-Critical Sequence Training", "answer": ["No"], "top_k_doc_id": [6928, 1258, 3092, 4035, 6930, 7647, 890, 2165, 6931, 3026, 3027, 1544, 4452, 6865, 7646], "orig_top_k_doc_id": [3092, 3026, 7647, 6931, 890, 6928, 4035, 1258, 6930, 3027, 2165, 1544, 4452, 6865, 7646]}, {"qid": 2042, "question": "What baseline model is used for comparison? in A Better Variant of Self-Critical Sequence Training", "answer": ["No"], "top_k_doc_id": [6928, 1258, 3092, 4035, 6930, 7647, 890, 2165, 6931, 5587, 4374, 5277, 4036, 3093, 1526], "orig_top_k_doc_id": [3092, 2165, 1258, 6930, 4035, 5587, 6931, 4374, 5277, 7647, 4036, 6928, 3093, 890, 1526]}, {"qid": 4402, "question": "How is GPU-based self-critical Reinforcement Learing model designed? in Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach", "answer": ["No", "We used the self-critical model of BIBREF13 proposed for image captioning, Additionally, we have used enthttps://stackoverflow.com/questions/19053077/looping-over-data-and-creating-individual-figuresropy regularization., To the best of our knowledge, ours is the first GPU based implementation."], "top_k_doc_id": [6928, 1132, 4826, 6927, 2335, 6930, 6931, 3357, 3026, 3152, 6863, 1035, 3358, 4035, 5519], "orig_top_k_doc_id": [6928, 1132, 3357, 3026, 3152, 6863, 1035, 4826, 3358, 6930, 6927, 6931, 4035, 5519, 2335]}, {"qid": 4404, "question": "What was previous state of the art on factored dataset? in Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach", "answer": ["ROUGE-1 41.69\nROUGE-2 19.47\nROUGE-L 37.92", "41.69 ROUGE-1"], "top_k_doc_id": [6928, 1132, 4826, 6927, 2335, 6930, 6931, 5079, 4939, 7116, 4601, 1626, 5998, 2922, 527], "orig_top_k_doc_id": [6931, 1132, 5079, 6927, 6930, 6928, 2335, 4939, 7116, 4601, 1626, 5998, 2922, 527, 4826]}, {"qid": 2041, "question": "What baseline function is used in REINFORCE algorithm? in A Better Variant of Self-Critical Sequence Training", "answer": ["baseline for each sampled caption is defined as the average reward of the rest samples"], "top_k_doc_id": [6928, 1258, 3092, 4035, 6930, 7647, 3093, 6934, 6868, 4071, 3808, 1526, 5556, 1677, 5555], "orig_top_k_doc_id": [3092, 6928, 3093, 6934, 6930, 6868, 4035, 4071, 3808, 1526, 1258, 7647, 5556, 1677, 5555]}, {"qid": 4403, "question": "What are previoius similar models authors are referring to? in Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach", "answer": ["Abstractive and extractive models from Nallapati et al., 2016, Pointer generator models with and without coverage from See et al., 2017, and Reinforcement Learning models from Paulus et al., 2018, and Celikyilmaz et al., 2018.", "HierAttn \nabstractive model \nPointer Generator \nPointer Generator + coverage \nMLE+RL, with intra-attention\n DCA, MLE+RL\nPlain NSE"], "top_k_doc_id": [6928, 1132, 4826, 6927, 3983, 2074, 3985, 3986, 6043, 1930, 6955, 4292, 2684, 4878, 7351], "orig_top_k_doc_id": [6928, 3983, 2074, 4826, 6927, 3985, 3986, 6043, 1930, 6955, 1132, 4292, 2684, 4878, 7351]}]}
{"group_id": 309, "group_size": 6, "items": [{"qid": 2073, "question": "Which soft-selection approaches are evaluated? in Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis", "answer": ["LSTM and BERT "], "top_k_doc_id": [597, 2215, 2216, 598, 3152, 3153, 3154, 3155, 3156, 2873, 2874, 6181, 6183, 6640, 3102], "orig_top_k_doc_id": [3152, 3156, 3155, 3153, 3154, 597, 2216, 2874, 6181, 2215, 598, 6183, 6640, 3102, 2873]}, {"qid": 2075, "question": "Is the accuracy of the opinion snippet detection subtask reported? in Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [597, 2215, 2216, 598, 3152, 3153, 3154, 3155, 3156, 2873, 2874, 897, 893, 369, 3579], "orig_top_k_doc_id": [3152, 3156, 3153, 3155, 3154, 597, 598, 2216, 2215, 897, 2874, 893, 369, 2873, 3579]}, {"qid": 2058, "question": "Does the dataset contain non-English reviews? in Aspect and Opinion Term Extraction for Aspect Based Sentiment Analysis of Hotel Reviews Using Transfer Learning", "answer": ["Yes"], "top_k_doc_id": [597, 2215, 2216, 605, 897, 2874, 3129, 5159, 6181, 6183, 6640, 1043, 3102, 6616, 896], "orig_top_k_doc_id": [3129, 6181, 2216, 6183, 2215, 5159, 605, 6640, 1043, 3102, 2874, 597, 6616, 896, 897]}, {"qid": 2059, "question": "Does the paper report the performance of the method when is trained for more than 8 epochs? in Aspect and Opinion Term Extraction for Aspect Based Sentiment Analysis of Hotel Reviews Using Transfer Learning", "answer": ["No"], "top_k_doc_id": [597, 2215, 2216, 605, 897, 2874, 3129, 5159, 6181, 6183, 6640, 3130, 7472, 598, 3152], "orig_top_k_doc_id": [3129, 2215, 2216, 6183, 6181, 3130, 7472, 605, 5159, 897, 597, 6640, 598, 2874, 3152]}, {"qid": 2074, "question": "Is the model evaluated against the baseline also on single-aspect sentences? in Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis", "answer": ["No"], "top_k_doc_id": [597, 2215, 2216, 598, 3152, 3153, 3154, 3155, 3156, 6472, 893, 3129, 897, 605, 6183], "orig_top_k_doc_id": [3152, 3156, 3153, 3155, 3154, 597, 6472, 598, 893, 3129, 897, 2216, 2215, 605, 6183]}, {"qid": 3959, "question": "Which languages do they explore? in Sentiment Analysis On Indian Indigenous Languages: A Review On Multilingual Opinion Mining", "answer": ["Irish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia", "Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil", "Irish, Gujarati, Hindi, Arabic, English, Spanish, French, German, Tamil, Bengali, Odia, Marathi, Telugu, Hinglish"], "top_k_doc_id": [597, 2215, 7172, 6396, 6403, 2873, 5105, 3129, 2284, 6181, 6640, 2282, 1039, 3123, 6401], "orig_top_k_doc_id": [7172, 6396, 6403, 597, 2873, 5105, 3129, 2284, 6181, 6640, 2282, 2215, 1039, 3123, 6401]}]}
{"group_id": 310, "group_size": 6, "items": [{"qid": 2087, "question": "What metris are used for evaluation? in Improving Slot Filling by Utilizing Contextual Information", "answer": ["micro-averaged F1 score"], "top_k_doc_id": [940, 2646, 3172, 4075, 1296, 3480, 5015, 3481, 3482, 4348, 1988, 3169, 7154, 941, 3679], "orig_top_k_doc_id": [3169, 3172, 4075, 7154, 3481, 3480, 2646, 1988, 3482, 940, 1296, 5015, 4348, 941, 3679]}, {"qid": 2089, "question": "What are the baselines? in Improving Slot Filling by Utilizing Contextual Information", "answer": ["Adobe internal NLU tool, Pytext, Rasa"], "top_k_doc_id": [940, 2646, 3172, 4075, 1296, 3480, 5015, 3481, 3482, 4348, 1988, 3169, 7154, 767, 2643], "orig_top_k_doc_id": [3169, 1988, 3172, 4075, 5015, 2646, 3480, 3481, 1296, 7154, 940, 3482, 767, 4348, 2643]}, {"qid": 2090, "question": "How big is slot filing dataset? in Improving Slot Filling by Utilizing Contextual Information", "answer": ["Dataset has 1737 train, 497 dev and 559 test sentences."], "top_k_doc_id": [940, 2646, 3172, 4075, 1296, 3480, 5015, 3481, 3482, 4348, 1988, 3169, 2643, 2645, 941], "orig_top_k_doc_id": [3169, 3172, 3480, 2646, 1296, 940, 4348, 1988, 4075, 3481, 2643, 2645, 5015, 3482, 941]}, {"qid": 2222, "question": "What is the task of slot filling? in Impact of Coreference Resolution on Slot Filling", "answer": ["The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents."], "top_k_doc_id": [940, 2646, 3172, 4075, 1296, 3480, 5015, 3481, 3482, 4348, 2645, 2643, 943, 1300, 1034], "orig_top_k_doc_id": [3480, 3482, 3481, 2646, 940, 2645, 2643, 4075, 1296, 943, 5015, 4348, 1300, 3172, 1034]}, {"qid": 2088, "question": "How better is proposed model compared to baselines? in Improving Slot Filling by Utilizing Contextual Information", "answer": [" improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction"], "top_k_doc_id": [940, 2646, 3172, 4075, 1296, 3480, 5015, 3169, 1988, 2645, 1989, 7446, 1299, 7447, 1297], "orig_top_k_doc_id": [3169, 2646, 1988, 5015, 1296, 3172, 940, 3480, 2645, 1989, 7446, 1299, 4075, 7447, 1297]}, {"qid": 2086, "question": "How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting? in Improving Slot Filling by Utilizing Contextual Information", "answer": ["we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence"], "top_k_doc_id": [940, 2646, 3172, 4075, 3169, 7154, 1988, 3821, 4590, 7448, 7447, 7446, 4074, 941, 4076], "orig_top_k_doc_id": [3169, 3172, 4075, 7154, 1988, 3821, 940, 4590, 2646, 7448, 7447, 7446, 4074, 941, 4076]}]}
{"group_id": 311, "group_size": 6, "items": [{"qid": 2235, "question": "What were the evaluation metrics used? in Towards Task-Oriented Dialogue in Mixed Domains", "answer": ["entity match rate, BLEU score, Success F1 score"], "top_k_doc_id": [3191, 3507, 3508, 1168, 3509, 3510, 3679, 5800, 1171, 1677, 3192, 965, 3193, 1170, 3190], "orig_top_k_doc_id": [3507, 3192, 3191, 3509, 1171, 3508, 1168, 3510, 965, 3679, 3193, 5800, 1170, 1677, 3190]}, {"qid": 2236, "question": "What is the size of the dataset? in Towards Task-Oriented Dialogue in Mixed Domains", "answer": ["3029"], "top_k_doc_id": [3191, 3507, 3508, 1168, 3509, 3510, 3679, 5800, 1171, 1677, 3192, 196, 1169, 1678, 7379], "orig_top_k_doc_id": [3508, 3507, 3509, 5800, 3510, 3191, 196, 1168, 1677, 3192, 1169, 1678, 3679, 1171, 7379]}, {"qid": 2237, "question": "What multi-domain dataset is used? in Towards Task-Oriented Dialogue in Mixed Domains", "answer": ["KVRET"], "top_k_doc_id": [3191, 3507, 3508, 1168, 3509, 3510, 3679, 5800, 196, 197, 2276, 200, 2234, 3193, 1169], "orig_top_k_doc_id": [3507, 3508, 3509, 3510, 5800, 196, 3191, 1168, 3679, 197, 2276, 200, 2234, 3193, 1169]}, {"qid": 2238, "question": "Which domains did they explored? in Towards Task-Oriented Dialogue in Mixed Domains", "answer": ["calendar, weather, navigation"], "top_k_doc_id": [3191, 3507, 3508, 1168, 3509, 3510, 3679, 5800, 196, 197, 2276, 1171, 1075, 3194, 202], "orig_top_k_doc_id": [3507, 3508, 3510, 1171, 3509, 1075, 3194, 2276, 3191, 197, 1168, 196, 5800, 3679, 202]}, {"qid": 1768, "question": "How do slot binary classifiers improve performance? in Flexibly-Structured Model for Task-Oriented Dialogues", "answer": ["by adding extra supervision to generate the slots that will be present in the response"], "top_k_doc_id": [3191, 3507, 3508, 197, 2550, 2555, 5744, 7156, 200, 2554, 2551, 3509, 3192, 3679, 2234], "orig_top_k_doc_id": [2555, 2550, 200, 2554, 2551, 3507, 3191, 5744, 3508, 197, 3509, 7156, 3192, 3679, 2234]}, {"qid": 1769, "question": "What baselines have been used in this work? in Flexibly-Structured Model for Task-Oriented Dialogues", "answer": ["NDM, LIDM, KVRN, and TSCP/RL"], "top_k_doc_id": [3191, 3507, 3508, 197, 2550, 2555, 5744, 7156, 6036, 196, 1168, 6651, 6858, 4669, 5793], "orig_top_k_doc_id": [2555, 5744, 3191, 3508, 2550, 6036, 197, 196, 1168, 6651, 7156, 3507, 6858, 4669, 5793]}]}
{"group_id": 312, "group_size": 6, "items": [{"qid": 2241, "question": "What size are the corpora? in Assessing the Applicability of Authorship Verification Methods", "answer": ["80 excerpts from scientific works, collection of 1,645 chat conversations, collection of 200 aggregated postings"], "top_k_doc_id": [4002, 3511, 3512, 3513, 3514, 3515, 3517, 3518, 3519, 4896, 3516, 1601, 4007, 4898, 2720], "orig_top_k_doc_id": [3511, 3512, 3519, 3515, 3517, 3514, 3513, 4896, 3518, 4002, 3516, 4898, 1601, 2720, 4007]}, {"qid": 2242, "question": "What is a self-compiled corpus? in Assessing the Applicability of Authorship Verification Methods", "answer": [" restrict the content of each text to the abstract and conclusion of the original work, considered other parts of the original works such as introduction or discussion sections, extracted text portions are appropriate for the AV task, each original work was preprocessed manually, removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms"], "top_k_doc_id": [4002, 3511, 3512, 3513, 3514, 3515, 3517, 3518, 3519, 4896, 3516, 1601, 4007, 2788, 1755], "orig_top_k_doc_id": [3512, 3515, 3511, 3519, 3513, 3516, 3514, 3517, 4896, 3518, 4002, 1601, 2788, 4007, 1755]}, {"qid": 2243, "question": "What are the 12 AV approaches which are examined? in Assessing the Applicability of Authorship Verification Methods", "answer": ["MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD"], "top_k_doc_id": [4002, 3511, 3512, 3513, 3514, 3515, 3517, 3518, 3519, 4896, 3516, 3520, 2720, 2208, 4898], "orig_top_k_doc_id": [3511, 3519, 3517, 3512, 3518, 3515, 3514, 3516, 3513, 3520, 4896, 2720, 2208, 4898, 4002]}, {"qid": 2240, "question": "Which is the best performing method? in Assessing the Applicability of Authorship Verification Methods", "answer": ["Caravel, COAV and NNCD"], "top_k_doc_id": [4002, 3511, 3512, 3513, 3514, 3515, 3517, 3518, 3519, 4896, 5395, 4895, 4898, 3487, 1601], "orig_top_k_doc_id": [3512, 3511, 3517, 3519, 4896, 3514, 3515, 3513, 3518, 4002, 5395, 4895, 4898, 3487, 1601]}, {"qid": 2239, "question": "Do they report results only on English data? in Assessing the Applicability of Authorship Verification Methods", "answer": ["Yes"], "top_k_doc_id": [4002, 3511, 3512, 3513, 3514, 3515, 3517, 3518, 3519, 4896, 3487, 2720, 5473, 3486, 4006], "orig_top_k_doc_id": [3512, 3511, 3519, 3514, 3515, 4896, 3517, 3513, 3518, 4002, 3487, 2720, 5473, 3486, 4006]}, {"qid": 2456, "question": "Does this paper address the variation among English dialects regarding these hedges? in A Label Semantics Approach to Linguistic Hedges", "answer": ["No"], "top_k_doc_id": [4002, 4082, 4093, 4084, 4094, 4086, 4090, 4085, 4092, 4091, 4007, 4088, 1495, 4087, 6456], "orig_top_k_doc_id": [4082, 4093, 4084, 4094, 4086, 4090, 4085, 4092, 4091, 4002, 4007, 4088, 1495, 4087, 6456]}]}
{"group_id": 313, "group_size": 6, "items": [{"qid": 2267, "question": "What baseline is used? in Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter", "answer": ["SVM"], "top_k_doc_id": [3574, 5295, 243, 245, 412, 3575, 3577, 4515, 5292, 5145, 5146, 5288, 6176, 3576, 5291], "orig_top_k_doc_id": [3577, 3575, 5288, 3574, 5145, 245, 5146, 5292, 6176, 3576, 4515, 412, 5291, 243, 5295]}, {"qid": 2269, "question": "What are the near-offensive language categories? in Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter", "answer": ["inappropriate, discriminating"], "top_k_doc_id": [3574, 5295, 243, 245, 412, 3575, 3577, 4515, 5292, 5145, 5146, 5288, 6176, 6131, 5144], "orig_top_k_doc_id": [3575, 3577, 3574, 5145, 245, 6131, 6176, 5295, 5144, 5146, 5288, 4515, 5292, 412, 243]}, {"qid": 2826, "question": "Do they perform error analysis? in Offensive Language Detection: A Comparative Analysis", "answer": ["No", "No"], "top_k_doc_id": [3574, 5295, 3445, 4948, 4950, 5144, 5168, 5288, 245, 412, 3577, 5976, 3314, 5293, 1725], "orig_top_k_doc_id": [4948, 5295, 3574, 3445, 3314, 5168, 3577, 5144, 245, 4950, 5288, 5293, 412, 5976, 1725]}, {"qid": 2827, "question": "How do their results compare to state-of-the-art? in Offensive Language Detection: A Comparative Analysis", "answer": ["No", "No"], "top_k_doc_id": [3574, 5295, 3445, 4948, 4950, 5144, 5168, 5288, 245, 412, 3577, 5976, 32, 876, 6519], "orig_top_k_doc_id": [412, 5168, 4948, 5288, 5976, 3577, 3445, 3574, 4950, 32, 876, 245, 6519, 5295, 5144]}, {"qid": 2266, "question": "By how much does transfer learning improve performance on this task? in Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter", "answer": ["In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%"], "top_k_doc_id": [3574, 5295, 243, 245, 412, 3575, 3577, 4515, 5292, 5291, 3576, 2954, 5815, 1782, 2953], "orig_top_k_doc_id": [3577, 3574, 3575, 245, 5291, 3576, 2954, 5292, 5815, 4515, 1782, 243, 5295, 412, 2953]}, {"qid": 2828, "question": "What is the Random Kitchen Sink approach? in Offensive Language Detection: A Comparative Analysis", "answer": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "top_k_doc_id": [3574, 5295, 3445, 4948, 4950, 5144, 5168, 5288, 4949, 6131, 3575, 789, 876, 6519, 5169], "orig_top_k_doc_id": [4950, 4949, 4948, 5288, 5168, 3574, 5144, 6131, 3445, 3575, 789, 5295, 876, 6519, 5169]}]}
{"group_id": 314, "group_size": 6, "items": [{"qid": 2388, "question": "What evaluation metric were used for presenting results?  in Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents", "answer": ["F$_1$, precision, and recall"], "top_k_doc_id": [3857, 1625, 3859, 4439, 6392, 7283, 3568, 6910, 2310, 6604, 2268, 1091, 2308, 4620, 7285], "orig_top_k_doc_id": [3859, 3857, 4439, 1625, 6392, 2310, 6604, 2268, 7283, 3568, 1091, 2308, 6910, 4620, 7285]}, {"qid": 2389, "question": "Was the structure of regulatory filings exploited when training the model?  in Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents", "answer": ["No"], "top_k_doc_id": [3857, 1625, 3859, 4439, 6392, 7283, 3568, 6910, 3858, 804, 5956, 6193, 5817, 3094, 6383], "orig_top_k_doc_id": [3857, 3859, 3858, 804, 4439, 5956, 1625, 6392, 6193, 3568, 5817, 7283, 6910, 3094, 6383]}, {"qid": 2390, "question": "What type of documents are supported by the annotation platform? in Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents", "answer": ["Variety of formats supported (PDF, Word...), user can define content elements of document"], "top_k_doc_id": [3857, 1625, 3859, 4439, 6392, 7283, 4614, 4348, 1628, 5908, 2268, 5186, 131, 6005, 6882], "orig_top_k_doc_id": [3857, 3859, 4439, 1625, 4614, 4348, 1628, 7283, 5908, 2268, 5186, 131, 6392, 6005, 6882]}, {"qid": 2387, "question": "At what text unit/level were documents processed? in Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents", "answer": ["documents are segmented into paragraphs and processed at the paragraph level"], "top_k_doc_id": [3857, 1625, 3859, 4439, 6392, 3858, 6472, 2448, 7285, 259, 57, 6471, 6318, 7314, 1091], "orig_top_k_doc_id": [3857, 3859, 4439, 1625, 3858, 6472, 2448, 7285, 259, 57, 6471, 6318, 6392, 7314, 1091]}, {"qid": 2513, "question": "What codemixed language pairs are evaluated? in Small and Practical BERT Models for Sequence Labeling", "answer": ["Hindi-English"], "top_k_doc_id": [3857, 21, 1782, 2709, 3368, 3858, 4363, 4510, 4290, 7287, 2308, 6094, 1363, 4817, 2908], "orig_top_k_doc_id": [4290, 4363, 3368, 2709, 3857, 1782, 7287, 2308, 4510, 6094, 1363, 21, 4817, 2908, 3858]}, {"qid": 2514, "question": "How do they compress the model? in Small and Practical BERT Models for Sequence Labeling", "answer": ["we extract sentences from Wikipedia in languages for which public multilingual is pretrained. For each sentence, we use the open-source BERT wordpiece tokenizer BIBREF4 , BIBREF1 and compute cross-entropy loss for each wordpiece: INLINEFORM0"], "top_k_doc_id": [3857, 21, 1782, 2709, 3368, 3858, 4363, 4510, 4624, 7630, 4771, 2682, 7633, 1414, 7791], "orig_top_k_doc_id": [3857, 21, 4510, 4624, 1782, 3368, 7630, 4771, 2682, 3858, 2709, 4363, 7633, 1414, 7791]}]}
{"group_id": 315, "group_size": 6, "items": [{"qid": 2405, "question": "Do they compare their algorithm to voting without weights? in Dover: A Method for Combining Diarization Outputs", "answer": ["No"], "top_k_doc_id": [3924, 7793, 7794, 3922, 3923, 3925, 3998, 7795, 7796, 7799, 1381, 2784, 6772, 4588, 7012], "orig_top_k_doc_id": [3925, 3922, 3923, 3924, 7794, 2784, 7795, 7793, 7799, 3998, 7796, 1381, 6772, 4588, 7012]}, {"qid": 2406, "question": "How do they assign weights between votes in their DOVER algorithm? in Dover: A Method for Combining Diarization Outputs", "answer": ["No"], "top_k_doc_id": [3924, 7793, 7794, 3922, 3923, 3925, 3998, 7795, 7796, 7799, 1381, 4885, 743, 4886, 6806], "orig_top_k_doc_id": [3925, 3922, 3923, 3924, 4885, 7794, 743, 7795, 4886, 7796, 7793, 1381, 6806, 7799, 3998]}, {"qid": 2404, "question": "On average, by how much do they reduce the diarization error? in Dover: A Method for Combining Diarization Outputs", "answer": ["No"], "top_k_doc_id": [3924, 7793, 7794, 3922, 3923, 3925, 3998, 7795, 7796, 7799, 1838, 3999, 2473, 2474, 7404], "orig_top_k_doc_id": [3925, 3922, 3924, 3923, 7794, 7793, 7795, 3998, 7796, 7799, 1838, 3999, 2473, 2474, 7404]}, {"qid": 2438, "question": "How long are dialogue recordings used for evaluation? in Simultaneous Speech Recognition and Speaker Diarization for Monaural Dialogue Recordings with Target-Speaker Acoustic Models", "answer": ["average 12.8 min per recording"], "top_k_doc_id": [3924, 7793, 7794, 3922, 3923, 3925, 3998, 7795, 4000, 3999, 7404, 4001, 5219, 7832, 844], "orig_top_k_doc_id": [3998, 4000, 7794, 3924, 3999, 7793, 3925, 7795, 7404, 3922, 3923, 4001, 5219, 7832, 844]}, {"qid": 5006, "question": "Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand? in Advances in Online Audio-Visual Meeting Transcription", "answer": ["Face tracking is performed in an automatic tracklet module, face identification is performed by creating a face embedding from the output of a CNN, the embedding is then compared to a gallery of each person's face using a discriminative classifier (SVM) and localization is modelled with a complex angular central Gaussian model. All are merged in a statistical model. ", "Input in ML model"], "top_k_doc_id": [3924, 7793, 7794, 521, 2800, 7138, 7797, 7799, 7798, 7796, 929, 7037, 3175, 2923, 3432], "orig_top_k_doc_id": [7799, 7794, 7793, 7797, 2800, 7138, 7798, 7796, 929, 3924, 7037, 521, 3175, 2923, 3432]}, {"qid": 5007, "question": "What are baselines used? in Advances in Online Audio-Visual Meeting Transcription", "answer": ["A diarization system using only face identification and SSL", "The baseline system was a conventional speech recognition approach using single-output beamforming."], "top_k_doc_id": [3924, 7793, 7794, 521, 2800, 7138, 7797, 7799, 3949, 6995, 4861, 6858, 286, 285, 1161], "orig_top_k_doc_id": [7793, 7794, 7799, 3924, 2800, 3949, 6995, 4861, 6858, 7797, 286, 285, 1161, 521, 7138]}]}
{"group_id": 316, "group_size": 6, "items": [{"qid": 2427, "question": "What was the best team's system? in CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain", "answer": ["No"], "top_k_doc_id": [55, 5956, 5957, 6955, 3966, 415, 3967, 5521, 5577, 7795, 6305, 6309, 1795, 2280, 5959], "orig_top_k_doc_id": [3966, 3967, 55, 6955, 6305, 7795, 5956, 5577, 5957, 6309, 5521, 1795, 415, 2280, 5959]}, {"qid": 2428, "question": "What are the baselines? in CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain", "answer": ["CNN, LSTM, BERT"], "top_k_doc_id": [55, 5956, 5957, 6955, 3966, 415, 3967, 5521, 5577, 7795, 5522, 6657, 6957, 5518, 6713], "orig_top_k_doc_id": [3966, 3967, 6955, 5521, 5577, 5522, 5956, 6657, 5957, 55, 6957, 415, 5518, 7795, 6713]}, {"qid": 3953, "question": "What are their results on this task? in Long-length Legal Document Classification", "answer": ["98.11% accuracy with a 0.4% improvement upon the benchmark model", " BiLSTM based framework and the linear classifier reaches a 97.97% accuracy, SVM classifier reaches a remarkable 98.11% accuracy", "F1 score of 97.97 for a linear classifier and 98.11 for a SVM classifier"], "top_k_doc_id": [55, 5956, 5957, 6955, 3966, 4439, 4920, 5578, 6381, 6383, 5577, 4652, 6306, 5960, 6307], "orig_top_k_doc_id": [6381, 3966, 6955, 4439, 55, 5957, 4920, 5578, 6383, 5577, 5956, 4652, 6306, 5960, 6307]}, {"qid": 3954, "question": "How is the text segmented? in Long-length Legal Document Classification", "answer": ["dividing documents into chunks before processing", "No", "They simply split document in chunks, get embedding for each chunk and train BiLSTM models with embeddings."], "top_k_doc_id": [55, 5956, 5957, 6955, 3966, 4439, 4920, 5578, 6381, 6383, 167, 6386, 165, 5519, 7761], "orig_top_k_doc_id": [6381, 167, 4439, 5957, 6955, 3966, 4920, 5956, 55, 6386, 6383, 5578, 165, 5519, 7761]}, {"qid": 2548, "question": "How big is dataset used for fine-tuning BERT? in BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding", "answer": ["hundreds of thousands of legal agreements"], "top_k_doc_id": [55, 5956, 5957, 6955, 4439, 4440, 2268, 2265, 7460, 5960, 533, 5959, 3857, 393, 5958], "orig_top_k_doc_id": [4439, 4440, 2268, 6955, 5956, 2265, 7460, 5957, 5960, 533, 5959, 3857, 393, 5958, 55]}, {"qid": 3627, "question": "Was the automatic annotation evaluated? in A Dataset of German Legal Documents for Named Entity Recognition", "answer": ["No", "No", "Yes", "Yes"], "top_k_doc_id": [55, 5956, 5957, 1098, 5959, 5960, 5961, 4574, 65, 6384, 438, 2320, 6381, 5376, 4594], "orig_top_k_doc_id": [5956, 5957, 1098, 5959, 5960, 5961, 4574, 65, 6384, 438, 2320, 55, 6381, 5376, 4594]}]}
{"group_id": 317, "group_size": 6, "items": [{"qid": 2648, "question": "How are models evaluated in this human-machine communication game? in Learning Autocomplete Systems as a Communication Game", "answer": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "top_k_doc_id": [6344, 6347, 6348, 318, 558, 3789, 4660, 4661, 6185, 4506, 4535, 6345, 4509, 3790, 6354], "orig_top_k_doc_id": [4660, 6344, 6348, 6347, 318, 558, 4509, 4535, 4506, 3789, 6345, 3790, 4661, 6185, 6354]}, {"qid": 2651, "question": "What are the baselines used? in Learning Autocomplete Systems as a Communication Game", "answer": ["Unif and Stopword", "Unif and Stopword"], "top_k_doc_id": [6344, 6347, 6348, 318, 558, 3789, 4660, 4661, 6185, 4506, 4535, 6345, 4662, 5687, 562], "orig_top_k_doc_id": [4660, 4661, 6348, 6344, 558, 6185, 4662, 3789, 318, 4506, 6347, 5687, 4535, 6345, 562]}, {"qid": 2649, "question": "How many participants were trying this communication game? in Learning Autocomplete Systems as a Communication Game", "answer": ["100 ", "100 crowdworkers "], "top_k_doc_id": [6344, 6347, 6348, 318, 558, 3789, 4660, 4661, 6185, 4506, 4231, 1074, 4509, 1637, 562], "orig_top_k_doc_id": [4660, 6344, 6347, 558, 4231, 3789, 4661, 4506, 6348, 1074, 318, 4509, 1637, 6185, 562]}, {"qid": 2650, "question": "What user variations have been tested? in Learning Autocomplete Systems as a Communication Game", "answer": ["completion times and accuracies ", "No"], "top_k_doc_id": [6344, 6347, 6348, 318, 558, 3789, 4660, 4661, 6185, 12, 3772, 1082, 7261, 541, 1637], "orig_top_k_doc_id": [4660, 4661, 558, 6344, 12, 3772, 6348, 1082, 7261, 6185, 541, 3789, 318, 6347, 1637]}, {"qid": 3939, "question": "What dataset is used? in Translating Neuralese", "answer": ["the XKCD color dataset, the Caltech\u2013UCSD Birds dataset", "XKCD color dataset, Caltech\u2013UCSD Birds dataset, actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game", "XKCD color dataset; Caltech-UCSD Birds dataset; game data from Amazon Mechanical Turk workers "], "top_k_doc_id": [6344, 6347, 6348, 6345, 6346, 2273, 5872, 4769, 5704, 5908, 5773, 5792, 5871, 6857, 5844], "orig_top_k_doc_id": [6348, 6344, 6345, 6347, 6346, 2273, 5872, 4769, 5704, 5908, 5773, 5792, 5871, 6857, 5844]}, {"qid": 2566, "question": "What are three possible phases for language formation? in Phase transitions in a decentralized graph-based approach to human language", "answer": ["Phase I: $\\langle cc \\rangle $ increases smoothly for $\\wp < 0.4$, indicating that for this domain there is a small correlation between word neighborhoods. Full vocabularies are attained also for $\\wp < 0.4$, Phase II: a drastic transition appears at the critical domain $\\wp ^* \\in (0.4,0.6)$, in which $\\langle cc \\rangle $ shifts abruptly towards 1. An abrupt change in $V(t_f)$ versus $\\wp $ is also found (Fig. FIGREF16) for $\\wp ^*$, Phase III: single-word languages dominate for $\\wp > 0.6$. The maximum value of $\\langle cc \\rangle $ indicate that word neighborhoods are completely correlated"], "top_k_doc_id": [6344, 4508, 4506, 4507, 558, 559, 853, 6630, 407, 5704, 267, 6888, 562, 1588, 1380], "orig_top_k_doc_id": [4508, 4506, 4507, 558, 559, 6344, 853, 6630, 407, 5704, 267, 6888, 562, 1588, 1380]}]}
{"group_id": 318, "group_size": 6, "items": [{"qid": 2692, "question": "Do they report results only on English data? in Towards a Continuous Knowledge Learning Engine for Chatbots", "answer": ["No", "No"], "top_k_doc_id": [3357, 4719, 4726, 5431, 5432, 5429, 2970, 4738, 5428, 6587, 7514, 5430, 4725, 2464, 5977], "orig_top_k_doc_id": [5432, 3357, 5431, 2970, 5429, 4726, 4719, 5430, 4725, 2464, 4738, 7514, 6587, 5977, 5428]}, {"qid": 2694, "question": "What baseline is used in the experiments? in Towards a Continuous Knowledge Learning Engine for Chatbots", "answer": ["versions of LiLi", "various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS"], "top_k_doc_id": [3357, 4719, 4726, 5431, 5432, 5429, 2970, 4738, 5428, 6587, 7514, 4317, 4467, 6410, 3735], "orig_top_k_doc_id": [5432, 5431, 3357, 2970, 5429, 4726, 5428, 4719, 6587, 4317, 4738, 4467, 6410, 7514, 3735]}, {"qid": 2693, "question": "How much better than the baseline is LiLi? in Towards a Continuous Knowledge Learning Engine for Chatbots", "answer": ["In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. \n", "No"], "top_k_doc_id": [3357, 4719, 4726, 5431, 5432, 4720, 4721, 4724, 4725, 4723, 2970, 4722, 317, 3735, 6587], "orig_top_k_doc_id": [4725, 4726, 4719, 4720, 5432, 4721, 4724, 3357, 4723, 2970, 4722, 5431, 317, 3735, 6587]}, {"qid": 2695, "question": "In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation? in Towards a Continuous Knowledge Learning Engine for Chatbots", "answer": ["newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning", "Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. "], "top_k_doc_id": [3357, 4719, 4726, 5431, 5432, 4720, 4721, 4724, 4725, 5798, 5426, 5427, 5425, 6583, 4656], "orig_top_k_doc_id": [4719, 4720, 4725, 4726, 4721, 5432, 5798, 5426, 5427, 5425, 4724, 3357, 5431, 6583, 4656]}, {"qid": 2697, "question": "What are the components of the general knowledge learning engine? in Towards a Continuous Knowledge Learning Engine for Chatbots", "answer": ["Answer with content missing: (list)\nLiLi should have the following capabilities:\n1. to formulate an inference strategy for a given query that embeds processing and interactive actions.\n2. to learn interaction behaviors (deciding what to ask and when to ask the user).\n3. to leverage the acquired knowledge in the current and future inference process.\n4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.", "Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )"], "top_k_doc_id": [3357, 4719, 4726, 5431, 5432, 5429, 7038, 4317, 4424, 7037, 6479, 4725, 130, 166, 5800], "orig_top_k_doc_id": [5432, 4726, 5431, 7038, 4317, 4424, 5429, 7037, 6479, 3357, 4725, 4719, 130, 166, 5800]}, {"qid": 2426, "question": "Why they conclude that the usage of Gated-Attention provides no competitive advantage against concatenation in this setting? in Task-Oriented Language Grounding for Language Input with Multiple Sub-Goals of Non-Linear Order", "answer": ["concatenation consistently outperforms the gated-attention mechanism for both training and testing instructions"], "top_k_doc_id": [3357, 3963, 3964, 3965, 200, 317, 6583, 1675, 3912, 5744, 3825, 1541, 316, 4199, 2970], "orig_top_k_doc_id": [3963, 3964, 3965, 3357, 200, 317, 6583, 1675, 3912, 5744, 3825, 1541, 316, 4199, 2970]}]}
{"group_id": 319, "group_size": 6, "items": [{"qid": 3039, "question": "How many users do they look at? in Predicting the Industry of Users on Social Media", "answer": ["22,880 users", "20,000"], "top_k_doc_id": [5191, 5192, 241, 4989, 5195, 330, 521, 522, 1957, 6629, 3135, 5102, 5467, 743, 3131], "orig_top_k_doc_id": [5191, 5195, 5192, 330, 3135, 241, 521, 5102, 522, 4989, 6629, 5467, 743, 3131, 1957]}, {"qid": 3041, "question": "What model did they use for their system? in Predicting the Industry of Users on Social Media", "answer": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "top_k_doc_id": [5191, 5192, 241, 4989, 5195, 330, 521, 522, 1957, 6629, 3047, 242, 3486, 804, 803], "orig_top_k_doc_id": [5191, 5195, 5192, 241, 4989, 3047, 522, 1957, 330, 242, 3486, 804, 521, 6629, 803]}, {"qid": 3040, "question": "What do they mean by a person's industry? in Predicting the Industry of Users on Social Media", "answer": ["the aggregate of enterprises in a particular field", "the aggregate of enterprises in a particular field"], "top_k_doc_id": [5191, 5192, 241, 4989, 5195, 242, 803, 804, 5193, 5194, 6396, 742, 335, 2803, 59], "orig_top_k_doc_id": [5191, 5195, 5192, 241, 5194, 804, 4989, 803, 6396, 742, 242, 335, 5193, 2803, 59]}, {"qid": 3042, "question": "What social media platform did they look at? in Predicting the Industry of Users on Social Media", "answer": [" http://www.blogger.com", "http://www.blogger.com"], "top_k_doc_id": [5191, 5192, 241, 4989, 5195, 330, 5102, 5906, 3135, 742, 3131, 5812, 743, 6896, 3730], "orig_top_k_doc_id": [5191, 5102, 4989, 5192, 5195, 5906, 3135, 330, 742, 3131, 241, 5812, 743, 6896, 3730]}, {"qid": 3043, "question": "What are the industry classes defined in this paper? in Predicting the Industry of Users on Social Media", "answer": ["technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive", "Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive."], "top_k_doc_id": [5191, 5192, 241, 4989, 5195, 242, 803, 804, 5193, 5194, 3203, 4644, 6470, 1957, 6463], "orig_top_k_doc_id": [5191, 5192, 5195, 241, 4989, 5194, 3203, 4644, 5193, 242, 6470, 804, 1957, 803, 6463]}, {"qid": 2166, "question": "Is is known whether Sina Weibo posts are censored by humans or some automatic classifier? in Linguistic Fingerprints of Internet Censorship: the Case of SinaWeibo", "answer": ["No"], "top_k_doc_id": [5191, 5192, 3316, 3318, 3317, 3320, 2076, 5552, 2077, 3319, 5930, 2968, 579, 1481, 1205], "orig_top_k_doc_id": [3316, 3318, 3317, 3320, 2076, 5552, 5192, 2077, 3319, 5930, 5191, 2968, 579, 1481, 1205]}]}
{"group_id": 320, "group_size": 6, "items": [{"qid": 3082, "question": "What was the baseline model? in UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B", "answer": ["by answering always YES (in batch 2 and 3) "], "top_k_doc_id": [5231, 5232, 5233, 5235, 5237, 6244, 6245, 217, 1106, 1108, 1109, 302, 1107, 5236, 7459], "orig_top_k_doc_id": [217, 5231, 6244, 1108, 1109, 6245, 5233, 1107, 1106, 5237, 5232, 5235, 302, 5236, 7459]}, {"qid": 3083, "question": "What dataset did they use? in UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B", "answer": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "top_k_doc_id": [5231, 5232, 5233, 5235, 5237, 6244, 6245, 217, 1106, 1108, 1109, 302, 1107, 5236, 7459], "orig_top_k_doc_id": [217, 5231, 6244, 1108, 1109, 5233, 6245, 5232, 1106, 5237, 1107, 5236, 302, 5235, 7459]}, {"qid": 3084, "question": "What was their highest recall score? in UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B", "answer": ["0.7033", "0.7033"], "top_k_doc_id": [5231, 5232, 5233, 5235, 5237, 6244, 6245, 217, 1106, 1108, 1109, 302, 1107, 5236, 219], "orig_top_k_doc_id": [217, 5231, 6244, 1109, 1108, 5233, 5237, 6245, 5235, 1107, 5232, 1106, 302, 5236, 219]}, {"qid": 3085, "question": "What was their highest MRR score? in UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B", "answer": ["0.5115", "0.6103"], "top_k_doc_id": [5231, 5232, 5233, 5235, 5237, 6244, 6245, 217, 1106, 1108, 1109, 302, 1107, 5234, 5046], "orig_top_k_doc_id": [5231, 217, 6244, 1109, 1108, 5237, 5235, 5233, 6245, 1107, 5232, 1106, 302, 5234, 5046]}, {"qid": 3865, "question": "Are answers in this dataset guaranteed to be substrings of the text? If not, what is the coverage of answers being substrings? in Neural Question Answering at BioASQ 5B", "answer": ["Yes", "No, the answers can also be summaries or yes/no.", "No"], "top_k_doc_id": [5231, 5232, 5233, 5235, 5237, 6244, 6245, 217, 1106, 1108, 1109, 5046, 5045, 5234, 6953], "orig_top_k_doc_id": [6244, 6245, 5237, 5233, 5231, 5046, 5232, 1108, 5235, 5045, 1106, 217, 5234, 1109, 6953]}, {"qid": 3866, "question": "How much is the gap between pretraining on SQuAD and not pretraining on SQuAD? in Neural Question Answering at BioASQ 5B", "answer": ["No", "No", "No"], "top_k_doc_id": [5231, 5232, 5233, 5235, 5237, 6244, 6245, 5046, 5045, 5044, 5236, 4791, 3809, 5473, 7462], "orig_top_k_doc_id": [5046, 5045, 5044, 6245, 5236, 5233, 5237, 4791, 3809, 5231, 6244, 5473, 5232, 5235, 7462]}]}
{"group_id": 321, "group_size": 6, "items": [{"qid": 3139, "question": "What are the two neural embedding models? in Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "answer": ["Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)", "Concept Raw Context model, Concept-Concept Context model"], "top_k_doc_id": [5297, 904, 5298, 5299, 5300, 5301, 6475, 6545, 4274, 7140, 5305, 5967, 3079, 6392, 1196], "orig_top_k_doc_id": [5298, 5301, 5300, 5297, 5299, 904, 6475, 6545, 5305, 7140, 4274, 5967, 3079, 6392, 1196]}, {"qid": 3140, "question": "which neural embedding model works better? in Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "answer": ["the CRX model", "3C model"], "top_k_doc_id": [5297, 904, 5298, 5299, 5300, 5301, 6475, 6545, 4274, 7140, 1301, 4275, 5136, 1661, 3300], "orig_top_k_doc_id": [5298, 5301, 5300, 5299, 5297, 904, 6545, 1301, 4275, 4274, 7140, 6475, 5136, 1661, 3300]}, {"qid": 3138, "question": "What is the benchmark dataset? in Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "answer": ["a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data", "dataset created by ceccarelli2013learning from the CoNLL 2003 data"], "top_k_doc_id": [5297, 904, 5298, 5299, 5300, 5301, 6475, 6545, 5305, 2194, 2195, 5311, 4105, 1217, 6050], "orig_top_k_doc_id": [5301, 5300, 5298, 5299, 5297, 6545, 5305, 2194, 2195, 6475, 5311, 4105, 904, 1217, 6050]}, {"qid": 2027, "question": "What are their baseline methods? in Bag of Tricks for Efficient Text Classification", "answer": ["simple linear model with rank constraint, Hierarchical softmax, N-gram features"], "top_k_doc_id": [5297, 1878, 2063, 3068, 5754, 6534, 6752, 6771, 6850, 5755, 4218, 2803, 52, 502, 231], "orig_top_k_doc_id": [3068, 6534, 5754, 6752, 6850, 2063, 6771, 5755, 4218, 1878, 2803, 52, 5297, 502, 231]}, {"qid": 2028, "question": "Which datasets are used for evaluation? in Bag of Tricks for Efficient Text Classification", "answer": ["No"], "top_k_doc_id": [5297, 1878, 2063, 3068, 5754, 6534, 6752, 6771, 5486, 1877, 2058, 1092, 5274, 6070, 6579], "orig_top_k_doc_id": [6534, 3068, 6771, 5486, 2063, 1877, 2058, 1092, 5274, 1878, 5297, 6070, 6752, 5754, 6579]}, {"qid": 3141, "question": "What is the degree of dimension reduction of the efficient aggregation method? in Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "answer": ["The number of dimensions can be reduced by up to 212 times."], "top_k_doc_id": [5297, 904, 5298, 5299, 5300, 5301, 5305, 5311, 3300, 52, 5310, 3068, 3079, 5309, 7216], "orig_top_k_doc_id": [5298, 5297, 5301, 5300, 5299, 5305, 5311, 3300, 52, 5310, 3068, 904, 3079, 5309, 7216]}]}
{"group_id": 322, "group_size": 6, "items": [{"qid": 3162, "question": "Which datasets are used for evaluation? in Multi-class Hierarchical Question Classification for Multiple Choice Science Exams", "answer": ["ARC , TREC, GARD , MLBioMedLAT ", "ARC, TREC, GARD, MLBioMedLAT"], "top_k_doc_id": [346, 3097, 3098, 3099, 5327, 5328, 5330, 5331, 7071, 7590, 345, 5329, 7589, 1141, 4463], "orig_top_k_doc_id": [5328, 5327, 7590, 3098, 3097, 5331, 5330, 346, 3099, 1141, 345, 7589, 5329, 7071, 4463]}, {"qid": 3163, "question": "What previous methods is their model compared to? in Multi-class Hierarchical Question Classification for Multiple Choice Science Exams", "answer": ["bag-of-words model, CNN"], "top_k_doc_id": [346, 3097, 3098, 3099, 5327, 5328, 5330, 5331, 7071, 7590, 1141, 3094, 4463, 5329, 7072], "orig_top_k_doc_id": [5328, 5327, 5331, 5330, 3097, 3098, 7590, 3099, 346, 7071, 4463, 3094, 5329, 7072, 1141]}, {"qid": 3164, "question": "Did they use a crowdsourcing platform? in Multi-class Hierarchical Question Classification for Multiple Choice Science Exams", "answer": ["No", "No"], "top_k_doc_id": [346, 3097, 3098, 3099, 5327, 5328, 5330, 5331, 7071, 7590, 345, 5329, 7589, 8, 3094], "orig_top_k_doc_id": [5328, 5327, 7590, 3099, 3098, 5331, 3097, 5330, 346, 345, 7589, 7071, 8, 5329, 3094]}, {"qid": 3165, "question": "How was the dataset collected? in Multi-class Hierarchical Question Classification for Multiple Choice Science Exams", "answer": ["from 3rd to 9th grade science questions collected from 12 US states", "Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus."], "top_k_doc_id": [346, 3097, 3098, 3099, 5327, 5328, 5330, 5331, 7071, 7590, 1141, 3094, 4463, 345, 7591], "orig_top_k_doc_id": [5327, 5328, 7590, 3097, 5331, 3098, 1141, 5330, 7071, 3099, 346, 345, 4463, 7591, 3094]}, {"qid": 2043, "question": "Is Aristo just some modern NLP model (ex. BERT) finetuned od data specific for this task? in From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project", "answer": ["Yes"], "top_k_doc_id": [346, 3097, 3098, 3099, 5327, 5328, 3094, 3095, 3096, 3100, 4463, 4467, 4469, 6050, 3273], "orig_top_k_doc_id": [3098, 3094, 3099, 3095, 3100, 5327, 5328, 3097, 3096, 6050, 4463, 3273, 346, 4467, 4469]}, {"qid": 2044, "question": "On what dataset is Aristo system trained? in From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project", "answer": ["Aristo Corpus\nRegents 4th\nRegents 8th\nRegents `12th\nARC-Easy\nARC-challenge "], "top_k_doc_id": [346, 3097, 3098, 3099, 5327, 5328, 3094, 3095, 3096, 3100, 4463, 4467, 4469, 6050, 7831], "orig_top_k_doc_id": [3098, 3099, 3094, 3095, 3100, 5327, 5328, 3096, 4463, 6050, 3097, 4469, 346, 7831, 4467]}]}
{"group_id": 323, "group_size": 6, "items": [{"qid": 3225, "question": "What tasks are used for evaluation? in Adaptively Sparse Transformers", "answer": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "top_k_doc_id": [7579, 651, 5412, 5413, 5414, 5415, 2225, 5411, 1152, 3697, 2680, 2375, 3359, 4412, 4284], "orig_top_k_doc_id": [5415, 5412, 5411, 5413, 5414, 651, 7579, 1152, 3697, 2225, 2680, 2375, 3359, 4412, 4284]}, {"qid": 3227, "question": "How does their model improve interpretability compared to softmax transformers? in Adaptively Sparse Transformers", "answer": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "top_k_doc_id": [7579, 651, 5412, 5413, 5414, 5415, 2225, 5411, 97, 426, 371, 96, 2205, 7553, 3781], "orig_top_k_doc_id": [5415, 5411, 5413, 5412, 97, 651, 426, 371, 5414, 7579, 96, 2225, 2205, 7553, 3781]}, {"qid": 4884, "question": "What do they mean by explicit selection of most relevant segments? in Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection", "answer": ["It is meant that only most contributive k elements are reserved, while other elements are removed.", "focusing on the top-k segments that contribute the most in terms of correlation to the query"], "top_k_doc_id": [7579, 7580, 7581, 1832, 2601, 2602, 4704, 4770, 7582, 7583, 4447, 2213, 3161, 1955, 1950], "orig_top_k_doc_id": [7579, 7581, 7580, 7582, 7583, 4447, 2602, 2213, 4704, 3161, 1832, 1955, 1950, 4770, 2601]}, {"qid": 4885, "question": "What datasets they used for evaluation? in Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection", "answer": ["For En-De translation the newstest 2014 set from WMT 2014 En-De translation dataset, for En-Vi translation the tst2013 from IWSLT 2015 dataset. and for De-En translation the teste set from IWSLT 2014.", "newstest 2014, tst2013, Following BIBREF21, we used the same test set with around 7K sentences., MSCOCO 2014 test set, Enwiki8"], "top_k_doc_id": [7579, 7580, 7581, 1832, 2601, 2602, 4704, 4770, 7582, 7583, 2680, 4703, 2679, 1692, 5411], "orig_top_k_doc_id": [7579, 7580, 7582, 7583, 7581, 4704, 2680, 4703, 2601, 4770, 1832, 2679, 2602, 1692, 5411]}, {"qid": 2801, "question": "What are the language pairs explored in this paper? in Sparse and Constrained Attention for Neural Machine Translation", "answer": ["De-En, Ja-En, Ro-En", "De-En, Ja-En, Ro-En"], "top_k_doc_id": [7579, 7580, 7581, 4906, 1185, 1013, 6344, 4590, 5412, 2548, 4907, 5683, 564, 6025, 5240], "orig_top_k_doc_id": [4906, 1185, 1013, 6344, 4590, 7580, 5412, 7581, 7579, 2548, 4907, 5683, 564, 6025, 5240]}, {"qid": 3226, "question": "HOw does the method perform compared with baselines? in Adaptively Sparse Transformers", "answer": ["On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The \u03b1-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline."], "top_k_doc_id": [7579, 651, 5412, 5413, 5414, 5415, 7581, 2375, 3633, 4476, 7580, 426, 1257, 1256, 3693], "orig_top_k_doc_id": [5415, 5412, 7579, 7581, 5413, 2375, 5414, 651, 3633, 4476, 7580, 426, 1257, 1256, 3693]}]}
{"group_id": 324, "group_size": 6, "items": [{"qid": 3238, "question": "What datasets are used? in A Hybrid Architecture for Multi-Party Conversational Systems", "answer": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "top_k_doc_id": [6793, 4669, 5426, 5432, 7403, 400, 5425, 5441, 5917, 5916, 5800, 7371, 4189, 404, 3357], "orig_top_k_doc_id": [6793, 4669, 5432, 5917, 5426, 5916, 5425, 5441, 5800, 7371, 4189, 400, 404, 3357, 7403]}, {"qid": 3239, "question": "What is the state of the art described in the paper? in A Hybrid Architecture for Multi-Party Conversational Systems", "answer": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "top_k_doc_id": [6793, 4669, 5426, 5432, 7403, 400, 5425, 5441, 5917, 7504, 3508, 3432, 2995, 405, 6583], "orig_top_k_doc_id": [5426, 5432, 5425, 6793, 5917, 4669, 7504, 3508, 3432, 400, 2995, 7403, 405, 6583, 5441]}, {"qid": 4824, "question": "what is the average number of speakers in the dataset? in Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs", "answer": ["26.8", "26.8"], "top_k_doc_id": [6793, 1713, 3521, 5258, 5428, 7504, 7505, 7506, 7507, 7508, 5426, 5441, 6795, 7158, 4188], "orig_top_k_doc_id": [7504, 7507, 7505, 7508, 7506, 6793, 3521, 1713, 5258, 5428, 5441, 6795, 7158, 5426, 4188]}, {"qid": 4826, "question": "what are the previous state of the art systems? in Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs", "answer": ["Dynamic-RNN model from BIBREF4", "Dynamic-RNN model"], "top_k_doc_id": [6793, 1713, 3521, 5258, 5428, 7504, 7505, 7506, 7507, 7508, 5426, 5441, 3537, 3190, 7542], "orig_top_k_doc_id": [7504, 7507, 7508, 7505, 7506, 5428, 3521, 5258, 6793, 1713, 5426, 3537, 3190, 7542, 5441]}, {"qid": 3237, "question": "What evaluation metrics did look at? in A Hybrid Architecture for Multi-Party Conversational Systems", "answer": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "top_k_doc_id": [6793, 4669, 5426, 5432, 7403, 4441, 404, 5920, 6651, 1171, 2874, 6590, 5916, 3358, 6583], "orig_top_k_doc_id": [4441, 404, 5920, 6651, 6793, 1171, 2874, 6590, 4669, 7403, 5426, 5916, 3358, 5432, 6583]}, {"qid": 4825, "question": "by how much is accuracy improved? in Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs", "answer": ["In addressee selection around 12% in RES-CAND = 2 and 10% in RES-CAND = 10, in candidate responses around 2% in RES-CAND = 2 and 4% in RES-CAND = 10", "The accuracy of addressee selection is improved by 11.025 percent points on average, the accuracy of response selection is improved by 3.09 percent points on average."], "top_k_doc_id": [6793, 1713, 3521, 5258, 5428, 7504, 7505, 7506, 7507, 7508, 7379, 6795, 1712, 4298, 6794], "orig_top_k_doc_id": [7507, 7504, 7508, 7505, 7506, 3521, 1713, 5428, 6793, 7379, 5258, 6795, 1712, 4298, 6794]}]}
{"group_id": 325, "group_size": 6, "items": [{"qid": 3252, "question": "what dataset is used? in Using Monolingual Data in Neural Machine Translation: a Systematic Study", "answer": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "top_k_doc_id": [3260, 5672, 1720, 2074, 7190, 7191, 1244, 6039, 6040, 6870, 2705, 2329, 5699, 1458, 2044], "orig_top_k_doc_id": [5672, 2074, 7191, 6039, 7190, 6870, 2705, 1244, 2329, 6040, 5699, 1458, 2044, 1720, 3260]}, {"qid": 3254, "question": "what language is the data in? in Using Monolingual Data in Neural Machine Translation: a Systematic Study", "answer": ["English , German, French"], "top_k_doc_id": [3260, 5672, 1720, 2074, 7190, 7191, 1244, 6039, 6040, 6870, 5455, 1040, 629, 6190, 5841], "orig_top_k_doc_id": [5672, 2074, 6039, 1244, 3260, 6870, 5455, 7191, 7190, 1040, 1720, 629, 6040, 6190, 5841]}, {"qid": 3250, "question": "what data simulation techniques were introduced? in Using Monolingual Data in Neural Machine Translation: a Systematic Study", "answer": ["copy, copy-marked, copy-dummies", "copy, copy-marked, copy-dummies"], "top_k_doc_id": [3260, 5672, 1720, 2074, 7190, 7191, 4595, 5455, 802, 7285, 1453, 2044, 1458, 4695, 446], "orig_top_k_doc_id": [4595, 3260, 7191, 5672, 2074, 1720, 7190, 5455, 802, 7285, 1453, 2044, 1458, 4695, 446]}, {"qid": 3249, "question": "why are their techniques cheaper to implement? in Using Monolingual Data in Neural Machine Translation: a Systematic Study", "answer": ["They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper", "They do not require the availability of a backward translation engine."], "top_k_doc_id": [3260, 5672, 231, 1458, 5455, 5459, 5839, 7191, 7190, 5240, 6943, 6034, 2578, 5699, 2074], "orig_top_k_doc_id": [5455, 5459, 3260, 7190, 7191, 1458, 5240, 5672, 6943, 231, 6034, 2578, 5699, 5839, 2074]}, {"qid": 3251, "question": "what is their explanation for the effectiveness of back-translation? in Using Monolingual Data in Neural Machine Translation: a Systematic Study", "answer": ["when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources"], "top_k_doc_id": [3260, 5672, 231, 1458, 5455, 5459, 5839, 7191, 7828, 5838, 6266, 7825, 233, 7827, 5835], "orig_top_k_doc_id": [5672, 3260, 7828, 5839, 5838, 6266, 1458, 7825, 233, 231, 5455, 5459, 7827, 5835, 7191]}, {"qid": 3253, "question": "what language pairs are explored? in Using Monolingual Data in Neural Machine Translation: a Systematic Study", "answer": ["English-German, English-French.", "English-German, English-French"], "top_k_doc_id": [3260, 5672, 1720, 1040, 7429, 2705, 2329, 6870, 5029, 5240, 5868, 5699, 5455, 1185, 6943], "orig_top_k_doc_id": [1040, 5672, 7429, 3260, 2705, 2329, 6870, 5029, 5240, 1720, 5868, 5699, 5455, 1185, 6943]}]}
{"group_id": 326, "group_size": 6, "items": [{"qid": 3291, "question": "What were the baselines? in Resolving the Scope of Speculation and Negation using Transformer-Based Architectures", "answer": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "top_k_doc_id": [4501, 5510, 5511, 5512, 1456, 4900, 4901, 886, 4358, 4359, 3647, 6925, 1256, 1257, 566], "orig_top_k_doc_id": [5510, 5512, 5511, 4501, 1456, 4901, 4900, 4359, 886, 3647, 6925, 1256, 1257, 4358, 566]}, {"qid": 3293, "question": "Which multiple datasets did they train on during joint training? in Resolving the Scope of Speculation and Negation using Transformer-Based Architectures", "answer": ["BF, BA, SFU and Sherlock", "BioScope Abstracts, SFU, and BioScope Full Papers"], "top_k_doc_id": [4501, 5510, 5511, 5512, 1456, 4900, 4901, 886, 4358, 1988, 6968, 2602, 450, 4913, 1221], "orig_top_k_doc_id": [5512, 5510, 5511, 1456, 4501, 886, 4901, 1988, 6968, 2602, 4900, 4358, 450, 4913, 1221]}, {"qid": 3295, "question": "What is the size of SFU Review corpus? in Resolving the Scope of Speculation and Negation using Transformer-Based Architectures", "answer": ["No", "No"], "top_k_doc_id": [4501, 5510, 5511, 5512, 1456, 4900, 4901, 0, 1222, 1560, 3071, 4359, 4931, 2225, 277], "orig_top_k_doc_id": [5512, 5510, 5511, 4501, 1456, 4359, 4931, 4901, 1560, 0, 1222, 2225, 4900, 3071, 277]}, {"qid": 3296, "question": "What is the size of bioScope corpus? in Resolving the Scope of Speculation and Negation using Transformer-Based Architectures", "answer": ["No", "No"], "top_k_doc_id": [4501, 5510, 5511, 5512, 1456, 4900, 4901, 0, 1222, 1560, 3071, 4359, 4358, 48, 1664], "orig_top_k_doc_id": [5510, 5512, 5511, 4501, 1456, 4359, 4901, 1560, 0, 1222, 4900, 3071, 4358, 48, 1664]}, {"qid": 3294, "question": "What were the previously reported results? in Resolving the Scope of Speculation and Negation using Transformer-Based Architectures", "answer": ["Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution"], "top_k_doc_id": [4501, 5510, 5511, 5512, 1456, 2743, 4913, 3863, 427, 781, 4359, 1560, 1222, 2330, 566], "orig_top_k_doc_id": [5512, 5510, 5511, 4501, 1456, 2743, 4913, 3863, 427, 781, 4359, 1560, 1222, 2330, 566]}, {"qid": 3292, "question": "Does RoBERTa outperform BERT? in Resolving the Scope of Speculation and Negation using Transformer-Based Architectures", "answer": ["No", "No"], "top_k_doc_id": [4501, 5510, 5511, 5512, 533, 4704, 436, 4277, 439, 536, 4703, 5249, 106, 4705, 4414], "orig_top_k_doc_id": [5510, 5512, 5511, 533, 4704, 436, 4277, 4501, 439, 536, 4703, 5249, 106, 4705, 4414]}]}
{"group_id": 327, "group_size": 6, "items": [{"qid": 3300, "question": "what are their results on the constructed dataset? in Automatic Judgment Prediction via Legal Reading Comprehension", "answer": ["AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement"], "top_k_doc_id": [3966, 5518, 5519, 5520, 5521, 5522, 6381, 6955, 2011, 5577, 1822, 2446, 352, 1512, 3972], "orig_top_k_doc_id": [5518, 5519, 5522, 5521, 3966, 5520, 5577, 6381, 6955, 1822, 2446, 352, 1512, 2011, 3972]}, {"qid": 3302, "question": "what civil field is the dataset about? in Automatic Judgment Prediction via Legal Reading Comprehension", "answer": ["divorce ", "divorce"], "top_k_doc_id": [3966, 5518, 5519, 5520, 5521, 5522, 6381, 6955, 2011, 5577, 1822, 5523, 6305, 55, 2048], "orig_top_k_doc_id": [5518, 5519, 5522, 5520, 5521, 6381, 3966, 5523, 5577, 1822, 6955, 2011, 6305, 55, 2048]}, {"qid": 3301, "question": "what evaluation metrics are reported? in Automatic Judgment Prediction via Legal Reading Comprehension", "answer": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "top_k_doc_id": [3966, 5518, 5519, 5520, 5521, 5522, 6381, 6955, 2011, 5577, 3805, 2445, 6651, 1807, 2205], "orig_top_k_doc_id": [5519, 5518, 5522, 5521, 6955, 5520, 2445, 3805, 6651, 6381, 3966, 5577, 1807, 2011, 2205]}, {"qid": 3304, "question": "what is the size of the real-world civil case dataset? in Automatic Judgment Prediction via Legal Reading Comprehension", "answer": ["100 000 documents", " INLINEFORM1 cases"], "top_k_doc_id": [3966, 5518, 5519, 5520, 5521, 5522, 6381, 6955, 2011, 5577, 5260, 1630, 7728, 5523, 3972], "orig_top_k_doc_id": [5518, 5522, 5519, 5521, 5520, 5577, 5260, 6381, 2011, 3966, 1630, 7728, 5523, 6955, 3972]}, {"qid": 3305, "question": "what datasets are used in the experiment? in Automatic Judgment Prediction via Legal Reading Comprehension", "answer": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "top_k_doc_id": [3966, 5518, 5519, 5520, 5521, 5522, 6381, 6955, 2011, 5577, 3805, 4415, 7728, 4414, 2661], "orig_top_k_doc_id": [5518, 5519, 5522, 5521, 5520, 6381, 3966, 6955, 3805, 5577, 2011, 4415, 7728, 4414, 2661]}, {"qid": 3303, "question": "what are the state-of-the-art models? in Automatic Judgment Prediction via Legal Reading Comprehension", "answer": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "top_k_doc_id": [3966, 5518, 5519, 5520, 5521, 5522, 6381, 6955, 2445, 164, 4075, 7727, 4415, 2234, 3805], "orig_top_k_doc_id": [5518, 5519, 5522, 5521, 6381, 5520, 2445, 6955, 3966, 164, 4075, 7727, 4415, 2234, 3805]}]}
{"group_id": 328, "group_size": 6, "items": [{"qid": 3451, "question": "Does the corpus contain only English documents? in Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "answer": ["Yes", "No", "No"], "top_k_doc_id": [5718, 5719, 5720, 5722, 5723, 325, 6492, 6573, 7615, 3715, 6858, 3158, 606, 7243, 456], "orig_top_k_doc_id": [5722, 5718, 5719, 5723, 5720, 6492, 7615, 3715, 6858, 3158, 606, 6573, 7243, 325, 456]}, {"qid": 3452, "question": "What type of evaluation is proposed for this task? in Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "answer": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "top_k_doc_id": [5718, 5719, 5720, 5722, 5723, 495, 583, 2226, 4573, 5116, 7615, 6716, 6573, 544, 4698], "orig_top_k_doc_id": [5718, 5719, 5722, 5720, 5723, 583, 2226, 6716, 495, 6573, 544, 4573, 7615, 4698, 5116]}, {"qid": 3453, "question": "What baseline system is proposed? in Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "answer": ["Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction."], "top_k_doc_id": [5718, 5719, 5720, 5722, 5723, 495, 583, 2226, 4573, 5116, 7615, 3158, 329, 5434, 2339], "orig_top_k_doc_id": [5718, 5722, 5719, 5723, 5720, 3158, 495, 7615, 329, 5434, 2226, 5116, 2339, 583, 4573]}, {"qid": 3455, "question": "Which collections of web documents are included in the corpus? in Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "answer": ["DIP corpus BIBREF37"], "top_k_doc_id": [5718, 5719, 5720, 5722, 5723, 325, 6492, 6573, 185, 5122, 543, 4380, 855, 544, 329], "orig_top_k_doc_id": [5718, 5722, 5719, 5720, 5723, 185, 5122, 543, 4380, 6492, 325, 855, 6573, 544, 329]}, {"qid": 3454, "question": "How were crowd workers instructed to identify important elements in large document collections? in Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "answer": ["provide only a description of the document cluster's topic along with the propositions", "They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary."], "top_k_doc_id": [5718, 5719, 5720, 5722, 5723, 3681, 5873, 3225, 5874, 7615, 6143, 325, 5043, 6142, 6805], "orig_top_k_doc_id": [5719, 5718, 5722, 5720, 3681, 5723, 5873, 3225, 5874, 7615, 6143, 325, 5043, 6142, 6805]}, {"qid": 3456, "question": "How do the authors define a concept map? in Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "answer": ["concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges"], "top_k_doc_id": [5718, 5719, 5720, 5722, 5723, 5298, 5721, 39, 7312, 7303, 7304, 237, 5300, 3066, 2195], "orig_top_k_doc_id": [5718, 5722, 5719, 5720, 5723, 5298, 5721, 39, 7312, 7303, 7304, 237, 5300, 3066, 2195]}]}
{"group_id": 329, "group_size": 6, "items": [{"qid": 3509, "question": "Was PolyReponse evaluated against some baseline? in PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking", "answer": ["No", "No"], "top_k_doc_id": [5744, 2276, 2550, 3357, 5800, 5802, 5803, 7584, 3359, 1677, 3192, 3193, 6583, 3507, 5430], "orig_top_k_doc_id": [5800, 5802, 3193, 3192, 5744, 3357, 5803, 3359, 1677, 2276, 2550, 6583, 3507, 7584, 5430]}, {"qid": 3510, "question": "What metric is used to evaluate PolyReponse system? in PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking", "answer": ["No"], "top_k_doc_id": [5744, 2276, 2550, 3357, 5800, 5802, 5803, 7584, 3359, 1677, 3192, 3193, 965, 1676, 3360], "orig_top_k_doc_id": [5800, 5802, 3359, 1677, 965, 3192, 5744, 5803, 7584, 3193, 2276, 1676, 3360, 2550, 3357]}, {"qid": 3511, "question": "How does PolyResponse architecture look like? in PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking", "answer": ["Henderson:2017, MobileNet model"], "top_k_doc_id": [5744, 2276, 2550, 3357, 5800, 5802, 5803, 7584, 3359, 5801, 5430, 3507, 3679, 5435, 965], "orig_top_k_doc_id": [5800, 5802, 5801, 2550, 3357, 5803, 2276, 5744, 7584, 5430, 3359, 3507, 3679, 5435, 965]}, {"qid": 3512, "question": "In what 8 languages is PolyResponse engine used for restourant search and booking system? in PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking", "answer": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "top_k_doc_id": [5744, 2276, 2550, 3357, 5800, 5802, 5803, 7584, 7758, 7372, 1168, 1677, 965, 3679, 7760], "orig_top_k_doc_id": [5800, 5802, 5803, 7758, 7584, 2550, 5744, 7372, 1168, 1677, 965, 2276, 3679, 7760, 3357]}, {"qid": 4197, "question": "what are the evaluation metrics used? in Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems", "answer": ["BLEU-4, slot error rate", "informativeness , naturalness ", "BLEU-4, slot error rate, informativeness, naturalness"], "top_k_doc_id": [5744, 965, 2818, 3359, 3679, 6671, 6675, 1170, 6673, 6674, 6651, 3357, 3193, 3192, 573], "orig_top_k_doc_id": [6671, 965, 6675, 3359, 1170, 2818, 6673, 3679, 6674, 6651, 3357, 3193, 5744, 3192, 573]}, {"qid": 4198, "question": "what other training procedures were explored? in Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems", "answer": ["Scratch-NLG, MTL-NLG, Zero-NLG, Supervised-NLG", "Scratch-NLG, MTL-NLG, Zero-NLG, Supervised-NLG"], "top_k_doc_id": [5744, 965, 2818, 3359, 3679, 6671, 6675, 6036, 1075, 3507, 898, 5426, 3194, 5800, 2276], "orig_top_k_doc_id": [6671, 6675, 6036, 1075, 5744, 2818, 3507, 898, 965, 3359, 5426, 3194, 5800, 3679, 2276]}]}
{"group_id": 330, "group_size": 6, "items": [{"qid": 3604, "question": "What is their evaluation metric? in Spotting Rumors via Novelty Detection", "answer": ["accuracy to evaluate effectiveness, Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability, throughput per second", "The metrics are accuracy, detection error trade-off curves and computing efficiency", "accuracy , Detection Error Trade-off (DET) curves, efficiency of computing the proposed features, measured by the throughput per second", "accuracy to evaluate effectiveness, Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability, throughput per second", "Accuracy compared to two state-of-the-art baselines"], "top_k_doc_id": [6817, 7625, 5122, 5928, 5929, 5931, 5930, 247, 5927, 3949, 5932, 955, 5135, 4121, 999], "orig_top_k_doc_id": [6817, 5931, 3949, 5928, 5929, 5930, 5932, 7625, 247, 5927, 955, 5122, 5135, 4121, 999]}, {"qid": 3607, "question": "What languages do they evaluate their methods on? in Spotting Rumors via Novelty Detection", "answer": ["Chinese", "Mandarin Chinese", "Chinese", "Mandarin Chinese (see table 3)", "Chinese"], "top_k_doc_id": [6817, 7625, 5122, 5928, 5929, 5931, 5930, 247, 5927, 5406, 4120, 59, 4510, 1081, 3487], "orig_top_k_doc_id": [6817, 5122, 247, 5928, 7625, 5929, 5406, 4120, 5931, 59, 5930, 5927, 4510, 1081, 3487]}, {"qid": 3603, "question": "What previous methods do they compare against? in Spotting Rumors via Novelty Detection", "answer": ["two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented., Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter.", "Liu et. al (2015), Yang et. al (2012)", "They compare against two other methods that apply message-,user-, topic- and propagation-based features and rely on an SVM classifier. One perform early rumor detection and operates with a delay of 24 hrs, while the other requires a cluster of 5 repeated messages to judge them for rumors.", "Liu et. al (2015) , Yang et. al (2012)", "Liu et al. (2015) and Yang et al. (2012)"], "top_k_doc_id": [6817, 7625, 5122, 5928, 5929, 5931, 5930, 5135, 2814, 4121, 804, 1081, 3486, 4120, 7499], "orig_top_k_doc_id": [6817, 5135, 5929, 2814, 4121, 7625, 804, 1081, 3486, 5931, 5928, 4120, 7499, 5122, 5930]}, {"qid": 3605, "question": "Are their methods fully supervised? in Spotting Rumors via Novelty Detection", "answer": ["No. They additionally use similarity to previously detected rumors to make the decision of whether a document is likely to be a rumor"], "top_k_doc_id": [6817, 7625, 5122, 5928, 5929, 5931, 3669, 413, 2814, 803, 5927, 4120, 3926, 4487, 3160], "orig_top_k_doc_id": [6817, 7625, 5122, 3669, 5931, 413, 2814, 803, 5928, 5927, 5929, 4120, 3926, 4487, 3160]}, {"qid": 3606, "question": "Do they build a dataset of rumors? in Spotting Rumors via Novelty Detection", "answer": ["Yes", "Yes", "Yes", "Yes, consisting of trusted resources, rumours and non-rumours", "Yes"], "top_k_doc_id": [6817, 7625, 955, 3486, 3487, 3488, 3669, 3926, 5135, 5136, 6663, 7499, 7629, 3543, 3484], "orig_top_k_doc_id": [6817, 7625, 5135, 3487, 5136, 3486, 3669, 7499, 3543, 6663, 955, 3488, 7629, 3926, 3484]}, {"qid": 3608, "question": "How do they define rumors? in Spotting Rumors via Novelty Detection", "answer": ["the presence of information unconfirmed by the official media is construed as an indication of being a rumour. ", "information of doubtful or unconfirmed truth", "information that is not fact- and background-checked and thoroughly investigated for authenticity", "Information of doubtful or unconfirmed truth"], "top_k_doc_id": [6817, 7625, 955, 3486, 3487, 3488, 3669, 3926, 5135, 5136, 6663, 7499, 7629, 7534, 1172], "orig_top_k_doc_id": [6817, 7625, 3487, 5135, 3486, 3669, 955, 5136, 7499, 3488, 7629, 6663, 7534, 1172, 3926]}]}
{"group_id": 331, "group_size": 6, "items": [{"qid": 3611, "question": "Which NER dataset do they use? in TENER: Adapting Transformer Encoder for Named Entity Recognition", "answer": ["CoNLL2003, OntoNotes 5.0, OntoNotes 4.0., Chinese NER dataset MSRA, Weibo NER, Resume NER", "CoNLL2003 , OntoNotes 5.0, OntoNotes 4.0, MSRA , Weibo, Resume ", "CoNLL2003, OntoNotes 5.0, OntoNotes 4.0, MSRA, Weibo NER, Resume NER", "CoNLL2003, OntoNotes 5.0, BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part, Chinese NER dataset MSRA, Weibo NER, Resume NER"], "top_k_doc_id": [5941, 4755, 5938, 5942, 7776, 1773, 4531, 5184, 7002, 5940, 4858, 5956, 4947, 2973, 4756], "orig_top_k_doc_id": [5941, 5942, 5184, 5938, 4531, 1773, 4755, 7002, 5940, 7776, 4858, 5956, 4947, 2973, 4756]}, {"qid": 3613, "question": "Do they outperform current NER state-of-the-art models? in TENER: Adapting Transformer Encoder for Named Entity Recognition", "answer": ["No", "Yes", "Yes", "we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features"], "top_k_doc_id": [5941, 4755, 5938, 5942, 7776, 1773, 4531, 1781, 7111, 6031, 7287, 3344, 4758, 1422, 5775], "orig_top_k_doc_id": [5941, 5942, 4755, 5938, 1773, 1781, 4531, 7111, 7776, 6031, 7287, 3344, 4758, 1422, 5775]}, {"qid": 4052, "question": "How is \"complexity\" and \"confusability\" of entity mentions defined in this work? in Remedying BiLSTM-CNN Deficiency in Modeling Cross-Context for NER.", "answer": ["Complexity is defined by examples of a singular named entity (e.g. work-of-art and creative-work entities) being represented by multiple surface forms. Mapping all of these forms to a single NE requires a complex understanding of the variations, some of which are genre-specific. Confusability is defined by examples when it becomes more difficult to disambiguate named entities that share the same surface form, such as the \"language\" versus \"NORP\" distinction represented by the surface forms Dutch and English.", "disambiguating fine-grained entity types, entities could in principle take any surface forms \u2013 unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media"], "top_k_doc_id": [5941, 3647, 5084, 6503, 6505, 6506, 6507, 5938, 5939, 6504, 2856, 5132, 3845, 438, 2858], "orig_top_k_doc_id": [6507, 6506, 6503, 6505, 6504, 5938, 3647, 5939, 5941, 2856, 5132, 5084, 3845, 438, 2858]}, {"qid": 4053, "question": "What are the baseline models? in Remedying BiLSTM-CNN Deficiency in Modeling Cross-Context for NER.", "answer": ["BiLSTM-CNN", "BiLSTM-CNN proposed by BIBREF1", "Baseline-BiLSTM-CNN"], "top_k_doc_id": [5941, 3647, 5084, 6503, 6505, 6506, 6507, 5938, 5939, 6504, 5942, 2029, 2028, 6146, 5940], "orig_top_k_doc_id": [6507, 6506, 6503, 6505, 6504, 3647, 5941, 5939, 5938, 5942, 2029, 5084, 2028, 6146, 5940]}, {"qid": 3612, "question": "How do they incorporate direction and relative distance in attention? in TENER: Adapting Transformer Encoder for Named Entity Recognition", "answer": ["by using an relative sinusodial positional embedding and unscaled attention", "Yes", "calculate the attention scores  which can  distinguish different directions and distances", "Self-attention mechanism is changed to allow for direction-aware calculations"], "top_k_doc_id": [5941, 4755, 5938, 5942, 7776, 5940, 7169, 2238, 6737, 5939, 7171, 4756, 7553, 3162, 365], "orig_top_k_doc_id": [5941, 5942, 5938, 5940, 7169, 4755, 7776, 2238, 6737, 5939, 7171, 4756, 7553, 3162, 365]}, {"qid": 4051, "question": "Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017? in Remedying BiLSTM-CNN Deficiency in Modeling Cross-Context for NER.", "answer": ["suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities", "The WNUT 2017 dataset had entities already seen in the training set filtered out while the OntoNotes dataset did not. Cross-context patterns thus provided more significant information for NER in WNUT 2017 because the possibility of memorizing entity forms was removed.", "Ontonotes is less noisy than Wnut 2017"], "top_k_doc_id": [5941, 3647, 5084, 6503, 6505, 6506, 6507, 3646, 1782, 5940, 2179, 6297, 7613, 5942, 6299], "orig_top_k_doc_id": [6507, 6506, 6505, 6503, 3647, 3646, 5941, 5084, 1782, 5940, 2179, 6297, 7613, 5942, 6299]}]}
{"group_id": 332, "group_size": 6, "items": [{"qid": 3631, "question": "What datasets are used in this paper? in Look, Read and Enrich - Learning from Scientific Figures and their Captions", "answer": ["The Semantic Scholar corpus , Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K and COCO", "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, January 2018 English Wikipedia dataset, Flickr30K, COCO", "The Semantic Scholar corpus, Springer Nature's SciGraph, The Textbook Question Answering corpus, Wikipedia, Flickr30K, COCO", "Semantic Scholar corpus BIBREF21 (SemScholar), Springer Nature's SciGraph, Textbook Question Answering corpus BIBREF23, Wikipedia, Flickr30K, COCO"], "top_k_doc_id": [2547, 2717, 5966, 5967, 5968, 5971, 5972, 494, 5970, 2171, 2755, 5969, 6591, 7356, 7355], "orig_top_k_doc_id": [5966, 5971, 5972, 5968, 2547, 2717, 5970, 5967, 2755, 6591, 2171, 5969, 494, 7356, 7355]}, {"qid": 3635, "question": "Is the data specific to a domain? in Look, Read and Enrich - Learning from Scientific Figures and their Captions", "answer": ["No", "No", "No", "Yes"], "top_k_doc_id": [2547, 2717, 5966, 5967, 5968, 5971, 5972, 494, 5970, 2171, 2755, 5969, 7085, 2728, 2729], "orig_top_k_doc_id": [5966, 5971, 5968, 5972, 2547, 2717, 5970, 5967, 5969, 2171, 494, 7085, 2755, 2728, 2729]}, {"qid": 3634, "question": "What supervised baselines did they compare with? in Look, Read and Enrich - Learning from Scientific Figures and their Captions", "answer": ["direct combination, supervised pre-training", "direct combination baseline, supervised pre-training baseline", "The direct combination baseline , The supervised pre-training baseline", "direct combination baseline, supervised pre-training baseline"], "top_k_doc_id": [2547, 2717, 5966, 5967, 5968, 5971, 5972, 494, 5970, 2171, 2755, 5969, 4441, 2074, 1752], "orig_top_k_doc_id": [5966, 5968, 5972, 5971, 5970, 2717, 5969, 2547, 5967, 2171, 494, 4441, 2074, 1752, 2755]}, {"qid": 3632, "question": "What language are the captions in? in Look, Read and Enrich - Learning from Scientific Figures and their Captions", "answer": ["English", "No", "No", "English"], "top_k_doc_id": [2547, 2717, 5966, 5967, 5968, 5971, 5972, 2546, 2716, 4746, 7085, 7355, 7356, 494, 6998], "orig_top_k_doc_id": [5966, 5971, 5972, 2717, 5968, 2547, 5967, 7356, 7085, 494, 4746, 7355, 6998, 2546, 2716]}, {"qid": 3633, "question": "What ad-hoc approaches are explored? in Look, Read and Enrich - Learning from Scientific Figures and their Captions", "answer": ["HolE, Vecsigrafo", "Embedding network, 2WayNet, VSE++, DSVE-loc)", "No"], "top_k_doc_id": [2547, 2717, 5966, 5967, 5968, 5971, 5972, 494, 5970, 5069, 7638, 5068, 7085, 1039, 6344], "orig_top_k_doc_id": [5966, 5971, 5968, 5972, 2717, 5069, 2547, 7638, 5068, 7085, 5967, 5970, 494, 1039, 6344]}, {"qid": 3636, "question": "Where do their figure and captions come from? in Look, Read and Enrich - Learning from Scientific Figures and their Captions", "answer": ["The Semantic Scholar corpus, Springer Nature's SciGraph", "scientific publications, middle school science curricula", "scientific literature", "SN SciGraph and AI2 Semantic Scholar"], "top_k_doc_id": [2547, 2717, 5966, 5967, 5968, 5971, 5972, 2546, 2716, 4746, 7085, 7355, 7356, 2901, 2171], "orig_top_k_doc_id": [5966, 5971, 5972, 5968, 2547, 2717, 5967, 7085, 2546, 2716, 7355, 7356, 2901, 4746, 2171]}]}
{"group_id": 333, "group_size": 6, "items": [{"qid": 3696, "question": "what are the baselines? in A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation", "answer": ["bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21", " standard bidirectional RNN model with attention, A standard context-agnostic Transformer", "standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8"], "top_k_doc_id": [6041, 6044, 6045, 1250, 6042, 6043, 7346, 5669, 7661, 6294, 7658, 4541, 1244, 7267, 2839], "orig_top_k_doc_id": [6041, 6042, 6045, 6043, 6044, 1250, 7346, 6294, 4541, 7658, 1244, 7661, 7267, 5669, 2839]}, {"qid": 3697, "question": "what context aware models were experimented? in A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation", "answer": ["standard bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, standard context-agnostic Transformer, concat22, concat21, BIBREF8", "bidirectional RNN, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21, BIBREF8", "a standard bidirectional RNN model with attention, concat22 , s-hier, s-t-hier, s-hier-to-2, concat21 , BIBREF8 "], "top_k_doc_id": [6041, 6044, 6045, 1250, 6042, 6043, 7346, 5669, 7661, 6294, 7658, 5672, 5673, 7064, 4542], "orig_top_k_doc_id": [6041, 6042, 6045, 6044, 1250, 6294, 7658, 5669, 7661, 7346, 5672, 5673, 7064, 4542, 6043]}, {"qid": 3695, "question": "did they collect their own contrastive test set? in A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation", "answer": ["No", "Yes", "It is automatically created from the OpenSubtitles corpus.", "Yes"], "top_k_doc_id": [6041, 6044, 6045, 1250, 6042, 6043, 7346, 5669, 7661, 1348, 5670, 5673, 1347, 2135, 7064], "orig_top_k_doc_id": [6042, 6041, 6044, 6043, 5669, 6045, 1348, 7346, 5670, 5673, 1250, 1347, 2135, 7661, 7064]}, {"qid": 3698, "question": "what languages did they experiment on? in A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation", "answer": ["English, German", "English, German ", "English , German "], "top_k_doc_id": [6041, 6044, 6045, 1250, 6042, 6043, 7346, 7669, 7064, 6190, 7267, 5773, 6595, 3686, 1244], "orig_top_k_doc_id": [6041, 6042, 7346, 1250, 6045, 6044, 6043, 7669, 7064, 6190, 7267, 5773, 6595, 3686, 1244]}, {"qid": 2579, "question": "Is the proposed model more sensitive than previous context-aware models too? in Context-Aware Learning for Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [6041, 6044, 6045, 7659, 4542, 4541, 7350, 7660, 7658, 5673, 1303, 5669, 2917, 5672, 6661], "orig_top_k_doc_id": [7659, 4542, 6041, 4541, 7350, 7660, 7658, 6045, 5673, 1303, 5669, 2917, 5672, 6661, 6044]}, {"qid": 2580, "question": "In what ways the larger context is ignored for the models that do consider larger context? in Context-Aware Learning for Neural Machine Translation", "answer": ["No"], "top_k_doc_id": [6041, 4541, 4542, 4540, 3190, 1303, 1920, 1348, 2823, 7659, 7064, 1250, 2984, 1048, 3793], "orig_top_k_doc_id": [6041, 4541, 4542, 4540, 3190, 1303, 1920, 1348, 2823, 7659, 7064, 1250, 2984, 1048, 3793]}]}
{"group_id": 334, "group_size": 6, "items": [{"qid": 3775, "question": "In what tasks does fine-tuning all layers hurt performance? in What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning", "answer": ["SST-2", "No", "SST-2"], "top_k_doc_id": [1560, 6136, 2682, 4572, 5624, 7003, 7776, 3577, 2681, 6095, 4182, 4183, 5459, 6943, 2680], "orig_top_k_doc_id": [1560, 3577, 6136, 2681, 4572, 6095, 4182, 4183, 5459, 5624, 7776, 6943, 2680, 2682, 7003]}, {"qid": 3776, "question": "Do they test against the large version of RoBERTa? in What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning", "answer": ["For GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.", "Yes", "Yes"], "top_k_doc_id": [1560, 6136, 2682, 4572, 5624, 7003, 7776, 6368, 6135, 533, 7778, 5542, 4614, 5623, 436], "orig_top_k_doc_id": [6136, 5624, 6368, 7776, 7003, 4572, 6135, 533, 7778, 5542, 2682, 4614, 1560, 5623, 436]}, {"qid": 2848, "question": "Do the authors also analyze transformer-based architectures? in Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering", "answer": ["No", "No"], "top_k_doc_id": [1560, 6136, 6135, 6070, 7778, 6561, 7777, 2899, 2691, 7664, 5735, 4987, 7632, 3175, 4652], "orig_top_k_doc_id": [1560, 6135, 6070, 7778, 6561, 7777, 6136, 2899, 2691, 7664, 5735, 4987, 7632, 3175, 4652]}, {"qid": 2262, "question": "What datasets or tasks do they conduct experiments on? in A Tensorized Transformer for Language Modeling", "answer": ["Language Modeling (LM), PTB BIBREF25 , WikiText-103 BIBREF26 and One-Billion Word benchmark BIBREF27 datasets, neural machine translation (NMT), WMT 2016 English-German dataset"], "top_k_doc_id": [1560, 3560, 3561, 3559, 3556, 3069, 891, 1256, 6135, 7776, 1779, 890, 4624, 7646, 4760], "orig_top_k_doc_id": [3560, 3561, 3559, 3556, 3069, 891, 1560, 1256, 6135, 7776, 1779, 890, 4624, 7646, 4760]}, {"qid": 4997, "question": "How many GPUs do they use for this task? in Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding", "answer": ["No", "No"], "top_k_doc_id": [1560, 5586, 6448, 6656, 7776, 6367, 3489, 250, 3581, 7138, 2899, 3617, 436, 6368, 2146], "orig_top_k_doc_id": [6367, 3489, 6656, 250, 1560, 3581, 6448, 7138, 2899, 3617, 7776, 436, 5586, 6368, 2146]}, {"qid": 4998, "question": "Do they use all the hidden layer representations? in Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding", "answer": ["Yes", "Yes"], "top_k_doc_id": [1560, 5586, 6448, 6656, 7776, 1746, 6104, 1364, 123, 2238, 730, 3368, 805, 1747, 2149], "orig_top_k_doc_id": [6448, 7776, 1746, 6104, 1364, 5586, 6656, 123, 2238, 730, 1560, 3368, 805, 1747, 2149]}]}
{"group_id": 335, "group_size": 6, "items": [{"qid": 3803, "question": "did they experiment with other text embeddings? in Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding", "answer": ["No", "Yes", "No"], "top_k_doc_id": [1147, 1148, 2228, 6162, 6163, 6164, 4967, 5194, 6555, 1149, 3431, 5676, 1910, 1084, 4979], "orig_top_k_doc_id": [6162, 6164, 6163, 1147, 1148, 1149, 2228, 6555, 3431, 5676, 1910, 1084, 4979, 5194, 4967]}, {"qid": 3805, "question": "how was the new dataset collected? in Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding", "answer": ["The new dataset was collected from structured abstracts from PubMed and filtering abstract headings representative of the desired categories.", "collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories", "By searching for structured abstracts on PubMed using specific filters."], "top_k_doc_id": [1147, 1148, 2228, 6162, 6163, 6164, 1084, 3744, 5757, 6555, 7029, 1979, 4995, 3431, 6849], "orig_top_k_doc_id": [6162, 6164, 6163, 1147, 1148, 6555, 5757, 7029, 1979, 1084, 4995, 3431, 3744, 6849, 2228]}, {"qid": 3806, "question": "who annotated the new dataset? in Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding", "answer": ["The P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.", "automatic labeling, lemmatization of the abstract section labels in order to cluster similar categories, manually looked at a small number of samples for each label to determine if text was representative", "No"], "top_k_doc_id": [1147, 1148, 2228, 6162, 6163, 6164, 1084, 3744, 5757, 6195, 1773, 5404, 5739, 4979, 5403], "orig_top_k_doc_id": [6162, 6164, 6163, 1147, 1148, 6195, 1773, 5404, 5739, 5757, 4979, 2228, 3744, 5403, 1084]}, {"qid": 3807, "question": "what shortcomings of previous datasets are mentioned? in Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding", "answer": ["using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label., Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "In the previous dataset a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "Information about the intervention and study design is mistakenly marked by a P label; a P-labeled section that contained more than one sentence would be split into multiple P-labeled sentences."], "top_k_doc_id": [1147, 1148, 2228, 6162, 6163, 6164, 4967, 5194, 6555, 5492, 3433, 1329, 4826, 5740, 3744], "orig_top_k_doc_id": [6162, 6164, 6163, 1147, 1148, 2228, 5492, 3433, 1329, 6555, 4826, 5740, 3744, 4967, 5194]}, {"qid": 3802, "question": "what boosting techniques were used? in Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding", "answer": ["Light Gradient Boosting Machine (LGBM)", "Light Gradient Boosting Machine", "Light Gradient Boosting Machine"], "top_k_doc_id": [1147, 1148, 2228, 6162, 6163, 6164, 6555, 3431, 6711, 6195, 6071, 4868, 6155, 6070, 2239], "orig_top_k_doc_id": [6164, 6162, 6163, 1147, 1148, 6555, 3431, 6711, 6195, 6071, 4868, 2228, 6155, 6070, 2239]}, {"qid": 3804, "question": "what is the size of this improved dataset? in Enhancing PIO Element Detection in Medical Text Using Contextualized Embedding", "answer": ["363,078 structured abstracts", "363,078", "No"], "top_k_doc_id": [1147, 1148, 2228, 6162, 6163, 6164, 23, 1149, 5970, 1402, 1401, 2380, 6555, 2694, 1945], "orig_top_k_doc_id": [6164, 6162, 6163, 1147, 1148, 23, 1149, 5970, 2228, 1402, 1401, 2380, 6555, 2694, 1945]}]}
{"group_id": 336, "group_size": 6, "items": [{"qid": 3913, "question": "What supervised machine learning models do they use? in Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features", "answer": ["ZeroR, Na\u00efve Bayes, J48, and random forest classifiers", "ZeroR, Na\u00efve Bayes, J48, and random forest ", "They use four classifiers: ZeroR, Naive Bayes, J48, and random forest."], "top_k_doc_id": [7638, 7639, 7640, 5131, 6317, 6318, 6319, 3052, 7837, 3988, 4545, 4550, 5743, 2828, 2798], "orig_top_k_doc_id": [6317, 6318, 6319, 3052, 7638, 7640, 5131, 5743, 4550, 4545, 3988, 7837, 7639, 2828, 2798]}, {"qid": 3914, "question": "Does the supervised machine learning approach outperform previous work? in Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features", "answer": ["No", "No", "No"], "top_k_doc_id": [7638, 7639, 7640, 5131, 6317, 6318, 6319, 3052, 7837, 3988, 4545, 4550, 5743, 705, 6985], "orig_top_k_doc_id": [6317, 6318, 6319, 7638, 3052, 7640, 5131, 7639, 3988, 4545, 705, 7837, 6985, 4550, 5743]}, {"qid": 3915, "question": "How large is the released data set? in Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features", "answer": ["1470 sentences", "316 sentences in Hypertension corpus, 877 sentences in Rhinosinusitis corpus", "No"], "top_k_doc_id": [7638, 7639, 7640, 5131, 6317, 6318, 6319, 3052, 7837, 7314, 7835, 2641, 705, 7157, 7833], "orig_top_k_doc_id": [6317, 6318, 6319, 5131, 3052, 7638, 7640, 7314, 7639, 7835, 7837, 2641, 705, 7157, 7833]}, {"qid": 3916, "question": "What is an example of a condition-action pair? in Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features", "answer": ["No", "Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation", "If patients have asthma, then beta-blockers, including eye drops, are contraindicated"], "top_k_doc_id": [7638, 7639, 7640, 5131, 6317, 6318, 6319, 705, 4545, 4722, 4550, 7314, 6347, 3908, 700], "orig_top_k_doc_id": [6317, 6318, 6319, 5131, 7639, 7640, 7638, 705, 4545, 4722, 4550, 7314, 6347, 3908, 700]}, {"qid": 4925, "question": "are the protocols manually annotated? in An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols", "answer": ["Yes", "Yes"], "top_k_doc_id": [7638, 7639, 7640, 638, 1074, 2388, 3287, 6344, 1807, 2210, 5157, 6317, 5395, 6600, 6141], "orig_top_k_doc_id": [7638, 7640, 7639, 3287, 2388, 6344, 1807, 2210, 5157, 6317, 1074, 5395, 638, 6600, 6141]}, {"qid": 4926, "question": "what ML approaches did they experiment with? in An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols", "answer": ["maximum entropy, neural network tagging model", "MaxEnt, BiLSTM, BiLSTM+CRF"], "top_k_doc_id": [7638, 7639, 7640, 638, 1074, 2388, 2386, 2400, 2399, 2390, 5664, 2395, 5914, 2389, 2398], "orig_top_k_doc_id": [7638, 7640, 7639, 2388, 2386, 2400, 2399, 1074, 638, 2390, 5664, 2395, 5914, 2389, 2398]}]}
{"group_id": 337, "group_size": 6, "items": [{"qid": 3984, "question": "Which classifiers did they experiment with? in GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors", "answer": ["logistic regression classifier", "Long Short Term Memory (LSTM) language model, logistic regression model", "logistic regression classifier, trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier"], "top_k_doc_id": [29, 6443, 6444, 6445, 6446, 6447, 7327, 7330, 7332, 5841, 7328, 7664, 7665, 6178, 3011], "orig_top_k_doc_id": [6443, 6447, 6446, 6445, 6444, 7330, 7327, 7328, 7332, 29, 7664, 5841, 6178, 7665, 3011]}, {"qid": 3986, "question": "How did they identify what language the text was? in GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors", "answer": ["used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages", " We used NanigoNet, a language detector based on GCNNs", "NanigoNet"], "top_k_doc_id": [29, 6443, 6444, 6445, 6446, 6447, 7327, 7330, 7332, 5841, 7328, 7664, 1441, 7665, 1151], "orig_top_k_doc_id": [6443, 6446, 6447, 6445, 6444, 7332, 7327, 29, 7330, 7328, 7665, 1441, 7664, 1151, 5841]}, {"qid": 3988, "question": "Which three features do they use? in GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors", "answer": ["mechanical, spell, and grammatical edits", "Ratio of the target perplexity over the source calculated by a language model, Normalized edit distance between the source and the target, Binary variable indicating whether the edit purely consists of changes in numbers", "Ratio of the target perplexity over the source calculated by a language model, Normalized edit distance between the source and the target, Binary variable indicating whether the edit purely consists of changes in numbers"], "top_k_doc_id": [29, 6443, 6444, 6445, 6446, 6447, 7327, 7330, 7332, 5841, 7328, 7664, 7665, 3641, 7329], "orig_top_k_doc_id": [6443, 6447, 6446, 6445, 6444, 7330, 7327, 7332, 7664, 7328, 29, 7665, 5841, 3641, 7329]}, {"qid": 3989, "question": "Which languages are covered in the corpus? in GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors", "answer": ["the top 10 languages are English, simplified Chinese, Japanese, Russian, French, German, Portuguese, Spanish, Korean and Hindi", "English, Chinese, Japanese, Russian, French, German, Portugese, Spanish, Korean, Hindi and Others", "English,  Chinese (smpl.),  Japanese,  Russian,  French,  German,  Portuguese,  Spanish,  Korean , Hindi"], "top_k_doc_id": [29, 6443, 6444, 6445, 6446, 6447, 7327, 7330, 7332, 5841, 7328, 7664, 1441, 5846, 3641], "orig_top_k_doc_id": [6447, 6443, 6446, 6445, 6444, 7332, 7327, 7330, 29, 5846, 3641, 5841, 7328, 1441, 7664]}, {"qid": 3985, "question": "Is the distribution of the edits uniform across all languages? in GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors", "answer": ["Yes", "No"], "top_k_doc_id": [29, 6443, 6444, 6445, 6446, 6447, 7327, 7330, 7332, 5841, 6031, 7665, 6268, 3641, 3691], "orig_top_k_doc_id": [6446, 6443, 6447, 6445, 6444, 7327, 7330, 6031, 7665, 7332, 6268, 3641, 29, 5841, 3691]}, {"qid": 3987, "question": "Which repositories did they collect from? in GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors", "answer": ["Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.", "GitHub repositories", "Has at least one pull request or pull request review comment event between November 2017 and September 2019,, 50 or more starts, size between 1MB and 1GB, permissive license"], "top_k_doc_id": [29, 6443, 6444, 6445, 6446, 6447, 7327, 7330, 7332, 2272, 7549, 2270, 7328, 7664, 7550], "orig_top_k_doc_id": [6447, 6443, 6446, 6444, 6445, 2272, 7549, 7330, 2270, 7327, 7328, 7332, 29, 7664, 7550]}]}
{"group_id": 338, "group_size": 6, "items": [{"qid": 4068, "question": "how many activities are in the dataset? in Predicting Human Activities from User-Generated Content", "answer": ["29,494", "29537", "30,000"], "top_k_doc_id": [521, 6525, 6530, 6526, 6527, 6528, 6529, 7557, 4663, 7016, 5461, 1377, 6056, 60, 1168], "orig_top_k_doc_id": [6525, 6530, 6526, 6527, 5461, 6528, 521, 7557, 6529, 1377, 7016, 6056, 60, 4663, 1168]}, {"qid": 4069, "question": "who annotated the datset? in Predicting Human Activities from User-Generated Content", "answer": ["No", "1000 people"], "top_k_doc_id": [521, 6525, 6530, 6526, 6527, 6528, 6529, 7557, 4663, 7016, 1636, 5095, 6155, 1635, 5091], "orig_top_k_doc_id": [6525, 6530, 6526, 521, 6527, 6528, 4663, 1636, 7016, 5095, 6155, 6529, 1635, 7557, 5091]}, {"qid": 4066, "question": "what user traits are taken into account? in Predicting Human Activities from User-Generated Content", "answer": ["The hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.", "personal values", "Family, Nature, Work-Ethic, Religion"], "top_k_doc_id": [521, 6525, 6530, 6526, 6527, 6528, 343, 5793, 6589, 7016, 330, 5439, 15, 5091, 2789], "orig_top_k_doc_id": [6525, 6530, 521, 6528, 6589, 7016, 343, 6527, 6526, 330, 5793, 5439, 15, 5091, 2789]}, {"qid": 4067, "question": "does incorporating user traits help the task? in Predicting Human Activities from User-Generated Content", "answer": ["No", "No", "only in the 806-class task predicting <= 25 clusters"], "top_k_doc_id": [521, 6525, 6530, 6526, 6527, 6528, 343, 5793, 6589, 7016, 7261, 4124, 7260, 1081, 379], "orig_top_k_doc_id": [6525, 521, 6530, 5793, 7016, 7261, 6528, 343, 6589, 6527, 6526, 4124, 7260, 1081, 379]}, {"qid": 4070, "question": "how were the data instances chosen? in Predicting Human Activities from User-Generated Content", "answer": [" query contains a first-person, past-tense verb within a phrase that describes a common activity that people do", "By querying Twitter Search API for the tweets containing a first-person and a past-tense verb that describes a common activity."], "top_k_doc_id": [521, 6525, 6530, 6526, 6527, 6528, 6529, 7557, 7411, 6155, 5192, 1081, 27, 60, 6057], "orig_top_k_doc_id": [6525, 6530, 6526, 521, 6527, 6529, 7411, 6155, 5192, 7557, 1081, 27, 6528, 60, 6057]}, {"qid": 4071, "question": "what social media platform was the data collected from? in Predicting Human Activities from User-Generated Content", "answer": ["Twitter", "Twitter ", " Twitter"], "top_k_doc_id": [521, 6525, 6530, 5191, 6057, 7016, 1482, 5102, 3131, 5085, 6804, 6833, 7029, 3730, 5291], "orig_top_k_doc_id": [6525, 521, 5191, 6057, 7016, 1482, 5102, 3131, 6530, 5085, 6804, 6833, 7029, 3730, 5291]}]}
{"group_id": 339, "group_size": 6, "items": [{"qid": 4078, "question": "What linguistic model does the conventional method use? in Predicting Audience's Laughter Using Convolutional Neural Network", "answer": ["Random Forest to perform humor recognition by using the following two groups of features:  latent semantic structural features and semantic distance features.", "Random Forest BIBREF12", "Random Forest classifier using latent semantic structural features, semantic distance features and sentences' averaged Word2Vec representations"], "top_k_doc_id": [6477, 6540, 2083, 3951, 3950, 4940, 5421, 6541, 3300, 2216, 6070, 381, 1490, 6119, 2484], "orig_top_k_doc_id": [6540, 6477, 6541, 3951, 3300, 2083, 5421, 4940, 2216, 6070, 381, 1490, 3950, 6119, 2484]}, {"qid": 4081, "question": "Do they evaluate only on English data? in Predicting Audience's Laughter Using Convolutional Neural Network", "answer": ["No", "No", "Yes"], "top_k_doc_id": [6477, 6540, 2083, 3951, 3950, 4940, 5421, 742, 7549, 3952, 4814, 1755, 76, 2874, 3468], "orig_top_k_doc_id": [6540, 6477, 3951, 4940, 2083, 5421, 742, 7549, 3952, 4814, 1755, 76, 3950, 2874, 3468]}, {"qid": 4082, "question": "How many speakers are included in the dataset? in Predicting Audience's Laughter Using Convolutional Neural Network", "answer": ["No", "No", "No"], "top_k_doc_id": [6477, 6540, 2083, 3951, 3950, 4940, 3949, 6173, 6541, 7297, 4672, 5145, 418, 6351, 7007], "orig_top_k_doc_id": [6540, 3951, 4940, 6477, 7297, 2083, 3949, 3950, 6541, 4672, 5145, 418, 6351, 6173, 7007]}, {"qid": 4083, "question": "How are the positive instances annotated? e.g. by annotators, or by laughter from the audience? in Predicting Audience's Laughter Using Convolutional Neural Network", "answer": ["Laughter from the audience.", "by laughter", "By laughter from the audience"], "top_k_doc_id": [6477, 6540, 2083, 3951, 3950, 4940, 3949, 6173, 6541, 4942, 3952, 1755, 6174, 4941, 472], "orig_top_k_doc_id": [6540, 4940, 6173, 6477, 3951, 4942, 3949, 3952, 3950, 1755, 6174, 6541, 4941, 2083, 472]}, {"qid": 4080, "question": "What lexical cues are used for humor recogition? in Predicting Audience's Laughter Using Convolutional Neural Network", "answer": ["Incongruity, Ambiguity, Interpersonal Effect, Phonetic Style", "alliteration, antonymy, adult slang"], "top_k_doc_id": [6477, 6540, 2083, 3951, 6541, 1329, 7297, 2140, 2139, 3498, 5272, 2085, 4914, 6542, 3949], "orig_top_k_doc_id": [6540, 6477, 2083, 6541, 1329, 7297, 2140, 2139, 3498, 5272, 2085, 4914, 3951, 6542, 3949]}, {"qid": 4079, "question": "What is novel about the newly emerging CNN method, in comparison to well-established conventional method? in Predicting Audience's Laughter Using Convolutional Neural Network", "answer": ["No", "one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks"], "top_k_doc_id": [6477, 6540, 6541, 7007, 5421, 418, 466, 5104, 3300, 6070, 76, 2293, 1665, 1755, 6532], "orig_top_k_doc_id": [6540, 6477, 6541, 7007, 5421, 418, 466, 5104, 3300, 6070, 76, 2293, 1665, 1755, 6532]}]}
{"group_id": 340, "group_size": 6, "items": [{"qid": 4092, "question": "Which word embeddings do they compare against? in Character n-gram Embeddings to Improve RNN Language Models", "answer": ["No", "No"], "top_k_doc_id": [6512, 6551, 7687, 6552, 6554, 7688, 5286, 7260, 6522, 2621, 50, 5938, 2619, 6513, 6386], "orig_top_k_doc_id": [6551, 6522, 6512, 5286, 2621, 6554, 6552, 50, 7260, 5938, 7687, 2619, 6513, 6386, 7688]}, {"qid": 4094, "question": "What results do their embeddings obtain on machine translation? in Character n-gram Embeddings to Improve RNN Language Models", "answer": ["BLEU score of 35.48 on En-Fr, 23.27 on En-De, 34.43 on Fr-En, 28.86 on De-En", "BLEU scores are: En-Fr(35.84), En-De(23.27), Fr-En(34.43) and De-En(28.86).", "Bleu on IWSLT16: En-FR 35.48, En-De 23.27, Fr-En 34.43, De-En 28.86"], "top_k_doc_id": [6512, 6551, 7687, 6552, 6554, 7688, 5286, 7260, 7686, 7259, 2590, 6385, 6944, 6070, 6750], "orig_top_k_doc_id": [6551, 6554, 6552, 7687, 6512, 7686, 7259, 2590, 5286, 7688, 6385, 6944, 6070, 6750, 7260]}, {"qid": 4090, "question": "What sized character n-grams do they use? in Character n-gram Embeddings to Improve RNN Language Models", "answer": ["cahr3-MS-vec, char4-MS-vec, char2-MS-vec", "2, 3 and 4", "char3"], "top_k_doc_id": [6512, 6551, 7687, 6552, 2615, 6240, 6522, 7259, 48, 465, 1434, 7174, 7686, 5709, 2590], "orig_top_k_doc_id": [7687, 48, 6551, 2615, 6240, 7259, 465, 6512, 6522, 1434, 7174, 6552, 7686, 5709, 2590]}, {"qid": 4093, "question": "Which dataset do they evaluate on for headline generation? in Character n-gram Embeddings to Improve RNN Language Models", "answer": ["English Gigaword corpus", "English Gigaword corpus BIBREF35", " the annotated English Gigaword corpus"], "top_k_doc_id": [6512, 6551, 7687, 6552, 6554, 7688, 6555, 6553, 6556, 6067, 6750, 1863, 6716, 5889, 1989], "orig_top_k_doc_id": [6554, 6551, 6552, 7688, 6555, 6512, 6553, 6556, 7687, 6067, 6750, 1863, 6716, 5889, 1989]}, {"qid": 4095, "question": "How do they combine ordinary word embeddings and ones constructed from character n-grams? in Character n-gram Embeddings to Improve RNN Language Models", "answer": ["They use a sum of charn-MS-vec and the standard word embedding as an input of an RNN", "Yes"], "top_k_doc_id": [6512, 6551, 7687, 6552, 2615, 6240, 6522, 7259, 6982, 7260, 5302, 6771, 6148, 7688, 6521], "orig_top_k_doc_id": [7687, 6551, 7259, 6552, 6982, 6240, 7260, 5302, 6771, 6522, 6148, 7688, 6512, 6521, 2615]}, {"qid": 4091, "question": "Do they experiment with fine-tuning their embeddings? in Character n-gram Embeddings to Improve RNN Language Models", "answer": ["No", "No", "No"], "top_k_doc_id": [6512, 6551, 7687, 50, 6944, 5292, 5172, 5291, 5082, 5875, 1825, 6943, 2622, 2621, 6554], "orig_top_k_doc_id": [50, 6944, 5292, 6551, 5172, 6512, 5291, 5082, 5875, 1825, 6943, 2622, 2621, 6554, 7687]}]}
{"group_id": 341, "group_size": 6, "items": [{"qid": 4299, "question": "Is this an English language corpus? in Twitter Job/Employment Corpus: A Dataset of Job-Related Discourse Built with Humans in the Loop", "answer": ["No", "Yes", "Yes"], "top_k_doc_id": [6141, 6805, 2800, 6804, 6806, 6808, 7017, 61, 6140, 6807, 4188, 6364, 951, 7063, 2958], "orig_top_k_doc_id": [6805, 6808, 6804, 6806, 2800, 6140, 61, 6141, 7017, 6807, 4188, 6364, 951, 7063, 2958]}, {"qid": 4301, "question": "What type of annotation is performed? in Twitter Job/Employment Corpus: A Dataset of Job-Related Discourse Built with Humans in the Loop", "answer": ["human annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related", "multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics"], "top_k_doc_id": [6141, 6805, 2800, 6804, 6806, 6808, 7017, 61, 6140, 6807, 4188, 6364, 2801, 2805, 5913], "orig_top_k_doc_id": [6808, 6805, 6804, 6806, 6140, 6141, 2800, 4188, 6807, 6364, 7017, 2801, 2805, 61, 5913]}, {"qid": 4302, "question": "How are the tweets selected? in Twitter Job/Employment Corpus: A Dataset of Job-Related Discourse Built with Humans in the Loop", "answer": ["They collected tweets from US and then applied some filtering rules based on Lexicons", " multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics, to extract job-related tweets from personal and business accounts"], "top_k_doc_id": [6141, 6805, 2800, 6804, 6806, 6808, 7017, 61, 6140, 6807, 882, 951, 4135, 6771, 881], "orig_top_k_doc_id": [6805, 6808, 6804, 6806, 6140, 61, 6141, 6807, 7017, 882, 2800, 951, 4135, 6771, 881]}, {"qid": 3781, "question": "Which was the most helpful strategy? in Integrating Crowdsourcing and Active Learning for Classification of Work-Life Events from Tweets", "answer": ["Vote entropy and KL divergence,  all the active learning strategies we tested do not work well with deep learning model", "Entropy algorithm is the best way to build machine learning models. Vote entropy and KL divergence are helpful for the training of machine learning ensemble classifiers.", "entropy"], "top_k_doc_id": [6141, 6805, 331, 521, 579, 3623, 4130, 6140, 6142, 6143, 6632, 4279, 6804, 6808, 4119], "orig_top_k_doc_id": [6140, 6143, 6142, 6805, 6141, 4279, 579, 3623, 521, 6804, 4130, 331, 6808, 6632, 4119]}, {"qid": 3782, "question": "How large is their tweets dataset? in Integrating Crowdsourcing and Active Learning for Classification of Work-Life Events from Tweets", "answer": ["3,685,984 unique tweets", "3,685,984 unique tweets", "3,685,984 unique tweets"], "top_k_doc_id": [6141, 6805, 331, 521, 579, 3623, 4130, 6140, 6142, 6143, 6632, 5115, 3574, 4280, 7310], "orig_top_k_doc_id": [6140, 6142, 6143, 6805, 6141, 579, 4130, 3623, 331, 5115, 3574, 4280, 521, 6632, 7310]}, {"qid": 4300, "question": "The authors point out a relevant constraint on the previous corpora of workplace, do they authors mention any relevant constrains on this corpus? in Twitter Job/Employment Corpus: A Dataset of Job-Related Discourse Built with Humans in the Loop", "answer": ["No", "No"], "top_k_doc_id": [6141, 6805, 2800, 6804, 6806, 6808, 7017, 6771, 2109, 5195, 4629, 984, 1070, 6791, 7063], "orig_top_k_doc_id": [6805, 6804, 6808, 6806, 7017, 2800, 6771, 2109, 6141, 5195, 4629, 984, 1070, 6791, 7063]}]}
{"group_id": 342, "group_size": 6, "items": [{"qid": 4512, "question": "How is BERT optimized for this task? in BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction", "answer": ["We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.", "NER (Named Entity Recognition) is the first task in the joint multi-head selection model, relation classification task as a multi-head selection problem, auxiliary sentence-level relation classification prediction task"], "top_k_doc_id": [7251, 2217, 5133, 7055, 7056, 7057, 7058, 1763, 1764, 2215, 7250, 7553, 7254, 7253, 2306], "orig_top_k_doc_id": [7056, 7251, 7058, 7055, 7057, 7254, 7250, 7253, 2217, 2306, 1763, 7553, 5133, 1764, 2215]}, {"qid": 4513, "question": "What is a soft label? in BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction", "answer": [" To solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied, soft label embedding, which takes the logits as input to preserve probability of each entity type", "we proposed soft label embedding, which takes the logits as input to preserve probability of each entity type"], "top_k_doc_id": [7251, 2217, 5133, 7055, 7056, 7057, 7058, 1763, 1764, 2215, 7250, 7553, 6113, 6115, 1761], "orig_top_k_doc_id": [7056, 7058, 7055, 7057, 7251, 5133, 6113, 2217, 6115, 7553, 1764, 7250, 2215, 1763, 1761]}, {"qid": 4639, "question": "How do they perform the joint training? in Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text", "answer": ["They train a single model that integrates a BERT language model as a shared parameter layer on NER and RC tasks.", "They perform joint learning through shared parameters for NER and RC."], "top_k_doc_id": [7251, 2216, 3744, 4439, 4967, 7250, 7253, 7254, 7255, 2217, 3348, 3349, 7055, 1763, 3844], "orig_top_k_doc_id": [7254, 7253, 7250, 7251, 7255, 7055, 2217, 3744, 3349, 2216, 3348, 1763, 4967, 4439, 3844]}, {"qid": 4640, "question": "How many parameters does their model have? in Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text", "answer": ["No", "No"], "top_k_doc_id": [7251, 2216, 3744, 4439, 4967, 7250, 7253, 7254, 7255, 2217, 3348, 3349, 7055, 5133, 3858], "orig_top_k_doc_id": [7254, 7253, 7250, 7251, 3348, 7055, 7255, 3349, 3744, 4439, 5133, 2217, 2216, 4967, 3858]}, {"qid": 4511, "question": "What is the weak supervision signal used in Baidu Baike corpus? in BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction", "answer": ["consider the title of each sample as a pseudo label and conduct NER pre-training", "NER Pretraining"], "top_k_doc_id": [7251, 2217, 5133, 7055, 7056, 7057, 7058, 3538, 3539, 1240, 6661, 6113, 3541, 22, 1534], "orig_top_k_doc_id": [7056, 7055, 7057, 3538, 7251, 7058, 3539, 1240, 6661, 6113, 3541, 22, 5133, 1534, 2217]}, {"qid": 4641, "question": "What is the previous model that achieved state-of-the-art? in Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text", "answer": ["Joint Bi-LSTM", "RDCNN, Joint-Bi-LSTM"], "top_k_doc_id": [7251, 2216, 3744, 4439, 4967, 7250, 7253, 7254, 7255, 5133, 22, 20, 945, 4609, 7056], "orig_top_k_doc_id": [7251, 7254, 5133, 7250, 7253, 3744, 4967, 22, 20, 945, 7255, 4609, 7056, 2216, 4439]}]}
{"group_id": 343, "group_size": 6, "items": [{"qid": 4569, "question": "Where did the system place in the other sub-tasks? in Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter", "answer": ["which we scored lower", "No"], "top_k_doc_id": [451, 6684, 7131, 7132, 1843, 3542, 450, 5255, 6685, 7133, 2874, 3543, 5008, 6176, 756], "orig_top_k_doc_id": [7133, 7131, 7132, 3543, 6176, 2874, 450, 6684, 5255, 6685, 1843, 3542, 451, 756, 5008]}, {"qid": 4570, "question": "What were the five labels to be predicted in sub-task C? in Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter", "answer": ["very negative, negative, neutral, positive, very positive", "very negative, negative, neutral, positive, very positive"], "top_k_doc_id": [451, 6684, 7131, 7132, 1843, 3542, 450, 5255, 6685, 7133, 2874, 3543, 5008, 452, 448], "orig_top_k_doc_id": [7132, 7131, 3542, 1843, 6684, 5008, 451, 450, 3543, 7133, 452, 5255, 6685, 448, 2874]}, {"qid": 4568, "question": "Which Twitter sentiment treebank is used? in Amobee at SemEval-2017 Task 4: Deep Learning System for Sentiment Detection on Twitter", "answer": ["They built their own", "Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees"], "top_k_doc_id": [451, 6684, 7131, 7132, 1843, 3542, 450, 5255, 6685, 7133, 452, 448, 1844, 6176, 1841], "orig_top_k_doc_id": [7131, 7133, 7132, 6684, 450, 6685, 5255, 452, 451, 448, 1843, 3542, 1844, 6176, 1841]}, {"qid": 1342, "question": "What were the five English subtasks? in BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs", "answer": [" five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0"], "top_k_doc_id": [451, 6684, 7131, 7132, 1843, 3542, 1844, 1841, 1842, 5008, 5007, 6752, 2982, 448, 452], "orig_top_k_doc_id": [1843, 1844, 1841, 1842, 5008, 5007, 7131, 451, 6752, 2982, 6684, 448, 3542, 452, 7132]}, {"qid": 4207, "question": "what were the evaluation metrics? in Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification", "answer": ["No", "macro-average recall"], "top_k_doc_id": [451, 6684, 7131, 7132, 447, 448, 450, 2982, 4379, 5008, 6685, 7628, 7627, 6400, 2044], "orig_top_k_doc_id": [6685, 6684, 4379, 448, 7131, 447, 2982, 450, 7628, 5008, 451, 7132, 7627, 6400, 2044]}, {"qid": 4208, "question": "how many sentiment labels do they explore? in Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification", "answer": ["3", "3", "3"], "top_k_doc_id": [451, 6684, 7131, 7132, 447, 448, 450, 2982, 4379, 5008, 6685, 2215, 2306, 6752, 449], "orig_top_k_doc_id": [6685, 6684, 447, 7131, 450, 448, 2982, 451, 2215, 4379, 5008, 2306, 7132, 6752, 449]}]}
{"group_id": 344, "group_size": 6, "items": [{"qid": 4607, "question": "What are the most discriminating patterns which are analyzed? in And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "answer": ["Patterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes and  patterns with the preposition \u201cfor\u201d are substantially more common in the feel class than the fact class.", "forms associated with the fact and feel"], "top_k_doc_id": [5374, 5377, 5378, 7185, 7186, 7187, 7188, 7189, 5384, 5386, 1499, 1656, 4831, 3545, 5906], "orig_top_k_doc_id": [7188, 7189, 7185, 5378, 1499, 7186, 5377, 5386, 7187, 4831, 5374, 3545, 5906, 1656, 5384]}, {"qid": 4609, "question": "What patterns were extracted which were correlated with emotional arguments? in And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "answer": ["Examples of extracted patters with high probability that contain of are: MARRIAGE FOR, STANDING FOR, SAME FOR, TREATMENT FOR, DEMAND FOR, ATTENTION FOR, ADVOCATE FOR, NO EVIDENCE FOR, JUSTIFICATION FOR, EXCUSE FOR", "Pattrn based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible or they involve assessment or evaluations of the arguments of the other speaker.  They are typically also very creative and diverse."], "top_k_doc_id": [5374, 5377, 5378, 7185, 7186, 7187, 7188, 7189, 5384, 5386, 1499, 1656, 4831, 589, 1495], "orig_top_k_doc_id": [7188, 7189, 7185, 5378, 5377, 7186, 5374, 5384, 4831, 1656, 7187, 589, 5386, 1499, 1495]}, {"qid": 4606, "question": "Do they report results only on English data? in And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "answer": ["No", "Yes"], "top_k_doc_id": [5374, 5377, 5378, 7185, 7186, 7187, 7188, 7189, 5384, 5386, 3545, 589, 2158, 7222, 5136], "orig_top_k_doc_id": [7188, 7189, 7185, 5378, 3545, 5377, 5384, 7186, 589, 2158, 5374, 7187, 7222, 5386, 5136]}, {"qid": 4610, "question": "What patterns were extracted which were correlated with factual arguments? in And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "answer": [" patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases", "Examples of extracted patters with high probability that correlate with factual argument are: RESULT OF, ORIGIN OF, THEORY OF, EVIDENCE OF, PARTS OF, EVOLUTION OF, PERCENT OF, THOUSANDS OF, EXAMPLE OF, LAW OF"], "top_k_doc_id": [5374, 5377, 5378, 7185, 7186, 7187, 7188, 7189, 5384, 2158, 4901, 5136, 2159, 5375, 1656], "orig_top_k_doc_id": [7188, 7189, 7185, 7186, 7187, 5378, 2158, 5384, 5377, 4901, 5374, 5136, 2159, 5375, 1656]}, {"qid": 4611, "question": "How were the factual and feeling forum posts annotated? in And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "answer": ["binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class.", "manually"], "top_k_doc_id": [5374, 5377, 5378, 7185, 7186, 7187, 7188, 7189, 5379, 5386, 5381, 5382, 4831, 1656, 5258], "orig_top_k_doc_id": [7188, 7185, 7189, 7186, 7187, 5378, 5379, 5386, 5374, 5381, 5377, 5382, 4831, 1656, 5258]}, {"qid": 4608, "question": "What bootstrapping methodology was used to find new patterns? in And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue", "answer": ["flow diagram for the bootstrapping system is shown in Figure FIGREF10", "They embed AutoSlog-TS in a bootstrapping framework to learn additional linguistic expressions from the unannotated texts -  they give the labeled training data to AutoSlog-TS, which generates patterns and associated statistics and then identifies high-precision patterns that can be used to label some of the unannotated texts as factual or feeling."], "top_k_doc_id": [5374, 5377, 5378, 7185, 7186, 7187, 7188, 7189, 5906, 1655, 1656, 1712, 6375, 3545, 1659], "orig_top_k_doc_id": [7188, 7189, 7185, 7186, 7187, 5906, 1655, 5378, 1656, 1712, 6375, 3545, 5374, 5377, 1659]}]}
{"group_id": 345, "group_size": 6, "items": [{"qid": 4752, "question": "How does this single-system compares to system combination ones? in An Investigation into the Effectiveness of Enhancement in ASR Training and Test for Chime-5 Dinner Party Transcription", "answer": ["in terms of WER for the DEV (EVAL) set, the single proposed model (GSS1) has higher WER than the multiple proposed model  (GSS6) by 7.4% (4.1%). ", "WER of the best single system 48.6 (46.7) comapred to 41.6 (43.2) of the best multiple system."], "top_k_doc_id": [7406, 1337, 7403, 7405, 7407, 7404, 1161, 4239, 6776, 4367, 7793, 4923, 1268, 1338, 1163], "orig_top_k_doc_id": [7403, 7404, 7407, 7405, 7406, 1337, 4367, 6776, 4239, 4923, 7793, 1268, 1338, 1163, 1161]}, {"qid": 4753, "question": "What was previous single-system state of the art result on the CHiME-5 data? in An Investigation into the Effectiveness of Enhancement in ASR Training and Test for Chime-5 Dinner Party Transcription", "answer": ["BIBREF12 (H/UPB)", "Previous single system state of the art had WER of  58.3 (53.1)."], "top_k_doc_id": [7406, 1337, 7403, 7405, 7407, 7404, 1161, 4239, 6776, 4367, 7793, 4544, 4543, 4152, 1336], "orig_top_k_doc_id": [7403, 7407, 7404, 7405, 7406, 1337, 4544, 4543, 7793, 6776, 4239, 4367, 1161, 4152, 1336]}, {"qid": 4754, "question": "How much is error rate reduced by cleaning up training data? in An Investigation into the Effectiveness of Enhancement in ASR Training and Test for Chime-5 Dinner Party Transcription", "answer": ["No", "In case of singe model the WER was better by 10.%  (6.4%) and in case of multi model it was 3.5% ( 4.1%)"], "top_k_doc_id": [7406, 1337, 7403, 7405, 7407, 7404, 1161, 4239, 6776, 2531, 1162, 1163, 1164, 4152, 150], "orig_top_k_doc_id": [7403, 7407, 7404, 7406, 7405, 2531, 1337, 1161, 6776, 1162, 1163, 4239, 1164, 4152, 150]}, {"qid": 4751, "question": "What supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training? in An Investigation into the Effectiveness of Enhancement in ASR Training and Test for Chime-5 Dinner Party Transcription", "answer": ["we performed an extensive experimental evaluation on the acoustically very challenging CHiME-5 dinner party data", "accuracy improves with increasing power of the enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data"], "top_k_doc_id": [7406, 1337, 7403, 7405, 7407, 7404, 1336, 1338, 4543, 6314, 3271, 279, 280, 4271, 3266], "orig_top_k_doc_id": [7403, 7407, 7404, 7405, 7406, 1337, 1336, 1338, 4543, 6314, 3271, 279, 280, 4271, 3266]}, {"qid": 1018, "question": "Which frozen acoustic model do they use? in Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data", "answer": ["a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13"], "top_k_doc_id": [7406, 1337, 7403, 7405, 7407, 1336, 1338, 2296, 2528, 2294, 2293, 668, 6300, 2295, 4370], "orig_top_k_doc_id": [1336, 1338, 1337, 7403, 7406, 7407, 2296, 2528, 7405, 2294, 2293, 668, 6300, 2295, 4370]}, {"qid": 3527, "question": "What datasets are used to assess the performance of the system? in Topic Identification for Speech without ASR", "answer": ["Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program", "LORELEI datasets of Uzbek, Mandarin and Turkish"], "top_k_doc_id": [7406, 5822, 4653, 1266, 1161, 1268, 5768, 5764, 3401, 1336, 4373, 101, 6310, 5825, 6314], "orig_top_k_doc_id": [5822, 4653, 1266, 1161, 7406, 1268, 5768, 5764, 3401, 1336, 4373, 101, 6310, 5825, 6314]}]}
{"group_id": 346, "group_size": 6, "items": [{"qid": 4873, "question": "What language are the videos in? in Identifying Visible Actions in Lifestyle Vlogs", "answer": ["No", "No"], "top_k_doc_id": [7556, 7557, 7558, 7560, 7561, 7559, 593, 1483, 2212, 7532, 314, 3126, 5104, 1798, 285], "orig_top_k_doc_id": [7561, 7556, 7557, 7560, 7558, 7532, 7559, 314, 1483, 5104, 2212, 3126, 593, 1798, 285]}, {"qid": 4874, "question": "How long are the videos? in Identifying Visible Actions in Lifestyle Vlogs", "answer": ["length of our collected videos varies from two minutes to twenty minutes", "On average videos are 16.36 minutes long"], "top_k_doc_id": [7556, 7557, 7558, 7560, 7561, 7559, 593, 1483, 2212, 7532, 314, 3126, 3908, 594, 319], "orig_top_k_doc_id": [7556, 7561, 7557, 7560, 7558, 7532, 7559, 593, 314, 3908, 594, 1483, 3126, 2212, 319]}, {"qid": 4869, "question": "How many actions are present in the dataset? in Identifying Visible Actions in Lifestyle Vlogs", "answer": ["14,769", "14,769 actions"], "top_k_doc_id": [7556, 7557, 7558, 7560, 7561, 7559, 314, 319, 1072, 1798, 3681, 3680, 4545, 7639, 3475], "orig_top_k_doc_id": [7556, 7561, 7557, 7560, 7558, 7559, 314, 1072, 319, 3681, 1798, 3680, 4545, 7639, 3475]}, {"qid": 4870, "question": "How many videos did they use? in Identifying Visible Actions in Lifestyle Vlogs", "answer": ["177", "1,268"], "top_k_doc_id": [7556, 7557, 7558, 7560, 7561, 7559, 593, 1483, 2212, 7532, 3362, 285, 594, 319, 3909], "orig_top_k_doc_id": [7556, 7561, 7557, 7560, 7558, 1483, 7559, 593, 2212, 7532, 3362, 285, 594, 319, 3909]}, {"qid": 4871, "question": "What unimodal algorithms do they compare with? in Identifying Visible Actions in Lifestyle Vlogs", "answer": ["Concreteness, Feature-based Classifier, LSTM and ELMo, Yolo Object Detection", "SVM, LSTM, ELMo, Yolo Object Detection"], "top_k_doc_id": [7556, 7557, 7558, 7560, 7561, 7559, 314, 319, 1072, 1798, 930, 7532, 7037, 3362, 7141], "orig_top_k_doc_id": [7561, 7556, 7557, 7560, 7558, 930, 7532, 7559, 319, 1798, 7037, 3362, 314, 1072, 7141]}, {"qid": 4872, "question": "What platform was used for crowdsourcing? in Identifying Visible Actions in Lifestyle Vlogs", "answer": ["Amazon Mechanical Turk (AMT)", "Amazon Mechanical Turk "], "top_k_doc_id": [7556, 7557, 7558, 7560, 7561, 1075, 1072, 1074, 3362, 6995, 4785, 1071, 5911, 7532, 3126], "orig_top_k_doc_id": [7561, 7556, 7557, 7560, 1075, 1072, 1074, 7558, 3362, 6995, 4785, 1071, 5911, 7532, 3126]}]}
{"group_id": 347, "group_size": 6, "items": [{"qid": 4940, "question": "How long is their dataset? in Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin", "answer": ["21214", "Data used has total of 23315 sentences."], "top_k_doc_id": [3920, 7669, 2825, 7007, 7670, 2074, 4396, 6294, 6291, 7847, 2998, 6295, 231, 6082, 7472], "orig_top_k_doc_id": [7669, 7007, 7670, 2825, 3920, 7847, 6291, 2074, 2998, 6295, 231, 4396, 6082, 7472, 6294]}, {"qid": 4941, "question": "What metrics are used? in Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin", "answer": ["BLEU score", "BLEU"], "top_k_doc_id": [3920, 7669, 2825, 7007, 7670, 2074, 4396, 6294, 6291, 6063, 6491, 4734, 6957, 1886, 3750], "orig_top_k_doc_id": [7669, 7007, 7670, 2825, 6063, 3920, 6491, 2074, 4734, 6957, 6294, 6291, 4396, 1886, 3750]}, {"qid": 4942, "question": "What is the best performing system? in Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin", "answer": ["A supervised model with byte pair encoding was the best for English to Pidgin, while a supervised model with word-level encoding was the best for Pidgin to English.", "In English to Pidgin best was byte pair encoding tokenization superised model, while in Pidgin to English word-level tokenization supervised model was the best."], "top_k_doc_id": [3920, 7669, 2825, 7007, 7670, 117, 2165, 2972, 6063, 6291, 7472, 6294, 5737, 4396, 6295], "orig_top_k_doc_id": [7669, 7007, 7670, 6291, 2165, 7472, 6294, 2825, 2972, 5737, 4396, 117, 3920, 6063, 6295]}, {"qid": 4943, "question": "What tokenization methods are used? in Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin", "answer": ["word-level , subword-level", "word-level, Byte Pair Encoding (BPE) subword-level"], "top_k_doc_id": [3920, 7669, 2825, 7007, 7670, 2074, 4396, 6294, 7686, 231, 6661, 7687, 4734, 2971, 2972], "orig_top_k_doc_id": [7669, 7007, 7670, 7686, 231, 2825, 6661, 7687, 2074, 6294, 3920, 4396, 4734, 2971, 2972]}, {"qid": 4944, "question": "What baselines do they propose? in Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin", "answer": ["Transformer architecture of BIBREF7", "supervised translation models"], "top_k_doc_id": [3920, 7669, 2825, 7007, 7670, 117, 2165, 2972, 6063, 6291, 6064, 3633, 4734, 6955, 3750], "orig_top_k_doc_id": [7669, 7007, 7670, 117, 6064, 2972, 6291, 6063, 3633, 2825, 4734, 6955, 3920, 3750, 2165]}, {"qid": 2403, "question": "Which dataset do they use? in Neural Machine Translation with Supervised Attention", "answer": ["BTEC corpus, the CSTAR03 and IWSLT04 held out sets, the NIST2008 Open Machine Translation Campaign"], "top_k_doc_id": [3920, 7669, 117, 3917, 4766, 2836, 6295, 575, 2136, 2998, 2074, 231, 868, 2135, 1743], "orig_top_k_doc_id": [7669, 117, 3920, 3917, 4766, 2836, 6295, 575, 2136, 2998, 2074, 231, 868, 2135, 1743]}]}
{"group_id": 348, "group_size": 6, "items": [{"qid": 4978, "question": "What size of dataset is sufficiently large for the model performance to approach the inter-annotator agreement? in Multilingual Twitter Sentiment Classification: The Role of Human Annotators", "answer": ["around 100,000 annotations", "150,000 labeled tweets"], "top_k_doc_id": [6808, 7750, 7751, 7746, 7747, 7748, 7749, 7752, 3626, 6177, 6625, 7308, 7753, 7754, 2392], "orig_top_k_doc_id": [7746, 7751, 7747, 7750, 7752, 7749, 6808, 7308, 7748, 6177, 2392, 7753, 7754, 3626, 6625]}, {"qid": 4979, "question": "Which measures of inter-annotator agreement are used? in Multilingual Twitter Sentiment Classification: The Role of Human Annotators", "answer": ["Krippendorff's Alpha-reliability", "Krippendorff's Alpha-reliability ( INLINEFORM0 ) BIBREF6, F score ( INLINEFORM0 ), Accuracy ( INLINEFORM0 ), Accuracy within 1 ( INLINEFORM0 )"], "top_k_doc_id": [6808, 7750, 7751, 7746, 7747, 7748, 7749, 7752, 3626, 6177, 6625, 7308, 7753, 7754, 6806], "orig_top_k_doc_id": [7747, 7746, 7751, 7750, 7749, 7753, 7752, 7754, 6808, 7748, 7308, 6177, 6806, 3626, 6625]}, {"qid": 4789, "question": "What was the level of inter-annotator agreement? in CAp 2017 challenge: Twitter Named Entity Recognition", "answer": ["Average Cohen\u2019s Kappa score of inter-annotator agreement was 0.655", "score for Cohen's Kappa (0,70)"], "top_k_doc_id": [6808, 7750, 7751, 7746, 7747, 7748, 7749, 7752, 7456, 7457, 2329, 7458, 4105, 5911, 5145], "orig_top_k_doc_id": [7456, 7752, 7457, 7750, 7751, 2329, 6808, 7749, 7747, 7458, 4105, 7746, 7748, 5911, 5145]}, {"qid": 4257, "question": "what was the inter-annotator agreement? in A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking", "answer": ["For stance annotation the inter-annotator agreement was 0.7, for FGE annotation inter-annotator agreement was 0.55", "Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, $\\kappa = 0.55$ Cohen's Kappa"], "top_k_doc_id": [6808, 7750, 7751, 7746, 7747, 6740, 6742, 5382, 1835, 6745, 1834, 862, 6182, 5873, 3626], "orig_top_k_doc_id": [6740, 6742, 6808, 5382, 7751, 1835, 6745, 1834, 7747, 862, 7746, 7750, 6182, 5873, 3626]}, {"qid": 764, "question": "What was the inter-annotator agreement? in Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering", "answer": ["correctness of all the question answer pairs are verified by at least two annotators"], "top_k_doc_id": [6808, 7750, 7751, 3626, 2392, 2910, 5387, 7351, 3627, 957, 5873, 4841, 2393, 7752, 2387], "orig_top_k_doc_id": [3626, 2392, 2910, 7751, 5387, 7750, 7351, 3627, 6808, 957, 5873, 4841, 2393, 7752, 2387]}, {"qid": 4875, "question": "What was the inter-annotator agreement between the expert annotators? in A corpus of precise natural textual entailment problems", "answer": ["No", "No"], "top_k_doc_id": [6808, 7563, 7562, 5873, 5911, 6742, 5874, 7752, 5376, 4105, 6805, 1646, 2662, 6182, 2392], "orig_top_k_doc_id": [6808, 7563, 7562, 5873, 5911, 6742, 5874, 7752, 5376, 4105, 6805, 1646, 2662, 6182, 2392]}]}
{"group_id": 349, "group_size": 5, "items": [{"qid": 52, "question": "How is quality of the citation measured? in Citation Data of Czech Apex Courts", "answer": ["it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."], "top_k_doc_id": [1301, 3606, 3608, 55, 56, 57, 58, 1921, 2041, 3607, 305, 1924, 2046, 1925, 3611], "orig_top_k_doc_id": [58, 55, 57, 56, 305, 3608, 2041, 1921, 1924, 1301, 3606, 3607, 2046, 1925, 3611]}, {"qid": 53, "question": "How big is the dataset? in Citation Data of Czech Apex Courts", "answer": ["903019 references"], "top_k_doc_id": [1301, 3606, 3608, 55, 56, 57, 58, 1921, 2041, 3607, 2047, 1302, 4030, 6363, 2036], "orig_top_k_doc_id": [58, 57, 55, 56, 3606, 1301, 2047, 1302, 4030, 1921, 3608, 3607, 2041, 6363, 2036]}, {"qid": 51, "question": "Did they experiment on this dataset? in Citation Data of Czech Apex Courts", "answer": ["No", "Yes"], "top_k_doc_id": [1301, 3606, 3608, 55, 56, 57, 58, 1921, 1302, 3609, 6363, 1922, 2852, 3611, 6971], "orig_top_k_doc_id": [58, 57, 55, 56, 1302, 3609, 6363, 3608, 1921, 1922, 2852, 3606, 3611, 6971, 1301]}, {"qid": 2281, "question": "What are the citation intent labels in the datasets? in Structural Scaffolds for Citation Intent Classification in Scientific Publications", "answer": ["Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset."], "top_k_doc_id": [1301, 3606, 3608, 303, 2036, 2041, 3607, 3609, 3610, 3611, 302, 1925, 1924, 1922, 1921], "orig_top_k_doc_id": [3606, 3611, 3610, 3608, 3609, 3607, 1301, 2036, 2041, 302, 1925, 1924, 303, 1922, 1921]}, {"qid": 2282, "question": "What is the size of ACL-ARC datasets? in Structural Scaffolds for Citation Intent Classification in Scientific Publications", "answer": ["includes 1,941 citation instances from 186 papers"], "top_k_doc_id": [1301, 3606, 3608, 303, 2036, 2041, 3607, 3609, 3610, 3611, 3582, 2047, 2043, 1302, 5898], "orig_top_k_doc_id": [3610, 3606, 3611, 3609, 3608, 3607, 1301, 2036, 2041, 3582, 2047, 2043, 1302, 303, 5898]}]}
{"group_id": 350, "group_size": 5, "items": [{"qid": 115, "question": "What do you use to calculate word/sub-word embeddings in Important Attribute Identification in Knowledge Graph", "answer": ["FastText"], "top_k_doc_id": [130, 131, 132, 133, 134, 853, 2359, 4550, 1822, 3354, 3634, 339, 3313, 3355, 788], "orig_top_k_doc_id": [133, 131, 134, 132, 1822, 2359, 3354, 853, 3634, 339, 3313, 3355, 4550, 130, 788]}, {"qid": 116, "question": "What user generated text data do you use? in Important Attribute Identification in Knowledge Graph", "answer": ["No"], "top_k_doc_id": [130, 131, 132, 133, 134, 853, 2359, 4550, 4441, 145, 558, 4442, 4444, 3017, 1169], "orig_top_k_doc_id": [131, 132, 853, 4550, 133, 4441, 134, 145, 558, 2359, 4442, 4444, 130, 3017, 1169]}, {"qid": 114, "question": "What are the traditional methods to identifying important attributes? in Important Attribute Identification in Knowledge Graph", "answer": ["automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm", "TextRank, Word2vec BIBREF19, GloVe BIBREF20"], "top_k_doc_id": [130, 131, 132, 133, 134, 853, 3634, 337, 6603, 338, 1958, 4904, 3017, 7334, 1822], "orig_top_k_doc_id": [131, 134, 133, 853, 130, 3634, 337, 132, 6603, 338, 1958, 4904, 3017, 7334, 1822]}, {"qid": 4443, "question": "What were the baseline methods? in Semantic Product Search", "answer": ["DSSM, Match Pyramid, ARC-II, our model with frozen, randomly initialized embeddings", "DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 "], "top_k_doc_id": [130, 131, 132, 133, 3020, 4169, 6980, 6981, 7126, 7818, 4170, 6355, 6982, 791, 4382], "orig_top_k_doc_id": [133, 4169, 6981, 6980, 131, 4170, 6355, 7126, 3020, 130, 6982, 132, 791, 7818, 4382]}, {"qid": 4444, "question": "What dataset is used for training? in Semantic Product Search", "answer": ["a self-collected dataset of 11 months of search logs as query-product pairs", "11 months of search logs"], "top_k_doc_id": [130, 131, 132, 133, 3020, 4169, 6980, 6981, 7126, 7818, 2703, 7664, 2986, 118, 2337], "orig_top_k_doc_id": [6980, 131, 7126, 130, 2703, 133, 132, 4169, 6981, 7664, 7818, 3020, 2986, 118, 2337]}]}
{"group_id": 351, "group_size": 5, "items": [{"qid": 147, "question": "What datasets are used to evaluate this approach? in Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "answer": [" Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ", "WN18 and YAGO3-10"], "top_k_doc_id": [1601, 4207, 178, 179, 180, 181, 182, 183, 4205, 1894, 1896, 254, 4492, 4202, 4208], "orig_top_k_doc_id": [182, 178, 180, 183, 181, 4205, 179, 4207, 1894, 254, 1601, 1896, 4492, 4202, 4208]}, {"qid": 149, "question": "Can this adversarial approach be used to directly improve model accuracy? in Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "answer": ["Yes"], "top_k_doc_id": [1601, 4207, 178, 179, 180, 181, 182, 183, 4205, 1894, 1896, 4206, 1600, 1597, 1897], "orig_top_k_doc_id": [182, 178, 183, 180, 4205, 181, 1601, 179, 4207, 1894, 4206, 1600, 1597, 1896, 1897]}, {"qid": 148, "question": "How is this approach used to detect incorrect facts? in Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "answer": ["if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. "], "top_k_doc_id": [1601, 4207, 178, 179, 180, 181, 182, 183, 4205, 4206, 2426, 2193, 4638, 5664, 5351], "orig_top_k_doc_id": [182, 178, 181, 180, 183, 4205, 179, 4206, 2426, 1601, 2193, 4638, 5664, 5351, 4207]}, {"qid": 1186, "question": "Which dataset do they use? in Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation", "answer": ["Stanford Sentiment Treebank (SST) BIBREF15 and AG News BIBREF16"], "top_k_doc_id": [1601, 4207, 1597, 1599, 1600, 1598, 6820, 3957, 5543, 5247, 2243, 6510, 1602, 5709, 96], "orig_top_k_doc_id": [1601, 1599, 1597, 1598, 1600, 6820, 4207, 3957, 5543, 5247, 2243, 6510, 1602, 5709, 96]}, {"qid": 1793, "question": "How does their perturbation algorihm work? in On the Robustness of Projection Neural Networks For Efficient Text Representation: An Empirical Study", "answer": ["same sentences after applying character level perturbations"], "top_k_doc_id": [1601, 4207, 1597, 1599, 1600, 2617, 2616, 2615, 1887, 626, 3566, 4680, 3562, 2494, 627], "orig_top_k_doc_id": [2617, 1601, 2616, 1597, 2615, 1887, 1600, 626, 3566, 4680, 1599, 3562, 4207, 2494, 627]}]}
{"group_id": 352, "group_size": 5, "items": [{"qid": 180, "question": "Does the paper report the results of previous models applied to the same tasks? in Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian", "answer": ["Yes", "No"], "top_k_doc_id": [234, 237, 239, 235, 236, 238, 240, 241, 242, 330, 808, 7675, 2409, 369, 334], "orig_top_k_doc_id": [234, 242, 235, 238, 236, 240, 237, 239, 241, 808, 330, 2409, 369, 7675, 334]}, {"qid": 181, "question": "How is the quality of the discussion evaluated? in Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian", "answer": ["No"], "top_k_doc_id": [234, 237, 239, 235, 236, 238, 240, 241, 242, 330, 808, 2410, 7675, 4577, 7745], "orig_top_k_doc_id": [234, 242, 235, 236, 240, 238, 239, 237, 241, 2410, 7675, 808, 4577, 330, 7745]}, {"qid": 182, "question": "What is the technique used for text analysis and mining? in Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian", "answer": ["No"], "top_k_doc_id": [234, 237, 239, 235, 236, 238, 240, 241, 242, 330, 808, 2410, 2409, 5373, 597], "orig_top_k_doc_id": [234, 242, 236, 235, 238, 240, 239, 237, 241, 2409, 330, 2410, 5373, 597, 808]}, {"qid": 183, "question": "What are the causal mapping methods employed? in Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian", "answer": ["Axelrod's causal mapping method"], "top_k_doc_id": [234, 237, 239, 235, 236, 238, 240, 241, 242, 330, 808, 7675, 3162, 1282, 3161], "orig_top_k_doc_id": [234, 236, 242, 238, 235, 237, 240, 239, 241, 3162, 808, 330, 1282, 3161, 7675]}, {"qid": 2396, "question": "How large language sets are able to be explored using this approach? in An efficient automated data analytics approach to large scale computational comparative linguistics", "answer": ["No"], "top_k_doc_id": [234, 237, 239, 1711, 554, 7114, 6140, 2439, 2435, 2287, 3023, 3080, 7313, 3860, 1556], "orig_top_k_doc_id": [1711, 554, 7114, 6140, 2439, 2435, 234, 2287, 3023, 3080, 239, 237, 7313, 3860, 1556]}]}
{"group_id": 353, "group_size": 5, "items": [{"qid": 213, "question": "What dataset do they use? in SUM-QE: a BERT-based Summary Quality Estimation Model", "answer": ["datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks"], "top_k_doc_id": [265, 266, 3723, 247, 248, 264, 7108, 7109, 4383, 5804, 4510, 5805, 5806, 5116, 1971], "orig_top_k_doc_id": [264, 265, 266, 247, 7108, 3723, 248, 5804, 5805, 7109, 4383, 5806, 4510, 5116, 1971]}, {"qid": 214, "question": "What simpler models do they look at? in SUM-QE: a BERT-based Summary Quality Estimation Model", "answer": ["BiGRU s with attention, ROUGE, Language model (LM), Next sentence prediction", "BiGRUs with attention, ROUGE, Language model, and next sentence prediction "], "top_k_doc_id": [265, 266, 3723, 247, 248, 264, 7108, 7109, 4383, 5804, 4510, 5805, 5806, 39, 6957], "orig_top_k_doc_id": [264, 265, 266, 247, 7108, 3723, 5804, 248, 5805, 7109, 39, 5806, 6957, 4510, 4383]}, {"qid": 212, "question": "What are their correlation results? in SUM-QE: a BERT-based Summary Quality Estimation Model", "answer": ["High correlation results range from 0.472 to 0.936"], "top_k_doc_id": [265, 266, 3723, 247, 248, 264, 7108, 7109, 4383, 5804, 1971, 5151, 5152, 5712, 5711], "orig_top_k_doc_id": [264, 265, 266, 247, 248, 3723, 7108, 1971, 7109, 5151, 4383, 5152, 5804, 5712, 5711]}, {"qid": 215, "question": "What linguistic quality aspects are addressed? in SUM-QE: a BERT-based Summary Quality Estimation Model", "answer": ["Grammaticality, non-redundancy, referential clarity, focus, structure & coherence"], "top_k_doc_id": [265, 266, 3723, 247, 248, 264, 7108, 7109, 6862, 5152, 59, 7048, 5805, 6924, 1649], "orig_top_k_doc_id": [264, 265, 266, 7108, 247, 3723, 248, 6862, 7109, 5152, 59, 7048, 5805, 6924, 1649]}, {"qid": 5005, "question": "Which metrics are used for evaluating the quality? in BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model", "answer": ["BLEU, perplexity,  self-BLEU, percentage of $n$ -grams that are unique", "BLEU,  perplexity"], "top_k_doc_id": [265, 266, 3723, 7791, 3968, 2440, 137, 7002, 1971, 3129, 2438, 7818, 2439, 7299, 7250], "orig_top_k_doc_id": [7791, 3968, 2440, 137, 7002, 266, 265, 1971, 3129, 2438, 7818, 2439, 3723, 7299, 7250]}]}
{"group_id": 354, "group_size": 5, "items": [{"qid": 228, "question": "Is text-to-image synthesis trained is suppervized or unsuppervized manner? in A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis", "answer": ["unsupervised ", "Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis"], "top_k_doc_id": [277, 278, 279, 280, 281, 287, 276, 283, 285, 7145, 7146, 282, 284, 286, 7138], "orig_top_k_doc_id": [276, 277, 280, 287, 279, 281, 278, 283, 285, 7145, 286, 7146, 7138, 282, 284]}, {"qid": 229, "question": "What challenges remain unresolved? in A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis", "answer": ["give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images"], "top_k_doc_id": [277, 278, 279, 280, 281, 287, 276, 283, 285, 7145, 7146, 491, 4166, 5982, 844], "orig_top_k_doc_id": [280, 277, 276, 287, 279, 278, 281, 283, 285, 4166, 5982, 844, 491, 7145, 7146]}, {"qid": 230, "question": "What is the conclusion of comparison of proposed solution? in A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis", "answer": ["HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset, In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, text to image synthesis is continuously improving the results for better visual perception and interception"], "top_k_doc_id": [277, 278, 279, 280, 281, 287, 276, 283, 285, 7145, 7146, 491, 4166, 7147, 4199], "orig_top_k_doc_id": [276, 277, 280, 279, 287, 278, 281, 283, 491, 285, 7147, 4166, 4199, 7145, 7146]}, {"qid": 231, "question": "What is typical GAN architecture for each text-to-image synhesis group? in A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis", "answer": ["Semantic Enhancement GANs: DC-GANs, MC-GAN\nResolution Enhancement GANs: StackGANs, AttnGAN, HDGAN\nDiversity Enhancement GANs: AC-GAN, TAC-GAN etc.\nMotion Enhancement GAGs: T2S, T2V, StoryGAN"], "top_k_doc_id": [277, 278, 279, 280, 281, 287, 276, 283, 285, 7145, 7146, 282, 284, 286, 6686], "orig_top_k_doc_id": [280, 277, 276, 287, 279, 281, 278, 285, 283, 7145, 7146, 282, 284, 286, 6686]}, {"qid": 3283, "question": "What objective function is used in the GAN? in Creative GANs for generating poems, lyrics, and metaphors", "answer": ["language modeling objective"], "top_k_doc_id": [277, 278, 279, 280, 281, 287, 5504, 3328, 89, 3329, 5442, 5681, 567, 90, 282], "orig_top_k_doc_id": [5504, 279, 278, 280, 3328, 89, 287, 3329, 5442, 5681, 281, 277, 567, 90, 282]}]}
{"group_id": 355, "group_size": 5, "items": [{"qid": 248, "question": "Did the authors use crowdsourcing platforms? in Talk the Walk: Navigating New York City through Grounded Dialogue", "answer": ["Yes", "Yes"], "top_k_doc_id": [312, 313, 317, 318, 319, 7222, 7758, 1070, 1071, 3110, 6298, 314, 3451, 2392, 6805], "orig_top_k_doc_id": [312, 313, 318, 1070, 317, 1071, 319, 7758, 7222, 3110, 6298, 3451, 2392, 6805, 314]}, {"qid": 252, "question": "What data did they use? in Talk the Walk: Navigating New York City through Grounded Dialogue", "answer": [" dataset on Mechanical Turk involving human perception, action and communication"], "top_k_doc_id": [312, 313, 317, 318, 319, 7222, 7758, 1070, 1071, 3110, 6298, 314, 4901, 824, 7226], "orig_top_k_doc_id": [312, 313, 318, 317, 319, 1070, 7222, 1071, 6298, 3110, 7758, 4901, 824, 314, 7226]}, {"qid": 249, "question": "How was the dataset collected? in Talk the Walk: Navigating New York City through Grounded Dialogue", "answer": ["crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)"], "top_k_doc_id": [312, 313, 317, 318, 319, 7222, 7758, 1070, 1071, 3110, 6298, 314, 494, 1444, 7379], "orig_top_k_doc_id": [312, 313, 318, 317, 1070, 319, 1071, 7758, 6298, 314, 7222, 494, 1444, 3110, 7379]}, {"qid": 251, "question": "What evaluation metrics did the authors look at? in Talk the Walk: Navigating New York City through Grounded Dialogue", "answer": ["localization accuracy"], "top_k_doc_id": [312, 313, 317, 318, 319, 7222, 7758, 1070, 1071, 3110, 6298, 4441, 824, 6590, 2392], "orig_top_k_doc_id": [312, 313, 318, 319, 317, 4441, 824, 7222, 3110, 6590, 7758, 6298, 1071, 1070, 2392]}, {"qid": 250, "question": "What language do the agents talk in? in Talk the Walk: Navigating New York City through Grounded Dialogue", "answer": ["English"], "top_k_doc_id": [312, 313, 317, 318, 319, 7222, 7758, 314, 6586, 5428, 7226, 7299, 7225, 3451, 5425], "orig_top_k_doc_id": [312, 313, 318, 317, 314, 319, 7758, 6586, 5428, 7226, 7222, 7299, 7225, 3451, 5425]}]}
{"group_id": 356, "group_size": 5, "items": [{"qid": 258, "question": "What is the baseline? in RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension", "answer": [" path ranking-based KGC (PRKGC)"], "top_k_doc_id": [325, 326, 327, 328, 329, 2752, 4752, 6932, 7459, 7589, 7590, 2096, 4637, 5473, 2759], "orig_top_k_doc_id": [325, 329, 328, 326, 327, 7590, 7589, 2752, 4752, 7459, 6932, 5473, 4637, 2759, 2096]}, {"qid": 261, "question": "How was the dataset annotated? in RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension", "answer": ["True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable), why they are unsure from two choices (\u201cNot stated in the article\u201d or \u201cOther\u201d), The \u201csummary\u201d text boxes"], "top_k_doc_id": [325, 326, 327, 328, 329, 2752, 4752, 6932, 7459, 7589, 7590, 2096, 4637, 5473, 2264], "orig_top_k_doc_id": [325, 329, 326, 328, 7590, 7589, 327, 4752, 2264, 2752, 6932, 2096, 4637, 5473, 7459]}, {"qid": 259, "question": "What dataset was used in the experiment? in RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension", "answer": ["WikiHop"], "top_k_doc_id": [325, 326, 327, 328, 329, 2752, 4752, 6932, 7459, 7589, 7590, 2096, 2661, 4640, 2759], "orig_top_k_doc_id": [325, 329, 328, 326, 7590, 7589, 327, 2752, 6932, 7459, 4752, 2096, 2661, 4640, 2759]}, {"qid": 262, "question": "What is the source of the proposed dataset? in RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension", "answer": ["No"], "top_k_doc_id": [325, 326, 327, 328, 329, 2752, 4752, 6932, 7459, 7589, 7590, 968, 2264, 2759, 1141], "orig_top_k_doc_id": [325, 329, 328, 326, 7590, 7589, 6932, 4752, 7459, 2752, 968, 327, 2264, 2759, 1141]}, {"qid": 260, "question": "Did they use any crowdsourcing platform? in RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension", "answer": ["Yes", "Yes"], "top_k_doc_id": [325, 326, 327, 328, 329, 2752, 4752, 6932, 7459, 7589, 7590, 4074, 4075, 5519, 5473], "orig_top_k_doc_id": [325, 329, 326, 328, 7590, 7589, 7459, 4074, 4075, 327, 2752, 4752, 5519, 5473, 6932]}]}
{"group_id": 357, "group_size": 5, "items": [{"qid": 263, "question": "How many label options are there in the multi-label task? in Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds", "answer": [" two labels "], "top_k_doc_id": [330, 336, 5874, 7411, 331, 452, 332, 5651, 335, 5728, 390, 5038, 5727, 447, 1711], "orig_top_k_doc_id": [330, 336, 332, 331, 335, 5874, 452, 5728, 7411, 390, 5038, 5651, 5727, 447, 1711]}, {"qid": 265, "question": "Who are the experts? in Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds", "answer": ["political pundits of the Washington Post", "the experts in the field"], "top_k_doc_id": [330, 336, 5874, 7411, 331, 452, 1754, 5873, 5876, 3623, 332, 5060, 334, 5037, 7413], "orig_top_k_doc_id": [330, 336, 331, 3623, 452, 5874, 7411, 332, 5873, 5060, 5876, 334, 1754, 5037, 7413]}, {"qid": 266, "question": "Who is the crowd in these experiments? in Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds", "answer": [" peoples' sentiments expressed over social media"], "top_k_doc_id": [330, 336, 5874, 7411, 331, 452, 1754, 5873, 5876, 5115, 5112, 5651, 1958, 5728, 3225], "orig_top_k_doc_id": [330, 336, 5874, 452, 1754, 5115, 5112, 7411, 5651, 331, 1958, 5728, 5876, 3225, 5873]}, {"qid": 267, "question": "How do you establish the ground truth of who won a debate? in Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds", "answer": ["experts in Washington Post"], "top_k_doc_id": [330, 336, 5874, 7411, 331, 452, 332, 5651, 334, 2661, 333, 5114, 4940, 1272, 2405], "orig_top_k_doc_id": [330, 336, 331, 332, 334, 5874, 2661, 333, 7411, 5114, 4940, 1272, 5651, 2405, 452]}, {"qid": 264, "question": "What is the interannotator agreement of the crowd sourced users? in Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds", "answer": ["No"], "top_k_doc_id": [330, 336, 5874, 7411, 5727, 1754, 5728, 6014, 1958, 1755, 5873, 5729, 7789, 679, 5112], "orig_top_k_doc_id": [330, 336, 5874, 5727, 1754, 5728, 6014, 1958, 1755, 5873, 5729, 7789, 679, 7411, 5112]}]}
{"group_id": 358, "group_size": 5, "items": [{"qid": 295, "question": "What word embeddings were used? in Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models", "answer": ["Kyubyong Park, Edouard Grave et al BIBREF11"], "top_k_doc_id": [357, 3430, 358, 359, 930, 2449, 4575, 5316, 6151, 7056, 7759, 5317, 245, 4756, 7100], "orig_top_k_doc_id": [358, 357, 5316, 930, 2449, 6151, 3430, 5317, 245, 359, 4575, 7759, 4756, 7100, 7056]}, {"qid": 296, "question": "What type of errors were produced by the BLSTM-CNN-CRF system? in Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models", "answer": ["No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag"], "top_k_doc_id": [357, 3430, 358, 359, 930, 2449, 4575, 5316, 6151, 7056, 7759, 6534, 7420, 4945, 6422], "orig_top_k_doc_id": [357, 358, 359, 3430, 2449, 4575, 7759, 930, 6534, 7420, 7056, 4945, 6151, 5316, 6422]}, {"qid": 297, "question": "How much better was the BLSTM-CNN-CRF than the BLSTM-CRF? in Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models", "answer": ["Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF "], "top_k_doc_id": [357, 3430, 358, 359, 930, 2449, 2256, 2349, 6534, 7420, 4374, 4945, 4296, 4375, 5083], "orig_top_k_doc_id": [357, 358, 2256, 2349, 6534, 7420, 3430, 4374, 4945, 4296, 359, 2449, 930, 4375, 5083]}, {"qid": 1802, "question": "Do they use an NER system in their pipeline? in Vietnamese Open Information Extraction", "answer": ["No"], "top_k_doc_id": [357, 3430, 358, 359, 2959, 1422, 6676, 5818, 360, 2956, 295, 5837, 2965, 3344, 7250], "orig_top_k_doc_id": [3430, 359, 2959, 1422, 6676, 5818, 360, 2956, 295, 358, 5837, 2965, 357, 3344, 7250]}, {"qid": 2204, "question": "What datasets do they use for the tasks? in NNVLP: A Neural Network-Based Vietnamese Language Processing Toolkit", "answer": [" Viet Treebank corpus for POS tagging and chunking tasks, and on VLSP shared task 2016 corpus for NER task"], "top_k_doc_id": [357, 3430, 5319, 2965, 5316, 5318, 3249, 5317, 1795, 4424, 2966, 7355, 2957, 3197, 7356], "orig_top_k_doc_id": [3430, 5319, 2965, 5316, 5318, 357, 3249, 5317, 1795, 4424, 2966, 7355, 2957, 3197, 7356]}]}
{"group_id": 359, "group_size": 5, "items": [{"qid": 301, "question": "What is a strong feature-based method? in Recurrent Neural Network Encoder with Attention for Community Question Answering", "answer": ["No"], "top_k_doc_id": [361, 3175, 4841, 1154, 2752, 2910, 4258, 5736, 7351, 7148, 6847, 364, 3026, 5737, 3487], "orig_top_k_doc_id": [361, 3175, 7148, 6847, 2910, 2752, 364, 1154, 3026, 4258, 5737, 7351, 5736, 3487, 4841]}, {"qid": 302, "question": "Did they experimnet in other languages? in Recurrent Neural Network Encoder with Attention for Community Question Answering", "answer": ["Yes"], "top_k_doc_id": [361, 3175, 4841, 1154, 2752, 2910, 4258, 5736, 7351, 4590, 112, 2917, 510, 2915, 2136], "orig_top_k_doc_id": [361, 7351, 4590, 1154, 4258, 2752, 2910, 3175, 4841, 112, 2917, 510, 2915, 5736, 2136]}, {"qid": 298, "question": "What supplemental tasks are used for multitask learning? in Recurrent Neural Network Encoder with Attention for Community Question Answering", "answer": ["Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question"], "top_k_doc_id": [361, 3175, 4841, 1154, 2752, 2910, 4258, 4878, 4879, 2915, 6510, 4561, 2528, 2414, 2048], "orig_top_k_doc_id": [4878, 2910, 4879, 361, 2915, 6510, 3175, 4258, 4561, 2528, 2752, 2414, 1154, 4841, 2048]}, {"qid": 299, "question": "Is the improvement actually coming from using an RNN? in Recurrent Neural Network Encoder with Attention for Community Question Answering", "answer": ["No"], "top_k_doc_id": [361, 3175, 4841, 112, 3358, 7351, 709, 5367, 2096, 7148, 6847, 4258, 2414, 3823, 3177], "orig_top_k_doc_id": [361, 3175, 709, 112, 5367, 2096, 4841, 7148, 7351, 3358, 6847, 4258, 2414, 3823, 3177]}, {"qid": 300, "question": "How much performance gap between their approach and the strong handcrafted method? in Recurrent Neural Network Encoder with Attention for Community Question Answering", "answer": ["0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C"], "top_k_doc_id": [361, 3175, 4841, 112, 3358, 7351, 5736, 2205, 4273, 1154, 3920, 2253, 3338, 3487, 1141], "orig_top_k_doc_id": [361, 5736, 7351, 2205, 3175, 4273, 1154, 3920, 4841, 2253, 3338, 3358, 112, 3487, 1141]}]}
{"group_id": 360, "group_size": 5, "items": [{"qid": 314, "question": "What conclusions are drawn from the syntactic analysis? in Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection", "answer": [" our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them"], "top_k_doc_id": [377, 378, 379, 768, 6863, 7060, 7266, 5240, 6864, 7520, 7064, 5909, 5600, 1244, 2040], "orig_top_k_doc_id": [377, 379, 7064, 378, 768, 5240, 7060, 5909, 6864, 7520, 5600, 7266, 1244, 2040, 6863]}, {"qid": 315, "question": "What type of syntactic analysis is performed? in Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection", "answer": ["Speaker's Gender Effects, Interlocutors' Gender and Number Effects"], "top_k_doc_id": [377, 378, 379, 768, 6863, 7060, 7266, 5240, 6864, 7520, 5764, 703, 6041, 5241, 4766], "orig_top_k_doc_id": [377, 379, 5240, 7520, 378, 768, 7060, 6864, 5764, 703, 7266, 6863, 6041, 5241, 4766]}, {"qid": 317, "question": "Which neural machine translation system is used? in Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection", "answer": ["Google's machine translation system (GMT)"], "top_k_doc_id": [377, 378, 379, 768, 6863, 7060, 7266, 5240, 6864, 3821, 6041, 1303, 6042, 7059, 5669], "orig_top_k_doc_id": [377, 379, 7266, 7060, 3821, 5240, 378, 6864, 768, 6863, 6041, 1303, 6042, 7059, 5669]}, {"qid": 316, "question": "How is it demonstrated that the correct gender and number information is injected using this system? in Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection", "answer": [" correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline, Finally, the \u201cShe said\u201d prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference"], "top_k_doc_id": [377, 378, 379, 768, 6863, 7060, 7266, 6042, 1124, 6589, 7520, 6558, 703, 6041, 697], "orig_top_k_doc_id": [379, 377, 378, 7266, 7060, 6042, 1124, 6589, 6863, 7520, 6558, 703, 768, 6041, 697]}, {"qid": 318, "question": "What are the components of the black-box context injection system? in Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection", "answer": ["supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences"], "top_k_doc_id": [377, 378, 379, 768, 6863, 6864, 5240, 7520, 5558, 3956, 5242, 2386, 4203, 4199, 5241], "orig_top_k_doc_id": [377, 379, 6863, 6864, 5240, 768, 7520, 5558, 3956, 5242, 2386, 4203, 4199, 378, 5241]}]}
{"group_id": 361, "group_size": 5, "items": [{"qid": 541, "question": "Is the model evaluated against any baseline? in How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages", "answer": ["No"], "top_k_doc_id": [661, 4184, 4875, 2972, 2948, 6960, 2075, 6484, 6834, 7669, 5564, 3651, 633, 3603, 6371], "orig_top_k_doc_id": [4875, 6960, 661, 2972, 2948, 4184, 7669, 5564, 3651, 6484, 6834, 2075, 633, 3603, 6371]}, {"qid": 543, "question": "How is the performance of the model evaluated? in How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages", "answer": ["The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8., For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). , Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models."], "top_k_doc_id": [661, 4184, 4875, 2972, 2948, 6960, 2075, 6484, 6834, 7669, 3749, 632, 3750, 7825, 4908], "orig_top_k_doc_id": [4875, 661, 6960, 4184, 2948, 2972, 6484, 6834, 3749, 2075, 7669, 632, 3750, 7825, 4908]}, {"qid": 542, "question": "Does the paper report the accuracy of the model? in How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages", "answer": ["No"], "top_k_doc_id": [661, 4184, 4875, 2972, 2948, 6960, 632, 633, 2786, 1053, 2317, 1927, 7267, 1015, 3603], "orig_top_k_doc_id": [4875, 661, 632, 4184, 633, 6960, 2948, 2972, 2786, 1053, 2317, 1927, 7267, 1015, 3603]}, {"qid": 544, "question": "What are the different bilingual models employed? in How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages", "answer": [" Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target"], "top_k_doc_id": [661, 4184, 4875, 2972, 3749, 3748, 5026, 633, 632, 3750, 1044, 1053, 84, 5841, 7828], "orig_top_k_doc_id": [661, 4875, 3749, 3748, 5026, 633, 632, 3750, 1044, 1053, 2972, 84, 4184, 5841, 7828]}, {"qid": 545, "question": "How does the well-resourced language impact the quality of the output? in How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages", "answer": ["Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved."], "top_k_doc_id": [661, 4184, 4875, 45, 1049, 4695, 5716, 3250, 2948, 994, 6960, 50, 5845, 3651, 7190], "orig_top_k_doc_id": [661, 4875, 45, 1049, 4695, 5716, 3250, 2948, 4184, 994, 6960, 50, 5845, 3651, 7190]}]}
{"group_id": 362, "group_size": 5, "items": [{"qid": 578, "question": "What is the difference in recall score between the systems? in Detecting Potential Topics In News Using BERT, CRF and Wikipedia", "answer": ["Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference."], "top_k_doc_id": [105, 714, 715, 5611, 4575, 4612, 106, 2825, 4613, 4614, 5099, 4833, 6017, 5948, 4131], "orig_top_k_doc_id": [715, 714, 4612, 5611, 4614, 105, 5099, 4613, 4575, 4833, 2825, 106, 6017, 5948, 4131]}, {"qid": 579, "question": "What is their f1 score and recall? in Detecting Potential Topics In News Using BERT, CRF and Wikipedia", "answer": ["F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference."], "top_k_doc_id": [105, 714, 715, 5611, 4575, 4612, 106, 2825, 4613, 4614, 3446, 5947, 6924, 438, 7837], "orig_top_k_doc_id": [714, 715, 5611, 105, 4612, 2825, 106, 4614, 4575, 4613, 3446, 5947, 6924, 438, 7837]}, {"qid": 580, "question": "How many layers does their system have? in Detecting Potential Topics In News Using BERT, CRF and Wikipedia", "answer": ["4 layers"], "top_k_doc_id": [105, 714, 715, 5611, 4575, 4612, 3736, 4833, 6272, 7287, 2794, 6268, 7002, 5947, 3446], "orig_top_k_doc_id": [715, 714, 5611, 3736, 105, 4612, 4833, 6272, 7287, 2794, 6268, 7002, 5947, 3446, 4575]}, {"qid": 581, "question": "Which news corpus is used? in Detecting Potential Topics In News Using BERT, CRF and Wikipedia", "answer": ["No"], "top_k_doc_id": [105, 714, 715, 5611, 4742, 5099, 6016, 7289, 7499, 4574, 6268, 4741, 1077, 7285, 7287], "orig_top_k_doc_id": [714, 715, 5611, 105, 7289, 5099, 4574, 6016, 4742, 6268, 4741, 1077, 7285, 7499, 7287]}, {"qid": 582, "question": "How large is the dataset they used? in Detecting Potential Topics In News Using BERT, CRF and Wikipedia", "answer": ["English wikipedia dataset has more than 18 million, a dump of 15 million English news articles "], "top_k_doc_id": [105, 714, 715, 5611, 4742, 5099, 6016, 7289, 7499, 4833, 3467, 3736, 4612, 3784, 4131], "orig_top_k_doc_id": [714, 715, 105, 5611, 5099, 6016, 4833, 3467, 3736, 7499, 4612, 3784, 4742, 4131, 7289]}]}
{"group_id": 363, "group_size": 5, "items": [{"qid": 592, "question": "Which inter-annotator metric do they use? in Shallow Discourse Annotation for Chinese TED Talks", "answer": ["agreement rates, Kappa value"], "top_k_doc_id": [736, 737, 738, 739, 741, 1349, 1352, 7277, 1350, 740, 4345, 5387, 5390, 2393, 2392], "orig_top_k_doc_id": [736, 739, 737, 741, 1349, 738, 1352, 740, 4345, 1350, 7277, 5387, 5390, 2393, 2392]}, {"qid": 594, "question": "How are resources adapted to properties of Chinese text? in Shallow Discourse Annotation for Chinese TED Talks", "answer": ["removing AltLexC and adding Progression into our sense hierarchy"], "top_k_doc_id": [736, 737, 738, 739, 741, 1349, 1352, 7277, 1350, 6540, 6098, 7658, 1906, 6542, 7298], "orig_top_k_doc_id": [736, 739, 741, 737, 738, 1349, 1352, 7277, 6540, 1350, 6098, 7658, 1906, 6542, 7298]}, {"qid": 591, "question": "Do they build a model to recognize discourse relations on their dataset? in Shallow Discourse Annotation for Chinese TED Talks", "answer": ["No"], "top_k_doc_id": [736, 737, 738, 739, 741, 1349, 1352, 7277, 1920, 740, 1907, 4781, 1906, 2422, 4188], "orig_top_k_doc_id": [736, 741, 739, 737, 738, 7277, 1349, 1920, 740, 1907, 4781, 1906, 2422, 1352, 4188]}, {"qid": 593, "question": "How high is the inter-annotator agreement? in Shallow Discourse Annotation for Chinese TED Talks", "answer": ["agreement of 0.85 and Kappa value of 0.83"], "top_k_doc_id": [736, 737, 738, 739, 741, 1349, 740, 4345, 862, 3895, 5705, 6652, 7748, 6808, 5382], "orig_top_k_doc_id": [736, 739, 737, 741, 1349, 740, 738, 4345, 862, 3895, 5705, 6652, 7748, 6808, 5382]}, {"qid": 4661, "question": "Do they attempt to jointly learn connectives, arguments, senses and non-explicit identiifers end-to-end? in Shallow Discourse Parsing with Maximum Entropy Model", "answer": ["No", "No"], "top_k_doc_id": [736, 737, 738, 739, 741, 7277, 7278, 740, 4781, 4780, 7023, 4634, 3162, 4782, 3161], "orig_top_k_doc_id": [7277, 737, 736, 7278, 740, 4781, 741, 4780, 738, 7023, 739, 4634, 3162, 4782, 3161]}]}
{"group_id": 364, "group_size": 5, "items": [{"qid": 740, "question": "Do they report results only on English data? in Learning Twitter User Sentiments on Climate Change with Limited Labeled Data", "answer": ["Yes"], "top_k_doc_id": [330, 926, 927, 928, 1757, 3311, 331, 332, 3734, 7750, 6971, 7751, 6285, 5059, 5570], "orig_top_k_doc_id": [926, 927, 928, 3311, 7750, 6971, 330, 332, 331, 3734, 7751, 6285, 1757, 5059, 5570]}, {"qid": 741, "question": "Do the authors mention any confounds to their study? in Learning Twitter User Sentiments on Climate Change with Limited Labeled Data", "answer": ["No"], "top_k_doc_id": [330, 926, 927, 928, 1757, 3311, 331, 332, 235, 3731, 6285, 5466, 3730, 6897, 6402], "orig_top_k_doc_id": [926, 927, 928, 331, 332, 5466, 3311, 6285, 3730, 6897, 3731, 6402, 1757, 330, 235]}, {"qid": 742, "question": "Which machine learning models are used? in Learning Twitter User Sentiments on Climate Change with Limited Labeled Data", "answer": ["RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel"], "top_k_doc_id": [330, 926, 927, 928, 1757, 3311, 331, 332, 235, 3731, 6285, 3734, 3735, 6401, 7750], "orig_top_k_doc_id": [926, 927, 928, 330, 332, 3734, 3731, 1757, 6285, 3735, 331, 6401, 235, 3311, 7750]}, {"qid": 744, "question": "Which five natural disasters were examined? in Learning Twitter User Sentiments on Climate Change with Limited Labeled Data", "answer": ["the East Coast Bomb Cyclone,  the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, the California Camp Fires"], "top_k_doc_id": [330, 926, 927, 928, 1757, 3311, 331, 332, 3734, 7750, 4322, 6805, 235, 1173, 5879], "orig_top_k_doc_id": [926, 927, 928, 4322, 330, 6805, 3311, 7750, 332, 235, 1757, 1173, 5879, 331, 3734]}, {"qid": 743, "question": "What methodology is used to compensate for limited labelled data? in Learning Twitter User Sentiments on Climate Change with Limited Labeled Data", "answer": ["Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets."], "top_k_doc_id": [330, 926, 927, 928, 1757, 3311, 7750, 7308, 6285, 5060, 7307, 5059, 235, 2074, 241], "orig_top_k_doc_id": [926, 927, 928, 7750, 7308, 3311, 6285, 5060, 330, 7307, 5059, 235, 2074, 241, 1757]}]}
{"group_id": 365, "group_size": 5, "items": [{"qid": 783, "question": "What syntactic structure is used to model tones? in A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading", "answer": ["syllables"], "top_k_doc_id": [973, 974, 975, 2315, 976, 2839, 3024, 2316, 2317, 2906, 2907, 3462, 861, 3939, 7054], "orig_top_k_doc_id": [973, 974, 976, 975, 2315, 2317, 2906, 2316, 2907, 2839, 3024, 3462, 861, 3939, 7054]}, {"qid": 784, "question": "What visual information characterizes tones? in A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading", "answer": ["video sequence is first fed into the VGG model BIBREF9 to extract visual feature"], "top_k_doc_id": [973, 974, 975, 2315, 976, 2839, 3024, 2316, 2317, 3655, 594, 3656, 3459, 3658, 3657], "orig_top_k_doc_id": [973, 974, 975, 976, 2315, 2317, 2316, 2839, 3655, 594, 3656, 3459, 3658, 3657, 3024]}, {"qid": 782, "question": "What was the previous state of the art model for this task? in A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading", "answer": ["WAS, LipCH-Net-seq, CSSMCM-w/o video"], "top_k_doc_id": [973, 974, 975, 2315, 976, 2839, 3024, 7642, 2906, 1822, 2635, 2836, 2004, 2220, 6098], "orig_top_k_doc_id": [976, 973, 974, 975, 2839, 7642, 2315, 3024, 2906, 1822, 2635, 2836, 2004, 2220, 6098]}, {"qid": 1632, "question": "What dataset is used for training? in Representation Learning for Discovering Phonemic Tone Contours", "answer": ["Mandarin dataset, Cantonese dataset"], "top_k_doc_id": [973, 974, 975, 2315, 2316, 2317, 2967, 2948, 2930, 4615, 2292, 6260, 7537, 3459, 2931], "orig_top_k_doc_id": [2315, 2316, 2317, 2948, 2967, 2930, 4615, 2292, 6260, 973, 974, 7537, 3459, 2931, 975]}, {"qid": 1633, "question": "How close do clusters match to ground truth tone categories? in Representation Learning for Discovering Phonemic Tone Contours", "answer": ["NMI between cluster assignments and ground truth tones for all sylables is:\nMandarin: 0.641\nCantonese: 0.464"], "top_k_doc_id": [973, 974, 975, 2315, 2316, 2317, 2967, 976, 751, 1429, 6490, 2823, 678, 5794, 772], "orig_top_k_doc_id": [2315, 2316, 2317, 975, 976, 751, 1429, 6490, 2823, 678, 973, 5794, 974, 2967, 772]}]}
{"group_id": 366, "group_size": 5, "items": [{"qid": 867, "question": "How do they measure performance? in A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data", "answer": ["average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values"], "top_k_doc_id": [787, 1120, 1123, 4278, 788, 1217, 3175, 7806, 2733, 2899, 2900, 2377, 510, 491, 2578], "orig_top_k_doc_id": [1120, 1217, 4278, 1123, 2899, 787, 788, 2900, 3175, 7806, 2377, 510, 491, 2578, 2733]}, {"qid": 868, "question": "Do they measure the performance of a combined approach? in A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data", "answer": ["No"], "top_k_doc_id": [787, 1120, 1123, 4278, 788, 1217, 3175, 7806, 2733, 2899, 2900, 1322, 1528, 2370, 7352], "orig_top_k_doc_id": [1120, 4278, 1217, 1123, 2900, 1322, 787, 788, 3175, 2899, 1528, 7806, 2733, 2370, 7352]}, {"qid": 869, "question": "Which four QA systems do they use? in A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data", "answer": ["WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8"], "top_k_doc_id": [787, 1120, 1123, 4278, 575, 1322, 2661, 7351, 7352, 7610, 7163, 2910, 491, 510, 1121], "orig_top_k_doc_id": [1120, 787, 1123, 7351, 7352, 7610, 575, 7163, 2661, 2910, 491, 510, 1121, 4278, 1322]}, {"qid": 870, "question": "How many iterations of visual search are done on average until an answer is found? in A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data", "answer": ["No"], "top_k_doc_id": [787, 1120, 1123, 4278, 788, 1217, 3175, 7806, 2377, 3799, 510, 7147, 7163, 4523, 2378], "orig_top_k_doc_id": [1120, 2377, 3799, 1123, 510, 7147, 1217, 7163, 3175, 4523, 2378, 788, 4278, 787, 7806]}, {"qid": 871, "question": "Do they test performance of their approaches using human judgements? in A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data", "answer": ["Yes"], "top_k_doc_id": [787, 1120, 1123, 4278, 575, 1322, 2661, 7351, 1217, 788, 867, 2578, 3175, 1940, 7800], "orig_top_k_doc_id": [1120, 1217, 4278, 787, 788, 1123, 867, 2661, 1322, 7351, 2578, 3175, 1940, 575, 7800]}]}
{"group_id": 367, "group_size": 5, "items": [{"qid": 896, "question": "How is knowledge retrieved in the memory? in RelNet: End-to-End Modeling of Entities & Relations", "answer": ["the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector."], "top_k_doc_id": [1154, 1155, 2579, 2580, 160, 4980, 6543, 7802, 6338, 569, 2004, 1405, 4597, 4321, 2550], "orig_top_k_doc_id": [1154, 1155, 2579, 160, 2580, 6338, 569, 4980, 6543, 2004, 1405, 4597, 4321, 2550, 7802]}, {"qid": 897, "question": "How is knowledge stored in the memory? in RelNet: End-to-End Modeling of Entities & Relations", "answer": ["entity memory and relational memory."], "top_k_doc_id": [1154, 1155, 2579, 2580, 160, 4980, 6543, 7802, 7803, 3357, 6579, 1540, 267, 7836, 4546], "orig_top_k_doc_id": [1154, 1155, 2579, 7802, 7803, 160, 3357, 2580, 4980, 6543, 6579, 1540, 267, 7836, 4546]}, {"qid": 899, "question": "What is the architecture of the neural network? in RelNet: End-to-End Modeling of Entities & Relations", "answer": ["extends memory-augmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. , The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory."], "top_k_doc_id": [1154, 1155, 2579, 2580, 930, 2004, 2584, 4075, 4076, 295, 6086, 1761, 3162, 160, 1363], "orig_top_k_doc_id": [1155, 1154, 2579, 930, 295, 6086, 2580, 1761, 3162, 160, 2004, 2584, 4076, 4075, 1363]}, {"qid": 900, "question": "What methods is RelNet compared to? in RelNet: End-to-End Modeling of Entities & Relations", "answer": ["We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17"], "top_k_doc_id": [1154, 1155, 2579, 2580, 930, 2004, 2584, 4075, 4076, 2581, 1539, 1540, 338, 337, 562], "orig_top_k_doc_id": [1155, 1154, 2579, 2584, 2581, 2580, 1539, 930, 1540, 4076, 4075, 338, 2004, 337, 562]}, {"qid": 898, "question": "What are the relative improvements observed over existing methods? in RelNet: End-to-End Modeling of Entities & Relations", "answer": ["The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."], "top_k_doc_id": [1154, 1155, 4075, 4074, 3633, 4560, 4377, 6090, 5335, 1539, 6086, 4559, 3837, 930, 7609], "orig_top_k_doc_id": [1154, 1155, 4075, 4074, 3633, 4560, 4377, 6090, 5335, 1539, 6086, 4559, 3837, 930, 7609]}]}
{"group_id": 368, "group_size": 5, "items": [{"qid": 971, "question": "What task-specific features are used? in Recognizing Musical Entities in User-generated Content", "answer": ["6)Contributor first names, 7)Contributor last names, 8)Contributor types (\"soprano\", \"violinist\", etc.), 9)Classical work types (\"symphony\", \"overture\", etc.), 10)Musical instruments, 11)Opus forms (\"op\", \"opus\"), 12)Work number forms (\"no\", \"number\"), 13)Work keys (\"C\", \"D\", \"E\", \"F\" , \"G\" , \"A\", \"B\", \"flat\", \"sharp\"), 14)Work Modes (\"major\", \"minor\", \"m\")"], "top_k_doc_id": [1262, 1263, 1264, 1265, 4574, 929, 3810, 4755, 1130, 3879, 995, 1169, 4485, 1168, 27], "orig_top_k_doc_id": [1265, 1262, 1263, 1264, 4755, 995, 1169, 3810, 3879, 4485, 929, 4574, 1130, 1168, 27]}, {"qid": 973, "question": "Which machine learning algorithms did the explore? in Recognizing Musical Entities in User-generated Content", "answer": ["biLSTM-networks"], "top_k_doc_id": [1262, 1263, 1264, 1265, 4574, 929, 3810, 4755, 1130, 3879, 1661, 4667, 5390, 6589, 5434], "orig_top_k_doc_id": [1262, 1263, 1265, 1264, 1661, 4755, 4574, 929, 4667, 1130, 3810, 5390, 6589, 5434, 3879]}, {"qid": 970, "question": "What are their results on the entity recognition task? in Recognizing Musical Entities in User-generated Content", "answer": ["With both test sets performances decrease, varying between 94-97%"], "top_k_doc_id": [1262, 1263, 1264, 1265, 4574, 929, 3810, 4755, 1169, 4576, 5984, 4967, 65, 4154, 4575], "orig_top_k_doc_id": [1265, 1262, 1264, 1263, 929, 4755, 4574, 1169, 4576, 5984, 3810, 4967, 65, 4154, 4575]}, {"qid": 972, "question": "What kind of corpus-based features are taken into account? in Recognizing Musical Entities in User-generated Content", "answer": ["standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, series of features representing tokens' left and right context"], "top_k_doc_id": [1262, 1263, 1264, 1265, 4574, 1129, 617, 995, 1661, 1130, 4734, 3879, 1077, 4552, 4485], "orig_top_k_doc_id": [1265, 1262, 1263, 1264, 1129, 617, 995, 1661, 4574, 1130, 4734, 3879, 1077, 4552, 4485]}, {"qid": 974, "question": "What language is the Twitter content in? in Recognizing Musical Entities in User-generated Content", "answer": ["English"], "top_k_doc_id": [1262, 1263, 1264, 1265, 929, 7456, 1169, 6833, 6375, 521, 6630, 27, 3244, 4485, 5144], "orig_top_k_doc_id": [1262, 1265, 1263, 1264, 929, 7456, 1169, 6833, 6375, 521, 6630, 27, 3244, 4485, 5144]}]}
{"group_id": 369, "group_size": 5, "items": [{"qid": 1002, "question": "Where did this model place in the final evaluation of the shared task? in Multi-Task Bidirectional Transformer Representations for Irony Detection", "answer": ["$4th$"], "top_k_doc_id": [85, 1318, 1320, 1329, 1967, 3860, 1319, 5498, 1321, 5881, 7116, 1968, 1331, 6664, 87], "orig_top_k_doc_id": [1318, 1967, 1320, 1329, 85, 1319, 1968, 3860, 5498, 1331, 7116, 6664, 1321, 87, 5881]}, {"qid": 1003, "question": "What in-domain data is used to continue pre-training? in Multi-Task Bidirectional Transformer Representations for Irony Detection", "answer": ["dialectal tweet data"], "top_k_doc_id": [85, 1318, 1320, 1329, 1967, 3860, 1319, 5498, 1321, 5881, 7116, 5292, 5175, 4378, 7047], "orig_top_k_doc_id": [1318, 1320, 1967, 85, 1329, 5292, 7116, 3860, 5881, 5175, 5498, 1319, 4378, 7047, 1321]}, {"qid": 1004, "question": "What dialect is used in the Google BERT model and what is used in the task data? in Multi-Task Bidirectional Transformer Representations for Irony Detection", "answer": ["Modern Standard Arabic (MSA), MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine"], "top_k_doc_id": [85, 1318, 1320, 1329, 1967, 3860, 1319, 5498, 3688, 5292, 1321, 88, 2330, 2072, 87], "orig_top_k_doc_id": [1318, 85, 1320, 1321, 88, 2330, 2072, 1967, 87, 1329, 3860, 5292, 3688, 1319, 5498]}, {"qid": 1005, "question": "What are the tasks used in the mulit-task learning setup? in Multi-Task Bidirectional Transformer Representations for Irony Detection", "answer": ["Author profiling and deception detection in Arabic, LAMA+DINA Emotion detection, Sentiment analysis in Arabic tweets"], "top_k_doc_id": [85, 1318, 1320, 1329, 1967, 3860, 1319, 5498, 3688, 5292, 7116, 2238, 2329, 6666, 7115], "orig_top_k_doc_id": [1318, 1967, 1320, 1329, 85, 3860, 7116, 3688, 5292, 5498, 2238, 1319, 2329, 6666, 7115]}, {"qid": 1001, "question": "Why is being feature-engineering free an advantage? in Multi-Task Bidirectional Transformer Representations for Irony Detection", "answer": ["No"], "top_k_doc_id": [85, 1318, 1320, 1329, 1967, 3860, 1321, 5175, 3162, 2088, 3273, 7047, 7116, 3161, 4378], "orig_top_k_doc_id": [1318, 1320, 1321, 5175, 3860, 1967, 1329, 85, 3162, 2088, 3273, 7047, 7116, 3161, 4378]}]}
{"group_id": 370, "group_size": 5, "items": [{"qid": 1016, "question": "Does the model incorporate coreference and entailment? in SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering", "answer": ["As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution."], "top_k_doc_id": [4978, 1332, 1334, 6880, 2917, 4603, 7244, 2413, 6257, 6256, 6879, 7632, 2234, 6543, 2844], "orig_top_k_doc_id": [1332, 1334, 2413, 6257, 4978, 7244, 6880, 6256, 2917, 6879, 4603, 7632, 2234, 6543, 2844]}, {"qid": 1017, "question": "Is the incorporation of context separately evaluated? in SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering", "answer": ["No"], "top_k_doc_id": [4978, 1332, 1334, 6880, 2917, 4603, 7244, 2238, 3836, 3834, 2705, 3835, 826, 2228, 2242], "orig_top_k_doc_id": [1332, 1334, 2238, 4978, 3836, 7244, 3834, 2705, 3835, 6880, 4603, 826, 2228, 2917, 2242]}, {"qid": 1015, "question": "Is the model evaluated on other datasets? in SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering", "answer": ["No"], "top_k_doc_id": [4978, 1332, 1334, 6880, 511, 2238, 5736, 3836, 6879, 968, 709, 3835, 510, 22, 7760], "orig_top_k_doc_id": [1332, 1334, 511, 4978, 2238, 5736, 6880, 3836, 6879, 968, 709, 3835, 510, 22, 7760]}, {"qid": 2846, "question": "Is fine-tuning required to incorporate these embeddings into existing models? in DOLORES: Deep Contextualized Knowledge Graph Embeddings", "answer": ["No", "No"], "top_k_doc_id": [4978, 4979, 4980, 4981, 4982, 7288, 1332, 4278, 7630, 945, 5624, 7002, 125, 5026, 5621], "orig_top_k_doc_id": [4978, 4979, 4982, 4980, 4981, 7288, 1332, 4278, 7630, 945, 5624, 7002, 125, 5026, 5621]}, {"qid": 2847, "question": "How are meaningful chains in the graph selected? in DOLORES: Deep Contextualized Knowledge Graph Embeddings", "answer": ["No", "utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings."], "top_k_doc_id": [4978, 4979, 4980, 4981, 4982, 3299, 2096, 2101, 4274, 340, 4493, 3628, 4318, 7072, 4983], "orig_top_k_doc_id": [4979, 4978, 4980, 4982, 4981, 3299, 2096, 2101, 4274, 340, 4493, 3628, 4318, 7072, 4983]}]}
{"group_id": 371, "group_size": 5, "items": [{"qid": 1087, "question": "Is their implementation on CNN-DSA compared to GPU implementation in terms of power consumption, accuracy and speed? in Multi-modal Sentiment Analysis using Super Characters Method on Low-power CNN Accelerator Device", "answer": ["No"], "top_k_doc_id": [3976, 1436, 1437, 1438, 2615, 5475, 830, 3977, 747, 502, 7405, 666, 1666, 3646, 3978], "orig_top_k_doc_id": [1436, 1438, 3976, 1437, 5475, 3977, 747, 2615, 502, 7405, 666, 1666, 3646, 3978, 830]}, {"qid": 1088, "question": "Does this implementation on CNN-DSA lead to diminishing of performance? in Multi-modal Sentiment Analysis using Super Characters Method on Low-power CNN Accelerator Device", "answer": ["No"], "top_k_doc_id": [3976, 1436, 1437, 1438, 2615, 5475, 830, 6405, 2119, 7139, 1860, 462, 2982, 5328, 756], "orig_top_k_doc_id": [1436, 1438, 1437, 3976, 2615, 6405, 830, 2119, 7139, 5475, 1860, 462, 2982, 5328, 756]}, {"qid": 1089, "question": "How is Super Character method modified to handle tabular data also? in Multi-modal Sentiment Analysis using Super Characters Method on Low-power CNN Accelerator Device", "answer": ["simply split the image into two parts. One for the text input, and the other for the tabular data"], "top_k_doc_id": [3976, 1436, 1437, 1438, 2615, 5475, 330, 3634, 2206, 876, 4320, 4837, 332, 2165, 336], "orig_top_k_doc_id": [1436, 1438, 1437, 3976, 5475, 330, 3634, 2206, 2615, 876, 4320, 4837, 332, 2165, 336]}, {"qid": 1934, "question": "How much does it minimally cost to fine-tune some model according to benchmarking framework? in HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing", "answer": ["$1,728"], "top_k_doc_id": [3976, 2885, 2886, 2887, 3244, 7572, 4627, 1144, 3859, 4031, 6367, 3374, 5991, 3394, 200], "orig_top_k_doc_id": [2887, 2885, 2886, 4627, 1144, 3976, 3859, 4031, 6367, 7572, 3374, 5991, 3394, 3244, 200]}, {"qid": 1935, "question": "What models are included in baseline benchmarking results? in HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing", "answer": ["BERT, XLNET RoBERTa, ALBERT, DistilBERT"], "top_k_doc_id": [3976, 2885, 2886, 2887, 3244, 7572, 5761, 194, 3977, 6408, 2188, 6692, 2531, 1899, 5246], "orig_top_k_doc_id": [2887, 2885, 2886, 7572, 3244, 3976, 5761, 194, 3977, 6408, 2188, 6692, 2531, 1899, 5246]}]}
{"group_id": 372, "group_size": 5, "items": [{"qid": 1103, "question": "What improvement does the MOE model make over the SOTA on machine translation? in Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "answer": ["1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3, perplexity scores are also better, On the Google Production dataset, our model achieved 1.01 higher test BLEU score"], "top_k_doc_id": [1459, 1460, 1461, 1464, 1768, 1462, 1463, 3781, 1465, 1466, 4214, 5012, 4455, 418, 6944], "orig_top_k_doc_id": [1459, 1460, 1464, 1461, 1466, 3781, 1465, 1462, 1768, 1463, 4214, 4455, 418, 5012, 6944]}, {"qid": 1104, "question": "What improvement does the MOE model make over the SOTA on language modelling? in Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "answer": ["Perpexity is improved from 34.7 to 28.0."], "top_k_doc_id": [1459, 1460, 1461, 1464, 1768, 1462, 1463, 3781, 1465, 1466, 4214, 5012, 7372, 6016, 1325], "orig_top_k_doc_id": [1459, 1460, 1464, 1461, 1466, 1465, 1462, 1463, 3781, 5012, 1768, 4214, 7372, 6016, 1325]}, {"qid": 1102, "question": "Approximately how much computational cost is saved by using this model? in Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "answer": ["No"], "top_k_doc_id": [1459, 1460, 1461, 1464, 1768, 1462, 1463, 3781, 4487, 27, 6387, 7139, 24, 2556, 3477], "orig_top_k_doc_id": [1459, 1460, 1464, 1461, 1768, 4487, 27, 6387, 7139, 24, 2556, 3781, 1462, 1463, 3477]}, {"qid": 1106, "question": "What equations are used for the trainable gating network? in Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "answer": ["DISPLAYFORM0, DISPLAYFORM0 DISPLAYFORM1"], "top_k_doc_id": [1459, 1460, 1461, 1464, 1768, 7494, 5595, 3823, 7493, 4811, 4487, 1176, 1326, 2556, 1466], "orig_top_k_doc_id": [1459, 1460, 7494, 5595, 1464, 3823, 7493, 1768, 4811, 4487, 1461, 1176, 1326, 2556, 1466]}, {"qid": 1105, "question": "How is the correct number of experts to use decided? in Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "answer": ["varied the number of experts between models"], "top_k_doc_id": [1459, 1460, 1461, 1464, 1768, 2557, 355, 4455, 4583, 2391, 1199, 4584, 354, 1466, 6381], "orig_top_k_doc_id": [1459, 1460, 2557, 1464, 355, 4455, 1461, 4583, 2391, 1199, 4584, 354, 1768, 1466, 6381]}]}
{"group_id": 373, "group_size": 5, "items": [{"qid": 1135, "question": "How better are results for pmra algorithm  than Doc2Vec in human evaluation?  in Doc2Vec on the PubMed corpus: study of a new approach to generate related articles", "answer": ["The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents."], "top_k_doc_id": [1506, 1507, 1510, 6555, 1396, 4997, 5759, 302, 1508, 1509, 1451, 6556, 5758, 6383, 2730], "orig_top_k_doc_id": [1506, 1510, 1507, 302, 1509, 4997, 1508, 1396, 5759, 1451, 6556, 6555, 5758, 6383, 2730]}, {"qid": 1137, "question": "What four evaluation tasks are defined to determine what influences proximity? in Doc2Vec on the PubMed corpus: study of a new approach to generate related articles", "answer": ["String length, Words co-occurrences, Stems co-occurrences, MeSH similarity"], "top_k_doc_id": [1506, 1507, 1510, 6555, 1396, 4997, 5759, 302, 1508, 1509, 5757, 5133, 4839, 3744, 1958], "orig_top_k_doc_id": [1506, 1507, 1510, 302, 5759, 5757, 1509, 5133, 4839, 1396, 4997, 3744, 1958, 6555, 1508]}, {"qid": 1134, "question": "How long it took for each Doc2Vec model to be trained? in Doc2Vec on the PubMed corpus: study of a new approach to generate related articles", "answer": ["No"], "top_k_doc_id": [1506, 1507, 1510, 6555, 1396, 4997, 5759, 5758, 6556, 5757, 1518, 6383, 5002, 6382, 1401], "orig_top_k_doc_id": [1506, 1507, 5759, 5758, 1510, 4997, 1396, 6556, 5757, 6555, 1518, 6383, 5002, 6382, 1401]}, {"qid": 1136, "question": "What Doc2Vec architectures other than PV-DBOW have been tried? in Doc2Vec on the PubMed corpus: study of a new approach to generate related articles", "answer": ["PV-DM"], "top_k_doc_id": [1506, 1507, 1510, 6555, 1396, 4997, 1508, 1509, 1327, 1395, 7738, 1450, 5313, 7739, 6383], "orig_top_k_doc_id": [1506, 1510, 1507, 4997, 1396, 1508, 1509, 1327, 6555, 1395, 7738, 1450, 5313, 7739, 6383]}, {"qid": 1138, "question": "What six parameters were optimized with grid search? in Doc2Vec on the PubMed corpus: study of a new approach to generate related articles", "answer": ["window_size, alpha, sample, dm, hs, vector_size"], "top_k_doc_id": [1506, 1507, 1510, 6555, 7129, 302, 217, 5759, 4096, 2730, 5757, 7128, 5438, 6162, 5790], "orig_top_k_doc_id": [1506, 1507, 1510, 7129, 302, 217, 5759, 4096, 6555, 2730, 5757, 7128, 5438, 6162, 5790]}]}
{"group_id": 374, "group_size": 5, "items": [{"qid": 1181, "question": "Do they use pretrained models as part of their parser? in An Incremental Parser for Abstract Meaning Representation", "answer": ["Yes"], "top_k_doc_id": [3759, 1586, 1591, 5138, 3489, 3490, 3912, 5943, 7053, 7054, 2014, 3255, 1994, 2022, 4344], "orig_top_k_doc_id": [3759, 1586, 1591, 3489, 5138, 3912, 3490, 5943, 7054, 2014, 3255, 1994, 7053, 2022, 4344]}, {"qid": 1182, "question": "Which subtasks do they evaluate on? in An Incremental Parser for Abstract Meaning Representation", "answer": [" entity recognition, semantic role labeling and co-reference resolution"], "top_k_doc_id": [3759, 1586, 1591, 5138, 3489, 3490, 3912, 5943, 7053, 1592, 2153, 5718, 1590, 3296, 2917], "orig_top_k_doc_id": [1591, 1592, 3759, 3912, 1586, 2153, 3489, 3490, 5718, 1590, 5138, 7053, 3296, 5943, 2917]}, {"qid": 1350, "question": "What is the SemEval-2016 task 8? in AMR-to-text Generation with Synchronous Node Replacement Grammar", "answer": ["No"], "top_k_doc_id": [3759, 1586, 1591, 5138, 1856, 1858, 1859, 1857, 1590, 4002, 1587, 2267, 5140, 5141, 3761], "orig_top_k_doc_id": [1856, 1858, 1859, 1591, 1857, 1590, 5138, 4002, 1587, 2267, 5140, 5141, 1586, 3759, 3761]}, {"qid": 2348, "question": "Do the authors test their annotation projection techniques on tasks other than AMR? in Cross-lingual Abstract Meaning Representation Parsing", "answer": ["No"], "top_k_doc_id": [3759, 247, 627, 1040, 1053, 3761, 3762, 6852, 6854, 3763, 3760, 1586, 1592, 5138, 1856], "orig_top_k_doc_id": [3762, 3759, 3763, 3760, 6852, 1586, 3761, 1592, 6854, 247, 1053, 5138, 1856, 627, 1040]}, {"qid": 2349, "question": "How is annotation projection done when languages have different word order? in Cross-lingual Abstract Meaning Representation Parsing", "answer": ["Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments."], "top_k_doc_id": [3759, 247, 627, 1040, 1053, 3761, 3762, 6852, 6854, 2806, 248, 3547, 1048, 2162, 4184], "orig_top_k_doc_id": [3759, 6852, 247, 3762, 1040, 2806, 248, 3547, 1053, 627, 1048, 3761, 2162, 4184, 6854]}]}
{"group_id": 375, "group_size": 5, "items": [{"qid": 1188, "question": "Which tasks do they apply their method to? in Quantifying Similarity between Relations with Fact Distribution", "answer": ["relation prediction, relation extraction, Open IE"], "top_k_doc_id": [1603, 1604, 1606, 1607, 4835, 1605, 3207, 5701, 4839, 5151, 4983, 5709, 6251, 2422, 6149], "orig_top_k_doc_id": [1603, 1604, 4835, 1607, 4839, 1606, 5151, 3207, 4983, 1605, 5709, 5701, 6251, 2422, 6149]}, {"qid": 1190, "question": "How do they gather human judgements for similarity between relations? in Quantifying Similarity between Relations with Fact Distribution", "answer": ["By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4"], "top_k_doc_id": [1603, 1604, 1606, 1607, 4835, 1605, 3207, 5701, 985, 3208, 6129, 5700, 6126, 5646, 3354], "orig_top_k_doc_id": [1603, 1604, 985, 3208, 4835, 1605, 1607, 6129, 3207, 5700, 6126, 1606, 5701, 5646, 3354]}, {"qid": 1187, "question": "Which competitive relational classification models do they test? in Quantifying Similarity between Relations with Fact Distribution", "answer": ["For relation prediction they test TransE and for relation extraction they test position aware neural sequence model"], "top_k_doc_id": [1603, 1604, 1606, 1607, 4835, 3535, 3672, 6129, 4216, 5700, 7055, 3533, 4218, 5212, 3015], "orig_top_k_doc_id": [1603, 3535, 1606, 3672, 1607, 1604, 6129, 4216, 5700, 7055, 3533, 4835, 4218, 5212, 3015]}, {"qid": 1191, "question": "Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs? in Quantifying Similarity between Relations with Fact Distribution", "answer": ["monte-carlo, sequential sampling"], "top_k_doc_id": [1603, 1604, 1606, 1607, 5761, 1354, 1605, 4259, 2593, 3237, 1052, 4260, 6429, 7782, 3539], "orig_top_k_doc_id": [1603, 1604, 1606, 1607, 5761, 1354, 1605, 4259, 2593, 3237, 1052, 4260, 6429, 7782, 3539]}, {"qid": 1189, "question": "Which knowledge bases do they use? in Quantifying Similarity between Relations with Fact Distribution", "answer": ["Wikidata, ReVerb, FB15K, TACRED"], "top_k_doc_id": [1603, 1604, 4719, 68, 1169, 6904, 4074, 4835, 4348, 145, 5212, 5943, 7517, 7520, 3409], "orig_top_k_doc_id": [1603, 1604, 4719, 68, 1169, 6904, 4074, 4835, 4348, 145, 5212, 5943, 7517, 7520, 3409]}]}
{"group_id": 376, "group_size": 5, "items": [{"qid": 1241, "question": "Which SOTA models are outperformed? in Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM", "answer": ["Attention-based LSTM with emojis"], "top_k_doc_id": [756, 757, 1685, 1686, 1687, 1688, 1689, 1690, 462, 5422, 7174, 3575, 4378, 6401, 7116], "orig_top_k_doc_id": [1687, 1686, 1685, 1688, 1689, 1690, 756, 5422, 757, 462, 6401, 7174, 3575, 4378, 7116]}, {"qid": 1242, "question": "What is the baseline for experiments? in Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM", "answer": ["LSTM with text embedding, LSTM with emoji embedding, Attention-based LSTM with emojis"], "top_k_doc_id": [756, 757, 1685, 1686, 1687, 1688, 1689, 1690, 462, 5422, 7174, 3575, 4378, 6159, 6329], "orig_top_k_doc_id": [1687, 1686, 1685, 1688, 1689, 1690, 756, 5422, 757, 462, 7174, 3575, 4378, 6159, 6329]}, {"qid": 1239, "question": "Do they evaluate only on English datasets? in Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM", "answer": ["Yes"], "top_k_doc_id": [756, 757, 1685, 1686, 1687, 1688, 1689, 1690, 462, 5422, 7174, 6158, 6159, 5417, 7173], "orig_top_k_doc_id": [1687, 1686, 1685, 1688, 1689, 1690, 756, 7174, 5422, 462, 757, 6158, 6159, 5417, 7173]}, {"qid": 1243, "question": "What is the motivation for training bi-sense embeddings? in Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM", "answer": [" previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts BIBREF5 , or when the emojis convey multiple senses of semantics and sentiments "], "top_k_doc_id": [756, 757, 1685, 1686, 1687, 1688, 1689, 1690, 462, 5422, 7174, 2241, 5314, 2857, 1489], "orig_top_k_doc_id": [1687, 1688, 1686, 1689, 1685, 1690, 5422, 756, 757, 462, 7174, 2241, 5314, 2857, 1489]}, {"qid": 1240, "question": "What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments? in Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM", "answer": ["The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments"], "top_k_doc_id": [756, 757, 1685, 1686, 1687, 1688, 1689, 1690, 330, 256, 4756, 4378, 6402, 728, 1560], "orig_top_k_doc_id": [1687, 1685, 1689, 1686, 1690, 1688, 756, 757, 330, 256, 4756, 4378, 6402, 728, 1560]}]}
{"group_id": 377, "group_size": 5, "items": [{"qid": 1297, "question": "Is the new model evaluated on the tasks that BERT and ELMo are evaluated on? in Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks", "answer": ["Yes"], "top_k_doc_id": [4561, 1779, 1780, 4209, 4562, 5678, 5679, 1773, 7630, 2238, 5505, 5183, 5941, 3499, 7005], "orig_top_k_doc_id": [1779, 5678, 1780, 5679, 4561, 4562, 1773, 7630, 2238, 5505, 5183, 5941, 4209, 3499, 7005]}, {"qid": 1298, "question": "Does the additional training on supervised tasks hurt performance in some tasks? in Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks", "answer": ["Yes"], "top_k_doc_id": [4561, 1779, 1780, 4209, 4562, 5678, 3571, 1533, 3293, 1745, 5504, 2457, 369, 5992, 2177], "orig_top_k_doc_id": [1779, 1780, 5678, 4561, 3571, 1533, 4209, 4562, 3293, 1745, 5504, 2457, 369, 5992, 2177]}, {"qid": 1170, "question": "What subset of GLUE tasks is used? in Revealing the Dark Secrets of BERT", "answer": ["MRPC, STS-B, SST-2, QQP, RTE, QNLI, MNLI"], "top_k_doc_id": [4561, 1779, 1780, 1560, 3074, 2680, 948, 1144, 1562, 2681, 1561, 6368, 2086, 5249, 7818], "orig_top_k_doc_id": [1560, 3074, 2680, 1780, 948, 1144, 1562, 1779, 2681, 4561, 1561, 6368, 2086, 5249, 7818]}, {"qid": 2543, "question": "What state-of-the-art general-purpose pretrained models are made available under the unified API?  in HuggingFace's Transformers: State-of-the-art Natural Language Processing", "answer": ["BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, XLM"], "top_k_doc_id": [4561, 3069, 4414, 4415, 5540, 4412, 4413, 5678, 6135, 4908, 6165, 4609, 5711, 436, 6479], "orig_top_k_doc_id": [4412, 4413, 4415, 5678, 4414, 6135, 5540, 4908, 4561, 6165, 4609, 5711, 436, 6479, 3069]}, {"qid": 2589, "question": "Do some pretraining objectives perform better than others for sentence level understanding tasks? in Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling", "answer": ["Yes"], "top_k_doc_id": [4561, 3069, 4414, 4415, 5540, 4562, 4563, 7157, 5473, 3277, 320, 1779, 6440, 7158, 3276], "orig_top_k_doc_id": [4414, 4415, 5540, 4561, 4562, 4563, 3069, 7157, 5473, 3277, 320, 1779, 6440, 7158, 3276]}]}
{"group_id": 378, "group_size": 5, "items": [{"qid": 1369, "question": "What dataset did they use? in Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks", "answer": ["16 different datasets from several popular review corpora used in BIBREF20, CoNLL 2000 BIBREF22"], "top_k_doc_id": [6256, 6260, 19, 20, 1880, 2120, 3116, 6258, 6671, 2121, 2874, 7472, 727, 1744, 1821], "orig_top_k_doc_id": [1880, 6260, 19, 6258, 3116, 2120, 2121, 2874, 6256, 6671, 20, 7472, 727, 1744, 1821]}, {"qid": 1370, "question": "What tasks did they experiment with? in Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks", "answer": ["Sentiment Classification, Transferability of Shared Sentence Representation, Introducing Sequence Labeling as Auxiliary Task"], "top_k_doc_id": [6256, 6260, 19, 20, 1880, 2120, 3116, 6258, 6671, 2121, 2874, 7472, 4561, 6448, 6532], "orig_top_k_doc_id": [1880, 19, 2120, 7472, 6260, 6258, 2121, 3116, 4561, 6448, 6256, 6671, 20, 2874, 6532]}, {"qid": 1368, "question": "What evaluation metrics are used? in Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks", "answer": ["Accuracy on each dataset and the average accuracy on all datasets."], "top_k_doc_id": [6256, 6260, 19, 20, 1880, 2120, 3116, 6258, 6671, 727, 6448, 491, 6259, 1410, 6355], "orig_top_k_doc_id": [1880, 3116, 19, 2120, 727, 6256, 6260, 20, 6448, 491, 6258, 6259, 1410, 6355, 6671]}, {"qid": 3874, "question": "what was the margin their system outperformed previous ones? in Attention-Based Convolutional Neural Network for Machine Comprehension", "answer": ["15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500", "15.6 and 16.5 for accuracy and NDCG on MCTest-150, 7.3 and 4.6 on MCTest-500.", "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500"], "top_k_doc_id": [6256, 6260, 2127, 3825, 4212, 4149, 1345, 1651, 2188, 7472, 2442, 7116, 4428, 2839, 4972], "orig_top_k_doc_id": [3825, 4212, 4149, 1345, 6260, 2127, 6256, 1651, 2188, 7472, 2442, 7116, 4428, 2839, 4972]}, {"qid": 3875, "question": "what prior approaches did they compare to? in Attention-Based Convolutional Neural Network for Machine Comprehension", "answer": ["Addition, Addition-proj, Neural Reasoner, Attentive Reader", "Neural Reasoner, Attentive Reader", "The Neural Reasoner, The Attentive Reader"], "top_k_doc_id": [6256, 6260, 2127, 3825, 3416, 6261, 6257, 2048, 2917, 259, 6943, 2752, 4878, 6258, 886], "orig_top_k_doc_id": [6256, 3416, 2127, 6261, 6257, 2048, 2917, 259, 6943, 3825, 2752, 6260, 4878, 6258, 886]}]}
{"group_id": 379, "group_size": 5, "items": [{"qid": 1439, "question": "How much in-domain data is enough for joint models to outperform baselines? in Joint Contextual Modeling for ASR Correction and Language Understanding", "answer": ["No"], "top_k_doc_id": [4370, 4373, 4374, 4376, 1300, 1738, 1987, 1988, 1989, 4375, 7349, 5015, 1338, 6468, 1812], "orig_top_k_doc_id": [1987, 1300, 1988, 1989, 1738, 4370, 5015, 1338, 7349, 6468, 1812, 4376, 4374, 4375, 4373]}, {"qid": 1440, "question": "How many parameters does their proposed joint model have? in Joint Contextual Modeling for ASR Correction and Language Understanding", "answer": ["No"], "top_k_doc_id": [4370, 4373, 4374, 4376, 1300, 1738, 1987, 1988, 1989, 4375, 7349, 4377, 2197, 4312, 4371], "orig_top_k_doc_id": [1987, 4377, 1988, 4376, 4374, 4375, 4370, 4373, 2197, 1738, 1989, 4312, 1300, 7349, 4371]}, {"qid": 2532, "question": "How is the discriminative training formulation different from the standard ones? in Progressive Joint Modeling in Unsupervised Single-channel Overlapped Speech Recognition", "answer": ["the best permutation is decided by $\\mathcal {J}_{\\text{SEQ}}(\\mathbf {L}_{un}^{(s^{\\prime })},\\mathbf {L}_{un}^{(r)})$ , which is the sequence discriminative criterion of taking the $s^{\\prime }$ -th permutation in $n$ -th output inference stream at utterance $u$"], "top_k_doc_id": [4370, 4373, 4374, 4376, 4367, 4368, 4369, 4371, 4372, 4377, 7793, 7795, 4375, 7642, 5822], "orig_top_k_doc_id": [4367, 4371, 4370, 4377, 4376, 4368, 4369, 4372, 4375, 4374, 7642, 4373, 7795, 7793, 5822]}, {"qid": 2533, "question": "How are the two datasets artificially overlapped? in Progressive Joint Modeling in Unsupervised Single-channel Overlapped Speech Recognition", "answer": ["we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together"], "top_k_doc_id": [4370, 4373, 4374, 4376, 4367, 4368, 4369, 4371, 4372, 4377, 7793, 7795, 7799, 7403, 4000], "orig_top_k_doc_id": [4367, 4370, 4371, 4368, 4377, 4372, 4376, 4369, 4373, 7795, 7799, 7793, 7403, 4000, 4374]}, {"qid": 2503, "question": "How long is new model trained on 3400 hours of data? in Domain Adaptation via Teacher-Student Learning for End-to-End Speech Recognition", "answer": ["No"], "top_k_doc_id": [4370, 4242, 4243, 4239, 4244, 4369, 4240, 5476, 4371, 5992, 2862, 7630, 4241, 381, 4377], "orig_top_k_doc_id": [4242, 4243, 4239, 4244, 4369, 4370, 4240, 5476, 4371, 5992, 2862, 7630, 4241, 381, 4377]}]}
{"group_id": 380, "group_size": 5, "items": [{"qid": 1524, "question": "Do they perform some annotation? in Mind Your Language: Abuse and Offense Detection for Code-Switched Languages", "answer": ["No"], "top_k_doc_id": [2137, 3593, 5144, 3582, 3583, 3588, 3589, 2237, 3575, 4948, 3585, 5170, 7172, 2633, 4137], "orig_top_k_doc_id": [2137, 5144, 3589, 3593, 3588, 3575, 3583, 5170, 2237, 3582, 4948, 3585, 2633, 4137, 7172]}, {"qid": 1525, "question": "Do they use dropout? in Mind Your Language: Abuse and Offense Detection for Code-Switched Languages", "answer": ["Yes"], "top_k_doc_id": [2137, 3593, 5144, 3582, 3583, 3588, 3589, 2237, 3575, 4948, 3585, 5170, 7172, 2138, 3581], "orig_top_k_doc_id": [2137, 5144, 3593, 3575, 3589, 3583, 3588, 7172, 2138, 5170, 3582, 3581, 2237, 4948, 3585]}, {"qid": 1523, "question": "What embeddings do they use? in Mind Your Language: Abuse and Offense Detection for Code-Switched Languages", "answer": ["Glove, Twitter word2vec"], "top_k_doc_id": [2137, 3593, 5144, 3582, 3583, 3588, 3589, 2237, 3575, 4948, 3581, 2138, 4551, 1789, 243], "orig_top_k_doc_id": [2137, 5144, 3593, 3589, 3575, 3583, 3581, 3588, 2138, 2237, 3582, 4551, 4948, 1789, 243]}, {"qid": 1526, "question": "What definition of hate speech do they use? in Mind Your Language: Abuse and Offense Detection for Code-Switched Languages", "answer": ["No"], "top_k_doc_id": [2137, 3593, 5144, 3582, 3583, 3588, 3589, 3581, 6176, 3574, 5170, 2138, 4137, 1788, 3584], "orig_top_k_doc_id": [5144, 2137, 3589, 3583, 3581, 6176, 3582, 3574, 3588, 5170, 2138, 4137, 1788, 3584, 3593]}, {"qid": 1522, "question": "Do all the instances contain code-switching? in Mind Your Language: Abuse and Offense Detection for Code-Switched Languages", "answer": ["No"], "top_k_doc_id": [2137, 3593, 5144, 1000, 3023, 7007, 7172, 3938, 6871, 1488, 5272, 2633, 2138, 5977, 3575], "orig_top_k_doc_id": [2137, 1000, 3023, 7007, 7172, 3938, 6871, 5144, 1488, 3593, 5272, 2633, 2138, 5977, 3575]}]}
{"group_id": 381, "group_size": 5, "items": [{"qid": 1576, "question": "What are the differences between FHIR and RDF? in Semantic Enrichment of Streaming Healthcare Data", "answer": ["One of the several formats into which FHIR can be serialized is RDF, there is the potential for a slight mismatch between the models"], "top_k_doc_id": [2200, 2202, 2203, 2204, 854, 879, 2201, 5266, 5817, 4154, 5431, 6317, 4796, 1486, 6713], "orig_top_k_doc_id": [2202, 2204, 2203, 2201, 2200, 5266, 854, 4154, 5817, 4796, 5431, 879, 1486, 6713, 6317]}, {"qid": 1577, "question": "What do FHIR and RDF stand for? in Semantic Enrichment of Streaming Healthcare Data", "answer": ["Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR), Resource Description Framework (RDF)"], "top_k_doc_id": [2200, 2202, 2203, 2204, 854, 879, 2201, 5266, 5817, 4154, 5431, 6317, 5258, 1120, 3271], "orig_top_k_doc_id": [2204, 2202, 2203, 2201, 2200, 854, 879, 6317, 5266, 5817, 5431, 4154, 5258, 1120, 3271]}, {"qid": 1575, "question": "How are FHIR and RDF combined? in Semantic Enrichment of Streaming Healthcare Data", "answer": ["RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting, RDF makes statements of fact, whereas FHIR makes records of events, RDF is intended to have the property of monotonicity, meaning that previous facts cannot be invalidated by new facts"], "top_k_doc_id": [2200, 2202, 2203, 2204, 854, 879, 2201, 5266, 5817, 982, 4462, 1758, 7176, 6579, 3271], "orig_top_k_doc_id": [2203, 2202, 2204, 2201, 2200, 5817, 854, 5266, 982, 4462, 1758, 7176, 6579, 879, 3271]}, {"qid": 1574, "question": "What type of simulations of real-time data feeds are used for validaton? in Semantic Enrichment of Streaming Healthcare Data", "answer": ["simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting"], "top_k_doc_id": [2200, 2202, 2203, 2204, 982, 853, 1486, 1758, 7772, 5257, 6317, 4108, 1754, 5258, 1756], "orig_top_k_doc_id": [2202, 2203, 2204, 2200, 982, 853, 1486, 1758, 7772, 5257, 6317, 4108, 1754, 5258, 1756]}, {"qid": 4603, "question": "What is the Semantic Web? in A Holistic Natural Language Generation Framework for the Semantic Web", "answer": ["No", "aims to make information available on the Web easier to process for machines and humans,  in RDF are expressed as so-called triples of the form (subject, predicate, object), OWL is equivalent to the description logic $\\mathcal {SROIQ}(D)$"], "top_k_doc_id": [2200, 2818, 7700, 7194, 3220, 6184, 2201, 7072, 1181, 1120, 6187, 1184, 4424, 3681, 854], "orig_top_k_doc_id": [2818, 7700, 7194, 3220, 6184, 2201, 7072, 1181, 2200, 1120, 6187, 1184, 4424, 3681, 854]}]}
{"group_id": 382, "group_size": 5, "items": [{"qid": 1702, "question": "What is the size of the model? in Neural Word Segmentation with Rich Pretraining", "answer": ["No"], "top_k_doc_id": [2432, 2433, 2430, 2429, 2431, 2434, 4561, 5540, 650, 2063, 2587, 1779, 5473, 3274, 3647], "orig_top_k_doc_id": [2433, 2429, 2434, 2432, 4561, 1779, 2431, 5473, 2063, 2430, 5540, 2587, 650, 3274, 3647]}, {"qid": 1703, "question": "What external sources are used? in Neural Word Segmentation with Rich Pretraining", "answer": ["Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily"], "top_k_doc_id": [2432, 2433, 2430, 2429, 2431, 2434, 4561, 5540, 650, 2063, 2587, 363, 5716, 5709, 2590], "orig_top_k_doc_id": [2433, 2429, 2434, 2431, 363, 2587, 2430, 2432, 5716, 2063, 5709, 650, 4561, 2590, 5540]}, {"qid": 1704, "question": "What submodules does the model consist of? in Neural Word Segmentation with Rich Pretraining", "answer": ["five-character window context"], "top_k_doc_id": [2432, 2433, 2430, 2429, 2431, 2434, 4561, 5540, 3021, 5473, 1779, 5046, 656, 4852, 655], "orig_top_k_doc_id": [2433, 2429, 2434, 2432, 3021, 2431, 4561, 5473, 2430, 5540, 1779, 5046, 656, 4852, 655]}, {"qid": 1980, "question": "Do they use pretrained word embeddings? in Learning to Describe Phrases with Local and Global Contexts", "answer": ["Yes"], "top_k_doc_id": [2432, 2433, 2430, 2984, 2983, 2985, 4598, 5336, 2059, 2058, 4885, 3077, 4597, 1761, 1447], "orig_top_k_doc_id": [2984, 2983, 2985, 2433, 4598, 5336, 2432, 2059, 2058, 4885, 3077, 4597, 1761, 1447, 2430]}, {"qid": 2323, "question": "Which dataset do they use? in Future Word Contexts in Neural Network Language Models", "answer": [" AMI IHM meeting corpus"], "top_k_doc_id": [2432, 2433, 1182, 1203, 4755, 2234, 887, 2983, 203, 3034, 5700, 425, 4756, 446, 1183], "orig_top_k_doc_id": [1182, 1203, 4755, 2234, 887, 2983, 2433, 203, 3034, 5700, 425, 4756, 446, 2432, 1183]}]}
{"group_id": 383, "group_size": 5, "items": [{"qid": 1712, "question": "What conclusions are drawn from these experiments? in Evaluating KGR10 Polish word embeddings in the recognition of temporal expressions using BiLSTM-CRF", "answer": ["best results were obtained using new word embeddings, best group of word embeddings is EC, The highest type F1-score was obtained for EC1 model, built using binary FastText Skip-gram method utilising subword information, ability of the model to provide vector representation for the unknown words seems to be the most important"], "top_k_doc_id": [2447, 2448, 2449, 5961, 357, 1781, 1782, 1094, 2694, 4575, 5133, 5960, 6644, 4883, 1578], "orig_top_k_doc_id": [2448, 2449, 2447, 2694, 5961, 1094, 5960, 6644, 4575, 1781, 357, 4883, 1782, 5133, 1578]}, {"qid": 1713, "question": "What experiments are presented? in Evaluating KGR10 Polish word embeddings in the recognition of temporal expressions using BiLSTM-CRF", "answer": ["identify the boundaries of timexes and assign them to one of the following classes: date, time, duration, set,  Then we evaluated these results using more detailed measures for timexes"], "top_k_doc_id": [2447, 2448, 2449, 5961, 357, 1781, 1782, 1094, 2694, 4575, 5133, 5960, 6644, 1091, 6698], "orig_top_k_doc_id": [2448, 2449, 2447, 1094, 5133, 6644, 2694, 5961, 1091, 5960, 4575, 1781, 6698, 1782, 357]}, {"qid": 1714, "question": "What is specific about the specific embeddings? in Evaluating KGR10 Polish word embeddings in the recognition of temporal expressions using BiLSTM-CRF", "answer": ["predicting the word given its context"], "top_k_doc_id": [2447, 2448, 2449, 5961, 357, 1781, 1782, 1578, 4967, 5083, 5960, 6644, 6698, 438, 1329], "orig_top_k_doc_id": [2448, 2449, 2447, 5960, 1782, 1781, 6644, 6698, 4967, 5961, 438, 357, 1329, 1578, 5083]}, {"qid": 1715, "question": "What embedding algorithm is used to build the embeddings? in Evaluating KGR10 Polish word embeddings in the recognition of temporal expressions using BiLSTM-CRF", "answer": ["CBOW and Skip-gram methods in the FastText tool BIBREF9"], "top_k_doc_id": [2447, 2448, 2449, 5961, 357, 1781, 1782, 1578, 4967, 5083, 4575, 7056, 499, 1096, 5133], "orig_top_k_doc_id": [2448, 2449, 2447, 1781, 4967, 4575, 7056, 1578, 5083, 5961, 1782, 499, 357, 1096, 5133]}, {"qid": 1716, "question": "How was the KGR10 corpus created? in Evaluating KGR10 Polish word embeddings in the recognition of temporal expressions using BiLSTM-CRF", "answer": ["most relevant content of the website, including all subsites"], "top_k_doc_id": [2447, 2448, 2449, 5961, 1094, 2694, 1091, 4575, 1096, 5083, 5082, 5960, 714, 438, 6698], "orig_top_k_doc_id": [2448, 2447, 2449, 1094, 2694, 1091, 4575, 1096, 5083, 5082, 5960, 5961, 714, 438, 6698]}]}
{"group_id": 384, "group_size": 5, "items": [{"qid": 1735, "question": "Which languages are evaluated? in Named Entity Recognition with Partially Annotated Training Data", "answer": ["Bengali, English, German, Spanish, Dutch, Amharic, Arabic, Hindi, Somali "], "top_k_doc_id": [4575, 5051, 6153, 7287, 1773, 4573, 4574, 2477, 4589, 497, 4750, 2318, 6151, 2320, 6036], "orig_top_k_doc_id": [2477, 5051, 4573, 4574, 6153, 7287, 4589, 497, 4750, 2318, 4575, 1773, 6151, 2320, 6036]}, {"qid": 2597, "question": "what ner models were evaluated? in pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "answer": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "top_k_doc_id": [4575, 5051, 6153, 7287, 1773, 4573, 4574, 4576, 7100, 7759, 5879, 858, 5956, 930, 438], "orig_top_k_doc_id": [4575, 4574, 4576, 4573, 6153, 7100, 7759, 5051, 1773, 5879, 858, 5956, 930, 438, 7287]}, {"qid": 1734, "question": "What was their F1 score on the Bengali NER corpus? in Named Entity Recognition with Partially Annotated Training Data", "answer": ["52.0%"], "top_k_doc_id": [4575, 5051, 6153, 7287, 2477, 4750, 618, 357, 2482, 607, 5132, 2327, 4858, 7172, 4749], "orig_top_k_doc_id": [2477, 4750, 618, 357, 6153, 4575, 2482, 607, 5132, 2327, 4858, 7172, 4749, 5051, 7287]}, {"qid": 2598, "question": "what is the source of the news sentences? in pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "answer": ["ilur.am", "links between Wikipedia articles to generate sequences of named-entity annotated tokens"], "top_k_doc_id": [4575, 4573, 4574, 4576, 3974, 7389, 4790, 2973, 929, 4750, 2886, 3945, 6036, 5189, 2823], "orig_top_k_doc_id": [4574, 4575, 4573, 4576, 3974, 7389, 4790, 2973, 929, 4750, 2886, 3945, 6036, 5189, 2823]}, {"qid": 2599, "question": "did they use a crowdsourcing platform for manual annotations? in pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "answer": ["No", "No"], "top_k_doc_id": [4575, 4573, 4574, 4576, 6140, 854, 5145, 4583, 4610, 5294, 4587, 6995, 4669, 5038, 7672], "orig_top_k_doc_id": [4573, 4575, 4574, 4576, 6140, 854, 5145, 4583, 4610, 5294, 4587, 6995, 4669, 5038, 7672]}]}
{"group_id": 385, "group_size": 5, "items": [{"qid": 1820, "question": "What is the size of their dataset? in CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding", "answer": ["10,001 utterances"], "top_k_doc_id": [200, 400, 2643, 2644, 2645, 2646, 2647, 4149, 3532, 4281, 5160, 6879, 4763, 733, 3020], "orig_top_k_doc_id": [2646, 2643, 2647, 2644, 2645, 3532, 400, 4763, 4281, 5160, 733, 4149, 3020, 200, 6879]}, {"qid": 1821, "question": "What is the source of the CAIS dataset? in CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding", "answer": ["the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS)"], "top_k_doc_id": [200, 400, 2643, 2644, 2645, 2646, 2647, 4149, 3532, 4281, 5160, 6879, 4928, 3127, 285], "orig_top_k_doc_id": [2646, 2643, 2647, 2645, 2644, 400, 4928, 3532, 3127, 4281, 200, 6879, 5160, 4149, 285]}, {"qid": 1818, "question": "What is the domain of their collected corpus? in CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding", "answer": ["speaker systems in the real world"], "top_k_doc_id": [200, 400, 2643, 2644, 2645, 2646, 2647, 4149, 2630, 4928, 5917, 201, 199, 4281, 7839], "orig_top_k_doc_id": [2646, 2643, 2647, 2645, 2644, 200, 400, 5917, 2630, 201, 199, 4928, 4281, 7839, 4149]}, {"qid": 1819, "question": "What was the performance on the self-collected corpus? in CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding", "answer": ["F1 scores of 86.16 on slot filling and 94.56 on intent detection"], "top_k_doc_id": [200, 400, 2643, 2644, 2645, 2646, 2647, 4149, 2630, 4928, 5917, 5075, 4652, 3532, 3069], "orig_top_k_doc_id": [2646, 2643, 2645, 2647, 2644, 400, 200, 2630, 4928, 5075, 5917, 4652, 3532, 4149, 3069]}, {"qid": 1822, "question": "What were the baselines models? in CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding", "answer": ["BiLSTMs + CRF architecture BIBREF36, sententce-state LSTM BIBREF21"], "top_k_doc_id": [200, 400, 2643, 2644, 2645, 2646, 2647, 4149, 3532, 3020, 5075, 3019, 5970, 6225, 5699], "orig_top_k_doc_id": [2646, 2643, 2647, 2644, 2645, 400, 3020, 5075, 3019, 3532, 5970, 6225, 4149, 200, 5699]}]}
{"group_id": 386, "group_size": 5, "items": [{"qid": 1823, "question": "Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose? in \"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach", "answer": ["Yes"], "top_k_doc_id": [2648, 5913, 6256, 302, 5368, 5306, 5367, 1476, 1937, 2334, 1651, 122, 123, 3207, 303], "orig_top_k_doc_id": [302, 5368, 2648, 5306, 5367, 6256, 1476, 1937, 2334, 1651, 122, 123, 3207, 303, 5913]}, {"qid": 1824, "question": "According to the authors, why does the CNN model exhibit a higher level of explainability? in \"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach", "answer": ["CNN model which additionally learns intermediate hidden layer representations and convolutional filters. Moreover the CNN model can take advantage of the semantic similarity encoded in the distributed word2vec representations"], "top_k_doc_id": [2648, 5913, 6256, 1955, 2655, 307, 252, 121, 6258, 2653, 1667, 3530, 6260, 2785, 6879], "orig_top_k_doc_id": [1955, 2655, 307, 2648, 252, 121, 6258, 5913, 6256, 2653, 1667, 3530, 6260, 2785, 6879]}, {"qid": 1825, "question": "Does the LRP method work in settings that contextualize the words with respect to one another? in \"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach", "answer": ["Yes"], "top_k_doc_id": [2648, 2649, 4513, 2653, 4512, 4514, 2654, 2656, 2655, 6025, 3866, 2652, 39, 38, 2657], "orig_top_k_doc_id": [2648, 2649, 4513, 2653, 4512, 4514, 2654, 2656, 2655, 6025, 3866, 2652, 39, 38, 2657]}, {"qid": 2392, "question": "Which datasets are used for evaluation? in Explaining Recurrent Neural Network Predictions in Sentiment Analysis", "answer": ["Stanford Sentiment Treebank"], "top_k_doc_id": [2648, 256, 2888, 3487, 6692, 3865, 3731, 7237, 3863, 3501, 5485, 96, 5106, 2874, 1685], "orig_top_k_doc_id": [3487, 6692, 2648, 3865, 256, 3731, 7237, 3863, 3501, 5485, 96, 5106, 2874, 1685, 2888]}, {"qid": 2568, "question": "Do the experiments explore how various architectures and layers contribute towards certain decisions? in Explaining Predictions of Non-Linear Classifiers in NLP", "answer": ["No"], "top_k_doc_id": [2648, 256, 2888, 4512, 1809, 4816, 7514, 6560, 1492, 253, 254, 1664, 7011, 5977, 1921], "orig_top_k_doc_id": [256, 2648, 4512, 1809, 4816, 7514, 6560, 1492, 253, 254, 2888, 1664, 7011, 5977, 1921]}]}
{"group_id": 387, "group_size": 5, "items": [{"qid": 1998, "question": "What is the model accuracy? in Attributed Multi-Relational Attention Network for Fact-checking URL Recommendation", "answer": ["Overall, our AMRAN outperforms all baselines, achieving 0.657 HR@10 and 0.410 NDCG@10."], "top_k_doc_id": [3019, 3020, 2158, 3015, 3016, 3018, 3021, 3022, 466, 1835, 2571, 3017, 3287, 3288, 3860], "orig_top_k_doc_id": [3016, 3015, 3019, 3020, 3021, 3018, 3022, 3017, 3288, 1835, 466, 2571, 2158, 3860, 3287]}, {"qid": 2000, "question": "What dataset is used? in Attributed Multi-Relational Attention Network for Fact-checking URL Recommendation", "answer": ["Twitter dataset obtained from the authors of BIBREF12"], "top_k_doc_id": [3019, 3020, 2158, 3015, 3016, 3018, 3021, 3022, 466, 1835, 2571, 3017, 3287, 3288, 7671], "orig_top_k_doc_id": [3016, 3015, 3019, 3020, 3021, 3018, 3022, 3017, 3288, 2158, 1835, 3287, 466, 2571, 7671]}, {"qid": 1410, "question": "Is this a task other people have worked on? in CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation", "answer": ["No"], "top_k_doc_id": [3019, 3020, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 3034, 5281, 3021, 7502], "orig_top_k_doc_id": [1955, 1948, 1956, 1949, 1950, 1954, 1953, 1951, 3020, 1952, 3019, 5281, 3034, 3021, 7502]}, {"qid": 1411, "question": "Where did they get the data for this project? in CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation", "answer": ["Twitter"], "top_k_doc_id": [3019, 3020, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 3034, 5281, 6543, 2838], "orig_top_k_doc_id": [1955, 1956, 1948, 1949, 1954, 1950, 1953, 1951, 1952, 3020, 3019, 5281, 6543, 3034, 2838]}, {"qid": 1999, "question": "How do the authors define fake news? in Attributed Multi-Relational Attention Network for Fact-checking URL Recommendation", "answer": ["No"], "top_k_doc_id": [3019, 3020, 2158, 3015, 3016, 3018, 3021, 3022, 2157, 3860, 5783, 6743, 2159, 2160, 1494], "orig_top_k_doc_id": [3016, 3015, 2158, 3019, 3018, 3021, 3020, 3022, 2157, 3860, 5783, 6743, 2159, 2160, 1494]}]}
{"group_id": 388, "group_size": 5, "items": [{"qid": 2007, "question": "What accuracy do they approach with their proposed method? in VQABQ: Visual Question Answering by Basic Questions", "answer": ["our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%"], "top_k_doc_id": [3033, 3037, 1120, 3034, 3035, 3036, 7805, 787, 7147, 788, 510, 2661, 2737, 511, 1217], "orig_top_k_doc_id": [3033, 3037, 787, 3034, 788, 1120, 3036, 3035, 7805, 7147, 510, 2661, 2737, 511, 1217]}, {"qid": 2008, "question": "What they formulate the question generation as? in VQABQ: Visual Question Answering by Basic Questions", "answer": ["LASSO optimization problem"], "top_k_doc_id": [3033, 3037, 1120, 3034, 3035, 3036, 7805, 787, 7147, 2210, 7804, 3799, 491, 2414, 3175], "orig_top_k_doc_id": [3033, 3037, 3034, 2210, 7805, 3035, 7804, 3799, 3036, 1120, 787, 491, 2414, 7147, 3175]}, {"qid": 2006, "question": "In which setting they achieve the state of the art? in VQABQ: Visual Question Answering by Basic Questions", "answer": ["in open-ended task esp. for counting-type questions "], "top_k_doc_id": [3033, 3037, 1120, 3034, 3035, 3036, 7805, 2737, 4278, 1217, 160, 4523, 1527, 7163, 7800], "orig_top_k_doc_id": [3033, 3037, 3035, 3034, 3036, 2737, 4278, 1120, 1217, 160, 4523, 1527, 7163, 7800, 7805]}, {"qid": 2009, "question": "What two main modules their approach consists of? in VQABQ: Visual Question Answering by Basic Questions", "answer": ["the basic question generation module (Module 1) and co-attention visual question answering module (Module 2)"], "top_k_doc_id": [3033, 3037, 1120, 3034, 3035, 3036, 2737, 2733, 160, 791, 3175, 7147, 790, 2578, 7148], "orig_top_k_doc_id": [3033, 3037, 2737, 2733, 160, 3034, 791, 3175, 7147, 790, 2578, 7148, 1120, 3035, 3036]}, {"qid": 649, "question": "From when are many VQA datasets collected? in An Analysis of Visual Question Answering Algorithms", "answer": ["late 2014"], "top_k_doc_id": [3033, 3037, 787, 788, 7163, 5727, 7147, 7800, 867, 3175, 413, 791, 793, 3176, 5730], "orig_top_k_doc_id": [787, 788, 7163, 5727, 7147, 7800, 867, 3175, 413, 791, 3033, 793, 3176, 3037, 5730]}]}
{"group_id": 389, "group_size": 5, "items": [{"qid": 2048, "question": "How is faithfulness of the resulting text evaluated? in Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation", "answer": ["manually inspect"], "top_k_doc_id": [1888, 3107, 3109, 1082, 5518, 7448, 848, 252, 5230, 3719, 255, 7700, 3299, 96, 203], "orig_top_k_doc_id": [252, 3109, 1888, 5230, 3719, 848, 3107, 1082, 255, 7700, 7448, 5518, 3299, 96, 203]}, {"qid": 2050, "question": "What is the effectiveness plan generation? in Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation", "answer": ["clear mapping between plans and text helps to reduce these issues greatly, the system in BIBREF0 still has 2% errors, work in neural text generation and summarization attempt to address these issues"], "top_k_doc_id": [1888, 3107, 3109, 1082, 5518, 7448, 848, 7447, 3108, 4444, 6678, 7711, 5495, 1081, 2149], "orig_top_k_doc_id": [1888, 1082, 3107, 3109, 7448, 7447, 3108, 848, 4444, 6678, 7711, 5518, 5495, 1081, 2149]}, {"qid": 2049, "question": "How are typing hints suggested? in Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation", "answer": [" concatenating to the embedding vector"], "top_k_doc_id": [1888, 3107, 3109, 1082, 5518, 7448, 3108, 3225, 7688, 6237, 568, 4830, 4639, 3226, 4659], "orig_top_k_doc_id": [3108, 3109, 1888, 3225, 3107, 7688, 6237, 568, 4830, 4639, 1082, 3226, 4659, 5518, 7448]}, {"qid": 2051, "question": "How is neural planning component trained? in Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation", "answer": ["plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller"], "top_k_doc_id": [1888, 3107, 3109, 1081, 3108, 1171, 3299, 4444, 2519, 100, 6065, 4934, 7199, 4442, 7195], "orig_top_k_doc_id": [3107, 1081, 1888, 3108, 1171, 3299, 4444, 2519, 100, 3109, 6065, 4934, 7199, 4442, 7195]}, {"qid": 2047, "question": "How is fluency of generated text evaluated? in Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation", "answer": ["manually reviewed"], "top_k_doc_id": [1888, 3107, 1887, 7447, 4796, 848, 734, 6068, 110, 1082, 7721, 6338, 2261, 1866, 7791], "orig_top_k_doc_id": [1888, 1887, 7447, 4796, 848, 734, 3107, 6068, 110, 1082, 7721, 6338, 2261, 1866, 7791]}]}
{"group_id": 390, "group_size": 5, "items": [{"qid": 2078, "question": "What did they pretrain the model on? in Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "answer": ["hree years of online news articles from June 2016 to June 2019"], "top_k_doc_id": [3157, 3158, 3160, 7281, 7280, 1253, 7137, 3159, 3717, 4282, 4481, 3201, 3194, 3593, 1697], "orig_top_k_doc_id": [3160, 3157, 3158, 7281, 3159, 7137, 1253, 7280, 3201, 4481, 3717, 3194, 3593, 1697, 4282]}, {"qid": 2080, "question": "What unlabeled corpus did they use? in Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "answer": ["three years of online news articles from June 2016 to June 2019"], "top_k_doc_id": [3157, 3158, 3160, 7281, 7280, 1253, 7137, 3159, 3717, 4282, 4481, 5207, 110, 4577, 5140], "orig_top_k_doc_id": [3157, 3160, 3158, 7137, 7281, 4282, 3159, 5207, 1253, 110, 4481, 3717, 7280, 4577, 5140]}, {"qid": 2076, "question": "What were the baselines? in Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "answer": ["$\\textsc {Lead-X}$, $\\textsc {PTGen}$, $\\textsc {DRM}$, $\\textsc {TConvS2S}$,  $\\textsc {BottomUp}$, ABS, DRGD, SEQ$^3$, BottleSum, GPT-2"], "top_k_doc_id": [3157, 3158, 3160, 7281, 7280, 1253, 7137, 6955, 6564, 5140, 6227, 1254, 3200, 3201, 6327], "orig_top_k_doc_id": [3160, 3157, 3158, 7281, 6955, 7280, 6564, 5140, 1253, 6227, 1254, 7137, 3200, 3201, 6327]}, {"qid": 2079, "question": "What does the data cleaning and filtering process consist of? in Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "answer": ["many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total, we try to remove articles whose top three sentences may not form a relevant summary"], "top_k_doc_id": [3157, 3158, 3160, 7281, 7280, 6859, 3159, 3974, 4280, 6858, 4137, 2864, 7285, 6840, 1697], "orig_top_k_doc_id": [3157, 3158, 3160, 6859, 7281, 3159, 3974, 4280, 6858, 4137, 2864, 7280, 7285, 6840, 1697]}, {"qid": 2077, "question": "What metric was used in the evaluation step? in Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization", "answer": ["ROUGE-1, ROUGE-2 and ROUGE-L, F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC"], "top_k_doc_id": [3157, 3158, 3160, 7281, 5140, 3715, 1669, 6861, 6860, 5147, 5138, 5142, 3717, 6862, 3200], "orig_top_k_doc_id": [3160, 3157, 7281, 5140, 3715, 3158, 1669, 6861, 6860, 5147, 5138, 5142, 3717, 6862, 3200]}]}
{"group_id": 391, "group_size": 5, "items": [{"qid": 2109, "question": "How do they generate a graphic representation of a query from a query? in Explaining Queries over Web Tables to Non-Experts", "answer": ["No"], "top_k_doc_id": [3216, 3218, 3219, 3220, 3221, 3222, 3223, 3217, 6847, 6842, 6845, 4317, 4320, 4558, 334], "orig_top_k_doc_id": [3220, 3221, 3216, 3223, 3218, 3217, 3222, 6847, 6842, 3219, 6845, 4317, 4320, 4558, 334]}, {"qid": 2110, "question": "How do they gather data for the query explanation problem? in Explaining Queries over Web Tables to Non-Experts", "answer": ["hand crafted by users"], "top_k_doc_id": [3216, 3218, 3219, 3220, 3221, 3222, 3223, 3217, 6847, 6842, 6845, 4461, 6526, 7072, 3296], "orig_top_k_doc_id": [3220, 3216, 3221, 3223, 3218, 3217, 3222, 3219, 4461, 6842, 6847, 6526, 6845, 7072, 3296]}, {"qid": 2111, "question": "Which query explanation method was preffered by the users in terms of correctness? in Explaining Queries over Web Tables to Non-Experts", "answer": ["hybrid approach"], "top_k_doc_id": [3216, 3218, 3219, 3220, 3221, 3222, 3223, 3217, 6847, 5119, 7071, 7072, 7176, 4558, 7074], "orig_top_k_doc_id": [3221, 3220, 3223, 3222, 3218, 3216, 3219, 3217, 7071, 4558, 7072, 6847, 7074, 7176, 5119]}, {"qid": 2112, "question": "Do they conduct a user study where they show an NL interface with and without their explanation? in Explaining Queries over Web Tables to Non-Experts", "answer": ["No"], "top_k_doc_id": [3216, 3218, 3219, 3220, 3221, 3222, 3223, 3217, 6847, 5119, 7071, 7072, 7176, 6842, 4317], "orig_top_k_doc_id": [3216, 3220, 3221, 3223, 3222, 3218, 3219, 5119, 3217, 6847, 7072, 7176, 7071, 6842, 4317]}, {"qid": 2113, "question": "How do the users in the user studies evaluate reliability of a NL interface? in Explaining Queries over Web Tables to Non-Experts", "answer": ["No"], "top_k_doc_id": [3216, 3218, 3219, 3220, 3221, 3222, 3223, 5578, 3225, 5119, 7411, 581, 1835, 6177, 144], "orig_top_k_doc_id": [3216, 3220, 3223, 3221, 3218, 3222, 3219, 5578, 3225, 5119, 7411, 581, 1835, 6177, 144]}]}
{"group_id": 392, "group_size": 5, "items": [{"qid": 2118, "question": "Are results reported only for English data? in Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency", "answer": ["No"], "top_k_doc_id": [1970, 3229, 3230, 3231, 3232, 3233, 3234, 5893, 972, 6566, 7242, 1777, 6063, 7134, 6496], "orig_top_k_doc_id": [3229, 3231, 3234, 3232, 3230, 1970, 7242, 1777, 972, 6063, 3233, 5893, 7134, 6566, 6496]}, {"qid": 2119, "question": "Which existing models does this approach outperform? in Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency", "answer": ["RNN-context, SRB, CopyNet, RNN-distract, DRGD"], "top_k_doc_id": [1970, 3229, 3230, 3231, 3232, 3233, 3234, 5893, 972, 6566, 3157, 1132, 3489, 1969, 6036], "orig_top_k_doc_id": [3229, 3231, 3234, 3232, 3157, 3230, 3233, 1970, 972, 1132, 3489, 6566, 5893, 1969, 6036]}, {"qid": 1425, "question": "What models are evaluated with QAGS? in Asking and Answering Questions to Evaluate the Factual Consistency of Summaries", "answer": ["bert-large-wwm, bert-base, bert-large"], "top_k_doc_id": [1970, 491, 560, 1969, 1971, 1972, 1973, 3798, 5157, 2334, 3790, 5893, 3232, 3231, 1637], "orig_top_k_doc_id": [1969, 1970, 1972, 1973, 1971, 491, 560, 2334, 3790, 3798, 5157, 5893, 3232, 3231, 1637]}, {"qid": 1426, "question": "Do they use crowdsourcing to collect human judgements? in Asking and Answering Questions to Evaluate the Factual Consistency of Summaries", "answer": ["Yes"], "top_k_doc_id": [1970, 491, 560, 1969, 1971, 1972, 1973, 3798, 5157, 7615, 5610, 8, 2340, 7222, 3805], "orig_top_k_doc_id": [1969, 1970, 1973, 1971, 1972, 7615, 5610, 8, 2340, 5157, 7222, 491, 560, 3798, 3805]}, {"qid": 2120, "question": "What human evaluation method is proposed? in Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency", "answer": ["comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant"], "top_k_doc_id": [1970, 3229, 3230, 3231, 3232, 3233, 3234, 5893, 1969, 1132, 5554, 1694, 5142, 1135, 4619], "orig_top_k_doc_id": [3229, 3231, 3234, 3232, 3233, 3230, 1970, 1969, 5893, 1132, 5554, 1694, 5142, 1135, 4619]}]}
{"group_id": 393, "group_size": 5, "items": [{"qid": 2191, "question": "How were the feature representations evaluated? in Visualizing and Measuring the Geometry of BERT", "answer": ["attention probes, using visualizations of the activations created by different pieces of text"], "top_k_doc_id": [4603, 4604, 264, 3368, 3370, 3372, 2417, 4034, 3371, 290, 3030, 6167, 2900, 3031, 3026], "orig_top_k_doc_id": [4603, 3370, 3372, 4604, 2417, 3368, 4034, 3371, 264, 290, 3030, 6167, 2900, 3031, 3026]}, {"qid": 2192, "question": "What linguistic features were probed for? in Visualizing and Measuring the Geometry of BERT", "answer": ["dependency relation between two words, word sense"], "top_k_doc_id": [4603, 4604, 264, 3368, 3370, 3372, 4330, 398, 4912, 4007, 5702, 3500, 4701, 3369, 1560], "orig_top_k_doc_id": [3372, 4603, 3368, 4330, 398, 3370, 4912, 4007, 5702, 3500, 4701, 4604, 3369, 1560, 264]}, {"qid": 2613, "question": "What experiments are proposed to test that upper layers produce context-specific embeddings? in How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings", "answer": ["They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.", "They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."], "top_k_doc_id": [4603, 4604, 2241, 4330, 4605, 4606, 4607, 5292, 6093, 2242, 6697, 7630, 6698, 7789, 4331], "orig_top_k_doc_id": [4603, 4607, 4606, 4605, 4604, 6093, 4330, 2242, 6697, 5292, 7630, 2241, 6698, 7789, 4331]}, {"qid": 2614, "question": "How do they calculate a static embedding for each word? in How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings", "answer": ["They use the first principal component of a word's contextualized representation in a given layer as its static embedding.", " by taking the first principal component (PC) of its contextualized representations in a given layer"], "top_k_doc_id": [4603, 4604, 2241, 4330, 4605, 4606, 4607, 5292, 6093, 5805, 3370, 5172, 5341, 3499, 50], "orig_top_k_doc_id": [4603, 4604, 4607, 4605, 4606, 4330, 6093, 5805, 3370, 2241, 5172, 5341, 3499, 5292, 50]}, {"qid": 3659, "question": "What do they mean by intrinsic geometry of spaces of learned representations? in Improved Representation Learning for Predicting Commonsense Ontologies", "answer": ["In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.", "The intrinsic geometry is that the general concept embedding should be smaller than the specific concept embedding in every coordinate of the embeddings", "the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than local relation predictions"], "top_k_doc_id": [4603, 4604, 6002, 3207, 3372, 7275, 5305, 6050, 3750, 945, 3746, 3747, 3028, 690, 7517], "orig_top_k_doc_id": [6002, 3207, 3372, 7275, 5305, 6050, 4603, 3750, 4604, 945, 3746, 3747, 3028, 690, 7517]}]}
{"group_id": 394, "group_size": 5, "items": [{"qid": 2330, "question": "Do they measure the number of created No-Arc long sequences? in Non-Projective Dependency Parsing with Non-Local Transitions", "answer": ["No"], "top_k_doc_id": [3255, 3259, 1586, 3257, 3258, 3726, 3727, 4043, 4044, 1587, 3600, 3728, 4042, 4063, 3256], "orig_top_k_doc_id": [3728, 3726, 3727, 4044, 3600, 1587, 3257, 4043, 3255, 1586, 3258, 4063, 3259, 3256, 4042]}, {"qid": 2331, "question": "By how much does the new parser outperform the current state-of-the-art? in Non-Projective Dependency Parsing with Non-Local Transitions", "answer": ["Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS."], "top_k_doc_id": [3255, 3259, 1586, 3257, 3258, 3726, 3727, 4043, 4044, 1587, 3600, 3728, 4042, 4063, 3780], "orig_top_k_doc_id": [3727, 3726, 3728, 4044, 3259, 4043, 3258, 3257, 1586, 3255, 3600, 4063, 1587, 3780, 4042]}, {"qid": 2039, "question": "How much better is performance of proposed approach compared to greedy decoding baseline? in A Better Variant of Self-Critical Sequence Training", "answer": ["No"], "top_k_doc_id": [3255, 3259, 2024, 3092, 5283, 5284, 7602, 1526, 3093, 2028, 5635, 1258, 6934, 6782, 1621], "orig_top_k_doc_id": [3092, 1526, 3093, 3255, 5283, 2028, 5635, 1258, 2024, 6934, 7602, 5284, 3259, 6782, 1621]}, {"qid": 2135, "question": "What are performance compared to former models? in Global Greedy Dependency Parsing", "answer": ["model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF"], "top_k_doc_id": [3255, 3259, 1586, 3257, 3258, 3726, 3727, 4043, 4044, 3256, 896, 4825, 2024, 5284, 6853], "orig_top_k_doc_id": [3255, 3258, 3259, 3256, 1586, 896, 4825, 3257, 3726, 2024, 3727, 4043, 4044, 5284, 6853]}, {"qid": 2136, "question": "How faster is training and decoding compared to former models? in Global Greedy Dependency Parsing", "answer": ["Proposed vs best baseline:\nDecoding: 8541 vs 8532 tokens/sec\nTraining: 8h vs 8h"], "top_k_doc_id": [3255, 3259, 2024, 3092, 5283, 5284, 7602, 3258, 3257, 3256, 7319, 4825, 6767, 4043, 7320], "orig_top_k_doc_id": [3255, 3258, 3257, 3259, 3256, 2024, 7319, 4825, 3092, 6767, 4043, 5283, 7602, 5284, 7320]}]}
{"group_id": 395, "group_size": 5, "items": [{"qid": 2350, "question": "What is the reasoning method that is used? in Canonicalizing Knowledge Base Literals", "answer": ["SPARQL"], "top_k_doc_id": [4154, 1224, 3764, 3768, 3770, 6580, 1091, 2272, 7177, 7147, 946, 4462, 4463, 4277, 7176], "orig_top_k_doc_id": [3764, 3770, 7147, 3768, 6580, 2272, 946, 4154, 1224, 4462, 4463, 7177, 4277, 1091, 7176]}, {"qid": 2352, "question": "What's the precision of the system? in Canonicalizing Knowledge Base Literals", "answer": ["0.8320 on semantic typing, 0.7194 on entity matching"], "top_k_doc_id": [4154, 1224, 3764, 3768, 3770, 6580, 1091, 2272, 7177, 4440, 145, 353, 6579, 1586, 1098], "orig_top_k_doc_id": [3764, 3770, 3768, 4440, 6580, 7177, 1224, 145, 2272, 4154, 353, 1091, 6579, 1586, 1098]}, {"qid": 2351, "question": "What KB is used in this work? in Canonicalizing Knowledge Base Literals", "answer": ["DBpedia"], "top_k_doc_id": [4154, 1224, 3764, 3768, 3770, 6580, 3765, 4720, 4719, 3533, 4126, 4640, 7677, 4555, 144], "orig_top_k_doc_id": [3764, 3770, 1224, 3765, 3768, 4720, 4719, 3533, 4126, 4640, 7677, 4154, 4555, 144, 6580]}, {"qid": 3523, "question": "Were any of the pipeline components based on deep learning models? in From Textual Information Sources to Linked Data in the Agatha Project", "answer": ["No", "No"], "top_k_doc_id": [4154, 754, 1823, 4158, 4159, 5816, 5818, 7100, 7386, 7285, 3481, 2202, 2447, 2792, 2455], "orig_top_k_doc_id": [5818, 4159, 7386, 7100, 5816, 7285, 4158, 1823, 4154, 3481, 754, 2202, 2447, 2792, 2455]}, {"qid": 3524, "question": "How is the effectiveness of this pipeline approach evaluated? in From Textual Information Sources to Linked Data in the Agatha Project", "answer": ["proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents."], "top_k_doc_id": [4154, 754, 1823, 4158, 4159, 5816, 5818, 7100, 7386, 6467, 7671, 1519, 3538, 3623, 7385], "orig_top_k_doc_id": [5818, 5816, 4159, 1823, 4158, 7100, 4154, 7386, 6467, 7671, 1519, 3538, 754, 3623, 7385]}]}
{"group_id": 396, "group_size": 5, "items": [{"qid": 2451, "question": "What is the road exam metric? in Rethinking Exposure Bias In Language Modeling", "answer": ["a new metric to reveal a model's robustness against exposure bias"], "top_k_doc_id": [3979, 3985, 1922, 1923, 2938, 2939, 4071, 4072, 4073, 6899, 3100, 2225, 3098, 1798, 6898], "orig_top_k_doc_id": [4072, 4071, 3100, 4073, 2939, 3979, 3985, 2225, 2938, 1922, 1923, 3098, 1798, 6898, 6899]}, {"qid": 2452, "question": "What are the competing models? in Rethinking Exposure Bias In Language Modeling", "answer": ["TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN."], "top_k_doc_id": [3979, 3985, 1922, 1923, 2938, 2939, 4071, 4072, 4073, 6899, 5786, 7320, 4371, 5442, 7848], "orig_top_k_doc_id": [4071, 4073, 4072, 3979, 3985, 5786, 7320, 4371, 6899, 1922, 2938, 5442, 2939, 1923, 7848]}, {"qid": 2435, "question": "What architectural factors were investigated? in Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks", "answer": ["type of recurrent unit, type of attention, choice of sequential vs. tree-based model structure"], "top_k_doc_id": [3979, 3985, 3986, 3982, 3984, 3983, 3981, 7052, 5183, 2555, 3980, 2702, 2189, 7053, 1389], "orig_top_k_doc_id": [3985, 3979, 3986, 3982, 3984, 3983, 3981, 7052, 5183, 2555, 3980, 2702, 2189, 7053, 1389]}, {"qid": 3409, "question": "Can the findings of this paper be generalized to a general-purpose task? in The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models", "answer": ["Yes", "Yes"], "top_k_doc_id": [3979, 1139, 1237, 2148, 2701, 5638, 5639, 3985, 2584, 2583, 3984, 6368, 3611, 274, 2017], "orig_top_k_doc_id": [5638, 2701, 3985, 2584, 1237, 5639, 2583, 3979, 3984, 6368, 3611, 274, 1139, 2148, 2017]}, {"qid": 3410, "question": "Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks? in The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models", "answer": ["The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization."], "top_k_doc_id": [3979, 1139, 1237, 2148, 2701, 5638, 5639, 2708, 3023, 2146, 1618, 1744, 2682, 2149, 4760], "orig_top_k_doc_id": [5638, 2148, 2701, 1237, 2708, 1139, 3023, 2146, 1618, 1744, 2682, 2149, 3979, 4760, 5639]}]}
{"group_id": 397, "group_size": 5, "items": [{"qid": 2512, "question": "What exactly is new about this stochastic gradient descent algorithm? in Applications of Online Deep Learning for Crisis Response Using Social Media Information", "answer": ["CNN model can be trained in a purely online setting. We first initialize the model parameters $\\theta _0$ (line 1), which can be a trained model from other disaster events or it can be initialized randomly to start from scratch.\n\nAs a new batch of labeled tweets $B_t= \\lbrace \\mathbf {s}_1 \\ldots \\mathbf {s}_n \\rbrace $ arrives, we first compute the log-loss (cross entropy) in Equation 11 for $B_t$ with respect to the current parameters $\\theta _t$ (line 2a). Then, we use backpropagation to compute the gradients $f^{\\prime }(\\theta _{t})$ of the loss with respect to the current parameters (line 2b). Finally, we update the parameters with the learning rate $\\eta _t$ and the mean of the gradients (line 2c). We take the mean of the gradients to deal with minibatches of different sizes. Notice that we take only the current minibatch into account to get an updated model. "], "top_k_doc_id": [4279, 4280, 4284, 4287, 3926, 4288, 4322, 6005, 6991, 447, 955, 6107, 185, 4285, 5935], "orig_top_k_doc_id": [4284, 6005, 4287, 4280, 6991, 447, 4322, 955, 3926, 4288, 4279, 6107, 185, 4285, 5935]}, {"qid": 2526, "question": "what was their baseline comparison? in Rapid Classification of Crisis-Related Data on Social Networks using Convolutional Neural Networks", "answer": ["Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF)"], "top_k_doc_id": [4279, 4280, 4284, 4287, 3926, 4288, 4322, 6005, 6016, 3401, 3015, 2649, 4323, 3487, 96], "orig_top_k_doc_id": [4284, 4287, 4322, 6005, 3926, 4288, 6016, 3401, 3015, 4279, 4280, 2649, 4323, 3487, 96]}, {"qid": 2510, "question": "What were the model's results on flood detection? in Localized Flood DetectionWith Minimal Labeled Social Media Data Using Transfer Learning", "answer": ["Queensland flood which provided 96% accuracy, Alberta flood with the same configuration of train-test split which provided 95% accuracy"], "top_k_doc_id": [4279, 4280, 243, 3574, 4281, 4282, 4283, 4322, 1726, 4360, 6817, 4359, 7750, 6729, 3733], "orig_top_k_doc_id": [4282, 4283, 4280, 4281, 4279, 1726, 4360, 4322, 6817, 4359, 7750, 6729, 243, 3733, 3574]}, {"qid": 2511, "question": "What dataset did they use? in Localized Flood DetectionWith Minimal Labeled Social Media Data Using Transfer Learning", "answer": [" disaster data from BIBREF5, Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada"], "top_k_doc_id": [4279, 4280, 243, 3574, 4281, 4282, 4283, 4322, 5812, 2343, 770, 2051, 6285, 1481, 6005], "orig_top_k_doc_id": [4282, 4283, 4280, 4281, 4279, 243, 4322, 5812, 2343, 770, 2051, 6285, 1481, 3574, 6005]}, {"qid": 4786, "question": "How is representation learning decoupled from memory management in this architecture? in On the Unintended Social Bias of Training Language Generation Models with Data from Local Media", "answer": ["considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network", " based on the use of an external memory in which word embeddings are associated to gender information"], "top_k_doc_id": [4279, 4280, 4284, 4287, 7455, 6558, 6107, 3861, 2700, 3581, 7454, 930, 2343, 6543, 3357], "orig_top_k_doc_id": [7455, 6558, 4280, 6107, 3861, 4284, 4279, 2700, 3581, 7454, 930, 4287, 2343, 6543, 3357]}]}
{"group_id": 398, "group_size": 5, "items": [{"qid": 2630, "question": "What evaluations methods do they take? in Stereotyping and Bias in the Flickr30K Dataset", "answer": ["No", "No"], "top_k_doc_id": [4629, 4630, 4631, 718, 6700, 910, 2729, 694, 696, 697, 7270, 5949, 6651, 1244, 4785], "orig_top_k_doc_id": [4631, 4629, 4630, 2729, 718, 6700, 5949, 6651, 1244, 910, 697, 7270, 694, 4785, 696]}, {"qid": 2631, "question": "What is the size of the dataset? in Stereotyping and Bias in the Flickr30K Dataset", "answer": ["30,000", "collection of over 30,000 images with 5 crowdsourced descriptions each"], "top_k_doc_id": [4629, 4630, 4631, 718, 6700, 910, 2729, 694, 696, 697, 7270, 873, 114, 2901, 5969], "orig_top_k_doc_id": [4631, 4629, 4630, 6700, 2729, 697, 910, 873, 696, 114, 718, 7270, 694, 2901, 5969]}, {"qid": 2633, "question": "What biases are found in the dataset? in Stereotyping and Bias in the Flickr30K Dataset", "answer": ["Ethnic bias", "adjectives are used to create \u201cmore narrow labels [or subtypes] for individuals who do not fit with general social category expectations\u201d"], "top_k_doc_id": [4629, 4630, 4631, 718, 6700, 910, 2729, 1443, 787, 5011, 3985, 6560, 3048, 788, 3011], "orig_top_k_doc_id": [4631, 4629, 6700, 4630, 1443, 718, 787, 5011, 3985, 6560, 910, 3048, 2729, 788, 3011]}, {"qid": 2632, "question": "Which methods are considered to find examples of biases and unwarranted inferences?? in Stereotyping and Bias in the Flickr30K Dataset", "answer": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "top_k_doc_id": [4629, 4630, 4631, 718, 6700, 5250, 52, 3988, 1443, 5249, 586, 1445, 6558, 3354, 3953], "orig_top_k_doc_id": [4631, 4629, 4630, 5250, 6700, 718, 52, 3988, 1443, 5249, 586, 1445, 6558, 3354, 3953]}, {"qid": 2230, "question": "Which dataset do they use? in Harnessing the richness of the linguistic signal in predicting pragmatic inferences", "answer": ["the annotated dataset reported by degen2015investigating, a dataset of the utterances from the Switchboard corpus of telephone dialogues BIBREF21 that contain the word some"], "top_k_doc_id": [4629, 4630, 4631, 3502, 3498, 587, 3503, 742, 4230, 805, 3397, 4579, 743, 2124, 5250], "orig_top_k_doc_id": [3502, 3498, 587, 3503, 742, 4631, 4629, 4230, 805, 3397, 4630, 4579, 743, 2124, 5250]}]}
{"group_id": 399, "group_size": 5, "items": [{"qid": 2658, "question": "What datasets do they evaluate on? in Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "answer": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "No"], "top_k_doc_id": [3661, 3662, 3663, 4676, 1389, 2166, 4674, 4675, 4678, 4679, 1255, 2917, 3069, 4561, 3072], "orig_top_k_doc_id": [4679, 4676, 4674, 3663, 3662, 4675, 4678, 3661, 1389, 3069, 4561, 1255, 2917, 2166, 3072]}, {"qid": 2659, "question": "Do they evaluate only on English datasets? in Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "answer": ["Yes", "No"], "top_k_doc_id": [3661, 3662, 3663, 4676, 1389, 2166, 4674, 4675, 4678, 4679, 1255, 2917, 5417, 2918, 2906], "orig_top_k_doc_id": [4679, 4676, 3663, 4674, 3662, 4675, 4678, 3661, 5417, 1389, 2918, 2166, 2917, 1255, 2906]}, {"qid": 2302, "question": "How do they evaluate the sentence representations? in Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning", "answer": ["The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset., Supervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 ."], "top_k_doc_id": [3661, 3662, 3663, 4676, 1232, 1744, 3659, 3660, 7766, 7767, 4674, 6295, 730, 5019, 5845], "orig_top_k_doc_id": [3662, 3663, 3661, 3659, 3660, 7766, 4674, 7767, 1744, 6295, 4676, 730, 5019, 5845, 1232]}, {"qid": 2303, "question": "What are the two decoding functions? in Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning", "answer": ["a linear projection and a bijective function with continuous transformation though  \u2018affine coupling layer\u2019 of (Dinh et al.,2016). "], "top_k_doc_id": [3661, 3662, 3663, 4676, 1232, 1744, 3659, 3660, 7766, 7767, 1233, 4178, 7768, 4675, 7770], "orig_top_k_doc_id": [3659, 3662, 3663, 3660, 3661, 1233, 7767, 4676, 4178, 7768, 1232, 4675, 7766, 7770, 1744]}, {"qid": 2660, "question": "What is the invertibility condition? in Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "answer": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "top_k_doc_id": [3661, 3662, 3663, 4676, 1389, 2166, 4674, 4675, 4678, 4679, 3660, 703, 1390, 2615, 3659], "orig_top_k_doc_id": [4679, 4675, 4676, 4678, 3663, 3662, 3660, 4674, 3661, 1389, 703, 1390, 2615, 2166, 3659]}]}
{"group_id": 400, "group_size": 5, "items": [{"qid": 2675, "question": "After how many hops does accuracy decrease? in What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge", "answer": ["1-hop links to 2-hops", "one additional hop"], "top_k_doc_id": [4698, 4699, 4703, 4705, 4331, 4332, 4704, 6606, 781, 4328, 5873, 2101, 4702, 4700, 7677], "orig_top_k_doc_id": [4703, 4698, 4705, 4704, 4332, 4699, 4331, 2101, 4702, 4700, 4328, 781, 7677, 6606, 5873]}, {"qid": 2676, "question": "How do they control for annotation artificats? in What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge", "answer": [" we use several of the MCQA baseline models first introduced in BIBREF0", "Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score"], "top_k_doc_id": [4698, 4699, 4703, 4705, 4331, 4332, 4704, 6606, 781, 4328, 5873, 6653, 4333, 4330, 5578], "orig_top_k_doc_id": [4705, 4698, 4703, 4699, 4331, 4332, 4704, 6653, 4333, 4328, 5873, 781, 4330, 6606, 5578]}, {"qid": 2673, "question": "Are the automatically constructed datasets subject to quality control? in What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge", "answer": ["No", "No"], "top_k_doc_id": [4698, 4699, 4703, 4705, 4331, 4332, 4704, 6606, 781, 6829, 7072, 3070, 6828, 6653, 3069], "orig_top_k_doc_id": [4705, 4698, 4703, 4699, 4704, 6829, 4331, 4332, 7072, 6606, 3070, 6828, 6653, 781, 3069]}, {"qid": 2677, "question": "Is WordNet useful for taxonomic reasoning for this task? in What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge", "answer": ["No", "Yes"], "top_k_doc_id": [4698, 4699, 4703, 4705, 4331, 4332, 4704, 6606, 4700, 1090, 5873, 145, 5943, 5328, 346], "orig_top_k_doc_id": [4698, 4699, 4703, 4705, 4704, 4700, 4331, 4332, 1090, 5873, 145, 6606, 5943, 5328, 346]}, {"qid": 2674, "question": "Do they focus on Reading Comprehension or multiple choice question answering? in What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge", "answer": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "top_k_doc_id": [4698, 4699, 4703, 4705, 2519, 2661, 6879, 4189, 1146, 1512, 6256, 2234, 2910, 7359, 4257], "orig_top_k_doc_id": [4698, 4703, 4699, 4705, 2519, 2661, 6879, 4189, 1146, 1512, 6256, 2234, 2910, 7359, 4257]}]}
{"group_id": 401, "group_size": 5, "items": [{"qid": 2725, "question": "What is masked document generation? in STEP: Sequence-to-Sequence Transformer Pre-training for Document Summarization", "answer": ["A task for seq2seq model pra-training that recovers a masked document to its original form.", "recovers a masked document to its original form"], "top_k_doc_id": [730, 4760, 732, 4761, 4762, 5540, 6839, 1132, 4764, 4765, 731, 5806, 5541, 4763, 6060], "orig_top_k_doc_id": [4760, 4761, 4764, 4765, 4762, 732, 5540, 731, 5806, 5541, 6839, 1132, 4763, 730, 6060]}, {"qid": 2726, "question": "Which of the three pretraining tasks is the most helpful? in STEP: Sequence-to-Sequence Transformer Pre-training for Document Summarization", "answer": ["SR", "SR"], "top_k_doc_id": [730, 4760, 732, 4761, 4762, 5540, 6839, 1132, 4764, 4765, 3157, 3158, 7281, 734, 733], "orig_top_k_doc_id": [4760, 3157, 5540, 4761, 4764, 3158, 730, 4765, 1132, 7281, 4762, 6839, 732, 734, 733]}, {"qid": 590, "question": "Is the baseline a non-heirarchical model like BERT? in HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization", "answer": ["There were hierarchical and non-hierarchical baselines; BERT was one of those baselines"], "top_k_doc_id": [730, 4760, 732, 4761, 4762, 5540, 6839, 734, 735, 733, 731, 123, 4652, 5541, 4481], "orig_top_k_doc_id": [734, 735, 730, 733, 731, 732, 4760, 123, 4652, 5540, 5541, 4762, 6839, 4761, 4481]}, {"qid": 2644, "question": "What datasets did they use for evaluation? in Hierarchical Transformers for Long Document Classification", "answer": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "top_k_doc_id": [730, 4760, 123, 371, 734, 4652, 735, 6696, 6381, 6386, 7688, 6692, 5329, 6585, 5898], "orig_top_k_doc_id": [4652, 123, 735, 4760, 371, 734, 6696, 6381, 730, 6386, 7688, 6692, 5329, 6585, 5898]}, {"qid": 2645, "question": "On top of BERT does the RNN layer work better or the transformer layer? in Hierarchical Transformers for Long Document Classification", "answer": ["Transformer over BERT (ToBERT)", "The transformer layer"], "top_k_doc_id": [730, 4760, 123, 371, 734, 4652, 3010, 5541, 5540, 1560, 4653, 1683, 3273, 1988, 6839], "orig_top_k_doc_id": [4652, 3010, 371, 5541, 5540, 730, 4760, 1560, 4653, 1683, 123, 734, 3273, 1988, 6839]}]}
{"group_id": 402, "group_size": 5, "items": [{"qid": 2752, "question": "Do they report results only on English? in NeuronBlocks: Building Your NLP DNN Models Like Playing Lego", "answer": ["No", "Yes"], "top_k_doc_id": [4441, 558, 4444, 4816, 4817, 1488, 2915, 5814, 1933, 4199, 6485, 4550, 4288, 382, 4015], "orig_top_k_doc_id": [4817, 4816, 4441, 558, 1488, 5814, 4444, 2915, 1933, 4199, 6485, 4550, 4288, 382, 4015]}, {"qid": 2753, "question": "What neural network modules are included in NeuronBlocks? in NeuronBlocks: Building Your NLP DNN Models Like Playing Lego", "answer": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "top_k_doc_id": [4441, 558, 4444, 4816, 4817, 1488, 2915, 2733, 2738, 3186, 4424, 7839, 150, 7162, 2484], "orig_top_k_doc_id": [4816, 4817, 4441, 558, 4444, 2733, 2738, 1488, 3186, 4424, 2915, 7839, 150, 7162, 2484]}, {"qid": 862, "question": "What is the architecture of the model? in Classifying topics in speech when all you have is crummy translations.", "answer": ["BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14"], "top_k_doc_id": [4441, 1111, 1112, 1113, 1114, 2074, 4442, 5385, 5984, 7534, 2075, 4444, 1927, 876, 715], "orig_top_k_doc_id": [1111, 4441, 1112, 1113, 4442, 2074, 5984, 5385, 1114, 7534, 2075, 4444, 1927, 876, 715]}, {"qid": 863, "question": "What language do they look at? in Classifying topics in speech when all you have is crummy translations.", "answer": ["Spanish"], "top_k_doc_id": [4441, 1111, 1112, 1113, 1114, 2074, 4442, 5385, 5984, 7534, 6132, 5913, 6190, 4577, 3972], "orig_top_k_doc_id": [4441, 1111, 1112, 2074, 1113, 1114, 6132, 4442, 7534, 5913, 6190, 4577, 5984, 3972, 5385]}, {"qid": 2754, "question": "How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques? in NeuronBlocks: Building Your NLP DNN Models Like Playing Lego", "answer": ["By conducting a survey among engineers", "No"], "top_k_doc_id": [4441, 558, 4444, 4816, 4817, 7813, 277, 7814, 4413, 5380, 7817, 1649, 4424, 3374, 6414], "orig_top_k_doc_id": [4816, 4817, 4444, 7813, 277, 7814, 4441, 4413, 5380, 7817, 1649, 558, 4424, 3374, 6414]}]}
{"group_id": 403, "group_size": 5, "items": [{"qid": 2757, "question": "Is pre-training effective in their evaluation? in English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor", "answer": ["Yes", "Yes"], "top_k_doc_id": [7348, 7347, 7349, 4387, 4822, 4823, 4824, 5835, 7350, 3830, 6975, 5839, 2373, 6039, 7661], "orig_top_k_doc_id": [4822, 4824, 4823, 7348, 7347, 7349, 5835, 7350, 3830, 6975, 5839, 2373, 4387, 6039, 7661]}, {"qid": 2758, "question": "What parallel corpus did they use? in English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor", "answer": ["Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1", "Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus "], "top_k_doc_id": [7348, 7347, 7349, 4387, 4822, 4823, 4824, 5835, 274, 5838, 7434, 3687, 7669, 2906, 7346], "orig_top_k_doc_id": [4823, 4822, 4824, 7347, 7348, 7349, 5835, 4387, 274, 5838, 7434, 3687, 7669, 2906, 7346]}, {"qid": 4718, "question": "Is Chinese a pro-drop language? in One Model to Learn Both: Zero Pronoun Prediction and Translation", "answer": ["Yes", "Yes"], "top_k_doc_id": [7348, 7347, 7349, 1557, 2847, 6042, 7267, 7270, 7346, 4752, 1874, 6063, 148, 1778, 2135], "orig_top_k_doc_id": [7346, 7347, 7348, 7267, 7270, 7349, 4752, 1874, 1557, 6063, 2847, 148, 1778, 2135, 6042]}, {"qid": 4719, "question": "Is English a pro-drop language? in One Model to Learn Both: Zero Pronoun Prediction and Translation", "answer": ["No", "No"], "top_k_doc_id": [7348, 7347, 7349, 1557, 2847, 6042, 7267, 7270, 7346, 7269, 6041, 4570, 5773, 7249, 1350], "orig_top_k_doc_id": [7346, 7347, 7267, 7270, 7348, 1557, 6042, 7269, 6041, 4570, 5773, 2847, 7349, 7249, 1350]}, {"qid": 3955, "question": "what are the state of the art models? in Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese", "answer": ["The character-aware neural language model BIBREF1, Hierarchical attention networks BIBREF10, FastText BIBREF11 ", "character-aware neural language model BIBREF1, Hierarchical attention networks BIBREF10, FastText BIBREF11", "The character-aware neural language model, Hierarchical attention networks, FastText"], "top_k_doc_id": [7348, 6387, 6385, 6386, 309, 729, 2590, 4822, 861, 6630, 2219, 368, 2220, 5242, 728], "orig_top_k_doc_id": [6387, 6385, 6386, 309, 729, 7348, 2590, 4822, 861, 6630, 2219, 368, 2220, 5242, 728]}]}
{"group_id": 404, "group_size": 5, "items": [{"qid": 2989, "question": "Do the authors report results only on English data? in Revisiting Summarization Evaluation for Scientific Articles", "answer": ["Yes", "No"], "top_k_doc_id": [5147, 5148, 5149, 5152, 2334, 4826, 5150, 1921, 6226, 6227, 6955, 3716, 4825, 3717, 3715], "orig_top_k_doc_id": [5147, 6227, 1921, 5148, 6226, 2334, 5150, 6955, 4826, 5149, 5152, 3716, 4825, 3717, 3715]}, {"qid": 2990, "question": "In the proposed metric, how is content relevance measured? in Revisiting Summarization Evaluation for Scientific Articles", "answer": ["The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. ", "On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval."], "top_k_doc_id": [5147, 5148, 5149, 5152, 2334, 4826, 5150, 1921, 6226, 6227, 6053, 5127, 3234, 2342, 1866], "orig_top_k_doc_id": [5147, 5148, 5150, 5152, 5149, 2334, 6053, 6226, 1921, 4826, 6227, 5127, 3234, 2342, 1866]}, {"qid": 2991, "question": "What different correlations result when using different variants of ROUGE scores? in Revisiting Summarization Evaluation for Scientific Articles", "answer": ["we observe that many variants of Rouge scores do not have high correlations with human pyramid scores", "Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878."], "top_k_doc_id": [5147, 5148, 5149, 5152, 5151, 6860, 6861, 6924, 5150, 1971, 1972, 6326, 3715, 3717, 4829], "orig_top_k_doc_id": [5147, 5151, 5152, 5150, 5148, 5149, 1971, 6860, 1972, 6326, 3715, 6924, 6861, 3717, 4829]}, {"qid": 2992, "question": "What manual Pyramid scores are used? in Revisiting Summarization Evaluation for Scientific Articles", "answer": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "top_k_doc_id": [5147, 5148, 5149, 5152, 2334, 4826, 5150, 5151, 264, 3157, 266, 6861, 265, 4825, 1028], "orig_top_k_doc_id": [5147, 5149, 5150, 5151, 5152, 264, 5148, 3157, 266, 4826, 6861, 2334, 265, 4825, 1028]}, {"qid": 2993, "question": "What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable' in Revisiting Summarization Evaluation for Scientific Articles", "answer": ["correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization"], "top_k_doc_id": [5147, 5148, 5149, 5152, 5151, 6860, 6861, 6924, 1134, 235, 4107, 2555, 6345, 3936, 3160], "orig_top_k_doc_id": [5147, 6861, 1134, 5152, 235, 5148, 6860, 5151, 5149, 4107, 2555, 6924, 6345, 3936, 3160]}]}
{"group_id": 405, "group_size": 5, "items": [{"qid": 2995, "question": "what dataset was used? in Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "answer": ["Daily Mail news articles released by BIBREF9 ", "Daily Mail news articles"], "top_k_doc_id": [5153, 5155, 7267, 52, 3941, 7268, 3010, 3011, 5154, 6737, 6560, 6738, 525, 5949, 7064], "orig_top_k_doc_id": [5153, 5155, 52, 6737, 525, 3011, 7267, 5949, 5154, 3010, 7268, 6560, 6738, 3941, 7064]}, {"qid": 2998, "question": "what bias evaluation metrics are used? in Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "answer": ["No", "gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1"], "top_k_doc_id": [5153, 5155, 7267, 52, 3941, 7268, 3010, 3011, 5154, 6737, 6560, 6738, 53, 42, 6735], "orig_top_k_doc_id": [5153, 5155, 5154, 52, 7267, 6737, 3941, 3010, 53, 3011, 42, 6560, 6735, 7268, 6738]}, {"qid": 2994, "question": "which existing strategies are compared? in Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "answer": ["CDA, REG", "No"], "top_k_doc_id": [5153, 5155, 7267, 52, 3941, 7268, 3010, 3011, 5154, 6737, 525, 718, 6965, 5949, 3942], "orig_top_k_doc_id": [5153, 5155, 3011, 5154, 3941, 6737, 52, 525, 7267, 3010, 718, 7268, 6965, 5949, 3942]}, {"qid": 4651, "question": "What metrics are used to measure bias reduction? in Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem", "answer": ["Accuracy, $\\mathbf {\\Delta G}$, $\\mathbf {\\Delta S}$, BLEU", "$\\mathbf {\\Delta G}$ \u2013 difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities, $\\mathbf {\\Delta S}$ \u2013 difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities"], "top_k_doc_id": [5153, 5155, 7267, 52, 3941, 7268, 7266, 7271, 6735, 6965, 7064, 5768, 6558, 7269, 53], "orig_top_k_doc_id": [7266, 7268, 7267, 52, 7271, 5153, 6735, 6965, 5155, 7064, 5768, 6558, 7269, 53, 3941]}, {"qid": 2996, "question": "what kinds of male and female words are looked at? in Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function", "answer": ["gendered word pairs like he and she"], "top_k_doc_id": [5153, 5155, 7267, 6737, 6341, 5154, 7063, 5949, 7064, 6738, 1444, 6343, 3354, 1900, 53], "orig_top_k_doc_id": [5153, 6737, 6341, 5154, 5155, 7267, 7063, 5949, 7064, 6738, 1444, 6343, 3354, 1900, 53]}]}
{"group_id": 406, "group_size": 5, "items": [{"qid": 3000, "question": "What baselines are presented? in ReviewQA: a relational aspect-based opinion reading dataset", "answer": ["Logistic regression, LSTM, End-to-end memory networks, Deep projective reader", "Logistic regression, LSTM, End-to-end memory networks, Deep projective reader"], "top_k_doc_id": [2264, 5157, 5158, 5159, 5161, 896, 6472, 2873, 2874, 2875, 3129, 6183, 3154, 4510, 5901], "orig_top_k_doc_id": [5159, 5161, 5158, 6183, 6472, 2874, 5157, 3129, 2264, 2875, 896, 2873, 3154, 4510, 5901]}, {"qid": 3001, "question": "What tasks were evaluated? in ReviewQA: a relational aspect-based opinion reading dataset", "answer": ["ReviewQA's test set", "Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review"], "top_k_doc_id": [2264, 5157, 5158, 5159, 5161, 896, 6472, 2873, 2874, 2875, 3129, 6183, 1155, 598, 3130], "orig_top_k_doc_id": [5159, 5161, 5158, 2874, 5157, 6472, 2264, 896, 1155, 2873, 598, 6183, 3129, 3130, 2875]}, {"qid": 3002, "question": "What language are the reviews in? in ReviewQA: a relational aspect-based opinion reading dataset", "answer": ["No", "English"], "top_k_doc_id": [2264, 5157, 5158, 5159, 5161, 896, 6472, 2873, 2874, 2875, 3129, 3102, 6181, 897, 6640], "orig_top_k_doc_id": [5159, 5158, 5161, 2264, 5157, 2874, 6472, 3129, 896, 3102, 2875, 6181, 2873, 897, 6640]}, {"qid": 2999, "question": "What kind of questions are present in the dataset? in ReviewQA: a relational aspect-based opinion reading dataset", "answer": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "top_k_doc_id": [2264, 5157, 5158, 5159, 5161, 896, 6472, 242, 3155, 7572, 5900, 3154, 6181, 4577, 5901], "orig_top_k_doc_id": [5159, 5158, 5161, 5157, 2264, 242, 3155, 7572, 5900, 6472, 3154, 6181, 4577, 896, 5901]}, {"qid": 3003, "question": "Where are the hotel reviews from? in ReviewQA: a relational aspect-based opinion reading dataset", "answer": ["TripAdvisor", "TripAdvisor"], "top_k_doc_id": [2264, 5157, 5158, 5159, 5161, 6182, 3129, 6181, 6183, 2216, 6548, 6547, 3102, 895, 1043], "orig_top_k_doc_id": [5159, 5158, 5161, 6182, 3129, 6181, 6183, 5157, 2264, 2216, 6548, 6547, 3102, 895, 1043]}]}
{"group_id": 407, "group_size": 5, "items": [{"qid": 3004, "question": "What was the baseline used? in Artificial Error Generation with Machine Translation and Syntactic Patterns", "answer": ["error detection system by Rei2016", "error detection system by Rei2016"], "top_k_doc_id": [3173, 3938, 5162, 5163, 5164, 2906, 1163, 2135, 5456, 6591, 1185, 4863, 7437, 820, 6595], "orig_top_k_doc_id": [5163, 5162, 5164, 3938, 2135, 1163, 3173, 6591, 1185, 4863, 5456, 2906, 7437, 820, 6595]}, {"qid": 3005, "question": "What are their results on both datasets? in Artificial Error Generation with Machine Translation and Syntactic Patterns", "answer": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "top_k_doc_id": [3173, 3938, 5162, 5163, 5164, 2906, 1163, 2135, 5456, 6591, 153, 6443, 3174, 4766, 2136], "orig_top_k_doc_id": [5164, 5163, 5162, 3173, 2135, 3938, 1163, 6591, 5456, 2906, 153, 6443, 3174, 4766, 2136]}, {"qid": 3007, "question": "Which annotated corpus did they use? in Artificial Error Generation with Machine Translation and Syntactic Patterns", "answer": [" FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) ", "FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset"], "top_k_doc_id": [3173, 3938, 5162, 5163, 5164, 2906, 1163, 2135, 2136, 3298, 6443, 4863, 6096, 4351, 6595], "orig_top_k_doc_id": [5162, 5164, 5163, 3173, 2906, 2135, 1163, 2136, 3298, 6443, 4863, 6096, 3938, 4351, 6595]}, {"qid": 3008, "question": "Which languages are explored in this paper? in Artificial Error Generation with Machine Translation and Syntactic Patterns", "answer": ["English ", "English "], "top_k_doc_id": [3173, 3938, 5162, 5163, 5164, 2906, 7429, 6595, 7156, 2136, 5599, 564, 897, 650, 7042], "orig_top_k_doc_id": [5164, 5162, 5163, 3938, 7429, 6595, 7156, 2136, 5599, 2906, 564, 897, 3173, 650, 7042]}, {"qid": 3006, "question": "What textual patterns are extracted? in Artificial Error Generation with Machine Translation and Syntactic Patterns", "answer": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "top_k_doc_id": [3173, 3938, 5162, 5163, 5164, 5071, 153, 7302, 7306, 4351, 2416, 897, 4269, 1163, 3161], "orig_top_k_doc_id": [5164, 5163, 5162, 5071, 153, 7302, 3938, 7306, 4351, 2416, 897, 4269, 1163, 3173, 3161]}]}
{"group_id": 408, "group_size": 5, "items": [{"qid": 3024, "question": "How were the tweets annotated? in Stance Detection in Turkish Tweets", "answer": ["tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbah\u00e7e", "No"], "top_k_doc_id": [5180, 5181, 5182, 7598, 7599, 4392, 5570, 4113, 4114, 2829, 4302, 6180, 7597, 6741, 2830], "orig_top_k_doc_id": [5180, 5181, 5182, 7599, 7598, 5570, 2829, 4392, 4302, 6180, 7597, 6741, 2830, 4113, 4114]}, {"qid": 3027, "question": "How many tweets did they collect? in Stance Detection in Turkish Tweets", "answer": ["700 ", "700"], "top_k_doc_id": [5180, 5181, 5182, 7598, 7599, 4392, 5570, 4113, 4114, 4137, 2828, 2396, 7499, 6520, 331], "orig_top_k_doc_id": [5180, 5181, 5182, 7599, 5570, 7598, 4113, 4137, 2828, 2396, 4114, 4392, 7499, 6520, 331]}, {"qid": 3025, "question": "Which SVM approach resulted in the best performance? in Stance Detection in Turkish Tweets", "answer": ["Target-1"], "top_k_doc_id": [5180, 5181, 5182, 7598, 7599, 3543, 6666, 4392, 3287, 2832, 6179, 2834, 6665, 2833, 5420], "orig_top_k_doc_id": [5180, 5181, 5182, 7599, 4392, 3287, 3543, 2832, 6179, 7598, 2834, 6665, 6666, 2833, 5420]}, {"qid": 3026, "question": "What are hashtag features? in Stance Detection in Turkish Tweets", "answer": ["hashtag features contain whether there is any hashtag in the tweet", "No"], "top_k_doc_id": [5180, 5181, 5182, 7598, 7599, 3543, 6666, 6457, 2828, 6667, 4302, 4136, 5256, 6173, 2329], "orig_top_k_doc_id": [5182, 5180, 7598, 5181, 7599, 6457, 3543, 2828, 6667, 4302, 6666, 4136, 5256, 6173, 2329]}, {"qid": 3028, "question": "Which sports clubs are the targets? in Stance Detection in Turkish Tweets", "answer": ["Galatasaray, Fenerbah\u00e7e", "Galatasaray , Fenerbah\u00e7e "], "top_k_doc_id": [5180, 5181, 5182, 7598, 7599, 4392, 5570, 3576, 5573, 5169, 5574, 2874, 2873, 6157, 4302], "orig_top_k_doc_id": [5180, 5181, 5182, 7599, 7598, 5570, 3576, 5573, 5169, 5574, 2874, 2873, 6157, 4302, 4392]}]}
{"group_id": 409, "group_size": 5, "items": [{"qid": 3029, "question": "Does this method help in sentiment classification task improvement? in Shallow Syntax in Deep Water", "answer": ["Yes", "No"], "top_k_doc_id": [5183, 5184, 5185, 7762, 3039, 3042, 485, 3826, 7168, 5105, 3043, 978, 460, 4678, 5980], "orig_top_k_doc_id": [5184, 5185, 3042, 3826, 7168, 7762, 5105, 3043, 978, 5183, 460, 3039, 485, 4678, 5980]}, {"qid": 3030, "question": "For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo\u2019s embedding? in Shallow Syntax in Deep Water", "answer": ["performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks", "3"], "top_k_doc_id": [5183, 5184, 5185, 7762, 3039, 3042, 485, 3826, 4333, 1256, 1743, 2917, 4332, 4331, 3598], "orig_top_k_doc_id": [5185, 5183, 3042, 3039, 7762, 4333, 5184, 3826, 1256, 1743, 2917, 4332, 4331, 3598, 485]}, {"qid": 3033, "question": "Which syntactic features are obtained automatically on downstream task data? in Shallow Syntax in Deep Water", "answer": ["token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels"], "top_k_doc_id": [5183, 5184, 5185, 7762, 3039, 3042, 3147, 1872, 7332, 3043, 3148, 1389, 7383, 863, 5303], "orig_top_k_doc_id": [5183, 5184, 5185, 3039, 3147, 1872, 3042, 7332, 3043, 3148, 1389, 7762, 7383, 863, 5303]}, {"qid": 3032, "question": "What are improvements for these two approaches relative to ELMo-only baselines? in Shallow Syntax in Deep Water", "answer": ["only modest gains on three of the four downstream tasks", " the performance differences across all tasks are small enough "], "top_k_doc_id": [5183, 5184, 5185, 7762, 1260, 3609, 864, 7763, 863, 1256, 5517, 1743, 4561, 6299, 1329], "orig_top_k_doc_id": [5183, 5184, 5185, 1260, 3609, 864, 7763, 863, 1256, 5517, 1743, 7762, 4561, 6299, 1329]}, {"qid": 3031, "question": "What are the black-box probes used? in Shallow Syntax in Deep Water", "answer": ["CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection", "Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases."], "top_k_doc_id": [5183, 5184, 5185, 1643, 7763, 4331, 4203, 4332, 4704, 4204, 4202, 6864, 6613, 6863, 5240], "orig_top_k_doc_id": [5183, 5185, 5184, 1643, 7763, 4331, 4203, 4332, 4704, 4204, 4202, 6864, 6613, 6863, 5240]}]}
{"group_id": 410, "group_size": 5, "items": [{"qid": 3034, "question": "Do they report results only on English data? in Open Event Extraction from Online Text using a Generative Adversarial Network", "answer": ["No", "No"], "top_k_doc_id": [5186, 5190, 5187, 5472, 6589, 6606, 7351, 3450, 3927, 5681, 1794, 5504, 4288, 4159, 1363], "orig_top_k_doc_id": [5186, 5187, 3450, 5190, 5681, 5472, 7351, 6589, 1794, 3927, 5504, 4288, 4159, 6606, 1363]}, {"qid": 3036, "question": "What datasets are used? in Open Event Extraction from Online Text using a Generative Adversarial Network", "answer": ["FSD BIBREF12 , Twitter, and Google datasets", "FSD dataset, Twitter dataset, Google dataset"], "top_k_doc_id": [5186, 5190, 5187, 5472, 6589, 6606, 7351, 3450, 3927, 5681, 5189, 7382, 5649, 4154, 1364], "orig_top_k_doc_id": [5186, 5187, 5190, 5189, 7382, 5681, 5649, 5472, 6606, 3450, 4154, 1364, 7351, 3927, 6589]}, {"qid": 3035, "question": "What baseline approaches does this approach out-perform? in Open Event Extraction from Online Text using a Generative Adversarial Network", "answer": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "top_k_doc_id": [5186, 5190, 5187, 5472, 6589, 6606, 7351, 5649, 5189, 4135, 7472, 1007, 4353, 1005, 4322], "orig_top_k_doc_id": [5186, 5187, 5190, 5649, 5189, 5472, 7351, 4135, 7472, 6589, 6606, 1007, 4353, 1005, 4322]}, {"qid": 3038, "question": "How does this model overcome the assumption that all words in a document are generated from a single event? in Open Event Extraction from Online Text using a Generative Adversarial Network", "answer": ["flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns", "by learning a projection function between the document-event distribution and four event related word distributions "], "top_k_doc_id": [5186, 5190, 5187, 5189, 6889, 7820, 4352, 2405, 6892, 4347, 6603, 5751, 4359, 2621, 7382], "orig_top_k_doc_id": [5186, 5187, 5190, 5189, 6889, 7820, 4352, 2405, 6892, 4347, 6603, 5751, 4359, 2621, 7382]}, {"qid": 2529, "question": "Which datasets are used in this work? in Detecting and Extracting Events from Text Documents", "answer": ["GENIA corpus"], "top_k_doc_id": [5186, 5190, 4131, 5189, 4334, 4140, 3161, 7411, 7314, 3623, 4359, 2105, 4279, 6916, 5647], "orig_top_k_doc_id": [4131, 5189, 5190, 4334, 4140, 5186, 3161, 7411, 7314, 3623, 4359, 2105, 4279, 6916, 5647]}]}
{"group_id": 411, "group_size": 5, "items": [{"qid": 3044, "question": "Do they report results only on English data? in Emotion Detection in Text: Focusing on Latent Representation", "answer": ["Yes", "No"], "top_k_doc_id": [4741, 5197, 5105, 3622, 4740, 1967, 2968, 3543, 5973, 1979, 4742, 1981, 1982, 5199, 5881], "orig_top_k_doc_id": [5197, 5105, 5973, 4741, 4740, 1979, 2968, 4742, 3543, 1981, 3622, 1982, 5199, 5881, 1967]}, {"qid": 3047, "question": "What data is used in experiments? in Emotion Detection in Text: Focusing on Latent Representation", "answer": ["Wang et al., CrowdFlower dataset ", " tweet dataset created by Wang et al. , CrowdFlower dataset"], "top_k_doc_id": [4741, 5197, 5105, 3622, 4740, 1967, 2968, 3543, 59, 7008, 1329, 757, 756, 4268, 6610], "orig_top_k_doc_id": [5197, 4741, 5105, 4740, 59, 7008, 3543, 1329, 757, 3622, 756, 2968, 4268, 1967, 6610]}, {"qid": 3046, "question": "What baseline is used? in Emotion Detection in Text: Focusing on Latent Representation", "answer": [" Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets", "Wang et al. , maximum entropy classifier with bag of words model"], "top_k_doc_id": [4741, 5197, 5105, 3622, 4740, 5410, 3623, 4268, 59, 2970, 5199, 4742, 1320, 6016, 757], "orig_top_k_doc_id": [5197, 4741, 5410, 4740, 3622, 3623, 4268, 59, 2970, 5199, 4742, 1320, 6016, 5105, 757]}, {"qid": 3045, "question": "What are the hyperparameters of the bi-GRU? in Emotion Detection in Text: Focusing on Latent Representation", "answer": ["They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%."], "top_k_doc_id": [4741, 5197, 5105, 3165, 7009, 756, 6016, 7008, 1320, 77, 757, 1079, 2968, 2970, 3164], "orig_top_k_doc_id": [3165, 7009, 5197, 756, 6016, 7008, 1320, 77, 757, 1079, 5105, 4741, 2968, 2970, 3164]}, {"qid": 3048, "question": "What meaningful information does the GRU model capture, which traditional ML models do not? in Emotion Detection in Text: Focusing on Latent Representation", "answer": [" the context and sequential nature of the text", "information about the context and sequential nature of the text"], "top_k_doc_id": [4741, 5197, 2968, 4740, 7323, 7372, 5198, 4359, 1329, 2396, 2106, 756, 5199, 7008, 5973], "orig_top_k_doc_id": [5197, 2968, 4741, 4740, 7323, 7372, 5198, 4359, 1329, 2396, 2106, 756, 5199, 7008, 5973]}]}
{"group_id": 412, "group_size": 5, "items": [{"qid": 3090, "question": "Do they compare human-level performance to model performance for their dataset? in Adversarial NLI: A New Benchmark for Natural Language Understanding", "answer": ["No", "No"], "top_k_doc_id": [872, 2746, 5246, 4698, 5251, 5868, 1144, 4415, 7664, 2676, 4204, 5250, 5562, 3618, 5100], "orig_top_k_doc_id": [5246, 5251, 5868, 2746, 872, 4698, 1144, 2676, 4204, 5250, 4415, 5562, 3618, 7664, 5100]}, {"qid": 3091, "question": "What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models? in Adversarial NLI: A New Benchmark for Natural Language Understanding", "answer": ["state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotators\u2014be they seasoned NLP researchers or non-experts\u2014might easily be able to construct examples that expose model brittleness"], "top_k_doc_id": [872, 2746, 5246, 4698, 5251, 5868, 583, 4699, 6135, 5249, 5248, 875, 3069, 4415, 256], "orig_top_k_doc_id": [5251, 5246, 5249, 2746, 872, 4699, 5868, 4698, 5248, 6135, 875, 3069, 4415, 256, 583]}, {"qid": 3092, "question": "What data sources do they use for creating their dataset? in Adversarial NLI: A New Benchmark for Natural Language Understanding", "answer": ["Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5", "Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset"], "top_k_doc_id": [872, 2746, 5246, 4698, 5251, 5868, 583, 4699, 6135, 2661, 5562, 4201, 5702, 5209, 3582], "orig_top_k_doc_id": [5246, 4699, 5251, 872, 4698, 2746, 5868, 2661, 5562, 4201, 5702, 583, 5209, 6135, 3582]}, {"qid": 3093, "question": "Do they use active learning to create their dataset? in Adversarial NLI: A New Benchmark for Natural Language Understanding", "answer": ["Yes", "No"], "top_k_doc_id": [872, 2746, 5246, 4698, 5251, 5868, 1144, 4415, 7664, 1747, 3953, 3445, 583, 6135, 3617], "orig_top_k_doc_id": [5246, 5251, 4415, 5868, 872, 1747, 2746, 3953, 3445, 583, 1144, 6135, 4698, 3617, 7664]}, {"qid": 4775, "question": "How much improvement did they see on the NLI task? in What can we learn from Semantic Tagging?", "answer": ["0.5 improvement with LWS over the single-task model", "Accuracy: SNLI - .5, SICK-E - 3.27"], "top_k_doc_id": [872, 2746, 5246, 7441, 7440, 7439, 1145, 256, 5021, 5019, 438, 2064, 4699, 1874, 4702], "orig_top_k_doc_id": [7441, 7440, 7439, 1145, 2746, 256, 5021, 5019, 438, 872, 2064, 5246, 4699, 1874, 4702]}]}
{"group_id": 413, "group_size": 5, "items": [{"qid": 3176, "question": "What dataset is used? in Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization", "answer": ["English WIKIBIO, French WIKIBIO , German WIKIBIO ", "WikiBio dataset,  introduce two new biography datasets, one in French and one in German"], "top_k_doc_id": [5345, 5346, 5348, 5349, 5350, 6066, 5228, 5492, 5495, 7447, 5226, 5227, 5347, 5493, 7147], "orig_top_k_doc_id": [5349, 5348, 5350, 5345, 5346, 6066, 5495, 5492, 5226, 5347, 5493, 5227, 7447, 7147, 5228]}, {"qid": 3177, "question": "What is a bifocal attention mechanism? in Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization", "answer": ["At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels"], "top_k_doc_id": [5345, 5346, 5348, 5349, 5350, 6066, 5228, 5492, 5495, 7447, 5226, 5227, 5347, 5493, 112], "orig_top_k_doc_id": [5349, 5348, 5346, 5350, 5345, 6066, 5492, 5227, 5226, 5495, 5493, 5347, 7447, 5228, 112]}, {"qid": 3175, "question": "Do they use pretrained embeddings? in Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization", "answer": ["Yes", "Yes"], "top_k_doc_id": [5345, 5346, 5348, 5349, 5350, 6066, 5228, 5492, 5495, 7447, 5226, 5227, 5347, 5493, 6067], "orig_top_k_doc_id": [5349, 5348, 5350, 5345, 5346, 6066, 6067, 5227, 5492, 5228, 5226, 7447, 5495, 5347, 5493]}, {"qid": 3174, "question": "What metrics are used for evaluation? in Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization", "answer": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "top_k_doc_id": [5345, 5346, 5348, 5349, 5350, 6066, 5228, 5492, 5495, 7447, 5497, 6067, 7145, 5494, 4793], "orig_top_k_doc_id": [5349, 5348, 5350, 5345, 5346, 5495, 5492, 7447, 5497, 5228, 6066, 6067, 7145, 5494, 4793]}, {"qid": 2457, "question": "On which dataset is model trained? in Behavior Gated Language Models", "answer": ["Couples Therapy Corpus (CoupTher) BIBREF21"], "top_k_doc_id": [5345, 5346, 5348, 5349, 5350, 6066, 4096, 4097, 4095, 1326, 2072, 639, 382, 6272, 4269], "orig_top_k_doc_id": [4096, 5348, 4097, 4095, 5349, 5350, 5345, 1326, 2072, 639, 382, 6272, 5346, 4269, 6066]}]}
{"group_id": 414, "group_size": 5, "items": [{"qid": 3319, "question": "What is their ROUGE score? in Ranking Sentences for Extractive Summarization with Reinforcement Learning", "answer": ["No"], "top_k_doc_id": [4760, 5554, 4478, 5557, 734, 3200, 3201, 4764, 5555, 5556, 730, 4482, 6925, 4481, 6924], "orig_top_k_doc_id": [5557, 5556, 5554, 5555, 4482, 4478, 4764, 4760, 6925, 4481, 734, 730, 3200, 3201, 6924]}, {"qid": 3320, "question": "What are the baselines? in Ranking Sentences for Extractive Summarization with Reinforcement Learning", "answer": ["No", "Answer with content missing: (Experimental Setup missing subsections)\nTo be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.\nAnswer: LEAD"], "top_k_doc_id": [4760, 5554, 4478, 5557, 734, 3200, 3201, 4764, 5555, 5556, 730, 544, 5540, 1132, 2335], "orig_top_k_doc_id": [5557, 5554, 5556, 544, 4760, 5540, 1132, 5555, 4478, 734, 3201, 3200, 4764, 730, 2335]}, {"qid": 3318, "question": "Do they use other evaluation metrics besides ROUGE? in Ranking Sentences for Extractive Summarization with Reinforcement Learning", "answer": ["Yes", "No"], "top_k_doc_id": [4760, 5554, 4478, 5557, 734, 3200, 3201, 4764, 5555, 5556, 220, 4482, 5152, 3715, 1132], "orig_top_k_doc_id": [5556, 5557, 5554, 5555, 4478, 3200, 3201, 4760, 4764, 220, 4482, 5152, 734, 3715, 1132]}, {"qid": 2559, "question": "What's the method used here? in Summary Level Training of Sentence Rewriting for Abstractive Summarization", "answer": ["Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8."], "top_k_doc_id": [4760, 5554, 4478, 5557, 4482, 4621, 4620, 4619, 4483, 730, 5542, 4304, 4761, 4480, 6839], "orig_top_k_doc_id": [4482, 4478, 4621, 5554, 4620, 4760, 4619, 4483, 730, 5542, 4304, 5557, 4761, 4480, 6839]}, {"qid": 2536, "question": "How does nextsum work? in What comes next? Extractive summarization by next-sentence prediction", "answer": ["selects the next summary sentence based not only on properties of the source text, but also on the previously selected sentences in the summary"], "top_k_doc_id": [4760, 5554, 4382, 4385, 4381, 4380, 5540, 5555, 4826, 4764, 3715, 4384, 1695, 4825, 4829], "orig_top_k_doc_id": [4382, 4385, 4381, 4380, 5540, 5555, 4826, 5554, 4764, 3715, 4384, 1695, 4825, 4829, 4760]}]}
{"group_id": 415, "group_size": 5, "items": [{"qid": 3375, "question": "what datasets were used? in Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model", "answer": ["diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher", "the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts"], "top_k_doc_id": [5599, 5600, 5602, 3412, 5601, 5604, 7091, 7092, 6206, 6208, 5979, 6207, 1726, 2330, 1580], "orig_top_k_doc_id": [5599, 5600, 5601, 5602, 5604, 7091, 7092, 6208, 6206, 6207, 3412, 5979, 1726, 2330, 1580]}, {"qid": 3378, "question": "what linguistics features are used? in Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model", "answer": ["POS, gender/number and stem POS"], "top_k_doc_id": [5599, 5600, 5602, 3412, 5601, 5604, 7091, 7092, 6206, 6208, 5979, 6207, 2086, 1584, 6209], "orig_top_k_doc_id": [5599, 5600, 5601, 5602, 5604, 7091, 7092, 6206, 6208, 6207, 3412, 2086, 5979, 1584, 6209]}, {"qid": 3376, "question": "what are the previous state of the art? in Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model", "answer": ["Farasa, RDI", "Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28"], "top_k_doc_id": [5599, 5600, 5602, 3412, 5601, 5604, 7091, 7092, 6206, 6208, 400, 2330, 1329, 2329, 4300], "orig_top_k_doc_id": [5599, 5600, 5601, 5604, 5602, 7091, 7092, 6208, 400, 2330, 1329, 2329, 3412, 4300, 6206]}, {"qid": 3377, "question": "what surface-level features are used? in Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model", "answer": ["affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities"], "top_k_doc_id": [5599, 5600, 5602, 3412, 5601, 5604, 7091, 7092, 1584, 1583, 5603, 6505, 2330, 1585, 2331], "orig_top_k_doc_id": [5599, 5600, 5601, 5604, 5602, 1584, 7091, 1583, 7092, 5603, 6505, 2330, 1585, 3412, 2331]}, {"qid": 4810, "question": "What sources did they get the data from? in Improving Yor\\`ub\\'a Diacritic Restoration", "answer": ["online public-domain sources, private sources and actual books", "Various web resources and couple of private sources as listed in the table."], "top_k_doc_id": [5599, 5600, 5602, 7489, 1796, 2560, 1795, 3173, 46, 2561, 1797, 4441, 5660, 5662, 5656], "orig_top_k_doc_id": [5600, 5599, 7489, 1796, 2560, 1795, 3173, 46, 5602, 2561, 1797, 4441, 5660, 5662, 5656]}]}
{"group_id": 416, "group_size": 5, "items": [{"qid": 3431, "question": "What models are used for painting embedding and what for language style transfer? in Prose for a Painting", "answer": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "top_k_doc_id": [567, 1643, 5681, 5682, 1121, 278, 353, 4264, 5700, 1008, 5841, 5844, 6882, 518, 2154], "orig_top_k_doc_id": [5681, 5682, 1643, 1121, 353, 5700, 4264, 1008, 5841, 5844, 6882, 518, 567, 278, 2154]}, {"qid": 3435, "question": "How big is English poem description of the painting dataset? in Prose for a Painting", "answer": ["No"], "top_k_doc_id": [567, 1643, 5681, 5682, 1121, 278, 353, 4264, 5700, 5, 566, 6214, 10, 564, 4], "orig_top_k_doc_id": [5681, 5682, 567, 5, 1643, 566, 353, 1121, 5700, 6214, 10, 278, 4264, 564, 4]}, {"qid": 3433, "question": "What limitations do the authors demnostrate of their model? in Prose for a Painting", "answer": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "top_k_doc_id": [567, 1643, 5681, 5682, 1121, 2450, 566, 4713, 2045, 678, 2396, 4139, 4204, 5049, 6206], "orig_top_k_doc_id": [5682, 5681, 1643, 2450, 567, 566, 4713, 2045, 678, 2396, 4139, 4204, 5049, 1121, 6206]}, {"qid": 3432, "question": "What applicability of their approach is demonstrated by the authors? in Prose for a Painting", "answer": ["No"], "top_k_doc_id": [567, 1643, 5681, 5682, 2334, 3511, 5093, 7434, 6206, 2464, 4264, 5913, 6888, 3514, 5385], "orig_top_k_doc_id": [5681, 5682, 1643, 567, 2334, 3511, 5093, 7434, 6206, 2464, 4264, 5913, 6888, 3514, 5385]}, {"qid": 3436, "question": "What is best BLEU score of language style transfer authors got? in Prose for a Painting", "answer": ["seq2seq model with global attention gives the best results with an average target BLEU score of 29.65", "average target BLEU score of 29.65"], "top_k_doc_id": [567, 1643, 5681, 5682, 1008, 1005, 6682, 6310, 5177, 5178, 1006, 566, 1007, 6945, 6944], "orig_top_k_doc_id": [5682, 5681, 1008, 567, 1643, 1005, 6682, 6310, 5177, 5178, 1006, 566, 1007, 6945, 6944]}]}
{"group_id": 417, "group_size": 5, "items": [{"qid": 3483, "question": "Do the authors conduct experiments on the tasks mentioned? in Winograd Schemas and Machine Translation", "answer": ["Yes", "No"], "top_k_doc_id": [568, 717, 718, 5772, 5773, 690, 4415, 5774, 945, 947, 949, 5839, 1245, 950, 6136], "orig_top_k_doc_id": [5773, 5772, 5774, 718, 717, 4415, 568, 949, 945, 947, 5839, 690, 1245, 950, 6136]}, {"qid": 3484, "question": "Did they collect their own datasets? in Winograd Schemas and Machine Translation", "answer": ["No"], "top_k_doc_id": [568, 717, 718, 5772, 5773, 690, 4415, 5774, 945, 947, 6995, 2136, 2277, 4156, 697], "orig_top_k_doc_id": [5772, 5773, 5774, 718, 717, 4415, 690, 568, 945, 6995, 2136, 2277, 4156, 947, 697]}, {"qid": 3485, "question": "What data do they look at? in Winograd Schemas and Machine Translation", "answer": ["WSC collection"], "top_k_doc_id": [568, 717, 718, 5772, 5773, 690, 4415, 5774, 5871, 6190, 6591, 692, 3460, 2074, 1039], "orig_top_k_doc_id": [5773, 5772, 5774, 718, 717, 690, 4415, 568, 692, 6591, 3460, 5871, 2074, 1039, 6190]}, {"qid": 3486, "question": "What language do they explore? in Winograd Schemas and Machine Translation", "answer": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "top_k_doc_id": [568, 717, 718, 5772, 5773, 690, 4415, 5774, 5871, 6190, 6591, 3585, 950, 6419, 2135], "orig_top_k_doc_id": [5773, 5772, 5774, 718, 717, 568, 690, 4415, 3585, 6190, 950, 6591, 6419, 2135, 5871]}, {"qid": 2852, "question": "What is the previous state of the art? in TTTTTackling WinoGrande Schemas", "answer": ["RoBERTa", "RoBERTa"], "top_k_doc_id": [568, 717, 718, 5772, 5773, 4994, 4993, 6646, 6649, 4156, 722, 3681, 6419, 3682, 4467], "orig_top_k_doc_id": [4994, 4993, 6646, 5773, 6649, 4156, 722, 718, 3681, 568, 5772, 6419, 3682, 4467, 717]}]}
{"group_id": 418, "group_size": 5, "items": [{"qid": 3513, "question": "Why masking words in the decoder is helpful? in Pretraining-Based Natural Language Generation for Text Summarization", "answer": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "top_k_doc_id": [5540, 3157, 3159, 5804, 6716, 1925, 3160, 4760, 4761, 1864, 4762, 4571, 436, 3158, 1138], "orig_top_k_doc_id": [4761, 5540, 4760, 3157, 1925, 1864, 4762, 3159, 4571, 5804, 436, 3160, 3158, 6716, 1138]}, {"qid": 3516, "question": "When is this paper published? in Pretraining-Based Natural Language Generation for Text Summarization", "answer": ["No"], "top_k_doc_id": [5540, 3157, 3159, 5804, 6716, 1925, 3160, 4760, 7285, 1921, 3719, 2435, 7761, 5147, 4414], "orig_top_k_doc_id": [5540, 7285, 1925, 4760, 1921, 3157, 3160, 5804, 3719, 2435, 7761, 5147, 6716, 3159, 4414]}, {"qid": 3515, "question": "How are the different components of the model trained? Is it trained end-to-end? in Pretraining-Based Natural Language Generation for Text Summarization", "answer": ["the objective of our model is sum of the two processes, jointly trained using \"teacher-forcing\" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\\hat{y} = argmax_{y^{\\prime }} P(y^{\\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end", "the model can be trained end-to-end"], "top_k_doc_id": [5540, 3157, 3159, 5804, 6716, 3719, 4424, 5850, 250, 4381, 3298, 4619, 6313, 2148, 6310], "orig_top_k_doc_id": [3719, 4424, 5540, 6716, 5850, 3157, 250, 5804, 4381, 3298, 3159, 4619, 6313, 2148, 6310]}, {"qid": 3514, "question": "What is the ROUGE score of the highest performing model? in Pretraining-Based Natural Language Generation for Text Summarization", "answer": ["33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L ", "33.33"], "top_k_doc_id": [5540, 3157, 3159, 3160, 4649, 6925, 4482, 4382, 4764, 6924, 3158, 6860, 3719, 5808, 6861], "orig_top_k_doc_id": [3159, 3160, 5540, 4649, 6925, 4482, 3157, 4382, 4764, 6924, 3158, 6860, 3719, 5808, 6861]}, {"qid": 3114, "question": "What is the previous state-of-the-art in summarization? in Pre-trained Language Model Representations for Language Generation", "answer": ["BIBREF26 ", "BIBREF26"], "top_k_doc_id": [5540, 5804, 4760, 7134, 1132, 6060, 4424, 6931, 2146, 7761, 2335, 4482, 6036, 5808, 4761], "orig_top_k_doc_id": [5804, 4760, 5540, 7134, 1132, 6060, 4424, 6931, 2146, 7761, 2335, 4482, 6036, 5808, 4761]}]}
{"group_id": 419, "group_size": 5, "items": [{"qid": 3547, "question": "Are there elements, other than pitch, that can potentially result in out of key converted singing? in PitchNet: Unsupervised Singing Voice Conversion with Pitch Adversarial Network", "answer": ["No"], "top_k_doc_id": [5862, 2473, 2627, 2628, 2629, 2474, 5478, 5860, 5861, 7537, 7538, 7540, 2315, 2317, 7539], "orig_top_k_doc_id": [5862, 5860, 5861, 7537, 7540, 7538, 2628, 2629, 2473, 2474, 2315, 2317, 2627, 7539, 5478]}, {"qid": 3548, "question": "How is the quality of singing voice measured? in PitchNet: Unsupervised Singing Voice Conversion with Pitch Adversarial Network", "answer": ["To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.", "Automatic: Normalized cross correlation (NCC)\nManual: Mean Opinion Score (MOS)"], "top_k_doc_id": [5862, 2473, 2627, 2628, 2629, 2474, 5478, 5860, 5861, 7537, 7538, 7540, 1064, 2070, 1814], "orig_top_k_doc_id": [5862, 5860, 5861, 7537, 7540, 7538, 2628, 2629, 2473, 2627, 2474, 1064, 2070, 5478, 1814]}, {"qid": 1758, "question": "What datasets are experimented with? in Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining", "answer": ["the CMU ARCTIC database BIBREF33,  the M-AILABS speech dataset BIBREF34 "], "top_k_doc_id": [5862, 1064, 1814, 1815, 2528, 2529, 2530, 2531, 3157, 3274, 5564, 5860, 5861, 3158, 1288], "orig_top_k_doc_id": [2528, 5860, 2529, 2530, 5862, 2531, 5861, 5564, 1814, 3157, 1815, 3158, 3274, 1064, 1288]}, {"qid": 1759, "question": "What is the baseline model? in Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining", "answer": ["a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model"], "top_k_doc_id": [5862, 1064, 1814, 1815, 2528, 2529, 2530, 2531, 3157, 3274, 5564, 5860, 5861, 3265, 1813], "orig_top_k_doc_id": [2528, 5860, 2529, 2531, 2530, 5862, 1814, 1815, 5564, 5861, 3274, 1064, 3157, 3265, 1813]}, {"qid": 1803, "question": "What large corpus is used for experiments? in Excitation-based Voice Quality Analysis and Modification", "answer": ["The De7 database"], "top_k_doc_id": [5862, 2473, 2627, 2628, 2629, 3266, 6468, 5564, 1622, 2530, 7104, 6467, 1623, 1624, 5764], "orig_top_k_doc_id": [2627, 2628, 3266, 2629, 6468, 5564, 1622, 2473, 2530, 7104, 6467, 1623, 1624, 5862, 5764]}]}
{"group_id": 420, "group_size": 5, "items": [{"qid": 3564, "question": "Which toolkits do they use? in Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets", "answer": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "top_k_doc_id": [2388, 4300, 5879, 5880, 447, 450, 598, 4287, 6140, 7131, 7697, 4288, 5319, 4908, 1262], "orig_top_k_doc_id": [5879, 5880, 7697, 4288, 447, 2388, 5319, 4300, 4287, 450, 4908, 598, 7131, 1262, 6140]}, {"qid": 3566, "question": "Is datasets for sentiment analysis balanced? in Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets", "answer": ["No"], "top_k_doc_id": [2388, 4300, 5879, 5880, 447, 450, 598, 4287, 6140, 7131, 5181, 5405, 452, 756, 6910], "orig_top_k_doc_id": [5879, 5880, 447, 2388, 5181, 5405, 6140, 450, 4300, 7131, 452, 4287, 756, 598, 6910]}, {"qid": 3565, "question": "Which sentiment class is the most accurately predicted by ELS systems? in Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets", "answer": ["neutral sentiment"], "top_k_doc_id": [2388, 4300, 5879, 5880, 447, 450, 598, 4287, 5181, 4288, 5180, 597, 7172, 335, 2828], "orig_top_k_doc_id": [5880, 5879, 5181, 598, 4288, 450, 447, 4300, 5180, 4287, 2388, 597, 7172, 335, 2828]}, {"qid": 3567, "question": "What measures are used for evaluation? in Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets", "answer": ["correct classification rate (CCR)"], "top_k_doc_id": [2388, 4300, 5879, 5880, 447, 450, 7746, 3332, 5775, 4288, 987, 449, 7751, 6140, 5181], "orig_top_k_doc_id": [5879, 5880, 7746, 447, 3332, 450, 4300, 5775, 4288, 987, 449, 2388, 7751, 6140, 5181]}, {"qid": 3563, "question": "Who are the crowdworkers? in Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets", "answer": ["people in the US that use Amazon Mechanical Turk", "located in the US, hired on the BIBREF22 platform"], "top_k_doc_id": [2388, 4300, 5879, 5880, 3989, 2387, 326, 3975, 2396, 6140, 2390, 2206, 4288, 2400, 2391], "orig_top_k_doc_id": [5879, 5880, 2388, 3989, 2387, 326, 3975, 2396, 6140, 2390, 4300, 2206, 4288, 2400, 2391]}]}
{"group_id": 421, "group_size": 5, "items": [{"qid": 3573, "question": "Do they evaluate on English only datasets? in Structured Embedding Models for Grouped Data", "answer": ["No", "No"], "top_k_doc_id": [5886, 5887, 4296, 4777, 6756, 3868, 6764, 1339, 5888, 4995, 3007, 6755, 3008, 6853, 5712], "orig_top_k_doc_id": [4296, 5886, 3868, 5887, 4777, 6764, 1339, 5888, 4995, 3007, 6755, 3008, 6853, 6756, 5712]}, {"qid": 3574, "question": "What experiments are used to demonstrate the benefits of this approach? in Structured Embedding Models for Grouped Data", "answer": ["On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:", "Calculate test log-likelihood on the three considered datasets"], "top_k_doc_id": [5886, 5887, 4296, 4777, 6756, 5221, 4298, 5885, 7235, 4679, 1344, 4320, 4674, 5351, 163], "orig_top_k_doc_id": [5221, 5886, 5887, 4298, 5885, 6756, 4296, 7235, 4679, 1344, 4320, 4674, 5351, 163, 4777]}, {"qid": 3575, "question": "What hierarchical modelling approach is used? in Structured Embedding Models for Grouped Data", "answer": ["the group-specific embedding representations are tied through a global embedding"], "top_k_doc_id": [5886, 5887, 4296, 6208, 5885, 5627, 6586, 5217, 6585, 4293, 6854, 5888, 3468, 1632, 5613], "orig_top_k_doc_id": [5887, 5886, 6208, 5885, 5627, 6586, 5217, 6585, 4296, 4293, 6854, 5888, 3468, 1632, 5613]}, {"qid": 3577, "question": "Which words are used differently across ArXiv? in Structured Embedding Models for Grouped Data", "answer": ["intelligence"], "top_k_doc_id": [5886, 5887, 5885, 5888, 4530, 94, 6534, 5592, 2396, 2548, 4956, 5593, 7425, 2388, 2718], "orig_top_k_doc_id": [5885, 5886, 5888, 4530, 94, 6534, 5592, 2396, 2548, 4956, 5593, 5887, 7425, 2388, 2718]}, {"qid": 3576, "question": "How do co-purchase patterns vary across seasons? in Structured Embedding Models for Grouped Data", "answer": ["No"], "top_k_doc_id": [5886, 5885, 6635, 18, 6764, 4135, 6763, 6761, 5864, 4995, 3101, 5403, 7835, 6755, 3102], "orig_top_k_doc_id": [5885, 5886, 6635, 18, 6764, 4135, 6763, 6761, 5864, 4995, 3101, 5403, 7835, 6755, 3102]}]}
{"group_id": 422, "group_size": 5, "items": [{"qid": 3583, "question": "Which component is the least impactful? in Message Passing Attention Networks for Document Understanding", "answer": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "top_k_doc_id": [2101, 4216, 5894, 5898, 6545, 7822, 6381, 7546, 2151, 6546, 2152, 743, 744, 2096, 2202], "orig_top_k_doc_id": [5894, 5898, 6545, 4216, 2151, 743, 6381, 744, 2096, 2152, 6546, 7822, 2101, 7546, 2202]}, {"qid": 3584, "question": "Which component has the greatest impact on performance? in Message Passing Attention Networks for Document Understanding", "answer": ["Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations", "Removing the master node deteriorates performance across all datasets"], "top_k_doc_id": [2101, 4216, 5894, 5898, 6545, 7822, 6381, 7546, 2151, 6546, 2152, 5897, 1987, 3175, 5087], "orig_top_k_doc_id": [5894, 5898, 6545, 2151, 4216, 7546, 5897, 2152, 1987, 3175, 5087, 2101, 6381, 6546, 7822]}, {"qid": 3585, "question": "What is the state-of-the-art system? in Message Passing Attention Networks for Document Understanding", "answer": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "top_k_doc_id": [2101, 4216, 5894, 5898, 6545, 7822, 6381, 7546, 2151, 6546, 1679, 5429, 1987, 6348, 2584], "orig_top_k_doc_id": [5894, 5898, 6381, 6545, 1679, 4216, 7822, 2101, 2151, 5429, 1987, 6348, 6546, 2584, 7546]}, {"qid": 3586, "question": "Which datasets are used? in Message Passing Attention Networks for Document Understanding", "answer": ["Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013", " Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26"], "top_k_doc_id": [2101, 4216, 5894, 5898, 6545, 7822, 6381, 7546, 5897, 1679, 1683, 2096, 6382, 5895, 2759], "orig_top_k_doc_id": [5898, 5894, 4216, 6381, 6545, 7546, 5897, 7822, 2101, 1679, 1683, 2096, 6382, 5895, 2759]}, {"qid": 3587, "question": "What is the message passing framework? in Message Passing Attention Networks for Document Understanding", "answer": ["It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors."], "top_k_doc_id": [2101, 4216, 5894, 5898, 6545, 7822, 5435, 2151, 2152, 5897, 4220, 3901, 5827, 2202, 6348], "orig_top_k_doc_id": [5894, 5898, 4216, 7822, 5435, 2151, 2152, 5897, 4220, 3901, 2101, 5827, 6545, 2202, 6348]}]}
{"group_id": 423, "group_size": 5, "items": [{"qid": 3598, "question": "How big is the ANTISCAM dataset?  in End-to-End Trainable Non-Collaborative Dialog System", "answer": [" 3,044 sentences in 100 dialogs", "220 human-human dialogs", "220 human-human dialogs. , 3,044 sentences in 100 dialogs", "220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. ", "220 human-human dialogs", "3,044 sentences in 100 dialogs"], "top_k_doc_id": [5917, 5918, 7839, 5916, 5919, 5921, 7840, 3019, 4442, 5920, 7477, 4124, 4545, 6588, 4441], "orig_top_k_doc_id": [5917, 5916, 5918, 5921, 5919, 5920, 7839, 7477, 4124, 7840, 3019, 4545, 6588, 4442, 4441]}, {"qid": 3601, "question": "What are the evaluation metrics and criteria used to evaluate the model performance? in End-to-End Trainable Non-Collaborative Dialog System", "answer": ["Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP) , Extended Response-Slot Prediction (ERSP) , Fluency, Coherence , Engagement, Dialog length , Task Success Score (TaskSuc)", "Perplexity , Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), Extended Response-Slot Prediction (ERSP), Fluency , Coherence , Engagement , Dialog length (Length) , Task Success Score (TaskSuc)", "Fluency Fluency is used to explore different models' language generation quality.\n\nCoherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\n\nEngagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.\n\nDialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\n\nTask Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.", "Automatic evaluation metrics (Perplexity (PPl), Response-Intent Prediction (RIP), Response-Slot Prediction(RSP), Extended Response-Intent Prediction(ERIP),  Extended Response-Slot Prediction (ERSP)) and Human Evaluation Metrics (Fluency, Coherence, Engagement, Lenhth, TaskSuc)", "Automatic metrics used: Perplexity, RIP, RSP, ERIP ERSP.\nHuman evaluation metrics used: Fluency, Coherence, Engagement, Dialog length and Task Success Score."], "top_k_doc_id": [5917, 5918, 7839, 5916, 5919, 5921, 7840, 3019, 4442, 5920, 851, 7842, 4550, 5765, 2438], "orig_top_k_doc_id": [5917, 5916, 5921, 5918, 5919, 5920, 7839, 851, 7842, 4550, 5765, 2438, 4442, 7840, 3019]}, {"qid": 3600, "question": "What are the baselines outperformed by this work? in End-to-End Trainable Non-Collaborative Dialog System", "answer": ["TransferTransfo and Hybrid ", "TransferTransfo,  hybrid model", "TransferTransfo, Hybrid", "TransferTransfo, Hybrid", "TransferTransfo The vanilla TransferTransfo framework, Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA", "TransferTransfo, Hybrid"], "top_k_doc_id": [5917, 5918, 7839, 5916, 5919, 5921, 7840, 3019, 7842, 7504, 4545, 4550, 4128, 2197, 4553], "orig_top_k_doc_id": [5917, 5916, 5918, 7839, 5919, 5921, 3019, 7840, 7842, 7504, 4545, 4550, 4128, 2197, 4553]}, {"qid": 3599, "question": "How is intent annotated? in End-to-End Trainable Non-Collaborative Dialog System", "answer": ["using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations", "Separate on-task and off task intents and annotate on task for data set specific intents, while annotating  off task intents with a fixed set of general intents.", "On-task dialog are annotated as on-task intents , the other dialog are annotated as pre-defined off-task intents.", "separate on-task and off-task intents, on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task, off-task content is too general to design task-specific intents, we choose common dialog acts as the categories", "we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. , In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme, For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns.", "using a hierarchical scheme where on-task intents uses task-related intents for representation and off-task intents chooses dialog acts that convey the syntax information"], "top_k_doc_id": [5917, 5918, 7839, 5916, 5919, 5921, 7840, 3773, 7158, 5920, 3682, 1632, 2197, 1472, 4785], "orig_top_k_doc_id": [5917, 5916, 5918, 5919, 5921, 7839, 3773, 7840, 7158, 5920, 3682, 1632, 2197, 1472, 4785]}, {"qid": 4085, "question": "Which domain are the conversations in? in Augmenting End-to-End Dialog Systems with Commonsense Knowledge", "answer": ["open-domain", "open-domain Twitter dialogues"], "top_k_doc_id": [5917, 5918, 7839, 6543, 848, 6587, 5646, 4124, 7504, 2464, 4545, 1632, 4550, 4441, 4128], "orig_top_k_doc_id": [6543, 7839, 848, 6587, 5646, 4124, 5918, 5917, 7504, 2464, 4545, 1632, 4550, 4441, 4128]}]}
{"group_id": 424, "group_size": 5, "items": [{"qid": 3614, "question": "What was their accuracy score? in Knowledge Authoring and Question Answering with KALM", "answer": ["95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset", "KALM achieves an accuracy of 95.6%, KALM-QA achieves 100% accuracy", "KALM-QA achieves an accuracy of 95% for parsing the queries, The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy", "KALM achieves an accuracy of 95.6%, KALM-QA achieves 95% accuracy on the manually constructured general questions dataset based on the 50 logical frames and achieves 100% accuracy on MetaQA dataset"], "top_k_doc_id": [5943, 5944, 5945, 7073, 7074, 3853, 7679, 791, 2464, 2737, 3100, 7075, 3532, 3855, 690], "orig_top_k_doc_id": [5943, 5944, 5945, 7073, 7074, 3853, 791, 2464, 7679, 2737, 3100, 7075, 3532, 3855, 690]}, {"qid": 3616, "question": "What dataset did they evaluate on? in Knowledge Authoring and Question Answering with KALM", "answer": ["dataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset", "first dataset is manually constructed general questions based on the 50 logical frames, second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions", "a manually created dataset of 50 logical frames mostly derived from FrameNet, a manually constructed general questions dataset based on the 50 logical frames and MetaQA dataset", " manually constructed general questions based on the 50 logical frames, MetaQA dataset"], "top_k_doc_id": [5943, 5944, 5945, 7073, 7074, 3853, 7679, 4900, 4901, 4075, 5607, 491, 1120, 883, 2661], "orig_top_k_doc_id": [5944, 5943, 5945, 7073, 4900, 4901, 7074, 4075, 5607, 7679, 3853, 491, 1120, 883, 2661]}, {"qid": 3615, "question": "What are the state-of-the-art systems? in Knowledge Authoring and Question Answering with KALM", "answer": ["SEMAFOR, SLING, Stanford KBP ", "SEMAFOR, SLING, Stanford KBP system", "SEMAFOR, SLING, Stanford KBP system", "SEMAFOR, SLING, and Stanford KBP system, BIBREF14"], "top_k_doc_id": [5943, 5944, 5945, 7073, 7074, 1120, 7072, 7351, 7245, 7605, 2234, 1106, 3099, 1422, 3175], "orig_top_k_doc_id": [5943, 5945, 5944, 7073, 7074, 1120, 7072, 7351, 7245, 7605, 2234, 1106, 3099, 1422, 3175]}, {"qid": 3941, "question": "what evaluation metrics were used? in BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes", "answer": ["Accuracy,  MAE: Mean Absolute Error ", "MAE: Mean Absolute Error, Accuracy$\\pm k$", "MAE: Mean Absolute Error, Accuracy$\\pm k$"], "top_k_doc_id": [5943, 5944, 130, 1561, 4351, 5089, 5647, 6354, 6355, 6356, 3335, 2438, 2439, 3336, 2858], "orig_top_k_doc_id": [6354, 6355, 6356, 5943, 5944, 5647, 1561, 130, 3335, 2438, 4351, 2439, 5089, 3336, 2858]}, {"qid": 3942, "question": "What datasets are used? in BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes", "answer": ["Craigslist Bargaining dataset (CB)", "Craigslist Bargaining dataset (CB)", "Craigslist Bargaining dataset (CB) "], "top_k_doc_id": [5943, 5944, 130, 1561, 4351, 5089, 5647, 6354, 6355, 6356, 318, 7005, 4342, 4343, 1711], "orig_top_k_doc_id": [6354, 6356, 6355, 5647, 5944, 5943, 1561, 5089, 4351, 130, 318, 7005, 4342, 4343, 1711]}]}
{"group_id": 425, "group_size": 5, "items": [{"qid": 3669, "question": "Which model architecture do they opt for? in Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions", "answer": ["Recurrent Neural Networks, Convolutional Neural Networks", "RNNs and CNNs", "HAN BIBREF10, CNN BIBREF11", "CNN, RNN"], "top_k_doc_id": [854, 1308, 6016, 6017, 6018, 6474, 446, 4003, 5373, 3784, 6584, 988, 3289, 3679, 6314], "orig_top_k_doc_id": [6018, 6016, 6474, 6017, 854, 1308, 4003, 6584, 988, 3289, 446, 3679, 5373, 3784, 6314]}, {"qid": 3670, "question": "Which dataset do they use? in Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions", "answer": ["Clueweb09", "Clueweb09 derived dataset, new dataset based on Wikipedia crawl data", "the Clueweb09 derived dataset , dataset based on Wikipedia crawl data", "Clueweb09 derived dataset, Wikipedia crawl data"], "top_k_doc_id": [854, 1308, 6016, 6017, 6018, 6474, 446, 4003, 5373, 3784, 6833, 3445, 5812, 6443, 5907], "orig_top_k_doc_id": [6018, 6016, 6474, 6017, 1308, 854, 4003, 3784, 6833, 446, 3445, 5812, 6443, 5373, 5907]}, {"qid": 3672, "question": "Which weak signal data do they use? in Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions", "answer": ["No", "semantic representations of word embeddings"], "top_k_doc_id": [854, 1308, 6016, 6017, 6018, 6474, 446, 4003, 5373, 2162, 4998, 3072, 6314, 6833, 3161], "orig_top_k_doc_id": [6016, 6018, 6474, 6017, 854, 2162, 4998, 1308, 4003, 3072, 6314, 446, 5373, 6833, 3161]}, {"qid": 3673, "question": "Do they compare their semantic feature approach to lexical approaches? in Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions", "answer": ["Yes", "Yes", "Yes", "Yes"], "top_k_doc_id": [854, 1308, 6016, 6017, 6018, 6474, 5982, 7126, 3445, 1185, 5447, 3784, 7305, 3612, 5135], "orig_top_k_doc_id": [6016, 6018, 6017, 854, 6474, 1308, 5982, 7126, 3445, 1185, 5447, 3784, 7305, 3612, 5135]}, {"qid": 3671, "question": "Which setup shows proves to be the hardest: cross-topic, cross-domain, cross-temporal, or across annotators? in Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions", "answer": ["No", "No", "No"], "top_k_doc_id": [854, 1308, 6016, 6017, 6018, 6474, 2162, 4360, 5390, 6627, 5099, 6910, 7838, 6659, 6039], "orig_top_k_doc_id": [6018, 6016, 6017, 2162, 4360, 5390, 6627, 6474, 854, 5099, 6910, 7838, 6659, 6039, 1308]}]}
{"group_id": 426, "group_size": 5, "items": [{"qid": 3705, "question": "Are constructed datasets open sourced? in Common-Knowledge Concept Recognition for SEVA", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [945, 946, 6050, 6051, 6052, 1137, 4698, 4699, 3784, 2022, 4415, 5305, 2168, 6543, 6010], "orig_top_k_doc_id": [6050, 6051, 6052, 4698, 1137, 3784, 4699, 945, 2022, 4415, 5305, 946, 2168, 6543, 6010]}, {"qid": 3708, "question": "How big is constructed dataset? in Common-Knowledge Concept Recognition for SEVA", "answer": ["3700 sentences", "3700 sentences ", "roughly 3700 sentences at the word-token level"], "top_k_doc_id": [945, 946, 6050, 6051, 6052, 1137, 4698, 4699, 7804, 7855, 3209, 5911, 4273, 1136, 6714], "orig_top_k_doc_id": [6050, 6051, 6052, 946, 945, 7804, 7855, 3209, 4699, 4698, 1137, 5911, 4273, 1136, 6714]}, {"qid": 755, "question": "How do they select answer candidates for their QA task? in Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models", "answer": ["AMS method."], "top_k_doc_id": [945, 946, 947, 948, 949, 4273, 4277, 950, 7514, 7518, 4257, 3776, 1822, 4258, 5473], "orig_top_k_doc_id": [945, 946, 949, 950, 7514, 4273, 7518, 947, 4257, 4277, 3776, 948, 1822, 4258, 5473]}, {"qid": 3704, "question": "What is the performance of fine tuned model on this dataset? in Common-Knowledge Concept Recognition for SEVA", "answer": ["F1-score of $0.89$", "The model gives an F1-score of $0.89$ for the concept recognition task.", " F1-score of $0.89$"], "top_k_doc_id": [945, 946, 947, 948, 949, 4273, 4277, 6050, 6051, 6052, 7519, 4967, 1560, 4699, 5350], "orig_top_k_doc_id": [6050, 6051, 6052, 945, 948, 4277, 7519, 947, 4967, 946, 4273, 949, 1560, 4699, 5350]}, {"qid": 3707, "question": "What pretrained language model is used? in Common-Knowledge Concept Recognition for SEVA", "answer": ["BERT", "BERT", "BERT "], "top_k_doc_id": [945, 946, 6050, 6051, 6052, 6153, 5709, 5710, 3122, 4900, 5711, 4415, 4273, 6656, 4198], "orig_top_k_doc_id": [6050, 6052, 6051, 6153, 5709, 5710, 3122, 946, 4900, 5711, 945, 4415, 4273, 6656, 4198]}]}
{"group_id": 427, "group_size": 5, "items": [{"qid": 3730, "question": "which pretrained embeddings were experimented with? in Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity", "answer": ["word2vec , fastText , GloVe , Baroni , SL999 ", "word2vec, fastText, GloVe, Baroni, SL999", "word2vec, fastText, GloVe, Baroni, SL999"], "top_k_doc_id": [5710, 5711, 5716, 6070, 6071, 6072, 6073, 1517, 3872, 5540, 5709, 6119, 5713, 4415, 6135], "orig_top_k_doc_id": [6070, 6073, 6071, 5710, 5711, 6072, 5540, 3872, 5716, 5713, 1517, 5709, 6119, 4415, 6135]}, {"qid": 3731, "question": "what datasets where used? in Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity", "answer": ["STSB , SICK, MRPC", "STSB, SICK, MRPC", "SICK, STSB, MRPC"], "top_k_doc_id": [5710, 5711, 5716, 6070, 6071, 6072, 6073, 1517, 3872, 5540, 5709, 6119, 5097, 5590, 6121], "orig_top_k_doc_id": [6070, 6071, 6073, 6072, 6119, 5710, 5711, 3872, 5716, 5097, 5540, 5590, 1517, 5709, 6121]}, {"qid": 3449, "question": "How were the datasets annotated? in Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity", "answer": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "top_k_doc_id": [5710, 5711, 5716, 3749, 5699, 5700, 5701, 5702, 5703, 5708, 5709, 5713, 5714, 5715, 5712], "orig_top_k_doc_id": [5716, 5699, 5700, 5702, 5709, 5713, 5708, 5711, 5715, 5710, 5714, 3749, 5701, 5703, 5712]}, {"qid": 3450, "question": "What are the 12 languages covered? in Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity", "answer": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "top_k_doc_id": [5710, 5711, 5716, 3749, 5699, 5700, 5701, 5702, 5703, 5708, 5709, 5713, 5714, 5715, 7510], "orig_top_k_doc_id": [5716, 5699, 5711, 5700, 5715, 5709, 5713, 5708, 5702, 5710, 5714, 5701, 5703, 3749, 7510]}, {"qid": 3732, "question": "what are the state of the art methods they compare with? in Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity", "answer": ["ECNU, HCTI", "HCTI BIBREF5, InferSent BIBREF23 ", "ECNU BIBREF6, HCTI BIBREF5"], "top_k_doc_id": [5710, 5711, 5716, 6070, 6071, 6072, 6073, 5097, 6135, 6448, 6449, 5713, 5590, 4415, 6121], "orig_top_k_doc_id": [6070, 6073, 6071, 6072, 5097, 5710, 5711, 6135, 6448, 6449, 5716, 5713, 5590, 4415, 6121]}]}
{"group_id": 428, "group_size": 5, "items": [{"qid": 3814, "question": "what are the baseline models? in #SarcasmDetection is soooo general! Towards a Domain-Independent Approach for Detecting Sarcasm", "answer": ["the All Sarcasm case, the Random case", "All Sarcasm case assumes that every instance is sarcastic,  Random case randomly assigns each instance as sarcastic or non-sarcastic", "All Sarcasm, Random case"], "top_k_doc_id": [2103, 5406, 5879, 5904, 6172, 6173, 6175, 7114, 5900, 1329, 2106, 4552, 5903, 5410, 2110], "orig_top_k_doc_id": [6172, 2103, 5879, 1329, 6175, 7114, 5406, 5904, 5410, 4552, 5900, 2110, 2106, 5903, 6173]}, {"qid": 3816, "question": "what training data was used? in #SarcasmDetection is soooo general! Towards a Domain-Independent Approach for Detecting Sarcasm", "answer": ["Twitter dataset,  Amazon product reviews", "Twitter product reviews containing exactly one of the trailing hashtags \u201c#sarcasm,\u201d \u201c#happiness,\u201d \u201c#sadness,\u201d \u201c#anger,\u201d \u201c#surprise,\u201d \u201c#fear,\u201d and \u201c#disgust\u201d, and Amazon product reviews from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661.", "Twitter, and Amazon product reviews"], "top_k_doc_id": [2103, 5406, 5879, 5904, 6172, 6173, 6175, 7114, 5900, 1329, 2106, 4552, 5903, 6174, 7752], "orig_top_k_doc_id": [6172, 6175, 5879, 1329, 2103, 5406, 6173, 7114, 4552, 5900, 5903, 5904, 6174, 2106, 7752]}, {"qid": 3813, "question": "what was the previous best results model? in #SarcasmDetection is soooo general! Towards a Domain-Independent Approach for Detecting Sarcasm", "answer": [" F1 (0.744)", " BIBREF12 buschmeier-cimiano-klinger:2014:W14-26", "logistic regression classifier"], "top_k_doc_id": [2103, 5406, 5879, 5904, 6172, 6173, 6175, 7114, 5900, 1329, 2110, 2109, 6174, 1967, 2105], "orig_top_k_doc_id": [6172, 1329, 6175, 5879, 5900, 7114, 2103, 2110, 2109, 5904, 5406, 6174, 1967, 6173, 2105]}, {"qid": 3812, "question": "by how much did their approach outperform previous work? in #SarcasmDetection is soooo general! Towards a Domain-Independent Approach for Detecting Sarcasm", "answer": ["By 0,008 F1,  0, 02 Recall and 0,02 Precision.", "New best result is F1 score of 0.752 compared to 0.744 of the best previous work.", "by 0.008 in terms of F1 score,  and 0.02 in terms of recall and precision "], "top_k_doc_id": [2103, 5406, 5879, 5904, 6172, 6173, 6175, 7114, 5900, 2105, 6174, 5410, 2104, 2114, 4382], "orig_top_k_doc_id": [6175, 6172, 2105, 6174, 6173, 5410, 5879, 5406, 5904, 2104, 7114, 2114, 5900, 2103, 4382]}, {"qid": 3815, "question": "what domains are explored? in #SarcasmDetection is soooo general! Towards a Domain-Independent Approach for Detecting Sarcasm", "answer": ["Twitter, and Amazon product reviews", "Data was taken from two domains: Twitter, and Amazon product reviews. ", "Twitter, Amazon "], "top_k_doc_id": [2103, 5406, 5879, 5904, 6172, 6173, 6175, 7114, 448, 1039, 6881, 1329, 6174, 1711, 7752], "orig_top_k_doc_id": [6172, 6175, 2103, 7114, 6173, 448, 1039, 6881, 1329, 6174, 1711, 5879, 7752, 5904, 5406]}]}
{"group_id": 429, "group_size": 5, "items": [{"qid": 3836, "question": "What conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder? in A computational linguistic study of personal recovery in bipolar disorder", "answer": ["No", "Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects", "a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals, expect the proposed investigations of views on recovery by previously under-researched ethnic, language, and cultural groups to yield valuable insights on the appropriateness of the recovery approach for a wider population, The datasets collected in this project can serve as useful resources for future research"], "top_k_doc_id": [64, 6206, 6207, 6208, 7485, 643, 7323, 7481, 60, 7484, 7486, 6971, 6209, 1702, 1837], "orig_top_k_doc_id": [6206, 6207, 6208, 7485, 7481, 7323, 64, 7486, 643, 6209, 60, 1702, 6971, 1837, 7484]}, {"qid": 3839, "question": "Was permission sought from the bipolar patients to use this data? in A computational linguistic study of personal recovery in bipolar disorder", "answer": ["For Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.", "No", "No"], "top_k_doc_id": [64, 6206, 6207, 6208, 7485, 643, 7323, 7481, 60, 7484, 7486, 6971, 59, 7621, 644], "orig_top_k_doc_id": [6206, 64, 7485, 643, 7323, 7481, 59, 7486, 6208, 60, 6207, 7621, 6971, 644, 7484]}, {"qid": 3840, "question": "How are the individuals with bipolar disorder identified? in A computational linguistic study of personal recovery in bipolar disorder", "answer": ["characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12", " Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'.", "Twitter and Reddit users  are identified automatically  via self-reported diagnosis statements. Blog users are identified manually."], "top_k_doc_id": [64, 6206, 6207, 6208, 7485, 643, 7323, 7481, 60, 7484, 7486, 3553, 6209, 522, 4964], "orig_top_k_doc_id": [6206, 7485, 7481, 64, 643, 6208, 7486, 7323, 3553, 6207, 60, 7484, 6209, 522, 4964]}, {"qid": 3837, "question": "What languages were included in this multilingual population? in A computational linguistic study of personal recovery in bipolar disorder", "answer": ["No", "No", "No"], "top_k_doc_id": [64, 6206, 6207, 6208, 7485, 643, 7323, 7481, 6209, 1067, 1066, 59, 6833, 4964, 6210], "orig_top_k_doc_id": [6206, 6208, 6207, 6209, 643, 7485, 7323, 64, 1067, 1066, 7481, 59, 6833, 4964, 6210]}, {"qid": 3838, "question": "What computational linguistic methods were used for the analysis? in A computational linguistic study of personal recovery in bipolar disorder", "answer": ["No", "No", "language identification"], "top_k_doc_id": [64, 6206, 6207, 6208, 7485, 1066, 4140, 5537, 6209, 3553, 1067, 59, 7534, 5524, 3891], "orig_top_k_doc_id": [6206, 6208, 6207, 64, 1066, 4140, 7485, 5537, 6209, 3553, 1067, 59, 7534, 5524, 3891]}]}
{"group_id": 430, "group_size": 5, "items": [{"qid": 3860, "question": "What metric is used to measure translation accuracy? in Hint-Based Training for Non-Autoregressive Machine Translation", "answer": ["BLEU ", "BLEU score", "BLUE and the percentage of repetitive words"], "top_k_doc_id": [1244, 6237, 6238, 6239, 7248, 1234, 2495, 7247, 2494, 7246, 7249, 1231, 6502, 5660, 6501], "orig_top_k_doc_id": [6237, 6239, 6238, 2495, 7248, 7249, 2494, 1244, 7246, 7247, 1234, 1231, 6502, 5660, 6501]}, {"qid": 3862, "question": "Are the results applicable to other language pairs than German-English? in Hint-Based Training for Non-Autoregressive Machine Translation", "answer": ["No", "No", "No"], "top_k_doc_id": [1244, 6237, 6238, 6239, 7248, 1234, 2495, 7247, 2494, 7246, 4031, 661, 4027, 4212, 4814], "orig_top_k_doc_id": [6238, 6237, 7248, 6239, 4031, 2494, 661, 1244, 4027, 7246, 1234, 7247, 2495, 4212, 4814]}, {"qid": 3859, "question": "How slow is the unparallelizable ART model in the first place?   in Hint-Based Training for Non-Autoregressive Machine Translation", "answer": ["784 miliseconds", "No", "While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory"], "top_k_doc_id": [1244, 6237, 6238, 6239, 7248, 1234, 2495, 7247, 1231, 1139, 1235, 1140, 3659, 2998, 7117], "orig_top_k_doc_id": [6237, 6238, 6239, 1231, 1139, 1235, 7248, 1234, 1140, 1244, 3659, 2998, 7117, 2495, 7247]}, {"qid": 3861, "question": "Were any datasets other than WMT used to test the model? in Hint-Based Training for Non-Autoregressive Machine Translation", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [1244, 6237, 6238, 6239, 7248, 1234, 2494, 2056, 2055, 1245, 2741, 3781, 5618, 1246, 6294], "orig_top_k_doc_id": [6239, 6238, 1234, 6237, 1244, 2494, 7248, 2056, 2055, 1245, 2741, 3781, 5618, 1246, 6294]}, {"qid": 3858, "question": "How do you know the word alignments are correct? in Hint-Based Training for Non-Autoregressive Machine Translation", "answer": ["we use the word alignment information from the ART model", "In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model,", "we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48)."], "top_k_doc_id": [1244, 6237, 6238, 6239, 7248, 7158, 377, 3761, 4769, 1231, 7059, 4657, 4766, 4184, 2495], "orig_top_k_doc_id": [6238, 6239, 6237, 7158, 377, 3761, 4769, 7248, 1231, 7059, 4657, 4766, 4184, 1244, 2495]}]}
{"group_id": 431, "group_size": 5, "items": [{"qid": 4006, "question": "what other representations do they compare with? in KeyVec: Key-semantics Preserving Document Representations", "answer": ["word2vec averaging, Paragraph Vector", "Paragraph Vector, word2vec averagings", "Word2vec averaging (public release 300d), word2vec averaging (academic corpus), Paragraph Vector"], "top_k_doc_id": [6460, 6461, 6462, 6686, 7668, 476, 5701, 6260, 7682, 6256, 6240, 6258, 5554, 2733, 1040], "orig_top_k_doc_id": [6460, 6462, 6461, 5701, 6256, 6686, 7668, 7682, 476, 6240, 6258, 5554, 6260, 2733, 1040]}, {"qid": 4010, "question": "what dataset was used? in KeyVec: Key-semantics Preserving Document Representations", "answer": ["669 academic papers published by IEEE, 850 academic papers", "669 academic papers published by IEEE", "For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation."], "top_k_doc_id": [6460, 6461, 6462, 6686, 7668, 476, 5701, 6260, 7682, 1008, 2704, 2705, 5540, 6509, 7443], "orig_top_k_doc_id": [6460, 6461, 6462, 5701, 7668, 6686, 476, 1008, 7682, 2704, 6260, 2705, 5540, 6509, 7443]}, {"qid": 4007, "question": "how many layers are in the neural network? in KeyVec: Key-semantics Preserving Document Representations", "answer": ["No", "No"], "top_k_doc_id": [6460, 6461, 6462, 6686, 7668, 5092, 5540, 2705, 4638, 2733, 419, 5554, 7138, 5966, 1560], "orig_top_k_doc_id": [6460, 6462, 6461, 6686, 5092, 5540, 2705, 4638, 2733, 7668, 419, 5554, 7138, 5966, 1560]}, {"qid": 4008, "question": "what empirical evaluations performed? in KeyVec: Key-semantics Preserving Document Representations", "answer": ["document retrieval, document clustering", "document retrieval, document clustering", " we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering."], "top_k_doc_id": [6460, 6461, 6462, 6686, 7668, 5554, 5701, 6679, 856, 860, 1506, 6348, 6347, 5739, 3393], "orig_top_k_doc_id": [6460, 6462, 6461, 5554, 5701, 6679, 6686, 7668, 856, 860, 1506, 6348, 6347, 5739, 3393]}, {"qid": 4009, "question": "which document understanding tasks did they evaluate on? in KeyVec: Key-semantics Preserving Document Representations", "answer": ["document retrieval, document clustering", " document retrieval and document clustering", " document retrieval, document clustering"], "top_k_doc_id": [6460, 6461, 6462, 5540, 6716, 5701, 6715, 123, 1447, 3743, 1373, 6509, 476, 5313, 5997], "orig_top_k_doc_id": [6460, 6461, 6462, 5540, 6716, 5701, 6715, 123, 1447, 3743, 1373, 6509, 476, 5313, 5997]}]}
{"group_id": 432, "group_size": 5, "items": [{"qid": 4115, "question": "Is the re-ranking approach described in this paper a transductive learning technique? in Incremental Improvement of a Question Answering System by Re-ranking Answer Candidates using Machine Learning", "answer": ["Yes", "No", "Yes"], "top_k_doc_id": [3851, 3854, 1643, 4535, 5736, 6575, 6577, 6578, 4497, 4599, 6391, 5119, 3421, 972, 7071], "orig_top_k_doc_id": [6578, 6577, 6575, 5736, 3851, 4497, 6391, 4535, 4599, 5119, 3854, 3421, 972, 7071, 1643]}, {"qid": 4116, "question": "How big is the test set used for evaluating the proposed re-ranking approach? in Incremental Improvement of a Question Answering System by Re-ranking Answer Candidates using Machine Learning", "answer": ["3084 real user requests  assigned to suitable answers from the training corpus.", "3084 real user requests from a chat-log of T-Mobile Austria", "3084"], "top_k_doc_id": [3851, 3854, 1643, 4535, 5736, 6575, 6577, 6578, 4497, 4599, 6391, 6576, 6390, 1637, 4597], "orig_top_k_doc_id": [6577, 6578, 6575, 6576, 3851, 4497, 5736, 6390, 6391, 3854, 1637, 4599, 4597, 1643, 4535]}, {"qid": 2085, "question": "What is the TREC-CAR dataset? in Passage Re-ranking with BERT", "answer": ["in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section"], "top_k_doc_id": [3851, 3854, 3852, 3853, 3855, 3167, 3168, 5736, 5329, 3420, 972, 971, 7129, 6449, 7128], "orig_top_k_doc_id": [3167, 3168, 3855, 3854, 3851, 5736, 5329, 3420, 972, 3852, 971, 7129, 6449, 3853, 7128]}, {"qid": 2386, "question": "How is OpenBookQA different from other natural language QA? in Careful Selection of Knowledge to solve Open Book Question Answering", "answer": ["in the OpenBookQA setup the open book part is much larger, the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required"], "top_k_doc_id": [3851, 3854, 3852, 3853, 3855, 3856, 7351, 1422, 2910, 490, 1090, 7800, 1141, 4464, 2896], "orig_top_k_doc_id": [3851, 3856, 3855, 3852, 3853, 7351, 1422, 3854, 2910, 490, 1090, 7800, 1141, 4464, 2896]}, {"qid": 4114, "question": "What QA system was used in this work? in Incremental Improvement of a Question Answering System by Re-ranking Answer Candidates using Machine Learning", "answer": ["We implement our question answering system using state-of-the-art open source components. ", "Rasa natural language understanding framework"], "top_k_doc_id": [3851, 3854, 1643, 4535, 5736, 6575, 6577, 6578, 1637, 1121, 7071, 4819, 5741, 145, 2366], "orig_top_k_doc_id": [6578, 6575, 6577, 1643, 4535, 5736, 3854, 1637, 1121, 7071, 4819, 3851, 5741, 145, 2366]}]}
{"group_id": 433, "group_size": 5, "items": [{"qid": 4129, "question": "What is the reason this research was not adopted in the 1960s? in A Lost Croatian Cybernetic Machine Translation Program", "answer": ["the lack of funding", " poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers", "the lack of federal funding, Laszlo\u2019s group had to manage without an actual computer"], "top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 5773, 6596, 5049, 5872, 5906, 2922, 6296, 2274, 1250, 3589], "orig_top_k_doc_id": [6591, 6593, 6594, 6592, 6595, 5906, 5872, 5773, 6596, 5049, 2922, 6296, 2274, 1250, 3589]}, {"qid": 4130, "question": "What is included in the cybernetic methods mentioned? in A Lost Croatian Cybernetic Machine Translation Program", "answer": ["compile a dictionary of words sorted from the end of the word to the beginning, make a word frequency table, create a good thesaurus", "Separation of the dictionary from the MT algorithm, Separation of the understanding and generation modules of the MT algorithms, All words need to be lemmatized, The word lemma should be the key of the dictionary,, Use context to determine the meaning of polysemous words."], "top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 5773, 6596, 2074, 2818, 6539, 6006, 6187, 5049, 7260, 3457], "orig_top_k_doc_id": [6591, 6593, 6594, 6592, 6595, 5773, 6596, 6539, 6006, 6187, 5049, 7260, 2818, 3457, 2074]}, {"qid": 4131, "question": "What were the usual logical approaches of the time period? in A Lost Croatian Cybernetic Machine Translation Program", "answer": ["They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype., Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian., Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards.,  Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7.", "to have a logical intermediate language, under the working name \u201cInterlingua\u201d, which was the connector of both natural languages", "The idea was to have a logical intermediate language"], "top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 5773, 6596, 2074, 2818, 2691, 2040, 567, 2042, 6185, 6600], "orig_top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 6596, 2818, 5773, 2691, 2040, 567, 2042, 6185, 6600, 2074]}, {"qid": 4132, "question": "What language was this research published in? in A Lost Croatian Cybernetic Machine Translation Program", "answer": ["No", "No"], "top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 5773, 6596, 5049, 5872, 2270, 7190, 2038, 1410, 2767, 2040], "orig_top_k_doc_id": [6593, 6592, 6591, 6594, 6595, 6596, 5773, 5049, 2270, 7190, 2038, 1410, 5872, 2767, 2040]}, {"qid": 4128, "question": "How does this research compare to research going on in the US and USSR at this time? in A Lost Croatian Cybernetic Machine Translation Program", "answer": ["lagging only a couple of years behind the research of the superpowers", "Author of this research noted the USA prototype effort from 1954 and research papers in 1955as well as USSR effort from 1955. ", "It is worthwhile to note that both the USA and the USSR had access to state-of-the-art computers, and the political support for the production of such systems meant that computers were made available to researchers in machine translation. However, the results were poor in the late 1950s, and a working system was yet to be shown. All work was therefore theoretical work implemented on a computer, which proved to be sub-optimal."], "top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 2074, 242, 2041, 5792, 2358, 2270, 7866, 2767, 5049, 6539], "orig_top_k_doc_id": [6591, 6592, 6593, 6594, 6595, 2074, 242, 2041, 5792, 2358, 2270, 7866, 2767, 5049, 6539]}]}
{"group_id": 434, "group_size": 5, "items": [{"qid": 4138, "question": "What previous approaches are presented for comparison? in An Interactive Machine Translation Framework for Modernizing Historical Documents", "answer": ["Baseline system corresponds to considering the original document as the modernized version. They used two approaches SMT and NMT and compared to the baseline, SMT showed best results.", "prefix-based "], "top_k_doc_id": [6600, 6601, 6602, 3731, 6025, 6879, 7342, 1250, 7447, 1312, 1357, 1132, 6348, 2074, 564], "orig_top_k_doc_id": [6602, 6600, 6601, 1250, 7447, 6879, 1312, 6025, 1357, 1132, 3731, 7342, 6348, 2074, 564]}, {"qid": 4139, "question": "What kind of data is used to train the model? in An Interactive Machine Translation Framework for Modernizing Historical Documents", "answer": ["Modern and historical versions of literature like the Bible and a Spanish novel.", "Dutch Bible BIBREF1, El Quijote BIBREF2,  El Conde Lucanor BIBREF2", "Dutch Bible, El Quijote"], "top_k_doc_id": [6600, 6601, 6602, 3731, 6025, 6879, 7342, 2400, 1805, 3124, 5158, 4135, 1804, 1443, 117], "orig_top_k_doc_id": [6602, 6600, 6601, 2400, 1805, 3731, 3124, 5158, 6879, 4135, 7342, 1804, 6025, 1443, 117]}, {"qid": 4140, "question": "Does proposed approach use neural networks? in An Interactive Machine Translation Framework for Modernizing Historical Documents", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6600, 6601, 6602, 2648, 6879, 7342, 1303, 1308, 6636, 6025, 406, 1374, 5934, 7493, 4804], "orig_top_k_doc_id": [6602, 6600, 6601, 1303, 1308, 7342, 6636, 2648, 6025, 6879, 406, 1374, 5934, 7493, 4804]}, {"qid": 4141, "question": "What machine learning techniques are used in the model architecture? in An Interactive Machine Translation Framework for Modernizing Historical Documents", "answer": ["Classical IMT approaches, Prefix-based IMT , Neural Machine Translation, Prefix-based Interactive Neural Machine Translation", "NMT systems using NMT-Keras, SMT systems were trained with Moses, Statistical IMT systems", "classification for SMT and neural methods for NMT"], "top_k_doc_id": [6600, 6601, 6602, 2648, 6879, 7342, 3752, 3416, 274, 1053, 3758, 6399, 5119, 7420, 3731], "orig_top_k_doc_id": [6600, 6602, 6601, 3752, 3416, 274, 1053, 3758, 6879, 2648, 6399, 5119, 7420, 3731, 7342]}, {"qid": 4142, "question": "What language(s) is the model tested on? in An Interactive Machine Translation Framework for Modernizing Historical Documents", "answer": ["Dutch and Spanish", "Dutch, Spanish"], "top_k_doc_id": [6600, 6601, 6602, 3731, 6591, 34, 1250, 3124, 4135, 1801, 406, 3125, 559, 1805, 2040], "orig_top_k_doc_id": [6602, 6600, 6601, 6591, 34, 1250, 3124, 4135, 1801, 406, 3125, 559, 3731, 1805, 2040]}]}
{"group_id": 435, "group_size": 5, "items": [{"qid": 4145, "question": "What was their performance on this task? in Clinical Information Extraction via Convolutional Neural Network", "answer": ["Their average F1 score was 0.874 on span detection; 08115 on contextual modality detection; 0.8695 on degree detection; 0.839 on polarity detection; 0.844 on type detection", "Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 on span, modality, degree, polarity and type respectively."], "top_k_doc_id": [2127, 2621, 5675, 6603, 19, 4646, 6604, 1340, 6317, 6605, 5737, 7250, 1339, 5739, 644], "orig_top_k_doc_id": [6603, 2127, 19, 2621, 6317, 6604, 6605, 5675, 1340, 5737, 4646, 7250, 1339, 5739, 644]}, {"qid": 4146, "question": "What dataset did they use to evaluate? in Clinical Information Extraction via Convolutional Neural Network", "answer": ["Clinical TempEval corpus", "Clinical TempEval corpus", "Clinical TempEval corpus"], "top_k_doc_id": [2127, 2621, 5675, 6603, 19, 4646, 6604, 1340, 6317, 2048, 2049, 4839, 2051, 7026, 3743], "orig_top_k_doc_id": [6603, 2127, 5675, 19, 2048, 6317, 1340, 2049, 2621, 6604, 4839, 2051, 7026, 3743, 4646]}, {"qid": 4147, "question": "How did they obtain part-of-speech tags? in Clinical Information Extraction via Convolutional Neural Network", "answer": ["Answer with content missing: (We then use \u201dPerceptronTagger\u201d as our part-ofspeech tagger due to its fast tagging speed) PerceptronTagger.", "Using NLTK POS tagger"], "top_k_doc_id": [2127, 2621, 5675, 6603, 19, 4646, 6604, 6605, 7026, 6144, 4399, 5676, 6151, 7832, 2049], "orig_top_k_doc_id": [6603, 19, 6605, 5675, 7026, 6144, 4646, 2127, 4399, 5676, 6151, 7832, 6604, 2049, 2621]}, {"qid": 4143, "question": "By how much did their model outperform baselines? in Clinical Information Extraction via Convolutional Neural Network", "answer": ["Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.", "Their average F1 score is higher than that of baseline by 0.0234 ", "on event expression tasks average by 2.3% with respect to F1; on phase 2 subtask by 11.3% with respect to recall"], "top_k_doc_id": [2127, 2621, 5675, 6603, 460, 2049, 2013, 1340, 5335, 459, 6605, 2051, 2173, 5676, 4382], "orig_top_k_doc_id": [6603, 5675, 2127, 460, 2049, 2013, 1340, 5335, 459, 6605, 2621, 2051, 2173, 5676, 4382]}, {"qid": 4144, "question": "Which baselines did they compare against? in Clinical Information Extraction via Convolutional Neural Network", "answer": ["memorization, median report, max report", "memorization baseline", "memorization"], "top_k_doc_id": [2127, 2621, 5675, 6603, 2649, 3744, 7026, 460, 3743, 1953, 4611, 6605, 4648, 2651, 5968], "orig_top_k_doc_id": [6603, 2621, 2127, 5675, 2649, 3744, 7026, 460, 3743, 1953, 4611, 6605, 4648, 2651, 5968]}]}
{"group_id": 436, "group_size": 5, "items": [{"qid": 4149, "question": "what were the baselines? in A Question Answering Approach to Emotion Cause Extraction", "answer": ["RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM, Word2vec, Multi-kernel, CNN, Memnet", "RB (Rule based method), CB (Common-sense based method), RB+CB+ML, SVM, Word2vec, Multi-kernel, CNN", "RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM classifier using the unigram, bigram and trigram features, SVM classifier using word representations learned by Word2vec, multi-kernel method BIBREF31,  convolutional neural network for sentence classification BIBREF5"], "top_k_doc_id": [0, 3626, 6606, 6607, 6609, 6610, 6608, 1500, 3622, 3623, 3625, 6611, 3799, 5968, 3854], "orig_top_k_doc_id": [6606, 6610, 6609, 3626, 6607, 3623, 3625, 6611, 6608, 1500, 0, 3622, 3799, 5968, 3854]}, {"qid": 4150, "question": "what emotion cause dataset was used? in A Question Answering Approach to Emotion Cause Extraction", "answer": ["simplified Chinese emotion cause corpus BIBREF31", "a simplified Chinese emotion cause corpus BIBREF31", "Chinese emotion cause corpus"], "top_k_doc_id": [0, 3626, 6606, 6607, 6609, 6610, 6608, 1500, 3622, 3623, 3625, 6611, 6423, 7301, 3543], "orig_top_k_doc_id": [6606, 6610, 3626, 6607, 3623, 6609, 6608, 3622, 3625, 6611, 1500, 0, 6423, 7301, 3543]}, {"qid": 4151, "question": "what lexical features are extracted? in A Question Answering Approach to Emotion Cause Extraction", "answer": ["the distance between a clause and an emotion words", "No"], "top_k_doc_id": [0, 3626, 6606, 6607, 6609, 6610, 6608, 2127, 2405, 2801, 7147, 5406, 3096, 4348, 3623], "orig_top_k_doc_id": [6606, 6610, 6609, 2801, 6607, 3626, 5406, 3096, 6608, 4348, 0, 3623, 2127, 7147, 2405]}, {"qid": 4152, "question": "what word level sequences features are extracted? in A Question Answering Approach to Emotion Cause Extraction", "answer": ["Concatenation of three prediction output vectors", "concatenation of three output vectors"], "top_k_doc_id": [0, 3626, 6606, 6607, 6609, 6610, 6608, 2127, 2405, 2801, 7147, 7352, 1961, 6987, 5409], "orig_top_k_doc_id": [6610, 6606, 6609, 2801, 6607, 6608, 0, 7352, 3626, 7147, 1961, 2405, 2127, 6987, 5409]}, {"qid": 4148, "question": "what was their system's f1 score? in A Question Answering Approach to Emotion Cause Extraction", "answer": ["0.6955", "0.6955", "69.55"], "top_k_doc_id": [0, 3626, 6606, 6607, 6609, 6610, 4077, 6423, 6244, 5409, 3623, 2366, 5728, 4911, 4023], "orig_top_k_doc_id": [6606, 6610, 3626, 4077, 6423, 6244, 5409, 6607, 3623, 6609, 2366, 5728, 4911, 4023, 0]}]}
{"group_id": 437, "group_size": 5, "items": [{"qid": 4231, "question": "what evaluation metrics were used? in Outline Generation: Understanding the Inherent Content Structure of Documents", "answer": ["EM-outline, EM-sec, Rouge", "EMoutline, EMsec, Rougehead", "EM INLINEFORM0 , EM INLINEFORM0, Rouge INLINEFORM0"], "top_k_doc_id": [7382, 6721, 277, 6715, 6716, 6717, 6722, 7383, 1924, 5495, 4796, 1170, 491, 6882, 6472], "orig_top_k_doc_id": [6715, 6716, 6721, 7382, 6722, 7383, 277, 6717, 1924, 5495, 4796, 1170, 491, 6882, 6472]}, {"qid": 4232, "question": "what state of the art models did they compare with? in Outline Generation: Understanding the Inherent Content Structure of Documents", "answer": ["IG CRF+GHD", "HiStGen_P, HiStGen_S, HiStGen_H, HiStGen_R, HiStGen_PSHR, IGCRF+TextRank, IGCRF+TopicRank, IGCRF+Hier, IGCRF+GHD, IGGPD+TextRank, IGGPD+TopicRank, IGGPD+Hier, IGGPD+GHD, GATextRank, GATopicRank, GAHier, GAGHD"], "top_k_doc_id": [7382, 6721, 277, 6715, 6716, 6717, 6722, 7383, 5646, 5385, 7416, 5374, 5401, 1921, 3426], "orig_top_k_doc_id": [6715, 6716, 6722, 7382, 6721, 6717, 5646, 5385, 7416, 5374, 277, 7383, 5401, 1921, 3426]}, {"qid": 3276, "question": "What future possible improvements are listed? in A Hierarchical Model for Data-to-Text Generation", "answer": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "top_k_doc_id": [7382, 6721, 7447, 3193, 7336, 1806, 3986, 6904, 6068, 848, 5495, 3190, 7539, 2209, 3828], "orig_top_k_doc_id": [7447, 6721, 3193, 7336, 1806, 3986, 6904, 6068, 848, 5495, 3190, 7539, 2209, 3828, 7382]}, {"qid": 4743, "question": "What new advances are included in this dataset? in Creating a Real-Time, Reproducible Event Dataset", "answer": ["PETRARCH, PETRARCH2, realtime event data, geolocation", "PETRARCH , PETRARCH2 ,  scraping of news content from the web, geolocation of the coded events,  a comprehensive pipeline"], "top_k_doc_id": [7382, 883, 3671, 4280, 7383, 7386, 7387, 7389, 2083, 6492, 3593, 4322, 286, 5197, 2189], "orig_top_k_doc_id": [7382, 7386, 7383, 7389, 7387, 883, 3671, 2083, 6492, 3593, 4322, 286, 5197, 2189, 4280]}, {"qid": 4744, "question": "What language is this dataset in? in Creating a Real-Time, Reproducible Event Dataset", "answer": ["English", "English"], "top_k_doc_id": [7382, 883, 3671, 4280, 7383, 7386, 7387, 7389, 7388, 3523, 3521, 2957, 3581, 6140, 3522], "orig_top_k_doc_id": [7386, 7382, 7383, 7389, 883, 3671, 7388, 3523, 3521, 2957, 7387, 3581, 6140, 3522, 4280]}]}
{"group_id": 438, "group_size": 5, "items": [{"qid": 4241, "question": "How were the ngram models used to generate predictions on the data? in Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "answer": ["The n-gram models were used to calculate the logarithm of the probability for each tweet", "system sorts all the tweets for each hashtag and orders them based on their log probability score", "The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first"], "top_k_doc_id": [3543, 4515, 6732, 7361, 2968, 1843, 3486, 5739, 7132, 6070, 7362, 7688, 7131, 6558, 6082], "orig_top_k_doc_id": [7361, 6732, 7132, 6070, 3543, 7362, 5739, 2968, 4515, 3486, 7688, 7131, 1843, 6558, 6082]}, {"qid": 4243, "question": "What rank did the language model system achieve in the task evaluation? in Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "answer": ["4th place on SubtaskA; 1st place on Subtask B", "No"], "top_k_doc_id": [3543, 4515, 6732, 7361, 2968, 1843, 3486, 5739, 5738, 2874, 5737, 6619, 2982, 1329, 5255], "orig_top_k_doc_id": [7361, 6732, 5739, 3543, 5738, 2874, 5737, 2968, 6619, 3486, 4515, 2982, 1329, 5255, 1843]}, {"qid": 4240, "question": "What size ngram models performed best? e.g. bigram, trigram, etc. in Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "answer": ["bigram ", "the trigram language model performed better on Subtask B, the bigram language model performed better on Subtask A", "advantage of bigrams on Subtask A was very slight"], "top_k_doc_id": [3543, 4515, 6732, 7361, 5180, 5739, 6733, 7684, 7685, 7362, 6070, 5252, 7297, 5182, 4180], "orig_top_k_doc_id": [7361, 6732, 6733, 7684, 7685, 7362, 5739, 5180, 4515, 6070, 3543, 5252, 7297, 5182, 4180]}, {"qid": 4242, "question": "What package was used to build the ngram language models? in Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "answer": ["KenLM Toolkit", "KenLM Toolkit", "KenLM Toolkit"], "top_k_doc_id": [3543, 4515, 6732, 7361, 2968, 4948, 6541, 5272, 6070, 7131, 5168, 7362, 5274, 758, 2874], "orig_top_k_doc_id": [7361, 6732, 4948, 4515, 3543, 6541, 5272, 6070, 7131, 2968, 5168, 7362, 5274, 758, 2874]}, {"qid": 4244, "question": "What were subtasks A and B? in Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "answer": ["For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets., For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest."], "top_k_doc_id": [3543, 4515, 6732, 7361, 5180, 5739, 1843, 3580, 1842, 5698, 3578, 1844, 7115, 5737, 1788], "orig_top_k_doc_id": [7361, 6732, 1843, 3580, 1842, 5698, 3543, 3578, 4515, 5739, 5180, 1844, 7115, 5737, 1788]}]}
{"group_id": 439, "group_size": 5, "items": [{"qid": 4324, "question": "Do they encode sentences separately or together? in Fine-tune BERT for Extractive Summarization", "answer": ["Yes", "Together"], "top_k_doc_id": [4478, 5541, 5542, 5543, 6839, 6925, 4760, 5540, 6922, 1145, 6923, 5804, 1144, 6840, 1971], "orig_top_k_doc_id": [5541, 5540, 5542, 4760, 6925, 5543, 6922, 6839, 4478, 6923, 5804, 1144, 1145, 6840, 1971]}, {"qid": 4325, "question": "How do they use BERT to encode the whole text? in Fine-tune BERT for Extractive Summarization", "answer": ["insert a [CLS] token before each sentence and a [SEP] token after each sentence, use interval segment embeddings to distinguish multiple sentences within a document", "interval segment embeddings to distinguish multiple sentences within a document"], "top_k_doc_id": [4478, 5541, 5542, 5543, 6839, 6925, 4760, 5540, 6922, 1145, 6923, 2148, 7460, 5544, 7137], "orig_top_k_doc_id": [5541, 5540, 6839, 5542, 5543, 4478, 6925, 6922, 6923, 2148, 4760, 7460, 5544, 7137, 1145]}, {"qid": 4323, "question": "What other evaluation metrics did they use other than ROUGE-L?? in Fine-tune BERT for Extractive Summarization", "answer": ["they also use ROUGE-1 and ROUGE-2", "Rouge-1, Rouge-2, Rouge Recall, Rouge F1", "ROUGE-1 and ROUGE-2", "ROUGE-1 and ROUGE-2"], "top_k_doc_id": [4478, 5541, 5542, 5543, 6839, 6925, 4481, 4828, 5807, 6924, 7136, 7137, 1971, 3717, 6840], "orig_top_k_doc_id": [6924, 5542, 4481, 1971, 6925, 5543, 6839, 7137, 5541, 5807, 3717, 4828, 4478, 7136, 6840]}, {"qid": 4326, "question": "What is the ROUGE-L score of baseline method? in Fine-tune BERT for Extractive Summarization", "answer": ["37.17 for the baseline model using a non-pretrained Transformer", "37.17"], "top_k_doc_id": [4478, 5541, 5542, 5543, 6839, 6925, 4481, 4828, 5807, 6924, 7136, 7137, 4482, 734, 4764], "orig_top_k_doc_id": [6925, 5543, 4481, 5542, 6924, 4478, 4482, 5541, 734, 6839, 7137, 4828, 4764, 5807, 7136]}, {"qid": 4327, "question": "Which is the baseline method? in Fine-tune BERT for Extractive Summarization", "answer": ["non-pretrained Transformer baseline "], "top_k_doc_id": [4478, 5541, 5542, 5543, 6839, 6925, 4760, 5540, 6922, 4621, 2148, 6840, 1971, 4481, 734], "orig_top_k_doc_id": [6925, 5543, 5541, 5542, 4760, 4621, 2148, 6840, 5540, 6839, 1971, 6922, 4481, 734, 4478]}]}
{"group_id": 440, "group_size": 5, "items": [{"qid": 4328, "question": "What loss function is used? in Content-Based Table Retrieval for Web Queries", "answer": ["negative log-likelihood", "negative log-likelihood", "negative log-likelihood"], "top_k_doc_id": [144, 6842, 6845, 6847, 6846, 302, 6304, 7126, 6980, 2789, 6981, 5733, 6408, 5734, 334], "orig_top_k_doc_id": [6847, 6845, 6842, 6980, 144, 2789, 6981, 302, 7126, 6304, 5733, 6408, 5734, 334, 6846]}, {"qid": 4330, "question": "Does their method rely on the column headings of the table? in Content-Based Table Retrieval for Web Queries", "answer": ["Yes", "Yes"], "top_k_doc_id": [144, 6842, 6845, 6847, 6846, 302, 6304, 7126, 3221, 3216, 3220, 2820, 3222, 335, 3217], "orig_top_k_doc_id": [6847, 6842, 6845, 6304, 144, 6846, 7126, 302, 3221, 3216, 3220, 2820, 3222, 335, 3217]}, {"qid": 4331, "question": "Are all the tables in the dataset from the same website? in Content-Based Table Retrieval for Web Queries", "answer": ["No, they come from the top ranked web pages relevant to a query and from Wikipedia ", "Yes"], "top_k_doc_id": [144, 6842, 6845, 6847, 6846, 2789, 3216, 3220, 4463, 4558, 3287, 7126, 5149, 1308, 2457], "orig_top_k_doc_id": [6847, 6842, 6845, 144, 3220, 6846, 4558, 2789, 3216, 3287, 7126, 5149, 4463, 1308, 2457]}, {"qid": 4332, "question": "How are the tables extracted from the HTML? in Content-Based Table Retrieval for Web Queries", "answer": ["No", "No"], "top_k_doc_id": [144, 6842, 6845, 6847, 6846, 2789, 3216, 3220, 4463, 3665, 4594, 2792, 6411, 2448, 4425], "orig_top_k_doc_id": [6845, 6847, 6842, 144, 3220, 3665, 4594, 3216, 2792, 6411, 2448, 2789, 6846, 4425, 4463]}, {"qid": 4329, "question": "Do they use the unstructured text on the webpage that was the source of the table? in Content-Based Table Retrieval for Web Queries", "answer": ["No", "No"], "top_k_doc_id": [144, 6842, 6845, 6847, 6833, 7126, 3486, 6526, 6810, 6745, 2986, 3487, 5149, 302, 2457], "orig_top_k_doc_id": [6842, 6847, 144, 6833, 7126, 3486, 6845, 6526, 6810, 6745, 2986, 3487, 5149, 302, 2457]}]}
{"group_id": 441, "group_size": 5, "items": [{"qid": 4334, "question": "What datasets are used for experiments? in Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce", "answer": ["the AliMe and Quora dataset", "AliMe and Quora", "AliMe , Quora"], "top_k_doc_id": [3537, 6848, 6849, 6850, 6851, 7760, 411, 2455, 6218, 6219, 7758, 131, 410, 5257, 4969], "orig_top_k_doc_id": [6850, 6848, 6851, 6849, 3537, 7760, 7758, 411, 5257, 410, 6218, 4969, 131, 6219, 2455]}, {"qid": 4336, "question": "Is model compared to some baseline? in Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce", "answer": ["Yes"], "top_k_doc_id": [3537, 6848, 6849, 6850, 6851, 7760, 411, 2455, 6218, 6219, 7758, 131, 410, 5579, 6982], "orig_top_k_doc_id": [6850, 6848, 6851, 6849, 7760, 3537, 6219, 6218, 411, 7758, 410, 5579, 6982, 2455, 131]}, {"qid": 4337, "question": "What datasets are used in experiments? in Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce", "answer": [" the AliMe and Quora dataset "], "top_k_doc_id": [3537, 6848, 6849, 6850, 6851, 7760, 411, 2455, 6218, 6219, 7758, 131, 410, 5257, 4969], "orig_top_k_doc_id": [6850, 6848, 6851, 6849, 3537, 7760, 7758, 411, 5257, 410, 6218, 4969, 131, 6219, 2455]}, {"qid": 4335, "question": "Which natural language(s) is/are studied? in Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce", "answer": ["No", "Chinese and English", "No"], "top_k_doc_id": [3537, 6848, 6849, 6850, 6851, 7760, 411, 2455, 6218, 6219, 7758, 1325, 4969, 5257, 4597], "orig_top_k_doc_id": [6848, 6850, 6849, 6851, 3537, 7760, 7758, 6218, 2455, 1325, 6219, 411, 4969, 5257, 4597]}, {"qid": 4333, "question": "Does the query-bag matching model use a neural network? in Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [3537, 6848, 6849, 6850, 6851, 7760, 6982, 6334, 6983, 6846, 6984, 6845, 6649, 7542, 5579], "orig_top_k_doc_id": [6848, 6850, 6849, 6851, 7760, 6982, 6334, 3537, 6983, 6846, 6984, 6845, 6649, 7542, 5579]}]}
{"group_id": 442, "group_size": 5, "items": [{"qid": 4390, "question": "What two novel metrics proposed? in Recommendation Chart of Domains for Cross-Domain Sentiment Analysis:Findings of A 20 Domain Study", "answer": ["ULM4, ULM5", "LM3 (Chameleon Words Similarity) and LM4 (Entropy Change)"], "top_k_doc_id": [1048, 6910, 1052, 6915, 6550, 2468, 4727, 4732, 6911, 2030, 5365, 6547, 6912, 6914, 1949], "orig_top_k_doc_id": [6910, 6915, 1052, 6911, 6550, 2468, 2030, 4732, 5365, 6547, 1048, 6912, 4727, 6914, 1949]}, {"qid": 4392, "question": "What 20 domains are available for selection of source domain? in Recommendation Chart of Domains for Cross-Domain Sentiment Analysis:Findings of A 20 Domain Study", "answer": ["Amazon Instant Video,  Automotive, Baby,  Beauty, Books,  Clothing Accessories,  Electronics,  Health, Home, Kitchen, Movies, Music,  Office Products, Patio, Pet Supplies, Shoes,  Software,  Sports Outdoors,  Tools Home Improvement, Toys Games, Video Games.", "Amazon Instant Video\nAutomotive\nBaby\nBeauty\nBooks\nClothing Accessories\nElectronics\nHealth\nHome Kitchen\nMovies TV\nMusic\nOffice Products\nPatio\nPet Supplies\nShoes\nSoftware\nSports Outdoors\nTools Home Improvement\nToys Games\nVideo Games"], "top_k_doc_id": [1048, 6910, 1052, 6915, 6550, 2468, 4727, 4732, 6911, 6658, 2308, 6656, 67, 197, 199], "orig_top_k_doc_id": [6910, 6915, 6658, 1052, 6911, 1048, 6550, 2308, 4727, 6656, 67, 2468, 197, 4732, 199]}, {"qid": 4389, "question": "What datasets are available for CDSA task? in Recommendation Chart of Domains for Cross-Domain Sentiment Analysis:Findings of A 20 Domain Study", "answer": ["DRANZIERA benchmark dataset", "DRANZIERA "], "top_k_doc_id": [1048, 6910, 1052, 6915, 6550, 6914, 6913, 2308, 1039, 6547, 6912, 197, 196, 6644, 6658], "orig_top_k_doc_id": [6910, 6915, 6914, 6913, 1052, 1048, 2308, 1039, 6547, 6550, 6912, 197, 196, 6644, 6658]}, {"qid": 4391, "question": "What similarity metrics have been tried? in Recommendation Chart of Domains for Cross-Domain Sentiment Analysis:Findings of A 20 Domain Study", "answer": ["LM1: Significant Words Overlap,  LM2: Symmetric KL-Divergence (SKLD), LM3: Chameleon Words Similarity, LM4: Entropy Change,  ULM1: Word2Vec, ULM2: Doc2Vec, ULM3: GloVe, ULM4 and ULM5: FastText, ULM6: ELMo, ULM7: Universal Sentence Encoder", "LM1: Significant Words Overlap, LM2: Symmetric KL-Divergence (SKLD), LM3: Chameleon Words Similarity, LM4: Entropy Change, ULM1: Word2Vec, ULM2: Doc2Vec, ULM3: GloVe, ULM4 and ULM5: FastText,  ULM6: ELMo"], "top_k_doc_id": [1048, 6910, 1052, 6915, 6911, 3193, 2469, 1039, 1949, 6913, 6914, 5421, 3337, 5390, 1051], "orig_top_k_doc_id": [6910, 6915, 6911, 1052, 3193, 1048, 2469, 1039, 1949, 6913, 6914, 5421, 3337, 5390, 1051]}, {"qid": 1970, "question": "What dierse domains and languages are present in new datasets? in Text Length Adaptation in Sentiment Classification", "answer": ["movies , restaurants, English , Korean"], "top_k_doc_id": [1048, 6910, 1327, 2468, 2469, 1049, 2306, 2308, 67, 1325, 2310, 4727, 1625, 4730, 2950], "orig_top_k_doc_id": [1048, 6910, 1327, 2468, 2469, 1049, 2306, 2308, 67, 1325, 2310, 4727, 1625, 4730, 2950]}]}
{"group_id": 443, "group_size": 5, "items": [{"qid": 4400, "question": "How does their BERT-based model work? in Transforming Wikipedia into Augmented Data for Query-Focused Summarization", "answer": ["The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.", "It takes the query and document as input and encodes the query relevance, document context and salient meaning to be passed to the output layer to make the prediction."], "top_k_doc_id": [3198, 3202, 6922, 6925, 456, 460, 6923, 6924, 6926, 7243, 2048, 4278, 4396, 3719, 7617], "orig_top_k_doc_id": [6926, 6922, 6925, 6923, 7243, 6924, 2048, 3198, 460, 456, 3202, 4278, 4396, 3719, 7617]}, {"qid": 4401, "question": "How do they use Wikipedia to automatically collect a query-focused summarization dataset? in Transforming Wikipedia into Augmented Data for Query-Focused Summarization", "answer": ["To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query. ", "They use the article and section titles to build a query and use the body text of citation as the summary."], "top_k_doc_id": [3198, 3202, 6922, 6925, 456, 460, 6923, 6924, 6926, 7243, 6715, 3200, 3201, 6493, 7241], "orig_top_k_doc_id": [6922, 6926, 6923, 7243, 3198, 6925, 3202, 6924, 460, 6715, 3200, 3201, 6493, 456, 7241]}, {"qid": 2102, "question": "What models do they compare to? in AttSum: Joint Learning of Focusing and Summarization with Neural Attention", "answer": ["LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION"], "top_k_doc_id": [3198, 3202, 6922, 6925, 3201, 3200, 3199, 490, 4619, 1565, 37, 2335, 1253, 1567, 4826], "orig_top_k_doc_id": [3201, 3198, 3200, 3202, 3199, 6925, 490, 4619, 6922, 1565, 37, 2335, 1253, 1567, 4826]}, {"qid": 3461, "question": "what quantitative analysis is done? in A Multi-Task Architecture on Relevance-based Neural Query Translation", "answer": ["Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP)."], "top_k_doc_id": [3198, 2362, 3201, 4693, 5411, 5732, 5733, 7477, 1637, 4841, 2417, 7160, 2970, 5841, 6119], "orig_top_k_doc_id": [5732, 1637, 2362, 4841, 5733, 5411, 4693, 2417, 7160, 2970, 5841, 3198, 7477, 6119, 3201]}, {"qid": 3462, "question": "what are the baselines? in A Multi-Task Architecture on Relevance-based Neural Query Translation", "answer": ["the baseline transformer BIBREF8", "baseline transformer BIBREF8"], "top_k_doc_id": [3198, 2362, 3201, 4693, 5411, 5732, 5733, 7477, 3416, 6844, 6842, 4692, 3202, 460, 1865], "orig_top_k_doc_id": [5732, 5733, 3201, 4693, 3416, 7477, 6844, 6842, 5411, 4692, 3202, 460, 1865, 3198, 2362]}]}
{"group_id": 444, "group_size": 5, "items": [{"qid": 4561, "question": "What classification tasks do they experiment on? in r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection", "answer": ["fake news detection through text, image and text+image modes", "They experiment on 3 types of classification tasks with different inputs:\n2-way: True/False\n3-way: True/False news with text true in real world/False news with false text\n5-way: True/Parody/Missleading/Imposter/False Connection"], "top_k_doc_id": [3926, 3928, 3929, 3860, 6107, 7120, 7121, 7122, 3273, 6665, 3015, 3861, 6666, 5547, 4834], "orig_top_k_doc_id": [7122, 7120, 7121, 3860, 3926, 6665, 3273, 6107, 3929, 3861, 3928, 6666, 5547, 4834, 3015]}, {"qid": 4562, "question": "What categories of fake news are in the dataset? in r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection", "answer": ["Satire/Parody, Misleading Content, Imposter Content, False Connection", "Satire/Parody, Misleading Content, Imposter Content, False Connection"], "top_k_doc_id": [3926, 3928, 3929, 3860, 6107, 7120, 7121, 7122, 3273, 6665, 3015, 3861, 5135, 6663, 3927], "orig_top_k_doc_id": [7122, 7120, 7121, 3929, 3860, 3926, 5135, 3015, 6107, 3928, 6663, 3861, 3927, 6665, 3273]}, {"qid": 3750, "question": "Which datasets do they use? in Fake News Detection with Different Models", "answer": ["No", "https://github.com/Sairamvinay/Fake-News-Dataset\n\n", "No"], "top_k_doc_id": [3926, 3928, 3929, 3860, 6107, 7120, 7121, 7122, 3273, 6665, 27, 2157, 6663, 3277, 6666], "orig_top_k_doc_id": [3926, 7120, 6665, 3273, 3928, 7121, 27, 2157, 6663, 3929, 3860, 7122, 3277, 6107, 6666]}, {"qid": 3751, "question": "What models are explored in this paper? in Fake News Detection with Different Models", "answer": ["SVM, Logistic Regression, ANN, LSTM, and Random Forest", "Artificial Neural Network (ANN), Long Short Term Memory networks (LSTMs),  Random Forest, Logistic Regression,  Support Vector Machine (SVM)", "SVM, Logistic Regression, ANN, LSTM, Random Forest, TFIDF, CV, W2V"], "top_k_doc_id": [3926, 3928, 3929, 3860, 6107, 7120, 7121, 7122, 6667, 3277, 6663, 3927, 34, 6666, 2158], "orig_top_k_doc_id": [6667, 3277, 3926, 6663, 3927, 34, 6666, 3928, 6107, 7120, 2158, 3929, 7121, 3860, 7122]}, {"qid": 2407, "question": "What are state of the art methods authors compare their work with?  in Detecting Fake News with Capsule Neural Networks", "answer": ["ISOT dataset: LLVM\nLiar dataset: Hybrid CNN and LSTM with attention"], "top_k_doc_id": [3926, 3928, 3929, 3930, 3927, 27, 6663, 6666, 2157, 5135, 34, 6101, 1495, 3273, 33], "orig_top_k_doc_id": [3930, 3926, 3928, 3927, 3929, 27, 6663, 6666, 2157, 5135, 34, 6101, 1495, 3273, 33]}]}
{"group_id": 445, "group_size": 5, "items": [{"qid": 4617, "question": "how is user satisfaction estimated? in Learning from Dialogue after Deployment: Feed Yourself, Chatbot!", "answer": [" Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). ", "via crowdsourcing"], "top_k_doc_id": [2969, 7222, 7218, 7219, 7220, 2970, 5793, 6575, 7217, 7221, 7760, 2967, 7224, 3188, 3185], "orig_top_k_doc_id": [7218, 7219, 7222, 7217, 7221, 7220, 6575, 2967, 7224, 5793, 3188, 3185, 2970, 7760, 2969]}, {"qid": 4618, "question": "by how much did performance improve? in Learning from Dialogue after Deployment: Feed Yourself, Chatbot!", "answer": [" an increase of up to 9.4 accuracy points, a 31% improvement.", " the trained classifier significantly outperforms both the uncertainty-based methods and our original regular expression, by as much as 0.28 and 0.42 F1 points, respectively"], "top_k_doc_id": [2969, 7222, 7218, 7219, 7220, 2970, 5793, 6575, 7217, 7221, 7760, 6578, 824, 250, 3507], "orig_top_k_doc_id": [7217, 7218, 7222, 6575, 7219, 7221, 7220, 2970, 5793, 7760, 2969, 6578, 824, 250, 3507]}, {"qid": 1936, "question": "did they compare with other evaluation metrics? in Evaluation and Improvement of Chatbot Text Classification Data Quality Using Plausible Negative Examples", "answer": ["Yes"], "top_k_doc_id": [2969, 7222, 1817, 2888, 2889, 2890, 2891, 2892, 6578, 120, 121, 2970, 7089, 7760, 1139], "orig_top_k_doc_id": [2892, 2890, 2891, 2888, 2889, 2969, 120, 6578, 1817, 121, 2970, 7222, 7089, 7760, 1139]}, {"qid": 1937, "question": "which datasets were used in validation? in Evaluation and Improvement of Chatbot Text Classification Data Quality Using Plausible Negative Examples", "answer": ["No"], "top_k_doc_id": [2969, 7222, 1817, 2888, 2889, 2890, 2891, 2892, 6578, 7220, 7219, 98, 3307, 3445, 7756], "orig_top_k_doc_id": [2889, 2888, 2892, 2890, 2891, 7222, 6578, 2969, 7220, 7219, 98, 1817, 3307, 3445, 7756]}, {"qid": 2097, "question": "What model do they use a baseline to estimate satisfaction? in Improving Interaction Quality Estimation with BiLSTMs and the Impact on Dialogue Policy Learning", "answer": ["a BiLSTM without attention (BiLSTM) as well as a single forward-LSTM layer with attention (LSTM+att) and without attention (LSTM), baselines are defined by BIBREF32 who already proposed an LSTM-based architecture that only uses non-temporal features, and the SVM-based estimation model as originally used for reward estimation by BIBREF24"], "top_k_doc_id": [2969, 7222, 7218, 7219, 7220, 3185, 3188, 3187, 3186, 1082, 1677, 7086, 1168, 7087, 3189], "orig_top_k_doc_id": [3185, 3188, 3187, 3186, 7219, 7218, 1082, 7222, 1677, 7086, 2969, 1168, 7220, 7087, 3189]}]}
{"group_id": 446, "group_size": 5, "items": [{"qid": 4619, "question": "What datasets do they use in the experiment? in Meta Multi-Task Learning for Sequence Modeling", "answer": ["Wall Street Journal(WSJ) portion of Penn Treebank (PTB) , CoNLL 2000 chunking, CoNLL 2003 English NER , Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen, IMDB The movie reviews with labels of subjective or objective, MR The movie reviews with two classes", "CoNLL 2000 chunking, CoNLL 2003 English NER, Wall Street Journal(WSJ) portion of Penn Treebank (PTB), 14 datasets are product reviews, two sub-datasets about movie reviews"], "top_k_doc_id": [7231, 2556, 7067, 7227, 7228, 7066, 7068, 7230, 6673, 7069, 7070, 7229, 1768, 2619, 7283], "orig_top_k_doc_id": [7070, 7069, 7230, 7067, 7068, 7231, 7066, 7227, 7228, 2556, 6673, 7229, 1768, 2619, 7283]}, {"qid": 4621, "question": "What kind of meta learning algorithm do they use? in Meta Multi-Task Learning for Sequence Modeling", "answer": ["a function-level sharing scheme for multi-task learning", "a function-level sharing scheme for multi-task learning, in which a shared meta-network is used to learn the meta-knowledge of semantic composition among the different tasks"], "top_k_doc_id": [7231, 2556, 7067, 7227, 7228, 7066, 7068, 7230, 6673, 7069, 7070, 4161, 6672, 4160, 3568], "orig_top_k_doc_id": [7070, 7068, 7231, 7067, 7227, 4161, 7069, 7066, 2556, 6673, 6672, 4160, 7230, 7228, 3568]}, {"qid": 4620, "question": "What new tasks do they use to show the transferring ability of the shared meta-knowledge? in Meta Multi-Task Learning for Sequence Modeling", "answer": ["choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task", "we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task."], "top_k_doc_id": [7231, 2556, 7067, 7227, 7228, 7066, 7068, 7230, 4160, 7282, 4161, 7283, 4164, 3746, 2619], "orig_top_k_doc_id": [4160, 7231, 7230, 7282, 7227, 4161, 7066, 7228, 7283, 2556, 7068, 7067, 4164, 3746, 2619]}, {"qid": 1771, "question": "What is hard parameter sharing? in Latent Multi-task Architecture Learning", "answer": ["No"], "top_k_doc_id": [7231, 2556, 7067, 7227, 7228, 2557, 3344, 5858, 1552, 6664, 6041, 1881, 2258, 5859, 6663], "orig_top_k_doc_id": [2556, 2557, 3344, 5858, 1552, 7227, 7067, 6664, 7228, 6041, 1881, 2258, 5859, 6663, 7231]}, {"qid": 2437, "question": "What is the introduced meta-embedding method introduced in this paper? in Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words", "answer": ["proposed method comprises of two steps: a neighbourhood reconstruction step (Section \"Nearest Neighbour Reconstruction\" ), and a projection step (Section \"Projection to Meta-Embedding Space\" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. "], "top_k_doc_id": [7231, 3992, 3991, 3996, 3995, 3993, 3747, 6539, 6127, 4161, 4163, 3568, 7645, 7070, 3926], "orig_top_k_doc_id": [3992, 3991, 3996, 3995, 3993, 3747, 6539, 6127, 4161, 4163, 3568, 7645, 7070, 7231, 3926]}]}
{"group_id": 447, "group_size": 5, "items": [{"qid": 4788, "question": "What descriptive statistics are provided about the data? in CAp 2017 challenge: Twitter Named Entity Recognition", "answer": ["the number of entities, unique entities in the training and test sets", "Entity distribution in the training and test data."], "top_k_doc_id": [7456, 7458, 5879, 929, 2079, 2973, 5775, 4667, 4529, 7502, 933, 2405, 5739, 1266, 7172], "orig_top_k_doc_id": [7456, 929, 7458, 4667, 2973, 4529, 5879, 7502, 933, 2405, 5739, 2079, 1266, 7172, 5775]}, {"qid": 4790, "question": "What questions were asked in the annotation process? in CAp 2017 challenge: Twitter Named Entity Recognition", "answer": ["determine entities and annotate them based on the description that matched the type of entity", "Identify the entities occurring in the dataset and annotate them with one of the 13 possible types."], "top_k_doc_id": [7456, 7458, 5879, 4529, 5739, 7457, 3624, 1098, 5956, 7412, 607, 6244, 5735, 7534, 6676], "orig_top_k_doc_id": [7456, 7458, 3624, 1098, 5956, 5879, 7412, 607, 6244, 5735, 7534, 6676, 7457, 5739, 4529]}, {"qid": 4791, "question": "Why is NER for tweets more challenging as the number of entities increases? in CAp 2017 challenge: Twitter Named Entity Recognition", "answer": ["tweets contain informal text with multilingual content that becomes more difficult to classify when there are more options to choose from", "NER systems are usually trained using texts that follow particular morpho-syntactic rules. The tweets have a different style and don't follow these rules."], "top_k_doc_id": [7456, 7458, 5879, 929, 2079, 2973, 5775, 4302, 1262, 4749, 607, 7457, 2080, 4755, 618], "orig_top_k_doc_id": [7456, 7458, 929, 5775, 5879, 4302, 2973, 1262, 4749, 607, 7457, 2079, 2080, 4755, 618]}, {"qid": 4792, "question": "What data preparation steps were used to construct the dataset? in CAp 2017 challenge: Twitter Named Entity Recognition", "answer": ["The tweets were gathered using Twitter API plus tweets provided by the French National Railway Corporation; the tweets were split into training and test sets, and then annotated.", "collecting tweets both from the Twitter API and from SNCF to the identifying and annotating entities occurring in the tweets"], "top_k_doc_id": [7456, 7458, 5879, 4529, 5739, 7457, 2973, 716, 6153, 3743, 7131, 497, 3203, 5044, 7100], "orig_top_k_doc_id": [7456, 7458, 4529, 2973, 7457, 5739, 716, 6153, 3743, 7131, 5879, 497, 3203, 5044, 7100]}, {"qid": 4787, "question": "What method did the highest scoring team use? in CAp 2017 challenge: Twitter Named Entity Recognition", "answer": ["CRF model that used morphosyntactic and distributional features, as well as word clusters based on these learned representations.", "employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations."], "top_k_doc_id": [7456, 7458, 7679, 4529, 7677, 618, 7678, 7078, 2080, 4946, 4358, 7287, 929, 930, 7244], "orig_top_k_doc_id": [7456, 7458, 7679, 4529, 7677, 618, 7678, 7078, 2080, 4946, 4358, 7287, 929, 930, 7244]}]}
{"group_id": 448, "group_size": 5, "items": [{"qid": 4803, "question": "Were human evaluations conducted? in Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems", "answer": ["Yes", "Yes"], "top_k_doc_id": [7479, 7480, 3193, 7476, 7477, 7478, 495, 898, 3194, 2915, 2914, 7541, 5920, 6651, 7570], "orig_top_k_doc_id": [7479, 7478, 7480, 7476, 2915, 7477, 2914, 898, 3194, 7541, 3193, 495, 5920, 6651, 7570]}, {"qid": 4804, "question": "What datasets are used? in Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems", "answer": ["They create their own datasets from online text.", "To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs"], "top_k_doc_id": [7479, 7480, 3193, 7476, 7477, 7478, 495, 898, 3194, 7504, 7569, 4443, 4485, 2910, 1473], "orig_top_k_doc_id": [7479, 7478, 7480, 7476, 7477, 3193, 7504, 7569, 495, 4443, 898, 4485, 2910, 3194, 1473]}, {"qid": 2549, "question": "How big are datasets for 2019 Amazon Alexa competition? in Proposal Towards a Personalized Knowledge-powered Self-play Based Ensemble Dialog System", "answer": ["No"], "top_k_doc_id": [7479, 7480, 495, 4124, 4441, 4442, 4443, 4444, 4445, 3772, 898, 101, 3976, 102, 4120], "orig_top_k_doc_id": [4441, 4443, 4444, 4442, 4445, 3772, 898, 101, 4124, 3976, 495, 102, 4120, 7480, 7479]}, {"qid": 2550, "question": "What is novel in author's approach? in Proposal Towards a Personalized Knowledge-powered Self-play Based Ensemble Dialog System", "answer": ["They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data."], "top_k_doc_id": [7479, 7480, 495, 4124, 4441, 4442, 4443, 4444, 4445, 4508, 4128, 4125, 4126, 6880, 6588], "orig_top_k_doc_id": [4444, 4445, 4441, 7480, 7479, 495, 4442, 4508, 4128, 4124, 4443, 4125, 4126, 6880, 6588]}, {"qid": 4805, "question": "How does inference time compare to other methods? in Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems", "answer": ["No", "No"], "top_k_doc_id": [7479, 7480, 3193, 7476, 7477, 7478, 7541, 4719, 7569, 1474, 1921, 1940, 1473, 7570, 6334], "orig_top_k_doc_id": [7479, 7478, 7476, 7480, 7541, 7477, 3193, 4719, 7569, 1474, 1921, 1940, 1473, 7570, 6334]}]}
{"group_id": 449, "group_size": 5, "items": [{"qid": 4811, "question": "What language is the model tested on? in Fixed-Size Ordinally Forgetting Encoding Based Word Sense Disambiguation", "answer": ["No", "No"], "top_k_doc_id": [4476, 7491, 68, 350, 6061, 7490, 7492, 349, 351, 2149, 6438, 4737, 2494, 7785, 4946], "orig_top_k_doc_id": [7490, 7492, 7491, 68, 4476, 4737, 2494, 7785, 4946, 350, 351, 6061, 349, 2149, 6438]}, {"qid": 4814, "question": "What is a pseudo language model? in Fixed-Size Ordinally Forgetting Encoding Based Word Sense Disambiguation", "answer": ["different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence", "Pseudo language model abstracts context as embeddings using preceding and succeeding sequences."], "top_k_doc_id": [4476, 7491, 68, 350, 6061, 7490, 7492, 349, 351, 2149, 6438, 2148, 7827, 1722, 3691], "orig_top_k_doc_id": [7492, 7490, 7491, 2148, 7827, 4476, 350, 351, 6061, 1722, 349, 68, 2149, 6438, 3691]}, {"qid": 4813, "question": "What is the state-of-the-art model? in Fixed-Size Ordinally Forgetting Encoding Based Word Sense Disambiguation", "answer": ["BIBREF4", "LSTM"], "top_k_doc_id": [4476, 7491, 68, 350, 6061, 7490, 7492, 6768, 1560, 982, 4945, 3784, 5341, 4561, 4946], "orig_top_k_doc_id": [7490, 7492, 7491, 4476, 6768, 68, 1560, 6061, 982, 4945, 3784, 5341, 4561, 4946, 350]}, {"qid": 860, "question": "How is the fluctuation in the sense of the word and its neighbors measured? in Polysemy Detection in Distributed Representation of Word Sense", "answer": ["Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i \u2264 N) and w.\n4) Computing the mean m and the sample variance \u03c3 for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m \u2212 3\u03c3. If the value is less than m \u2212 3\u03c3, we may regard w as a polysemic word."], "top_k_doc_id": [4476, 7491, 68, 350, 1102, 1105, 1104, 803, 5761, 7425, 5341, 349, 7302, 72, 3737], "orig_top_k_doc_id": [1102, 1105, 1104, 803, 4476, 5761, 7425, 7491, 5341, 350, 349, 68, 7302, 72, 3737]}, {"qid": 4812, "question": "How much lower is the computational cost of the proposed model? in Fixed-Size Ordinally Forgetting Encoding Based Word Sense Disambiguation", "answer": ["BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days", "By 45 times."], "top_k_doc_id": [4476, 7491, 7492, 7490, 4475, 6791, 429, 6387, 4477, 3691, 3700, 7273, 1010, 73, 1107], "orig_top_k_doc_id": [7492, 7490, 7491, 4475, 4476, 6791, 429, 6387, 4477, 3691, 3700, 7273, 1010, 73, 1107]}]}
{"group_id": 450, "group_size": 5, "items": [{"qid": 4815, "question": "How significant is the performance compared to LSTM model? in Gated Recurrent Neural Tensor Network", "answer": ["0.03 absolute / 2.22% relative BPC, 11.29 absolute / 10.42% relative PPL", "GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN., From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN."], "top_k_doc_id": [3825, 7493, 2881, 7143, 7494, 7495, 7498, 4635, 6666, 4774, 1327, 1328, 6247, 2882, 167], "orig_top_k_doc_id": [7493, 7494, 7495, 2881, 4635, 7498, 4774, 6666, 1327, 1328, 3825, 6247, 7143, 2882, 167]}, {"qid": 4817, "question": "How much improvement do the introduced model achieve compared to the previous models? in Gated Recurrent Neural Tensor Network", "answer": ["we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN. , In this experiment, GRURNN made faster progress than LSTMRNN, but eventually LSTMRNN converged into a better BPC based on the development set. Our proposed model GRURNTN made faster and quicker progress than LSTMRNTN and converged into a similar BPC in the last epoch. Both proposed models produced lower BPC than our baseline models from the first epoch to the last epoch., Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task., we report our experiment results on PTB word-level language modeling using our baseline models GRURNN and LSTMRNN and our proposed models GRURNTN and LSTMRNTN. , In this experiment, GRURNN made faster progress than LSTMRNN. Our proposed GRURNTN's progress was also better than LSTMRNTN. The best model in this task was GRURNTN, which had a consistently lower PPL than the other models., GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin., In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. ", "GRURNTN, character: 0.06 absolute / 4.32% relative bits-per-character.\nLSTMRNTN, character: 0.03 absolute / 2.22% relative bits-per-character.\nGRURNTN, word: 10.4 absolute / 10.63% relative perplexity.\nLSTMRNTN, word: 11.29 absolute / 10.42% relative perplexity."], "top_k_doc_id": [3825, 7493, 2881, 7143, 7494, 7495, 7498, 4635, 6666, 4692, 4198, 5595, 4636, 7240, 3556], "orig_top_k_doc_id": [7493, 7498, 7495, 7494, 4692, 4198, 2881, 3825, 6666, 5595, 4635, 4636, 7240, 3556, 7143]}, {"qid": 3549, "question": "what data did they use? in Pyramidal Recurrent Unit for Language Modeling", "answer": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "top_k_doc_id": [3825, 7493, 4692, 5863, 5864, 5865, 5866, 7363, 3826, 5835, 1619, 7079, 2970, 1389, 3705], "orig_top_k_doc_id": [5863, 5866, 5864, 5865, 7493, 3825, 3826, 4692, 5835, 7363, 1619, 7079, 2970, 1389, 3705]}, {"qid": 3550, "question": "what previous RNN models do they compare with? in Pyramidal Recurrent Unit for Language Modeling", "answer": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "top_k_doc_id": [3825, 7493, 4692, 5863, 5864, 5865, 5866, 7363, 2607, 4926, 7364, 1739, 7391, 4213, 3034], "orig_top_k_doc_id": [5863, 5864, 5866, 5865, 7493, 3825, 2607, 4926, 7363, 4692, 7364, 1739, 7391, 4213, 3034]}, {"qid": 4816, "question": "How does the introduced model combine the both factors? in Gated Recurrent Neural Tensor Network", "answer": ["in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values., As with GRURNTN, we also applied the tensor product operation for the LSTM unit to improve its performance. In this architecture, the tensor product operation is applied between the current input and the previous hidden layers to calculate the current memory cell. The calculation is parameterized by the tensor weight. We call this architecture a Long Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN). ", "For the former limitation, the RecNN performance can be improved by adding more interaction between the two input vectors. Therefore, a new architecture called a Recursive Neural Tensor Network (RecNTN) tried to overcome the previous problem by adding interaction between two vectors using a tensor product, which is connected by tensor weight parameters. Each slice of the tensor weight can be used to capture the specific pattern between the left and right child vectors. For RecNTN, value $p_1$ from Eq. 13 and is defined by:\n\n$$p_1 &=& f\\left( \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} W_{tsr}^{[1:d]} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} W + b \\right) \\\\ p_2 &=& f\\left( \\begin{bmatrix} p_1 & x_3 \\end{bmatrix} W_{tsr}^{[1:d]} \\begin{bmatrix} p_1 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} p_1 & x_3 \\end{bmatrix} W + b \\right)$$ (Eq. 15)\n\nwhere $W_{tsr}^{[1:d]} \\in \\mathbb {R}^{2d \\times 2d \\times d}$ is the tensor weight to map the tensor product between two children vectors. Each slice $W_{tsr}^{[i]}$ is a matrix $\\mathbb {R}^{2d \\times 2d}$ . "], "top_k_doc_id": [3825, 7493, 2881, 7143, 7494, 7495, 7498, 2940, 5513, 3559, 4692, 4198, 3534, 6247, 7131], "orig_top_k_doc_id": [7493, 2881, 7495, 7498, 7494, 7143, 3825, 2940, 5513, 3559, 4692, 4198, 3534, 6247, 7131]}]}
{"group_id": 451, "group_size": 5, "items": [{"qid": 4830, "question": "Do they compare their approach to data-driven only methods? in Neuro-symbolic Architectures for Context Understanding", "answer": ["Yes", "No"], "top_k_doc_id": [1217, 7514, 3100, 5109, 7148, 7515, 7520, 490, 6723, 5430, 898, 2746, 3357, 2106, 6395], "orig_top_k_doc_id": [7514, 7148, 7520, 5430, 6723, 490, 898, 2746, 5109, 7515, 3357, 2106, 1217, 6395, 3100]}, {"qid": 4831, "question": "What are the two applications of neuro-symbolism? in Neuro-symbolic Architectures for Context Understanding", "answer": ["Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes, Neural Question-Answering using Commonsense Knowledge Bases", "Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes, Application II: Neural Question-Answering using Commonsense Knowledge Bases"], "top_k_doc_id": [1217, 7514, 3100, 5109, 7148, 7515, 7520, 490, 6723, 2150, 7685, 1216, 3368, 2149, 3612], "orig_top_k_doc_id": [7514, 7148, 7520, 5109, 490, 7515, 2150, 3100, 6723, 7685, 1216, 1217, 3368, 2149, 3612]}, {"qid": 2755, "question": "what datasets did they use? in A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification", "answer": ["Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.", "3500 questions collected from the internet and books."], "top_k_doc_id": [1217, 7514, 4819, 4821, 6606, 7805, 1547, 7174, 490, 3416, 425, 75, 7173, 1216, 1667], "orig_top_k_doc_id": [4819, 1547, 4821, 7174, 6606, 7805, 490, 3416, 7514, 1217, 425, 75, 7173, 1216, 1667]}, {"qid": 2756, "question": "what ml based approaches were compared? in A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification", "answer": ["Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)", "Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest"], "top_k_doc_id": [1217, 7514, 4819, 4821, 6606, 7805, 2386, 52, 6399, 3431, 7520, 2181, 6400, 2832, 6609], "orig_top_k_doc_id": [4819, 7514, 4821, 2386, 6606, 52, 6399, 7805, 3431, 7520, 2181, 6400, 2832, 6609, 1217]}, {"qid": 4829, "question": "How do they interpret the model? in Neuro-symbolic Architectures for Context Understanding", "answer": ["No", "They find relations that connect questions to the answer-options."], "top_k_doc_id": [1217, 7514, 3100, 5109, 7148, 7515, 7520, 686, 5832, 5966, 5240, 4878, 575, 2746, 2578], "orig_top_k_doc_id": [7514, 7148, 686, 5832, 5966, 5240, 5109, 4878, 7520, 575, 2746, 1217, 7515, 3100, 2578]}]}
{"group_id": 452, "group_size": 5, "items": [{"qid": 4841, "question": "What explanation do the authors offer for the super or sublinear urban scaling? in Scaling in Words on Twitter", "answer": ["abundance or lack of the elements of urban lifestyle"], "top_k_doc_id": [7530, 7533, 7531, 7532, 3080, 3081, 838, 1432, 1815, 5422, 3135, 998, 3133, 3691, 331], "orig_top_k_doc_id": [7532, 7533, 7530, 7531, 1432, 3080, 3081, 3135, 998, 3133, 3691, 5422, 331, 838, 1815]}, {"qid": 4842, "question": "Do the authors give examples of the core vocabulary which follows the scaling relationship of the bulk text? in Scaling in Words on Twitter", "answer": ["Yes"], "top_k_doc_id": [7530, 7533, 7531, 7532, 3080, 3081, 838, 1432, 1815, 5422, 1814, 3527, 3083, 5973, 3084], "orig_top_k_doc_id": [7533, 7532, 7531, 7530, 838, 1815, 3081, 1814, 3527, 3083, 3080, 5973, 5422, 1432, 3084]}, {"qid": 4840, "question": "Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter? in Scaling in Words on Twitter", "answer": ["No", "Yes", "No"], "top_k_doc_id": [7530, 7533, 7531, 7532, 3080, 3081, 3527, 3710, 3713, 3711, 3528, 4508, 5038, 3245, 3649], "orig_top_k_doc_id": [7530, 7533, 3527, 7532, 3710, 7531, 3713, 3711, 3081, 3528, 4508, 3080, 5038, 3245, 3649]}, {"qid": 4839, "question": "Is this analysis performed only on English data? in Scaling in Words on Twitter", "answer": ["No", "No", "Yes"], "top_k_doc_id": [7530, 7533, 7531, 7532, 3527, 5038, 5422, 1432, 2076, 7172, 6402, 951, 63, 59, 6396], "orig_top_k_doc_id": [3527, 7530, 7533, 7532, 5038, 5422, 1432, 7531, 2076, 7172, 6402, 951, 63, 59, 6396]}, {"qid": 2324, "question": "How do Zipf and Herdan-Heap's laws differ? in Universal and non-universal text statistics: Clustering coefficient for language identification", "answer": ["Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)"], "top_k_doc_id": [7530, 7533, 3711, 3713, 3710, 3712, 4949, 5428, 4506, 2771, 2806, 1773, 3600, 626, 3884], "orig_top_k_doc_id": [3711, 3713, 3710, 3712, 7533, 4949, 7530, 5428, 4506, 2771, 2806, 1773, 3600, 626, 3884]}]}
{"group_id": 453, "group_size": 5, "items": [{"qid": 4890, "question": "How do they deal with imbalanced datasets? in A hybrid text normalization system using multi-head self-attention for mandarin", "answer": ["data expansion using oversampling, add loss control", "data expansion using oversampling, add loss control in the model"], "top_k_doc_id": [7594, 7595, 7596, 1618, 7647, 2838, 4179, 6969, 2918, 6066, 1865, 2940, 7251, 2919, 652], "orig_top_k_doc_id": [7594, 7595, 7596, 2838, 1865, 7647, 6969, 2940, 6066, 1618, 7251, 2918, 4179, 2919, 652]}, {"qid": 4891, "question": "What models do they compare to? in A hybrid text normalization system using multi-head self-attention for mandarin", "answer": ["rule-based TN model", "six different variations of their multi-head attention model"], "top_k_doc_id": [7594, 7595, 7596, 1618, 7647, 2838, 4179, 6969, 2918, 6066, 7648, 2917, 1566, 5287, 6292], "orig_top_k_doc_id": [7594, 7595, 7596, 6066, 7647, 1618, 6969, 2838, 7648, 2917, 1566, 2918, 5287, 6292, 4179]}, {"qid": 4894, "question": "Did they collect their own corpus? in A hybrid text normalization system using multi-head self-attention for mandarin", "answer": ["No", "No"], "top_k_doc_id": [7594, 7595, 7596, 1618, 7647, 2838, 4179, 6969, 2049, 1410, 3163, 1017, 5939, 1865, 4178], "orig_top_k_doc_id": [7594, 7595, 7596, 2049, 7647, 4179, 6969, 1410, 1618, 3163, 1017, 5939, 1865, 4178, 2838]}, {"qid": 4892, "question": "What text preprocessing tasks do they focus on? in A hybrid text normalization system using multi-head self-attention for mandarin", "answer": ["normalize unreadable numbers, symbols or characters", "No"], "top_k_doc_id": [7594, 7595, 7596, 1618, 7647, 6066, 1411, 2940, 888, 7648, 2917, 7251, 3027, 2529, 1560], "orig_top_k_doc_id": [7594, 7595, 6066, 7596, 1411, 1618, 2940, 7647, 888, 7648, 2917, 7251, 3027, 2529, 1560]}, {"qid": 4893, "question": "What news sources did they get the dataset from? in A hybrid text normalization system using multi-head self-attention for mandarin", "answer": ["No", "No"], "top_k_doc_id": [7594, 7595, 7596, 1865, 2838, 2049, 5287, 652, 1410, 1084, 974, 1517, 3945, 6066, 3163], "orig_top_k_doc_id": [7594, 7595, 7596, 1865, 2838, 2049, 5287, 652, 1410, 1084, 974, 1517, 3945, 6066, 3163]}]}
{"group_id": 454, "group_size": 5, "items": [{"qid": 4895, "question": "Do the tweets fall under a specific domain? in To What Extent are Name Variants Used as Named Entities in Turkish Tweets?", "answer": ["No", "No"], "top_k_doc_id": [2973, 4301, 4302, 4303, 2329, 2974, 7597, 7598, 7599, 1262, 4300, 6050, 5421, 4131, 5420], "orig_top_k_doc_id": [7599, 7597, 7598, 4302, 4303, 2974, 2329, 4301, 6050, 5421, 4300, 1262, 2973, 4131, 5420]}, {"qid": 4896, "question": "How many tweets are in the dataset? in To What Extent are Name Variants Used as Named Entities in Turkish Tweets?", "answer": ["670 tweets ", "These 980 PLOs were annotated within a total of 670 tweets."], "top_k_doc_id": [2973, 4301, 4302, 4303, 2329, 2974, 7597, 7598, 7599, 1262, 4300, 5879, 7412, 7457, 3795], "orig_top_k_doc_id": [7597, 7599, 7598, 4302, 4303, 4301, 2329, 1262, 5879, 2974, 7412, 7457, 3795, 4300, 2973]}, {"qid": 2518, "question": "What type and size of word embeddings were used? in Named Entity Recognition on Twitter for Turkish using Semi-supervised Learning with Word Embeddings", "answer": ["word2vec, 200 as the dimension of the obtained word vectors"], "top_k_doc_id": [2973, 4301, 4302, 4303, 450, 4300, 5420, 2318, 930, 2875, 5912, 5775, 5698, 3845, 2974], "orig_top_k_doc_id": [4300, 4302, 4301, 4303, 2318, 450, 930, 5420, 2973, 2875, 5912, 5775, 5698, 3845, 2974]}, {"qid": 2519, "question": "What data was used to build the word embeddings? in Named Entity Recognition on Twitter for Turkish using Semi-supervised Learning with Word Embeddings", "answer": ["Turkish news-web corpus,  TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyal\u0131"], "top_k_doc_id": [2973, 4301, 4302, 4303, 450, 4300, 5420, 4399, 5421, 2448, 2321, 4529, 7100, 4755, 3703], "orig_top_k_doc_id": [4300, 4302, 4303, 4301, 450, 4399, 5421, 2448, 2321, 4529, 2973, 5420, 7100, 4755, 3703]}, {"qid": 4897, "question": "What categories do they look at? in To What Extent are Name Variants Used as Named Entities in Turkish Tweets?", "answer": ["PERSON, LOCATION, and ORGANIZATION", "PERSON, LOCATION, ORGANIZATION"], "top_k_doc_id": [2973, 4301, 4302, 4303, 2329, 2974, 7597, 7598, 7599, 4536, 2538, 1642, 2875, 6280, 3624], "orig_top_k_doc_id": [7597, 7598, 7599, 4536, 4302, 2538, 1642, 2875, 6280, 2973, 2329, 2974, 4303, 4301, 3624]}]}
{"group_id": 455, "group_size": 5, "items": [{"qid": 4910, "question": "What is the strong baseline model used? in Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning", "answer": ["an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0", "Passage-only heuristic baseline, QANet, QANet+BERT, BERT QA"], "top_k_doc_id": [1357, 7589, 7590, 2661, 3972, 7572, 4791, 4792, 5473, 7615, 7616, 4637, 7573, 7592, 884], "orig_top_k_doc_id": [7615, 7616, 4791, 7589, 7572, 1357, 7590, 3972, 5473, 4637, 4792, 7573, 7592, 2661, 884]}, {"qid": 4911, "question": "What crowdsourcing platform did they obtain the data from? in Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning", "answer": ["Mechanical Turk", "Mechanical Turk"], "top_k_doc_id": [1357, 7589, 7590, 2661, 3972, 7572, 4791, 4792, 5473, 7615, 7616, 4075, 5605, 4074, 5610], "orig_top_k_doc_id": [7615, 7616, 4791, 7589, 7590, 1357, 2661, 4075, 5605, 4074, 4792, 5473, 7572, 5610, 3972]}, {"qid": 1036, "question": "what are the existing models they compared with? in NumNet: Machine Reading Comprehension with Numerical Reasoning", "answer": ["Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+"], "top_k_doc_id": [1357, 7589, 7590, 2661, 3972, 7572, 1361, 1360, 1358, 5257, 3416, 7728, 4518, 7573, 5250], "orig_top_k_doc_id": [1357, 1361, 1360, 1358, 7589, 3972, 7590, 7572, 5257, 3416, 2661, 7728, 4518, 7573, 5250]}, {"qid": 4888, "question": "How big is this dataset? in ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning", "answer": ["6,138 logical reasoning questions", "6,138 pieces of logical reasoning questions"], "top_k_doc_id": [1357, 7589, 7590, 384, 1290, 4278, 7573, 7591, 7592, 7593, 7615, 7616, 7578, 3851, 7728], "orig_top_k_doc_id": [7589, 7590, 7593, 7592, 7591, 7615, 7616, 1357, 384, 7573, 1290, 7578, 3851, 4278, 7728]}, {"qid": 4889, "question": "How are biases identified in the dataset? in ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning", "answer": ["we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem,  identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question", "They identify biases as lexical choice and sentence length for right and wrong answer options in an isolated context, without the question and paragraph context that typically precedes answer options. Lexical choice was identified by calculating per-token correlation scores with \"right\" and \"wrong labels. They calculated the average sentence length for \"right\" and \"wrong\" sentences."], "top_k_doc_id": [1357, 7589, 7590, 384, 1290, 4278, 7573, 7591, 7592, 7593, 7615, 7616, 7572, 4915, 325], "orig_top_k_doc_id": [7589, 7590, 7593, 7592, 7591, 7572, 7573, 384, 7615, 4915, 4278, 325, 7616, 1357, 1290]}]}
{"group_id": 456, "group_size": 4, "items": [{"qid": 124, "question": "How does the conversation layer work? in QnAMaker: Data to Bot in 2 Minutes", "answer": ["No"], "top_k_doc_id": [1074, 144, 145, 146, 824, 7774, 4127, 7219, 7218, 3366, 7758, 3367, 4126, 7224, 103], "orig_top_k_doc_id": [144, 145, 146, 1074, 7774, 7218, 3366, 4127, 7219, 7758, 824, 3367, 4126, 7224, 103]}, {"qid": 125, "question": "What components is the QnAMaker composed of? in QnAMaker: Data to Bot in 2 Minutes", "answer": ["QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot", "QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot"], "top_k_doc_id": [1074, 144, 145, 146, 824, 7774, 4127, 7219, 6848, 4441, 4125, 1733, 4506, 4810, 4663], "orig_top_k_doc_id": [144, 145, 146, 7774, 1074, 824, 7219, 6848, 4441, 4127, 4125, 1733, 4506, 4810, 4663]}, {"qid": 123, "question": "What experiments do the authors present to validate their system? in QnAMaker: Data to Bot in 2 Minutes", "answer": [" we measure our system's performance for datasets across various domains, evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs"], "top_k_doc_id": [1074, 144, 145, 146, 824, 7774, 6157, 1729, 5440, 4506, 6523, 3367, 6156, 5441, 1730], "orig_top_k_doc_id": [144, 145, 146, 6157, 7774, 824, 1074, 1729, 5440, 4506, 6523, 3367, 6156, 5441, 1730]}, {"qid": 2190, "question": "Did they experiment with the corpus? in A Large-Scale Corpus for Conversation Disentanglement", "answer": ["Yes"], "top_k_doc_id": [1074, 3363, 3362, 3367, 3364, 6584, 2906, 6625, 3516, 6653, 6793, 7508, 1655, 196, 1163], "orig_top_k_doc_id": [3363, 3362, 3367, 3364, 6584, 2906, 6625, 3516, 6653, 6793, 7508, 1655, 196, 1074, 1163]}]}
{"group_id": 457, "group_size": 4, "items": [{"qid": 156, "question": "What models other than standalone BERT is new model compared to? in BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "answer": ["Only Bert base and Bert large are compared to proposed approach."], "top_k_doc_id": [203, 204, 205, 206, 207, 533, 22, 208, 1775, 4607, 6697, 4510, 5711, 21, 20], "orig_top_k_doc_id": [207, 203, 533, 205, 1775, 22, 208, 204, 6697, 4510, 206, 5711, 21, 20, 4607]}, {"qid": 157, "question": "How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work? in BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "answer": ["improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking"], "top_k_doc_id": [203, 204, 205, 206, 207, 533, 22, 208, 1775, 4607, 1345, 6163, 1910, 5715, 2306], "orig_top_k_doc_id": [207, 203, 205, 206, 208, 204, 533, 1775, 1345, 6163, 22, 1910, 4607, 5715, 2306]}, {"qid": 158, "question": "What are three downstream task datasets? in BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "answer": ["MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23", "MNLI, AG's News, DBPedia"], "top_k_doc_id": [203, 204, 205, 206, 207, 533, 709, 4603, 6697, 208, 2238, 1936, 23, 3606, 690], "orig_top_k_doc_id": [203, 6697, 207, 208, 205, 206, 709, 533, 4603, 2238, 1936, 23, 204, 3606, 690]}, {"qid": 159, "question": "What is dataset for word probing task? in BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "answer": ["WNLaMPro dataset"], "top_k_doc_id": [203, 204, 205, 206, 207, 533, 709, 4603, 6697, 4328, 4331, 6448, 4330, 4604, 6870], "orig_top_k_doc_id": [203, 4328, 4603, 4331, 205, 207, 6448, 6697, 4330, 206, 4604, 6870, 533, 204, 709]}]}
{"group_id": 458, "group_size": 4, "items": [{"qid": 160, "question": "How fast is the model compared to baselines? in Joint Entity Linking with Deep Reinforcement Learning", "answer": ["No"], "top_k_doc_id": [209, 214, 215, 3849, 4320, 4859, 7609, 3633, 4318, 1294, 2846, 300, 5335, 6046, 4601], "orig_top_k_doc_id": [215, 214, 209, 1294, 3849, 4320, 3633, 2846, 300, 4859, 7609, 5335, 4318, 6046, 4601]}, {"qid": 161, "question": "How big is the performance difference between this method and the baseline? in Joint Entity Linking with Deep Reinforcement Learning", "answer": ["Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores."], "top_k_doc_id": [209, 214, 215, 3849, 4320, 4859, 7609, 3633, 4318, 3634, 7843, 618, 4599, 5336, 2871], "orig_top_k_doc_id": [215, 214, 3634, 3633, 209, 4320, 7843, 3849, 618, 4599, 7609, 4318, 4859, 5336, 2871]}, {"qid": 162, "question": "What datasets used for evaluation? in Joint Entity Linking with Deep Reinforcement Learning", "answer": ["AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI", "AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI"], "top_k_doc_id": [209, 214, 215, 3849, 4320, 4859, 7609, 560, 3634, 6049, 7100, 6046, 212, 7605, 4576], "orig_top_k_doc_id": [215, 214, 209, 7609, 4320, 6049, 3634, 6046, 560, 3849, 7100, 212, 7605, 4576, 4859]}, {"qid": 163, "question": "what are the mentioned cues? in Joint Entity Linking with Deep Reinforcement Learning", "answer": ["output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7"], "top_k_doc_id": [209, 214, 215, 3849, 4320, 4859, 7609, 560, 3634, 6049, 7100, 5336, 4720, 210, 6950], "orig_top_k_doc_id": [215, 209, 214, 560, 7609, 4859, 3849, 5336, 4320, 7100, 6049, 4720, 210, 3634, 6950]}]}
{"group_id": 459, "group_size": 4, "items": [{"qid": 164, "question": "How did the author's work rank among other submissions on the challenge? in Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "answer": ["No"], "top_k_doc_id": [217, 219, 1094, 1322, 7352, 1694, 6449, 5231, 303, 5739, 6916, 1555, 1099, 302, 7351], "orig_top_k_doc_id": [217, 219, 5231, 1694, 7352, 303, 5739, 1322, 6449, 6916, 1555, 1099, 302, 1094, 7351]}, {"qid": 165, "question": "What approaches without reinforcement learning have been tried? in Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "answer": ["classification, regression, neural methods", " Support Vector Regression (SVR) and Support Vector Classification (SVC), deep learning regression models of BIBREF2 to convert them to classification models"], "top_k_doc_id": [217, 219, 1094, 1322, 7352, 220, 2362, 4845, 5368, 7351, 7664, 6863, 3201, 1637, 3202], "orig_top_k_doc_id": [217, 1322, 219, 7664, 7351, 7352, 6863, 220, 1094, 3201, 5368, 1637, 4845, 2362, 3202]}, {"qid": 166, "question": "What classification approaches were experimented for this task? in Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "answer": ["NNC SU4 F1, NNC top 5, Support Vector Classification (SVC)"], "top_k_doc_id": [217, 219, 1094, 1322, 7352, 220, 2362, 4845, 5368, 7351, 6449, 1694, 2661, 4833, 5736], "orig_top_k_doc_id": [217, 1322, 219, 7352, 220, 1094, 7351, 4845, 5368, 6449, 1694, 2661, 2362, 4833, 5736]}, {"qid": 167, "question": "Did classification models perform better than previous regression one? in Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "answer": ["Yes"], "top_k_doc_id": [217, 219, 1094, 1322, 7352, 1694, 6449, 1201, 4497, 6925, 5611, 1202, 5097, 6922, 6453], "orig_top_k_doc_id": [217, 219, 1201, 4497, 6925, 7352, 6449, 1322, 1694, 5611, 1202, 5097, 6922, 6453, 1094]}]}
{"group_id": 460, "group_size": 4, "items": [{"qid": 169, "question": "Do they look for inconsistencies between different languages' annotations in UniMorph? in Marrying Universal Dependencies and Universal Morphology", "answer": ["Yes"], "top_k_doc_id": [221, 222, 225, 1773, 2806, 6166, 6279, 6280, 224, 624, 396, 395, 223, 626, 4591], "orig_top_k_doc_id": [225, 222, 221, 224, 6166, 6280, 1773, 2806, 396, 395, 6279, 223, 626, 4591, 624]}, {"qid": 170, "question": "Do they look for inconsistencies between different UD treebanks? in Marrying Universal Dependencies and Universal Morphology", "answer": ["Yes"], "top_k_doc_id": [221, 222, 225, 1773, 2806, 6166, 6279, 6280, 224, 624, 4146, 3258, 7320, 2807, 6466], "orig_top_k_doc_id": [225, 222, 221, 2806, 6280, 6279, 6166, 4146, 1773, 3258, 224, 624, 7320, 2807, 6466]}, {"qid": 168, "question": "What are the main sources of recall errors in the mapping? in Marrying Universal Dependencies and Universal Morphology", "answer": ["irremediable annotation discrepancies, differences in choice of attributes to annotate, The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them, the two annotations encode distinct information, incorrectly applied UniMorph annotation, cross-lingual inconsistency in both resources"], "top_k_doc_id": [221, 222, 225, 1773, 2806, 6166, 6279, 6280, 224, 6152, 7318, 7327, 6466, 6467, 3358], "orig_top_k_doc_id": [221, 225, 6166, 222, 6280, 1773, 6152, 2806, 7318, 7327, 6279, 6466, 6467, 224, 3358]}, {"qid": 171, "question": "Which languages do they validate on? in Marrying Universal Dependencies and Universal Morphology", "answer": ["Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur", "We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish"], "top_k_doc_id": [221, 222, 225, 1773, 2806, 6166, 6279, 6280, 4290, 624, 7318, 4696, 5341, 4026, 6033], "orig_top_k_doc_id": [225, 221, 6280, 1773, 6279, 222, 2806, 4290, 624, 7318, 6166, 4696, 5341, 4026, 6033]}]}
{"group_id": 461, "group_size": 4, "items": [{"qid": 201, "question": "What faithfulness criteria does they propose? in Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "answer": ["Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."], "top_k_doc_id": [252, 253, 254, 255, 3719, 3109, 5230, 6978, 5226, 7660, 2225, 5765, 7572, 1279, 4140], "orig_top_k_doc_id": [252, 253, 254, 255, 5230, 3719, 7660, 2225, 5226, 6978, 5765, 7572, 1279, 3109, 4140]}, {"qid": 202, "question": "Which are three assumptions in current approaches for defining faithfulness? in Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "answer": ["Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.", "Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."], "top_k_doc_id": [252, 253, 254, 255, 3719, 3109, 5230, 6978, 5226, 5910, 7313, 3211, 2258, 3299, 1934], "orig_top_k_doc_id": [252, 253, 254, 255, 3719, 5230, 6978, 5226, 5910, 7313, 3211, 2258, 3299, 1934, 3109]}, {"qid": 203, "question": "Which are key points in guidelines for faithfulness evaluation? in Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "answer": ["Be explicit in what you evaluate., Faithfulness evaluation should not involve human-judgement on the quality of interpretation., Faithfulness evaluation should not involve human-provided gold labels., Do not trust \u201cinherent interpretability\u201d claims., Faithfulness evaluation of IUI systems should not rely on user performance."], "top_k_doc_id": [252, 253, 254, 255, 3719, 3109, 5230, 6978, 7660, 3588, 5910, 1905, 3309, 2327, 3593], "orig_top_k_doc_id": [252, 253, 255, 254, 3719, 6978, 5230, 7660, 3588, 5910, 3109, 1905, 3309, 2327, 3593]}, {"qid": 200, "question": "What approaches they propose? in Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "answer": ["Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."], "top_k_doc_id": [252, 253, 254, 255, 3719, 3208, 7572, 5913, 6882, 4825, 5910, 3211, 3207, 2258, 1905], "orig_top_k_doc_id": [252, 253, 254, 255, 3208, 7572, 5913, 6882, 4825, 5910, 3719, 3211, 3207, 2258, 1905]}]}
{"group_id": 462, "group_size": 4, "items": [{"qid": 205, "question": "What is the performance of their model? in Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference", "answer": ["No", "No"], "top_k_doc_id": [256, 5240, 531, 1560, 1743, 3175, 7661, 4774, 6344, 6847, 7371, 6082, 3298, 639, 7662], "orig_top_k_doc_id": [256, 6344, 1743, 1560, 5240, 3175, 6847, 7371, 7661, 4774, 531, 6082, 3298, 639, 7662]}, {"qid": 206, "question": "How many layers are there in their model? in Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference", "answer": ["two LSTM layers"], "top_k_doc_id": [256, 5240, 531, 1560, 1743, 3175, 7661, 4774, 6344, 3559, 6135, 4811, 891, 1618, 4812], "orig_top_k_doc_id": [256, 1560, 3559, 6344, 6135, 4774, 1743, 3175, 5240, 4811, 891, 531, 7661, 1618, 4812]}, {"qid": 204, "question": "Did they use the state-of-the-art model to analyze the attention? in Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference", "answer": ["we provide an extensive analysis of the state-of-the-art model"], "top_k_doc_id": [256, 5240, 531, 1560, 1743, 3175, 7661, 6135, 891, 3357, 2253, 533, 3298, 4033, 4276], "orig_top_k_doc_id": [256, 5240, 531, 1560, 6135, 891, 3357, 7661, 3175, 2253, 533, 3298, 4033, 1743, 4276]}, {"qid": 207, "question": "Did they compare with gradient-based methods? in Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference", "answer": ["No"], "top_k_doc_id": [256, 5240, 3298, 1155, 768, 3357, 3822, 6344, 3559, 1154, 4774, 7371, 3863, 4812, 1618], "orig_top_k_doc_id": [5240, 256, 3298, 1155, 768, 3357, 3822, 6344, 3559, 1154, 4774, 7371, 3863, 4812, 1618]}]}
{"group_id": 463, "group_size": 4, "items": [{"qid": 209, "question": "how much of improvement the adaptation model can get? in Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "answer": [" 69.10%/78.38%"], "top_k_doc_id": [259, 263, 4928, 3839, 6508, 7244, 1106, 6509, 7541, 6511, 6482, 6510, 1109, 2725, 4637], "orig_top_k_doc_id": [263, 4928, 259, 7244, 6511, 6482, 6510, 7541, 1109, 1106, 3839, 2725, 6508, 4637, 6509]}, {"qid": 210, "question": "what is the architecture of the baseline model? in Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "answer": ["word embedding, input encoder, alignment, aggregation, and prediction.", "Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction."], "top_k_doc_id": [259, 263, 4928, 3839, 6508, 7244, 1106, 6509, 7541, 3175, 7163, 7351, 5236, 6368, 575], "orig_top_k_doc_id": [7244, 259, 263, 4928, 7541, 3175, 7163, 7351, 1106, 5236, 6368, 6508, 3839, 575, 6509]}, {"qid": 208, "question": "What MC abbreviate for? in Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "answer": ["machine comprehension"], "top_k_doc_id": [259, 263, 4928, 3839, 6508, 7244, 2755, 2362, 5970, 2759, 6847, 7163, 7727, 7830, 490], "orig_top_k_doc_id": [4928, 2755, 7244, 259, 2362, 5970, 263, 2759, 6847, 7163, 7727, 3839, 7830, 490, 6508]}, {"qid": 211, "question": "What is the exact performance on SQUAD? in Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "answer": ["Our model achieves a 68.73% EM score and 77.39% F1 score"], "top_k_doc_id": [259, 263, 4928, 7727, 5231, 6368, 1357, 5236, 5044, 2268, 262, 1972, 5472, 5158, 407], "orig_top_k_doc_id": [263, 4928, 259, 7727, 5231, 6368, 1357, 5236, 5044, 2268, 262, 1972, 5472, 5158, 407]}]}
{"group_id": 464, "group_size": 4, "items": [{"qid": 233, "question": "Which model architecture do they use to obtain representations? in Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study", "answer": ["BiLSTM with max pooling"], "top_k_doc_id": [289, 290, 292, 293, 6556, 7493, 1345, 6503, 7498, 1518, 1342, 1683, 1373, 6279, 1651], "orig_top_k_doc_id": [293, 292, 290, 289, 1345, 1518, 6556, 7498, 1342, 7493, 1683, 1373, 6279, 6503, 1651]}, {"qid": 235, "question": "Which similarity datasets do they use? in Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study", "answer": ["MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S", "WS353S, SimLex999, SimVerb3500"], "top_k_doc_id": [289, 290, 292, 293, 6556, 7493, 1345, 6503, 7498, 1518, 291, 2759, 6190, 3846, 2755], "orig_top_k_doc_id": [293, 292, 290, 289, 291, 2759, 1345, 7498, 1518, 6556, 6503, 6190, 3846, 7493, 2755]}, {"qid": 234, "question": "Which downstream sentence-level tasks do they evaluate on? in Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study", "answer": ["BIBREF13 , BIBREF18"], "top_k_doc_id": [289, 290, 292, 293, 6556, 7493, 1345, 6503, 7498, 3834, 1342, 6448, 2759, 7167, 5351], "orig_top_k_doc_id": [293, 292, 289, 290, 3834, 1342, 6448, 1345, 6503, 7493, 2759, 7167, 5351, 7498, 6556]}, {"qid": 232, "question": "Where do they employ feature-wise sigmoid gating? in Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study", "answer": ["gating mechanism acts upon each dimension of the word and character-level vectors"], "top_k_doc_id": [289, 290, 292, 293, 6556, 7493, 2755, 2756, 1326, 7238, 6115, 3179, 6877, 2759, 1518], "orig_top_k_doc_id": [293, 289, 290, 292, 2755, 2756, 1326, 6556, 7493, 7238, 6115, 3179, 6877, 2759, 1518]}]}
{"group_id": 465, "group_size": 4, "items": [{"qid": 306, "question": "What datasets were used? in ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples", "answer": ["datasets given on the shared task, without using any additional external data"], "top_k_doc_id": [2310, 369, 448, 2267, 2309, 7474, 7679, 2308, 1141, 1143, 7163, 3722, 393, 2306, 2268], "orig_top_k_doc_id": [369, 7474, 2267, 2310, 2308, 2309, 7679, 1141, 1143, 448, 7163, 3722, 393, 2306, 2268]}, {"qid": 307, "question": "How did they do compared to other teams? in ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples", "answer": ["second on Subtask A with an F1 score of 77.78% among 33 other team submissions, performs well on Subtask B with an F1 score of 79.59%"], "top_k_doc_id": [2310, 369, 448, 2267, 2309, 7474, 7679, 7119, 1412, 2278, 2619, 1933, 1932, 3689, 397], "orig_top_k_doc_id": [369, 2310, 7119, 1412, 2278, 7474, 2619, 1933, 448, 7679, 1932, 3689, 2309, 2267, 397]}, {"qid": 1629, "question": "By how much does their model outperform the baseline in the cross-domain evaluation? in Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification", "answer": ["$2.2\\%$ absolute accuracy improvement on the laptops test set, $3.6\\%$ accuracy improvement on the restaurants test set"], "top_k_doc_id": [2310, 393, 1043, 2306, 2307, 2308, 3274, 4688, 4689, 4690, 7472, 1048, 2309, 6619, 3140], "orig_top_k_doc_id": [2310, 2308, 2306, 2307, 4688, 4689, 1048, 1043, 2309, 3274, 7472, 393, 6619, 4690, 3140]}, {"qid": 1630, "question": "What are the performance results? in Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification", "answer": ["results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset, new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively."], "top_k_doc_id": [2310, 393, 1043, 2306, 2307, 2308, 3274, 4688, 4689, 4690, 7472, 394, 6482, 3275, 2468], "orig_top_k_doc_id": [2308, 2306, 2310, 2307, 4688, 394, 4689, 1043, 3274, 393, 6482, 3275, 7472, 4690, 2468]}]}
{"group_id": 466, "group_size": 4, "items": [{"qid": 319, "question": "What normalization techniques are mentioned? in Exploring End-to-End Techniques for Low-Resource Speech Recognition", "answer": ["FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)"], "top_k_doc_id": [380, 381, 2351, 4863, 5481, 6310, 382, 1784, 2995, 5566, 4862, 1618, 7645, 6005, 2634], "orig_top_k_doc_id": [4863, 381, 2351, 4862, 1618, 382, 1784, 380, 6310, 5566, 2995, 7645, 5481, 6005, 2634]}, {"qid": 320, "question": "What features do they experiment with? in Exploring End-to-End Techniques for Low-Resource Speech Recognition", "answer": ["40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, deltas and delta-deltas (120 features in vector), spectrogram"], "top_k_doc_id": [380, 381, 2351, 4863, 5481, 6310, 1593, 2450, 4862, 6314, 5566, 622, 3439, 6005, 4550], "orig_top_k_doc_id": [381, 4863, 6314, 6310, 5566, 2450, 380, 5481, 622, 3439, 4862, 2351, 1593, 6005, 4550]}, {"qid": 321, "question": "Which architecture is their best model? in Exploring End-to-End Techniques for Low-Resource Speech Recognition", "answer": ["6-layer bLSTM with 1024 hidden units"], "top_k_doc_id": [380, 381, 2351, 4863, 5481, 6310, 382, 1784, 2995, 5566, 5053, 4975, 2451, 1593, 2450], "orig_top_k_doc_id": [381, 380, 4863, 6310, 5481, 5566, 2351, 5053, 4975, 2451, 1593, 2450, 1784, 2995, 382]}, {"qid": 322, "question": "What kind of spontaneous speech is used? in Exploring End-to-End Techniques for Low-Resource Speech Recognition", "answer": ["No"], "top_k_doc_id": [380, 381, 2351, 4863, 5481, 6310, 1593, 2450, 4862, 5264, 2633, 5765, 3338, 3340, 6110], "orig_top_k_doc_id": [5264, 381, 2450, 2633, 4863, 380, 4862, 5481, 5765, 6310, 2351, 3338, 1593, 3340, 6110]}]}
{"group_id": 467, "group_size": 4, "items": [{"qid": 329, "question": "Does the system trained only using XR loss outperform the fully supervised neural system? in Transfer Learning Between Related Tasks Using Expected Label Proportions", "answer": ["Yes"], "top_k_doc_id": [390, 391, 392, 393, 394, 2308, 1005, 7441, 6262, 3651, 4396, 5476, 4728, 5845, 5133], "orig_top_k_doc_id": [390, 392, 391, 393, 1005, 7441, 6262, 3651, 4396, 5476, 4728, 5845, 394, 2308, 5133]}, {"qid": 330, "question": "How accurate is the aspect based sentiment classifier trained only using the XR loss? in Transfer Learning Between Related Tasks Using Expected Label Proportions", "answer": ["BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n"], "top_k_doc_id": [390, 391, 392, 393, 394, 2308, 1005, 2306, 2215, 2307, 5177, 2030, 3129, 5980, 3289], "orig_top_k_doc_id": [390, 392, 391, 393, 394, 2306, 1005, 2215, 2307, 5177, 2030, 3129, 2308, 5980, 3289]}, {"qid": 328, "question": "How much more data does the model trained using XR loss have access to, compared to the fully supervised model? in Transfer Learning Between Related Tasks Using Expected Label Proportions", "answer": ["No"], "top_k_doc_id": [390, 391, 392, 393, 394, 2308, 1968, 3842, 4370, 7459, 2954, 4728, 2032, 2306, 2953], "orig_top_k_doc_id": [390, 393, 392, 391, 2308, 1968, 3842, 4370, 7459, 2954, 394, 4728, 2032, 2306, 2953]}, {"qid": 331, "question": "How is the expectation regularization loss defined? in Transfer Learning Between Related Tasks Using Expected Label Proportions", "answer": ["DISPLAYFORM0"], "top_k_doc_id": [390, 391, 392, 2032, 5114, 5207, 5204, 5112, 2468, 2033, 5980, 1553, 188, 4541, 5132], "orig_top_k_doc_id": [390, 2032, 5114, 392, 391, 5207, 5204, 5112, 2468, 2033, 5980, 1553, 188, 4541, 5132]}]}
{"group_id": 468, "group_size": 4, "items": [{"qid": 454, "question": "Do they perform manual evaluation? in Incorporating Sememes into Chinese Definition Modeling", "answer": ["Yes"], "top_k_doc_id": [527, 528, 529, 530, 531, 532, 736, 1625, 3642, 2194, 6097, 1626, 5788, 2889, 69], "orig_top_k_doc_id": [527, 531, 532, 530, 528, 529, 3642, 1625, 6097, 736, 1626, 2194, 5788, 2889, 69]}, {"qid": 456, "question": "What is a sememe? in Incorporating Sememes into Chinese Definition Modeling", "answer": ["Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes"], "top_k_doc_id": [527, 528, 529, 530, 531, 532, 736, 1625, 3642, 2194, 7295, 1116, 2917, 7053, 2590], "orig_top_k_doc_id": [527, 530, 531, 528, 529, 532, 3642, 1625, 2194, 736, 7295, 1116, 2917, 7053, 2590]}, {"qid": 453, "question": "Is there an online demo of their system? in Incorporating Sememes into Chinese Definition Modeling", "answer": ["No"], "top_k_doc_id": [527, 528, 529, 530, 531, 532, 736, 1625, 3642, 2042, 3537, 1790, 6051, 2043, 7662], "orig_top_k_doc_id": [527, 532, 531, 530, 528, 529, 3642, 2042, 3537, 1625, 1790, 6051, 2043, 7662, 736]}, {"qid": 455, "question": "Do they compare against Noraset et al. 2017? in Incorporating Sememes into Chinese Definition Modeling", "answer": ["Yes"], "top_k_doc_id": [527, 528, 529, 530, 531, 532, 4511, 4510, 2011, 7260, 7261, 5737, 690, 3345, 412], "orig_top_k_doc_id": [527, 532, 531, 530, 4511, 528, 4510, 2011, 7260, 7261, 529, 5737, 690, 3345, 412]}]}
{"group_id": 469, "group_size": 4, "items": [{"qid": 460, "question": "What result from experiments suggest that natural language based agents are more robust? in Natural Language State Representation for Reinforcement Learning", "answer": ["Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances"], "top_k_doc_id": [540, 6583, 6588, 539, 541, 2867, 5683, 6344, 537, 7839, 7514, 317, 7840, 4522, 6587], "orig_top_k_doc_id": [540, 537, 6344, 6583, 541, 7839, 6588, 7514, 539, 2867, 5683, 317, 7840, 4522, 6587]}, {"qid": 461, "question": "How better is performance of natural language based agents in experiments? in Natural Language State Representation for Reinforcement Learning", "answer": ["No"], "top_k_doc_id": [540, 6583, 6588, 539, 541, 2867, 5683, 6344, 537, 3805, 3475, 3357, 7843, 6589, 2871], "orig_top_k_doc_id": [540, 541, 6588, 5683, 537, 2867, 6344, 3805, 3475, 3357, 6583, 7843, 539, 6589, 2871]}, {"qid": 462, "question": "How much faster natural language agents converge in performed experiments? in Natural Language State Representation for Reinforcement Learning", "answer": ["No"], "top_k_doc_id": [540, 6583, 6588, 539, 541, 2867, 5683, 6344, 2871, 2872, 2584, 5556, 3358, 1144, 317], "orig_top_k_doc_id": [2871, 2867, 2872, 5683, 2584, 5556, 3358, 540, 1144, 317, 6588, 6583, 541, 6344, 539]}, {"qid": 4123, "question": "What GAN and RL approaches are used? in The Rapidly Changing Landscape of Conversational Agents", "answer": ["adversarial training for open-domain dialogue generation , trust region actor-critic with experience replay , episodic natural actor-critic with experience replay, multi-turn dialogue agent, on-policy Monte Carlo method ", "the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances., The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones, The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines.", "authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated, task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones"], "top_k_doc_id": [540, 6583, 6588, 6590, 7839, 3775, 6589, 3328, 3331, 7842, 3798, 3772, 1541, 3330, 575], "orig_top_k_doc_id": [6590, 6583, 7839, 3775, 6588, 6589, 3328, 3331, 7842, 3798, 3772, 540, 1541, 3330, 575]}]}
{"group_id": 470, "group_size": 4, "items": [{"qid": 534, "question": "How many layers do they use in their best performing network? in Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks", "answer": ["36"], "top_k_doc_id": [651, 652, 653, 2048, 426, 4816, 7630, 1795, 165, 1560, 3178, 2743, 7472, 6093, 244], "orig_top_k_doc_id": [653, 651, 652, 2048, 4816, 1795, 426, 165, 1560, 7630, 3178, 2743, 7472, 6093, 244]}, {"qid": 536, "question": "Does their model take more time to train than regular transformer models? in Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks", "answer": ["No"], "top_k_doc_id": [651, 652, 653, 2048, 426, 4816, 7630, 2088, 1948, 2529, 3559, 7117, 7595, 2709, 5724], "orig_top_k_doc_id": [653, 651, 652, 2088, 1948, 2529, 3559, 7630, 7117, 7595, 2048, 2709, 4816, 426, 5724]}, {"qid": 535, "question": "Do they just sum up all the loses the calculate to end up with one single loss? in Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks", "answer": ["No"], "top_k_doc_id": [651, 652, 653, 2048, 7648, 5724, 373, 2088, 6588, 7090, 2230, 244, 2815, 3576, 6256], "orig_top_k_doc_id": [653, 651, 652, 7648, 5724, 2048, 373, 2088, 6588, 7090, 2230, 244, 2815, 3576, 6256]}, {"qid": 533, "question": "Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness? in Deja-vu: Double Feature Presentation and Iterated Loss in Deep Transformer Networks", "answer": ["No"], "top_k_doc_id": [651, 652, 653, 5241, 2088, 4209, 7141, 6217, 2734, 7012, 4575, 2529, 2528, 3300, 7106], "orig_top_k_doc_id": [651, 653, 652, 5241, 2088, 4209, 7141, 6217, 2734, 7012, 4575, 2529, 2528, 3300, 7106]}]}
{"group_id": 471, "group_size": 4, "items": [{"qid": 655, "question": "What is the dataset that is used in the paper? in A framework for anomaly detection using language modeling, and its applications to finance", "answer": ["No"], "top_k_doc_id": [803, 804, 805, 806, 2699, 2700, 5004, 2688, 3770, 4135, 7256, 1870, 2850, 6519, 6375], "orig_top_k_doc_id": [806, 803, 2700, 2688, 5004, 2699, 7256, 805, 4135, 2850, 6519, 804, 3770, 1870, 6375]}, {"qid": 656, "question": "What is the performance of the models discussed in the paper? in A framework for anomaly detection using language modeling, and its applications to finance", "answer": ["No"], "top_k_doc_id": [803, 804, 805, 806, 2699, 2700, 5004, 2688, 3770, 4135, 7256, 1870, 2850, 1625, 3612], "orig_top_k_doc_id": [806, 803, 2700, 2699, 2688, 5004, 1870, 804, 2850, 805, 4135, 7256, 3770, 1625, 3612]}, {"qid": 658, "question": "Does the paper report a baseline for the task? in A framework for anomaly detection using language modeling, and its applications to finance", "answer": ["No"], "top_k_doc_id": [803, 804, 805, 806, 2699, 2700, 5004, 2688, 3770, 4135, 7256, 6519, 5106, 1625, 3015], "orig_top_k_doc_id": [806, 803, 2700, 5004, 804, 805, 2699, 3770, 6519, 7256, 5106, 4135, 2688, 1625, 3015]}, {"qid": 657, "question": "Does the paper consider the use of perplexity in order to identify text anomalies? in A framework for anomaly detection using language modeling, and its applications to finance", "answer": ["No"], "top_k_doc_id": [803, 804, 805, 806, 2699, 2700, 5004, 2570, 1625, 2850, 24, 6723, 3968, 1987, 1870], "orig_top_k_doc_id": [806, 803, 805, 804, 2570, 2700, 5004, 2699, 1625, 2850, 24, 6723, 3968, 1987, 1870]}]}
{"group_id": 472, "group_size": 4, "items": [{"qid": 672, "question": "Which metrics do they use to evaluate matching? in Lattice CNNs for Matching Based Chinese Question Answering", "answer": ["Precision@1, Mean Average Precision, Mean Reciprocal Rank"], "top_k_doc_id": [826, 828, 829, 831, 827, 830, 6779, 1188, 3644, 4791, 6257, 1187, 6980, 2442, 6848], "orig_top_k_doc_id": [826, 831, 829, 828, 827, 830, 1187, 6980, 2442, 6779, 4791, 6257, 6848, 1188, 3644]}, {"qid": 673, "question": "Which dataset(s) do they evaluate on? in Lattice CNNs for Matching Based Chinese Question Answering", "answer": ["DBQA, KBRE"], "top_k_doc_id": [826, 828, 829, 831, 827, 830, 6779, 1188, 3644, 4791, 6257, 3643, 4790, 3642, 3646], "orig_top_k_doc_id": [826, 831, 829, 828, 827, 830, 6257, 4791, 3644, 3643, 4790, 3642, 1188, 3646, 6779]}, {"qid": 671, "question": "How do they obtain word lattices from words? in Lattice CNNs for Matching Based Chinese Question Answering", "answer": ["By considering words as vertices and generating directed edges between neighboring words within a sentence"], "top_k_doc_id": [826, 828, 829, 831, 827, 830, 6779, 1185, 1189, 2313, 2197, 2314, 6777, 3707, 3708], "orig_top_k_doc_id": [826, 831, 828, 827, 830, 829, 1185, 6779, 1189, 2313, 2197, 2314, 6777, 3707, 3708]}, {"qid": 2768, "question": "What word level and character level model baselines are used? in Character-Level Question Answering with Attention", "answer": ["None", "Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)"], "top_k_doc_id": [826, 828, 829, 831, 259, 4845, 4844, 4841, 7541, 6255, 6864, 2228, 408, 2615, 2520], "orig_top_k_doc_id": [259, 4845, 4844, 4841, 826, 7541, 828, 829, 6255, 6864, 2228, 408, 2615, 831, 2520]}]}
{"group_id": 473, "group_size": 4, "items": [{"qid": 723, "question": "What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text? in Using word embeddings to improve the discriminability of co-occurrence text networks", "answer": ["long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach"], "top_k_doc_id": [903, 904, 908, 5896, 5898, 906, 907, 905, 2334, 2335, 4885, 5894, 6796, 3830, 2355], "orig_top_k_doc_id": [903, 908, 904, 906, 5898, 905, 4885, 2335, 6796, 907, 5894, 3830, 2334, 5896, 2355]}, {"qid": 725, "question": "On what model architectures are previous co-occurence networks based? in Using word embeddings to improve the discriminability of co-occurrence text networks", "answer": ["in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks"], "top_k_doc_id": [903, 904, 908, 5896, 5898, 906, 907, 905, 2334, 2335, 4885, 5894, 4890, 3651, 3178], "orig_top_k_doc_id": [903, 908, 904, 906, 5896, 2335, 5898, 4890, 907, 5894, 3651, 905, 4885, 3178, 2334]}, {"qid": 722, "question": "What other natural processing tasks authors think could be studied by using word embeddings? in Using word embeddings to improve the discriminability of co-occurrence text networks", "answer": ["general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques"], "top_k_doc_id": [903, 904, 908, 5896, 5898, 906, 907, 6796, 6635, 772, 2355, 2523, 155, 5627, 3830], "orig_top_k_doc_id": [903, 908, 904, 6796, 6635, 907, 906, 5898, 5896, 772, 2355, 2523, 155, 5627, 3830]}, {"qid": 724, "question": "Do the use word embeddings alone or they replace some previous features of the model with word embeddings? in Using word embeddings to improve the discriminability of co-occurrence text networks", "answer": ["They use it as addition to previous model - they add new edge between words if word embeddings are similar."], "top_k_doc_id": [903, 904, 908, 5896, 5898, 6635, 3293, 5336, 905, 5417, 2965, 6796, 7139, 2355, 7145], "orig_top_k_doc_id": [903, 908, 904, 6635, 5896, 3293, 5336, 5898, 905, 5417, 2965, 6796, 7139, 2355, 7145]}]}
{"group_id": 474, "group_size": 4, "items": [{"qid": 799, "question": "What is the performance of the CRF model on the task described? in An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines", "answer": ["the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)"], "top_k_doc_id": [1000, 1001, 1002, 1003, 1004, 2320, 3627, 7481, 438, 2321, 3626, 4532, 4610, 4574, 7483], "orig_top_k_doc_id": [1000, 1001, 1003, 1004, 1002, 3626, 3627, 2320, 7481, 438, 4574, 2321, 4610, 4532, 7483]}, {"qid": 800, "question": "Does the paper motivate the use of CRF as the baseline model? in An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines", "answer": ["the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data"], "top_k_doc_id": [1000, 1001, 1002, 1003, 1004, 2320, 3627, 7481, 438, 2321, 3626, 4532, 4610, 4757, 6505], "orig_top_k_doc_id": [1000, 1003, 1001, 1004, 1002, 3627, 438, 3626, 2320, 4610, 4757, 7481, 6505, 2321, 4532]}, {"qid": 801, "question": "What are the handcrafted features used? in An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines", "answer": ["Bias feature, Token feature, Uppercase feature (y/n), Titlecase feature (y/n), Character trigram feature, Quotation feature (y/n), Word suffix feature (last three characters), POS tag (provided by spaCy utilities), Word shape (provided by spaCy utilities), Word embedding (see Table TABREF26)"], "top_k_doc_id": [1000, 1001, 1002, 1003, 1004, 2320, 3627, 7481, 2797, 7483, 1863, 2319, 4533, 3624, 3327], "orig_top_k_doc_id": [1001, 1003, 1000, 1004, 1002, 2320, 7481, 2797, 7483, 1863, 2319, 4533, 3624, 3327, 3627]}, {"qid": 798, "question": "Does the paper mention other works proposing methods to detect anglicisms in Spanish? in An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines", "answer": ["Yes"], "top_k_doc_id": [1000, 1001, 1002, 1003, 1004, 2320, 4610, 2695, 3327, 1039, 2329, 5956, 1045, 5, 7748], "orig_top_k_doc_id": [1001, 1000, 1003, 1004, 1002, 2320, 4610, 2695, 3327, 1039, 2329, 5956, 1045, 5, 7748]}]}
{"group_id": 475, "group_size": 4, "items": [{"qid": 816, "question": "What baseline do they compare to? in Team Papelo: Transformer Networks at FEVER", "answer": ["For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 ."], "top_k_doc_id": [1017, 1018, 1019, 3161, 6677, 6744, 1982, 2455, 6745, 4833, 5495, 1981, 5448, 7581, 397], "orig_top_k_doc_id": [1017, 1018, 6744, 6677, 4833, 1019, 5495, 3161, 2455, 6745, 1981, 5448, 7581, 1982, 397]}, {"qid": 818, "question": "What is the FEVER task? in Team Papelo: Transformer Networks at FEVER", "answer": ["tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem"], "top_k_doc_id": [1017, 1018, 1019, 3161, 6677, 6744, 1982, 2455, 6745, 2458, 2457, 2459, 6676, 7722, 2460], "orig_top_k_doc_id": [1017, 1018, 1019, 6744, 6677, 2458, 2457, 2459, 6676, 7722, 6745, 2455, 3161, 1982, 2460]}, {"qid": 815, "question": "How big is their training set? in Team Papelo: Transformer Networks at FEVER", "answer": ["No"], "top_k_doc_id": [1017, 1018, 1019, 3161, 6677, 6744, 4833, 4680, 2458, 7116, 2739, 2744, 2459, 2743, 6994], "orig_top_k_doc_id": [1017, 1018, 4833, 6677, 4680, 3161, 2458, 1019, 7116, 2739, 6744, 2744, 2459, 2743, 6994]}, {"qid": 817, "question": "Which pre-trained transformer do they use? in Team Papelo: Transformer Networks at FEVER", "answer": ["BIBREF5"], "top_k_doc_id": [1017, 1018, 1019, 3161, 6677, 4833, 7116, 427, 4309, 426, 6678, 7646, 533, 1777, 730], "orig_top_k_doc_id": [1017, 6677, 1018, 4833, 7116, 3161, 427, 4309, 426, 6678, 7646, 1019, 533, 1777, 730]}]}
{"group_id": 476, "group_size": 4, "items": [{"qid": 829, "question": "Does API provide ability to connect to models written in some other deep learning framework? in Torch-Struct: Deep Structured Prediction Library", "answer": ["Yes"], "top_k_doc_id": [769, 1035, 1036, 1037, 1038, 3694, 4412, 4413, 7549, 7232, 4415, 879, 3679, 3764, 276], "orig_top_k_doc_id": [1035, 1037, 1036, 1038, 3694, 7549, 4412, 7232, 4413, 4415, 879, 3679, 3764, 769, 276]}, {"qid": 832, "question": "What general-purpose optimizations are included? in Torch-Struct: Deep Structured Prediction Library", "answer": ["Parallel Scan Inference, Vectorized Parsing, Semiring Matrix Operations"], "top_k_doc_id": [769, 1035, 1036, 1037, 1038, 3694, 4412, 4413, 1021, 3244, 4296, 1120, 610, 2892, 3701], "orig_top_k_doc_id": [1035, 1037, 1038, 1036, 4413, 3694, 4412, 1021, 3244, 4296, 769, 1120, 610, 2892, 3701]}, {"qid": 830, "question": "Is this library implemented into Torch or is framework agnostic? in Torch-Struct: Deep Structured Prediction Library", "answer": ["It uses deep learning framework (pytorch)"], "top_k_doc_id": [769, 1035, 1036, 1037, 1038, 3694, 4412, 4413, 1178, 5277, 6151, 4986, 88, 6781, 2859], "orig_top_k_doc_id": [1035, 1036, 1037, 1038, 769, 4413, 1178, 5277, 3694, 6151, 4986, 88, 4412, 6781, 2859]}, {"qid": 831, "question": "What baselines are used in experiments? in Torch-Struct: Deep Structured Prediction Library", "answer": ["Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient"], "top_k_doc_id": [769, 1035, 1036, 1037, 1038, 3694, 4296, 6666, 2737, 1498, 4297, 3609, 5631, 5277, 1851], "orig_top_k_doc_id": [1035, 1037, 1036, 1038, 3694, 4296, 6666, 2737, 1498, 769, 4297, 3609, 5631, 5277, 1851]}]}
{"group_id": 477, "group_size": 4, "items": [{"qid": 853, "question": "What is used a baseline? in Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling", "answer": ["As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12"], "top_k_doc_id": [1084, 7111, 7548, 217, 1086, 3816, 5489, 6016, 7100, 4869, 5228, 2429, 7455, 6410, 1170], "orig_top_k_doc_id": [1084, 7548, 1086, 5489, 7111, 6016, 3816, 7100, 4869, 5228, 2429, 7455, 217, 6410, 1170]}, {"qid": 854, "question": "What contextual features are used? in Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling", "answer": ["The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords."], "top_k_doc_id": [1084, 7111, 7548, 217, 1086, 3816, 5489, 6016, 1085, 1088, 1089, 3327, 7049, 6172, 7116], "orig_top_k_doc_id": [1084, 7548, 1086, 1085, 1088, 1089, 5489, 7111, 3327, 7049, 6016, 3816, 217, 6172, 7116]}, {"qid": 855, "question": "Where are the cybersecurity articles used in the model sourced from? in Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling", "answer": [" from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018"], "top_k_doc_id": [1084, 7111, 7548, 217, 1086, 3816, 1085, 6410, 5228, 3274, 2388, 2396, 3784, 5589, 6716], "orig_top_k_doc_id": [1084, 1086, 7548, 3816, 1085, 6410, 5228, 3274, 2388, 2396, 7111, 217, 3784, 5589, 6716]}, {"qid": 856, "question": "What type of hand-crafted features are used in state of the art IOC detection systems? in Collecting Indicators of Compromise from Unstructured Text of Cybersecurity Articles using Neural-Based Sequence Labelling", "answer": ["No"], "top_k_doc_id": [1084, 7111, 7548, 1087, 2160, 1088, 5589, 5675, 1089, 4609, 4781, 459, 6410, 1329, 1170], "orig_top_k_doc_id": [1084, 1087, 7111, 7548, 2160, 1088, 5589, 5675, 1089, 4609, 4781, 459, 6410, 1329, 1170]}]}
{"group_id": 478, "group_size": 4, "items": [{"qid": 920, "question": "Do they evaluate the quality of the paraphrasing model? in Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "answer": ["No"], "top_k_doc_id": [1185, 1186, 1187, 5780, 1188, 1887, 1888, 1886, 2893, 7664, 1884, 1005, 5779, 3235, 3236], "orig_top_k_doc_id": [1185, 1186, 1887, 7664, 5780, 2893, 1884, 1886, 1188, 1187, 1005, 5779, 3235, 3236, 1888]}, {"qid": 921, "question": "How many paraphrases are generated per question? in Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "answer": ["10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans"], "top_k_doc_id": [1185, 1186, 1187, 5780, 1188, 1887, 1888, 1886, 2893, 7664, 2691, 1189, 2894, 2897, 2895], "orig_top_k_doc_id": [1185, 1186, 1187, 2691, 2893, 5780, 7664, 1189, 1887, 1888, 1188, 2894, 2897, 2895, 1886]}, {"qid": 923, "question": "What are the baselines? in Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "answer": ["GraphParser without paraphrases, monolingual machine translation based model for paraphrase generation"], "top_k_doc_id": [1185, 1186, 1187, 5780, 1188, 1887, 1888, 5779, 1772, 1771, 3468, 3236, 6427, 6070, 1159], "orig_top_k_doc_id": [1185, 1186, 5780, 1187, 1188, 5779, 1772, 1771, 1888, 3468, 1887, 3236, 6427, 6070, 1159]}, {"qid": 922, "question": "What latent variables are modeled in the PCFG? in Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "answer": ["syntactic information, semantic and topical information"], "top_k_doc_id": [1185, 1186, 1187, 5780, 5779, 1390, 3236, 3235, 5781, 3240, 6428, 1189, 1771, 1768, 1770], "orig_top_k_doc_id": [1185, 1186, 5780, 5779, 1390, 3236, 1187, 3235, 5781, 3240, 6428, 1189, 1771, 1768, 1770]}]}
{"group_id": 479, "group_size": 4, "items": [{"qid": 927, "question": "What datasets are used for evaluation? in Rethinking travel behavior modeling representations through embeddings", "answer": ["Swissmetro dataset"], "top_k_doc_id": [1194, 1195, 1197, 1198, 3985, 5067, 5350, 7127, 5066, 5647, 7758, 411, 5868, 5709, 3979], "orig_top_k_doc_id": [1194, 1195, 1198, 1197, 5067, 3985, 7127, 5350, 5647, 5066, 7758, 411, 5868, 5709, 3979]}, {"qid": 929, "question": "How do they model travel behavior? in Rethinking travel behavior modeling representations through embeddings", "answer": ["The data from collected travel surveys is used to model travel behavior."], "top_k_doc_id": [1194, 1195, 1197, 1198, 3985, 5067, 5350, 7127, 5066, 5647, 7758, 6635, 2402, 2237, 5430], "orig_top_k_doc_id": [1194, 1195, 5067, 1198, 1197, 5350, 3985, 7758, 7127, 5066, 6635, 2402, 2237, 5430, 5647]}, {"qid": 928, "question": "How do their train their embeddings? in Rethinking travel behavior modeling representations through embeddings", "answer": ["The embeddings are learned several times using the training set, then the average is taken."], "top_k_doc_id": [1194, 1195, 1197, 1198, 3985, 5067, 5350, 7127, 6635, 3612, 4891, 7126, 5885, 6817, 3615], "orig_top_k_doc_id": [1194, 1195, 1198, 1197, 3985, 5067, 6635, 3612, 7127, 4891, 7126, 5350, 5885, 6817, 3615]}, {"qid": 930, "question": "How do their interpret the coefficients? in Rethinking travel behavior modeling representations through embeddings", "answer": ["The coefficients are projected back to the dummy variable space."], "top_k_doc_id": [1194, 1195, 1197, 1198, 3985, 5067, 752, 2121, 1382, 753, 2473, 4096, 4581, 5215, 1388], "orig_top_k_doc_id": [1195, 3985, 1198, 1194, 752, 1197, 2121, 5067, 1382, 753, 2473, 4096, 4581, 5215, 1388]}]}
{"group_id": 480, "group_size": 4, "items": [{"qid": 949, "question": "Does this model train faster than state of the art models? in FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow", "answer": ["No"], "top_k_doc_id": [1231, 1232, 1233, 1234, 1235, 1236, 1768, 1138, 1139, 1140, 2146, 6239, 7050, 7247, 4414], "orig_top_k_doc_id": [1231, 1235, 1234, 1232, 1236, 1233, 1140, 1138, 1139, 2146, 6239, 1768, 7050, 7247, 4414]}, {"qid": 950, "question": "What is the performance difference between proposed method and state-of-the-arts on these datasets? in FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow", "answer": ["Difference is around 1 BLEU score lower on average than state of the art methods."], "top_k_doc_id": [1231, 1232, 1233, 1234, 1235, 1236, 1768, 1138, 1139, 1140, 2146, 685, 2065, 6237, 7566], "orig_top_k_doc_id": [1231, 1234, 1233, 1235, 1232, 1236, 685, 1140, 1768, 1138, 1139, 2065, 2146, 6237, 7566]}, {"qid": 951, "question": "What non autoregressive NMT models are used for comparison? in FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow", "answer": ["NAT w/ Fertility, NAT-IR, NAT-REG, LV NAR, CTC Loss, CMLM"], "top_k_doc_id": [1231, 1232, 1233, 1234, 1235, 1236, 1768, 6502, 7246, 7247, 7248, 7249, 6501, 1140, 2146], "orig_top_k_doc_id": [1231, 1234, 1235, 1233, 1236, 1232, 6502, 7249, 7248, 7247, 6501, 1768, 1140, 7246, 2146]}, {"qid": 952, "question": "What are three neural machine translation (NMT) benchmark datasets used for evaluation? in FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow", "answer": ["WMT2014, WMT2016 and IWSLT-2014"], "top_k_doc_id": [1231, 1232, 1233, 1234, 1235, 1236, 1768, 6502, 7246, 7247, 7248, 2187, 28, 2065, 1139], "orig_top_k_doc_id": [1231, 1234, 1235, 1232, 1236, 1233, 7246, 2187, 28, 7247, 6502, 7248, 1768, 2065, 1139]}]}
{"group_id": 481, "group_size": 4, "items": [{"qid": 956, "question": "What are two models' architectures in proposed solution? in Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games", "answer": ["Reasoner model, also implemented with the MatchLSTM architecture, Ranker model"], "top_k_doc_id": [1146, 1240, 1241, 1242, 2096, 2366, 4216, 325, 1155, 1217, 6932, 575, 7164, 1141, 1357], "orig_top_k_doc_id": [1240, 1242, 1241, 4216, 2366, 6932, 575, 2096, 1155, 7164, 1146, 1217, 325, 1141, 1357]}, {"qid": 957, "question": "How do two models cooperate to select the most confident chains? in Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games", "answer": ["Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards"], "top_k_doc_id": [1146, 1240, 1241, 1242, 2096, 2366, 4216, 1243, 2101, 7072, 5267, 3299, 4597, 575, 6932], "orig_top_k_doc_id": [1240, 1242, 1241, 2096, 1243, 4216, 2366, 5267, 2101, 3299, 4597, 7072, 575, 6932, 1146]}, {"qid": 958, "question": "How many hand-labeled reasoning chains have been created? in Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games", "answer": ["No"], "top_k_doc_id": [1146, 1240, 1241, 1242, 2096, 2366, 4216, 1243, 2101, 7072, 325, 1141, 2369, 2097, 4637], "orig_top_k_doc_id": [1240, 1242, 1241, 2096, 1243, 4216, 325, 2101, 1146, 7072, 1141, 2369, 2097, 4637, 2366]}, {"qid": 959, "question": "What benchmarks are created? in Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games", "answer": ["Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples"], "top_k_doc_id": [1146, 1240, 1241, 1242, 2096, 2366, 4216, 325, 1155, 1217, 6932, 1637, 2369, 4496, 2368], "orig_top_k_doc_id": [1240, 1242, 1241, 2096, 2366, 4216, 1637, 1217, 6932, 1146, 325, 2369, 4496, 1155, 2368]}]}
{"group_id": 482, "group_size": 4, "items": [{"qid": 981, "question": "How many words are coded in the dictionary? in A Swiss German Dictionary: Variation in Speech and Writing", "answer": ["11'248"], "top_k_doc_id": [3664, 1286, 1288, 1289, 3667, 5574, 1287, 2774, 6111, 5462, 4015, 3896, 4016, 3666, 2558], "orig_top_k_doc_id": [1286, 1289, 1287, 1288, 3664, 3667, 6111, 5462, 4015, 3896, 4016, 3666, 2558, 5574, 2774]}, {"qid": 982, "question": "Is the model evaluated on the graphemes-to-phonemes task? in A Swiss German Dictionary: Variation in Speech and Writing", "answer": ["Yes"], "top_k_doc_id": [3664, 1286, 1288, 1289, 3667, 5574, 1287, 2774, 4616, 4615, 3456, 2315, 3457, 6351, 380], "orig_top_k_doc_id": [1287, 1288, 1286, 1289, 4616, 2774, 4615, 3664, 3667, 3456, 2315, 3457, 6351, 5574, 380]}, {"qid": 2304, "question": "How is language modelling evaluated? in Automatic Creation of Text Corpora for Low-Resource Languages from the Internet: The Case of Swiss German", "answer": ["perplexity of the models"], "top_k_doc_id": [3664, 1286, 1288, 1289, 3667, 5574, 45, 3665, 6110, 3293, 2789, 3668, 6791, 2906, 2630], "orig_top_k_doc_id": [3664, 45, 3667, 1286, 3665, 6110, 5574, 3293, 1289, 2789, 1288, 3668, 6791, 2906, 2630]}, {"qid": 1795, "question": "What is one of the first writing systems in the world? in Experiments in Cuneiform Language Identification", "answer": ["Cuneiform"], "top_k_doc_id": [3664, 2618, 2619, 404, 3465, 558, 986, 3353, 3458, 3457, 3459, 3456, 853, 2558, 4615], "orig_top_k_doc_id": [2618, 2619, 404, 3465, 558, 986, 3353, 3458, 3664, 3457, 3459, 3456, 853, 2558, 4615]}]}
{"group_id": 483, "group_size": 4, "items": [{"qid": 1032, "question": "What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space? in Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space", "answer": [" analogy query, analogy browsing"], "top_k_doc_id": [1353, 1354, 1355, 4560, 267, 268, 453, 3271, 5297, 7424, 7514, 7678, 3207, 6014, 4160], "orig_top_k_doc_id": [1353, 1355, 1354, 268, 3271, 5297, 7678, 4560, 7424, 3207, 6014, 4160, 7514, 453, 267]}, {"qid": 1033, "question": "What are the uncanny semantic structures of the embedding space? in Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space", "answer": ["Semantic similarity structure, Semantic direction structure"], "top_k_doc_id": [1353, 1354, 1355, 4560, 267, 268, 453, 3271, 5297, 7424, 7514, 7678, 4318, 134, 4275], "orig_top_k_doc_id": [1355, 1353, 1354, 268, 5297, 4560, 4318, 267, 7424, 3271, 7514, 134, 453, 4275, 7678]}, {"qid": 1034, "question": "What is the general framework for data exploration by semantic queries? in Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space", "answer": ["three main components, namely data processing, task processing, and query processing"], "top_k_doc_id": [1353, 1354, 1355, 4560, 1120, 1122, 4318, 4716, 4559, 4317, 2986, 7126, 134, 4319, 6980], "orig_top_k_doc_id": [1355, 1353, 1354, 1120, 4318, 4560, 4559, 4317, 4716, 2986, 7126, 134, 1122, 4319, 6980]}, {"qid": 1035, "question": "What data exploration is supported by the analysis of these semantic structures? in Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space", "answer": ["Task processing: converting data exploration tasks to algebraic operations on the embedding space, Query processing: executing semantic query on the embedding space and return results"], "top_k_doc_id": [1353, 1354, 1355, 4560, 1120, 1122, 4318, 4716, 5297, 6951, 268, 4718, 865, 460, 1356], "orig_top_k_doc_id": [1355, 1353, 1354, 1120, 5297, 6951, 4318, 4716, 1122, 4560, 268, 4718, 865, 460, 1356]}]}
{"group_id": 484, "group_size": 4, "items": [{"qid": 1140, "question": "What are the differences with previous applications of neural networks for this task? in Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks", "answer": ["This approach considers related images"], "top_k_doc_id": [1172, 1499, 1517, 1519, 6555, 6556, 6557, 1518, 1863, 6013, 5893, 816, 2590, 3928, 75], "orig_top_k_doc_id": [1519, 1517, 6555, 6557, 1172, 6013, 1499, 5893, 6556, 816, 1518, 2590, 3928, 1863, 75]}, {"qid": 4096, "question": "Which dataset do they use? in SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait Detection", "answer": ["A crowdsourced twitter dataset containing 19358 tweets", "BIBREF4", "19538 tweets  from BIBREF4"], "top_k_doc_id": [1172, 1499, 1517, 1519, 6555, 6556, 6557, 1498, 4967, 6980, 3287, 597, 6088, 6740, 6479], "orig_top_k_doc_id": [6557, 1517, 6555, 6556, 1172, 1519, 1499, 3287, 597, 6088, 1498, 6980, 4967, 6740, 6479]}, {"qid": 4097, "question": "By how much do they outperform previous state-of-the-art approaches? in SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait Detection", "answer": ["BiLSTM for 0.02 F1,  Feature Engineering SotA  for 0.08 F1, and Concatenated NN Architecture for  0.24 F1.", "Proposed model had 0.63 F1 score and 83.49% accuracy compared to the 0.61 F1 and 83.28% accuracy of best compared method.", "By more than 0.02 with F1 score and 0.21% with accuracy"], "top_k_doc_id": [1172, 1499, 1517, 1519, 6555, 6556, 6557, 1498, 4967, 6980, 123, 5417, 6338, 5369, 730], "orig_top_k_doc_id": [6557, 6555, 6556, 1519, 1517, 123, 6980, 1499, 1498, 5417, 1172, 6338, 5369, 730, 4967]}, {"qid": 4098, "question": "Do they analyze attention outputs to determine which terms in general contribute to clickbait titles? in SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait Detection", "answer": ["No", "No"], "top_k_doc_id": [1172, 1499, 1517, 1519, 6555, 6556, 6557, 1518, 1863, 1498, 1494, 1867, 1496, 1866, 1173], "orig_top_k_doc_id": [6557, 1517, 6555, 6556, 1519, 1498, 1172, 1494, 1867, 1499, 1863, 1496, 1866, 1173, 1518]}]}
{"group_id": 485, "group_size": 4, "items": [{"qid": 1145, "question": "What model was winner of the Visual Dialog challenge 2019? in Modality-Balanced Models for Visual Dialogue", "answer": ["No"], "top_k_doc_id": [576, 1527, 1529, 1530, 1531, 7164, 871, 1532, 2414, 4552, 4756, 5015, 3658, 1237, 4758], "orig_top_k_doc_id": [1531, 1527, 1532, 576, 1530, 4756, 3658, 1529, 4552, 5015, 871, 2414, 7164, 1237, 4758]}, {"qid": 1146, "question": "What model was winner of the Visual Dialog challenge 2018? in Modality-Balanced Models for Visual Dialogue", "answer": ["DL-61"], "top_k_doc_id": [576, 1527, 1529, 1530, 1531, 7164, 871, 1532, 2414, 4552, 4756, 5015, 7165, 5787, 4977], "orig_top_k_doc_id": [1531, 1527, 1530, 576, 7165, 1532, 5787, 4756, 1529, 4552, 5015, 7164, 4977, 871, 2414]}, {"qid": 3501, "question": "What was the baseline? in Ensemble based discriminative models for Visual Dialog Challenge 2018", "answer": ["No"], "top_k_doc_id": [576, 1527, 1529, 1530, 1531, 7164, 575, 1033, 1034, 5787, 4553, 1331, 7479, 1330, 4977], "orig_top_k_doc_id": [1531, 5787, 1527, 1530, 1033, 1034, 575, 1529, 7164, 4553, 1331, 7479, 1330, 4977, 576]}, {"qid": 3502, "question": "Which three discriminative models did they use? in Ensemble based discriminative models for Visual Dialog Challenge 2018", "answer": ["LF-RCNN, MN-RCNN, MN-RCNN-Wt", "LF-RCNN, MN-RCNN, MN-RCNN-Wt"], "top_k_doc_id": [576, 1527, 1529, 1530, 1531, 7164, 575, 1033, 1034, 5787, 1029, 1030, 1031, 6589, 4371], "orig_top_k_doc_id": [5787, 1531, 1527, 1530, 1033, 575, 1034, 1029, 1030, 1529, 1031, 6589, 4371, 7164, 576]}]}
{"group_id": 486, "group_size": 4, "items": [{"qid": 1156, "question": "Is accuracy the only metric they used to compare systems? in ISS-MULT: Intelligent Sample Selection for Multi-Task Learning in Question Answering", "answer": ["No"], "top_k_doc_id": [1547, 1549, 490, 575, 659, 946, 1548, 1550, 109, 3851, 6936, 6932, 1070, 2416, 3094], "orig_top_k_doc_id": [1547, 1549, 1550, 1548, 109, 490, 659, 6936, 6932, 1070, 3851, 946, 575, 2416, 3094]}, {"qid": 1157, "question": "How do they transfer the model? in ISS-MULT: Intelligent Sample Selection for Multi-Task Learning in Question Answering", "answer": ["In the MULT method, two datasets are simultaneously trained, and the weights are tuned based on the inputs which come from both datasets. The hyper-parameter $\\lambda \\in (0,1)$ is calculated based on a brute-force search or using general global search. This hyper parameter is used to calculate the final cost function which is computed from the combination of the cost function of the source dataset and the target datasets. , this paper proposes to use the most relevant samples from the source dataset to train on the target dataset. One way to find the most similar samples is finding the pair-wise distance between all samples of the development set of the target dataset and source dataset., we propose using a clustering algorithm on the development set. The clustering algorithm used ihere is a hierarchical clustering algorithm. The cosine similarity is used as a criteria to cluster each question and answer. Therefore, these clusters are representative of the development set of the target dataset and the corresponding center for each cluster is representative of all the samples on that cluster. In the next step, the distance of each center is used to calculate the cosine similarity. Finally, the samples in the source dataset which are far from these centers are ignored. In other words, the outliers do not take part in transfer learning."], "top_k_doc_id": [1547, 1549, 490, 575, 659, 946, 1548, 1550, 109, 3851, 3839, 1555, 3617, 2725, 1240], "orig_top_k_doc_id": [1547, 1549, 1550, 1548, 659, 3839, 490, 1555, 946, 109, 3617, 3851, 575, 2725, 1240]}, {"qid": 1155, "question": "Do transferring hurt the performance is the corpora are not related? in ISS-MULT: Intelligent Sample Selection for Multi-Task Learning in Question Answering", "answer": ["Yes"], "top_k_doc_id": [1547, 1549, 490, 575, 659, 946, 1548, 1550, 7056, 949, 2306, 2661, 3839, 2725, 2664], "orig_top_k_doc_id": [1549, 1547, 1550, 1548, 7056, 575, 946, 949, 659, 2306, 490, 2661, 3839, 2725, 2664]}, {"qid": 2384, "question": "How different is the dataset size of source and target? in Supervised and Unsupervised Transfer Learning for Question Answering", "answer": ["the training dataset is large while the target dataset is usually much smaller"], "top_k_doc_id": [1547, 1549, 3842, 3839, 3840, 2306, 3620, 5968, 2870, 5044, 4590, 633, 632, 2030, 2725], "orig_top_k_doc_id": [3842, 3839, 3840, 2306, 3620, 5968, 2870, 5044, 1547, 4590, 633, 632, 1549, 2030, 2725]}]}
{"group_id": 487, "group_size": 4, "items": [{"qid": 1212, "question": "What is the average length of the claims? in Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "answer": ["Average claim length is 8.9 tokens."], "top_k_doc_id": [1482, 1645, 1646, 1647, 1649, 928, 1648, 3287, 5375, 1495, 321, 7673, 746, 744, 322], "orig_top_k_doc_id": [1645, 1647, 1646, 1649, 5375, 1648, 321, 7673, 3287, 928, 746, 1482, 744, 1495, 322]}, {"qid": 1215, "question": "Which machine baselines are used? in Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "answer": ["Information Retrieval"], "top_k_doc_id": [1482, 1645, 1646, 1647, 1649, 928, 1648, 3287, 5375, 1495, 6260, 7499, 514, 6745, 5315], "orig_top_k_doc_id": [1645, 1647, 1646, 1649, 1648, 1482, 1495, 3287, 5375, 6260, 7499, 514, 928, 6745, 5315]}, {"qid": 1216, "question": "What challenges are highlighted? in Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "answer": ["one needs to develop mechanisms to recognize valid argumentative structures, we ignore trustworthiness and credibility issues"], "top_k_doc_id": [1482, 1645, 1646, 1647, 1649, 928, 1648, 3287, 5375, 5426, 5907, 609, 927, 433, 741], "orig_top_k_doc_id": [1645, 1646, 1649, 1647, 1482, 3287, 5375, 5426, 1648, 5907, 928, 609, 927, 433, 741]}, {"qid": 1214, "question": "What crowdsourcing platform did they use? in Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims", "answer": ["Amazon Mechanical Turk (AMT)"], "top_k_doc_id": [1482, 1645, 1646, 1647, 1649, 2400, 6995, 3486, 5910, 5908, 3316, 5907, 5911, 7675, 3318], "orig_top_k_doc_id": [1645, 1646, 2400, 1649, 6995, 3486, 5910, 1647, 1482, 5908, 3316, 5907, 5911, 7675, 3318]}]}
{"group_id": 488, "group_size": 4, "items": [{"qid": 1220, "question": "What are the linguistic differences between each class? in Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "answer": ["Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes"], "top_k_doc_id": [1655, 1658, 1659, 2109, 2114, 1330, 2115, 4138, 1656, 1669, 5903, 3587, 1715, 1657, 523], "orig_top_k_doc_id": [1659, 1655, 1658, 1656, 4138, 2115, 2114, 5903, 3587, 1715, 1330, 1657, 2109, 523, 1669]}, {"qid": 1221, "question": "What simple features are used? in Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "answer": ["unigrams, bigrams, and trigrams, including sequences of punctuation, Word2Vec word embeddings"], "top_k_doc_id": [1655, 1658, 1659, 2109, 2114, 1330, 2115, 4138, 1656, 1669, 7372, 7262, 3451, 2105, 3543], "orig_top_k_doc_id": [1655, 1659, 4138, 1669, 2109, 1658, 1656, 2115, 2114, 7372, 1330, 7262, 3451, 2105, 3543]}, {"qid": 1219, "question": "Do they report results only on English datasets? in Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "answer": ["No"], "top_k_doc_id": [1655, 1658, 1659, 2109, 2114, 1330, 2115, 4138, 2110, 2104, 2278, 6858, 3451, 6173, 4136], "orig_top_k_doc_id": [1655, 1659, 1330, 2114, 2109, 2110, 2115, 2104, 2278, 4138, 6858, 3451, 6173, 1658, 4136]}, {"qid": 1222, "question": "What lexico-syntactic cues are used to retrieve sarcastic utterances? in Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "answer": ["adjective and adverb patterns, verb, subject, and object arguments, verbal patterns"], "top_k_doc_id": [1655, 1658, 1659, 2109, 2114, 1656, 1657, 1329, 6328, 2104, 2105, 5900, 2113, 2110, 6173], "orig_top_k_doc_id": [1655, 1659, 1656, 1658, 1657, 1329, 2109, 6328, 2104, 2105, 2114, 5900, 2113, 2110, 6173]}]}
{"group_id": 489, "group_size": 4, "items": [{"qid": 1250, "question": "Is the dataset balanced across categories? in Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "answer": ["Yes"], "top_k_doc_id": [955, 1172, 1701, 1703, 1704, 1705, 6330, 1174, 3311, 4579, 6285, 6329, 6328, 1702, 5913], "orig_top_k_doc_id": [1701, 1704, 1705, 6330, 6329, 6328, 4579, 1702, 1172, 955, 5913, 1174, 6285, 1703, 3311]}, {"qid": 1251, "question": "What supervised methods are used? in Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "answer": ["Support Vector Machines (SVMs), logistic regression, Na\u00efve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)"], "top_k_doc_id": [955, 1172, 1701, 1703, 1704, 1705, 6330, 1702, 2083, 6207, 6209, 6328, 5905, 5913, 412], "orig_top_k_doc_id": [1701, 1704, 1705, 6207, 955, 6330, 1703, 1172, 6209, 6328, 5905, 5913, 412, 1702, 2083]}, {"qid": 1252, "question": "What labels are in the dataset? in Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "answer": ["binary label of stress or not stress"], "top_k_doc_id": [955, 1172, 1701, 1703, 1704, 1705, 6330, 1702, 2083, 6207, 6209, 6328, 6285, 5908, 5056], "orig_top_k_doc_id": [1701, 1704, 1705, 1172, 6330, 6285, 6207, 1703, 2083, 5908, 6328, 955, 6209, 5056, 1702]}, {"qid": 1253, "question": "What categories does the dataset come from? in Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "answer": ["abuse, social, anxiety, PTSD, and financial"], "top_k_doc_id": [955, 1172, 1701, 1703, 1704, 1705, 6330, 1174, 3311, 4579, 6285, 6329, 6207, 5908, 3627], "orig_top_k_doc_id": [1701, 1704, 1705, 955, 4579, 6329, 1703, 6285, 1172, 6330, 3311, 6207, 5908, 3627, 1174]}]}
{"group_id": 490, "group_size": 4, "items": [{"qid": 1376, "question": "How much in experiments is performance improved for models trained with generated adversarial examples? in Adversarial Examples with Difficult Common Words for Paraphrase Identification", "answer": ["Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original\nexamples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)"], "top_k_doc_id": [1893, 1894, 1898, 3565, 6864, 3563, 5354, 5562, 1896, 1897, 4204, 5561, 1597, 5560, 6865], "orig_top_k_doc_id": [1898, 1893, 1894, 1897, 1896, 4204, 6864, 5561, 5354, 5562, 1597, 3565, 3563, 5560, 6865]}, {"qid": 1377, "question": "How much dramatically results drop for models on generated adversarial examples? in Adversarial Examples with Difficult Common Words for Paraphrase Identification", "answer": ["BERT on Quora drops from 94.6% to 24.1%"], "top_k_doc_id": [1893, 1894, 1898, 3565, 6864, 3563, 5354, 5562, 1896, 1897, 4204, 5561, 6863, 6868, 4205], "orig_top_k_doc_id": [1898, 1893, 1894, 1897, 1896, 5561, 3563, 6864, 5354, 3565, 4204, 5562, 6863, 6868, 4205]}, {"qid": 1379, "question": "What are benhmark datasets for paraphrase identification? in Adversarial Examples with Difficult Common Words for Paraphrase Identification", "answer": ["Quora, MRPC"], "top_k_doc_id": [1893, 1894, 1898, 3565, 6864, 3563, 5354, 5562, 2807, 6561, 6564, 1187, 4987, 2808, 2895], "orig_top_k_doc_id": [1893, 1894, 1898, 2807, 3565, 6561, 6864, 6564, 1187, 5354, 4987, 2808, 5562, 2895, 3563]}, {"qid": 1378, "question": "What is discriminator in this generative adversarial setup? in Adversarial Examples with Difficult Common Words for Paraphrase Identification", "answer": [" current model"], "top_k_doc_id": [1893, 1894, 1898, 3565, 6864, 1006, 5442, 94, 4583, 1014, 5186, 5279, 278, 3328, 276], "orig_top_k_doc_id": [1893, 1894, 1898, 1006, 5442, 94, 4583, 6864, 1014, 5186, 5279, 3565, 278, 3328, 276]}]}
{"group_id": 491, "group_size": 4, "items": [{"qid": 1389, "question": "What is the size of the corpus? in Citation Text Generation", "answer": ["8.1 million scientific documents, 154K computer science articles, 622K citing sentences"], "top_k_doc_id": [307, 1921, 1922, 1923, 1924, 1925, 1301, 2046, 2047, 55, 304, 3608, 3609, 3606, 302], "orig_top_k_doc_id": [1922, 1921, 1924, 1925, 3608, 304, 2047, 1923, 55, 3609, 2046, 3606, 302, 307, 1301]}, {"qid": 1390, "question": "How was the evaluation corpus collected? in Citation Text Generation", "answer": ["No"], "top_k_doc_id": [307, 1921, 1922, 1923, 1924, 1925, 1301, 2046, 2047, 55, 304, 3608, 2041, 303, 1865], "orig_top_k_doc_id": [1921, 1922, 1924, 1925, 3608, 1923, 55, 307, 2041, 303, 1301, 2047, 304, 2046, 1865]}, {"qid": 1388, "question": "Which baselines are explored? in Citation Text Generation", "answer": ["GPT2, SciBERT model of BIBREF11"], "top_k_doc_id": [307, 1921, 1922, 1923, 1924, 1925, 1301, 2046, 2047, 302, 305, 2041, 306, 3606, 3609], "orig_top_k_doc_id": [1921, 302, 1922, 1925, 1924, 2047, 1301, 305, 2041, 306, 3606, 2046, 307, 1923, 3609]}, {"qid": 1387, "question": "Which baseline performs best? in Citation Text Generation", "answer": ["IR methods perform better than the best neural models"], "top_k_doc_id": [307, 1921, 1922, 1923, 1924, 1925, 3610, 306, 3609, 305, 302, 1302, 3611, 3936, 7337], "orig_top_k_doc_id": [307, 1921, 1924, 1922, 1925, 3610, 306, 1923, 3609, 305, 302, 1302, 3611, 3936, 7337]}]}
{"group_id": 492, "group_size": 4, "items": [{"qid": 1427, "question": "Which dataset(s) do they evaluate on? in Deep Text-to-Speech System with Seq2Seq Model", "answer": ["LJSpeech"], "top_k_doc_id": [1975, 1977, 2003, 2530, 491, 109, 2842, 6288, 6294, 4921, 7618, 6289, 4920, 569, 494], "orig_top_k_doc_id": [1977, 1975, 491, 109, 2842, 6288, 6294, 4921, 2003, 7618, 2530, 6289, 4920, 569, 494]}, {"qid": 1428, "question": "Which modifications do they make to well-established Seq2seq architectures? in Deep Text-to-Speech System with Seq2Seq Model", "answer": ["Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible"], "top_k_doc_id": [1975, 1977, 2528, 2531, 3173, 6067, 6066, 6065, 2739, 7619, 2529, 7594, 2530, 1976, 2188], "orig_top_k_doc_id": [1975, 6067, 6066, 6065, 3173, 2739, 7619, 2529, 7594, 2528, 2530, 2531, 1976, 2188, 1977]}, {"qid": 1429, "question": "How do they measure the size of models? in Deep Text-to-Speech System with Seq2Seq Model", "answer": ["Direct comparison of model parameters"], "top_k_doc_id": [1975, 1977, 2528, 2531, 3173, 6067, 491, 2842, 7479, 6068, 492, 7160, 7618, 1665, 898], "orig_top_k_doc_id": [1977, 491, 1975, 2842, 7479, 6068, 492, 7160, 2528, 6067, 7618, 2531, 1665, 3173, 898]}, {"qid": 1430, "question": "Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models? in Deep Text-to-Speech System with Seq2Seq Model", "answer": ["Yes"], "top_k_doc_id": [1975, 1977, 2003, 2530, 150, 2149, 3338, 4922, 4974, 2528, 3976, 1618, 4973, 4487, 4515], "orig_top_k_doc_id": [1977, 1975, 150, 2530, 2149, 2003, 3338, 4922, 4974, 2528, 3976, 1618, 4973, 4487, 4515]}]}
{"group_id": 493, "group_size": 4, "items": [{"qid": 1451, "question": "By how much do they outperform basic greedy and cross-entropy beam decoding? in A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models", "answer": ["2 accuracy points"], "top_k_doc_id": [2024, 2025, 2026, 2027, 2029, 1525, 6668, 2028, 6535, 5283, 3092, 3260, 1621, 5635, 382], "orig_top_k_doc_id": [2024, 2029, 2027, 2026, 2028, 6535, 2025, 5283, 3092, 3260, 1621, 6668, 5635, 1525, 382]}, {"qid": 1452, "question": "Do they provide a framework for building a sub-differentiable for any final loss metric? in A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models", "answer": ["Yes"], "top_k_doc_id": [2024, 2025, 2026, 2027, 2029, 381, 3359, 2426, 315, 4424, 7372, 2733, 4972, 5987, 3834], "orig_top_k_doc_id": [2024, 2029, 2025, 2426, 2026, 381, 2027, 315, 4424, 7372, 2733, 4972, 5987, 3359, 3834]}, {"qid": 1453, "question": "Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences? in A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models", "answer": ["Yes"], "top_k_doc_id": [2024, 2025, 2026, 2027, 2029, 1525, 6668, 7601, 7642, 28, 1253, 6067, 5633, 7644, 3359], "orig_top_k_doc_id": [2024, 2029, 2027, 2025, 2026, 7601, 1525, 7642, 28, 1253, 6067, 6668, 5633, 7644, 3359]}, {"qid": 1454, "question": "Which loss metrics do they try in their new training procedure evaluated on the output of beam search? in A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models", "answer": [" continuous relaxation to top-k-argmax"], "top_k_doc_id": [2024, 2025, 2026, 2027, 2029, 381, 3359, 382, 6270, 3808, 5632, 2028, 2431, 1056, 7642], "orig_top_k_doc_id": [2024, 2025, 2027, 2026, 2029, 3359, 382, 6270, 3808, 381, 5632, 2028, 2431, 1056, 7642]}]}
{"group_id": 494, "group_size": 4, "items": [{"qid": 1477, "question": "What is the highest accuracy score achieved? in Constructing a Natural Language Inference Dataset using Generative Neural Networks", "answer": ["82.0%"], "top_k_doc_id": [2064, 2065, 1967, 3159, 7371, 2071, 7734, 280, 6431, 94, 4273, 3102, 3255, 7053, 690], "orig_top_k_doc_id": [2065, 2064, 2071, 7734, 7371, 280, 6431, 94, 4273, 1967, 3102, 3159, 3255, 7053, 690]}, {"qid": 1478, "question": "What is the size range of the datasets? in Constructing a Natural Language Inference Dataset using Generative Neural Networks", "answer": ["No"], "top_k_doc_id": [2064, 2065, 1967, 3159, 7371, 1560, 533, 4698, 5779, 3956, 4699, 281, 3359, 3298, 453], "orig_top_k_doc_id": [2065, 2064, 1560, 533, 4698, 5779, 1967, 3956, 4699, 281, 7371, 3359, 3159, 3298, 453]}, {"qid": 2158, "question": "How is the generative model evaluated? in Generating Natural Language Inference Chains", "answer": ["Comparing BLEU score of model with and without attention"], "top_k_doc_id": [2064, 2065, 3299, 3298, 6888, 4354, 280, 7072, 3496, 281, 28, 1242, 1470, 533, 2425], "orig_top_k_doc_id": [2065, 3299, 3298, 2064, 6888, 4354, 280, 7072, 3496, 281, 28, 1242, 1470, 533, 2425]}, {"qid": 4876, "question": "How were missing hypotheses discovered? in A corpus of precise natural textual entailment problems", "answer": ["The problems were then re-rated by experts in logic and/or linguistics. , If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using.", "More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using. "], "top_k_doc_id": [2064, 2065, 7563, 7565, 7564, 7562, 1982, 584, 1980, 3298, 5735, 910, 3852, 1981, 4994], "orig_top_k_doc_id": [7563, 7565, 7564, 7562, 1982, 584, 1980, 2064, 3298, 5735, 910, 2065, 3852, 1981, 4994]}]}
{"group_id": 495, "group_size": 4, "items": [{"qid": 1510, "question": "Do they report results only on English data? in The Effect of Context on Metaphor Paraphrase Aptness Judgments", "answer": ["No"], "top_k_doc_id": [2122, 2123, 2124, 2125, 2126, 3183, 4796, 4839, 5869, 3724, 4791, 5, 5868, 6763, 6454], "orig_top_k_doc_id": [2125, 2126, 2123, 2124, 2122, 5869, 4839, 4796, 3724, 4791, 5, 5868, 6763, 6454, 3183]}, {"qid": 1513, "question": "What were the results of the first experiment? in The Effect of Context on Metaphor Paraphrase Aptness Judgments", "answer": ["Best performance achieved is 0.72 F1 score"], "top_k_doc_id": [2122, 2123, 2124, 2125, 2126, 3183, 4796, 4839, 5716, 7789, 1187, 7274, 1972, 3069, 5708], "orig_top_k_doc_id": [2126, 2125, 2122, 2123, 2124, 5716, 3183, 7789, 1187, 4796, 7274, 1972, 3069, 5708, 4839]}, {"qid": 1511, "question": "What provisional explanation do the authors give for the impact of document context? in The Effect of Context on Metaphor Paraphrase Aptness Judgments", "answer": ["adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence"], "top_k_doc_id": [2122, 2123, 2124, 2125, 2126, 307, 306, 5868, 5869, 7424, 1682, 1560, 6714, 3467, 305], "orig_top_k_doc_id": [2125, 2124, 2122, 2126, 2123, 307, 306, 5868, 5869, 7424, 1682, 1560, 6714, 3467, 305]}, {"qid": 1512, "question": "What document context was added? in The Effect of Context on Metaphor Paraphrase Aptness Judgments", "answer": ["Preceding and following sentence of each metaphor and paraphrase are added as document context"], "top_k_doc_id": [2122, 2123, 2124, 2125, 2126, 306, 6731, 7571, 7563, 5150, 4829, 2399, 6823, 7619, 2209], "orig_top_k_doc_id": [2123, 2122, 2126, 2125, 2124, 306, 6731, 7571, 7563, 5150, 4829, 2399, 6823, 7619, 2209]}]}
{"group_id": 496, "group_size": 4, "items": [{"qid": 1519, "question": "Do they perform a quantitative analysis of their model displaying knowledge distortions? in Compositional Neural Machine Translation by Removing the Lexicon from Syntax", "answer": ["Yes"], "top_k_doc_id": [2132, 2134, 2135, 2136, 2701, 2705, 4221, 4344, 2133, 1389, 6251, 4312, 3550, 273, 1637], "orig_top_k_doc_id": [2136, 2133, 2134, 2135, 2701, 4344, 6251, 2705, 4221, 4312, 3550, 273, 1389, 1637, 2132]}, {"qid": 1520, "question": "How do they damage different neural modules? in Compositional Neural Machine Translation by Removing the Lexicon from Syntax", "answer": ["Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information."], "top_k_doc_id": [2132, 2134, 2135, 2136, 2701, 2705, 4221, 4344, 2133, 1389, 2706, 2584, 2578, 2738, 2704], "orig_top_k_doc_id": [2133, 2134, 2705, 2135, 2706, 2584, 2701, 2136, 2578, 2132, 2738, 2704, 4344, 1389, 4221]}, {"qid": 1521, "question": "Which weights from their model do they analyze? in Compositional Neural Machine Translation by Removing the Lexicon from Syntax", "answer": ["No"], "top_k_doc_id": [2132, 2134, 2135, 2136, 2701, 2705, 4221, 4344, 2133, 2702, 2761, 2704, 4312, 4314, 4222], "orig_top_k_doc_id": [2135, 2132, 2705, 2134, 2702, 2136, 2133, 4344, 2761, 4221, 2701, 2704, 4312, 4314, 4222]}, {"qid": 1518, "question": "Does having constrained neural units imply word meanings are fixed across different context? in Compositional Neural Machine Translation by Removing the Lexicon from Syntax", "answer": ["No"], "top_k_doc_id": [2132, 2134, 2135, 2136, 2701, 2705, 4221, 4344, 2704, 4339, 4633, 4846, 5423, 4632, 1181], "orig_top_k_doc_id": [2132, 2136, 2705, 2134, 2135, 2704, 4221, 2701, 4339, 4633, 4344, 4846, 5423, 4632, 1181]}]}
{"group_id": 497, "group_size": 4, "items": [{"qid": 1552, "question": "What is the size of the released dataset? in Open Information Extraction on Scientific Text: An Evaluation", "answer": ["440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples."], "top_k_doc_id": [2168, 2171, 1903, 4008, 4105, 7082, 3743, 3582, 6955, 5968, 5400, 5495, 2172, 965, 2376], "orig_top_k_doc_id": [2168, 2171, 3743, 3582, 7082, 6955, 1903, 5968, 4008, 5400, 5495, 2172, 965, 2376, 4105]}, {"qid": 1554, "question": "What is the most common error type? in Open Information Extraction on Scientific Text: An Evaluation", "answer": ["all annotators that a triple extraction was incorrect"], "top_k_doc_id": [2168, 2171, 1903, 4008, 4105, 7082, 1539, 3857, 5674, 2170, 2755, 5401, 2127, 357, 5332], "orig_top_k_doc_id": [2168, 2171, 4105, 1539, 7082, 3857, 5674, 2170, 2755, 5401, 1903, 2127, 4008, 357, 5332]}, {"qid": 1556, "question": "What is the role of crowd-sourcing? in Open Information Extraction on Scientific Text: An Evaluation", "answer": ["Crowd workers were asked to mark whether a triple was correct, namely, did the triple reflect the consequence of the sentence."], "top_k_doc_id": [2168, 2171, 2172, 303, 2169, 302, 2170, 780, 2376, 2420, 1170, 1535, 5291, 1979, 7553], "orig_top_k_doc_id": [2168, 2171, 2172, 303, 2169, 302, 2170, 780, 2376, 2420, 1170, 1535, 5291, 1979, 7553]}, {"qid": 2460, "question": "How large is the dataset? in The STEM-ECR Dataset: Grounding Scientific Entity References in STEM Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources", "answer": ["6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities"], "top_k_doc_id": [2168, 2171, 4105, 4107, 4108, 4109, 4106, 6226, 476, 5121, 3931, 1188, 480, 812, 5125], "orig_top_k_doc_id": [4105, 4107, 4108, 4109, 4106, 6226, 476, 5121, 3931, 1188, 480, 2171, 812, 5125, 2168]}]}
{"group_id": 498, "group_size": 4, "items": [{"qid": 1580, "question": "What evaluation metrics were used in the experiment? in TutorialVQA: Question Answering Dataset for Tutorial Videos", "answer": ["For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy"], "top_k_doc_id": [2210, 2211, 2212, 2213, 2214, 7355, 576, 739, 5666, 787, 788, 2803, 7352, 491, 2347], "orig_top_k_doc_id": [2210, 2214, 2212, 2213, 2211, 576, 7355, 788, 787, 739, 2803, 5666, 7352, 491, 2347]}, {"qid": 1582, "question": "What baseline algorithms were presented? in TutorialVQA: Question Answering Dataset for Tutorial Videos", "answer": ["a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm"], "top_k_doc_id": [2210, 2211, 2212, 2213, 2214, 7355, 576, 739, 5666, 787, 788, 2803, 791, 7357, 5368], "orig_top_k_doc_id": [2214, 2210, 2211, 2213, 2212, 739, 2803, 791, 576, 787, 7355, 7357, 5368, 5666, 788]}, {"qid": 1581, "question": "What kind of instructional videos are in the dataset? in TutorialVQA: Question Answering Dataset for Tutorial Videos", "answer": ["tutorial videos for a photo-editing software"], "top_k_doc_id": [2210, 2211, 2212, 2213, 2214, 7355, 576, 739, 5666, 2922, 2923, 7557, 285, 2689, 7556], "orig_top_k_doc_id": [2214, 2210, 2213, 2211, 2212, 2922, 2923, 7557, 576, 285, 739, 5666, 7355, 2689, 7556]}, {"qid": 1583, "question": "What is the source of the triples? in TutorialVQA: Question Answering Dataset for Tutorial Videos", "answer": ["a tutorial website about an image editing program "], "top_k_doc_id": [2210, 2211, 2212, 2213, 2214, 7355, 946, 4699, 7557, 4490, 5368, 7351, 4900, 7805, 7804], "orig_top_k_doc_id": [2210, 2214, 2213, 2212, 2211, 946, 4699, 7557, 4490, 5368, 7355, 7351, 4900, 7805, 7804]}]}
{"group_id": 499, "group_size": 4, "items": [{"qid": 1623, "question": "Which is the baseline model? in Phonetic Temporal Neural Model for Language Identification", "answer": ["The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. "], "top_k_doc_id": [2293, 2296, 2300, 2292, 2294, 2295, 2298, 6370, 1266, 6605, 2297, 4671, 1784, 1268, 1286], "orig_top_k_doc_id": [2300, 2295, 2294, 2296, 2298, 2293, 1266, 6370, 2292, 2297, 4671, 1784, 6605, 1268, 1286]}, {"qid": 1625, "question": "What is the main contribution of the paper?  in Phonetic Temporal Neural Model for Language Identification", "answer": ["Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance"], "top_k_doc_id": [2293, 2296, 2300, 2292, 2294, 2295, 2298, 6370, 1266, 6605, 2768, 2769, 6959, 1760, 6896], "orig_top_k_doc_id": [2294, 2295, 2293, 2300, 2298, 6370, 2292, 2296, 2768, 2769, 1266, 6959, 6605, 1760, 6896]}, {"qid": 1624, "question": "How big is the Babel database? in Phonetic Temporal Neural Model for Language Identification", "answer": ["No"], "top_k_doc_id": [2293, 2296, 2300, 2292, 2294, 2295, 2298, 6370, 2299, 2297, 3891, 380, 381, 3885, 4671], "orig_top_k_doc_id": [2300, 2296, 2299, 2298, 2297, 2294, 2295, 2293, 3891, 6370, 380, 2292, 381, 3885, 4671]}, {"qid": 1019, "question": "By how much does using phonetic feedback improve state-of-the-art systems? in Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data", "answer": ["Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9"], "top_k_doc_id": [2293, 2296, 2300, 1338, 1336, 1337, 2299, 4863, 6310, 6491, 3648, 1286, 101, 2297, 6312], "orig_top_k_doc_id": [1338, 1336, 1337, 2299, 4863, 2296, 2300, 6310, 6491, 2293, 3648, 1286, 101, 2297, 6312]}]}
{"group_id": 500, "group_size": 4, "items": [{"qid": 1721, "question": "How do they train the retrieval modules? in Revealing the Importance of Semantic Retrieval for Machine Reading at Scale", "answer": ["We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss."], "top_k_doc_id": [2455, 2457, 1409, 2464, 6842, 117, 118, 2458, 2459, 4307, 5119, 6843, 7571, 7570, 3851], "orig_top_k_doc_id": [2455, 2458, 2457, 2464, 2459, 7571, 6842, 118, 117, 5119, 1409, 7570, 4307, 6843, 3851]}, {"qid": 1722, "question": "How do they model the neural retrieval modules? in Revealing the Importance of Semantic Retrieval for Machine Reading at Scale", "answer": ["BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling"], "top_k_doc_id": [2455, 2457, 1409, 2464, 6842, 117, 118, 2458, 2459, 4307, 5119, 6843, 7571, 3175, 490], "orig_top_k_doc_id": [2455, 2458, 2459, 2457, 2464, 7571, 6842, 118, 1409, 4307, 117, 6843, 5119, 3175, 490]}, {"qid": 1066, "question": "How can a neural model be used for a retrieval if the input is the entire Wikipedia? in Question Answering from Unstructured Text by Retrieval and Comprehension", "answer": ["Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question."], "top_k_doc_id": [2455, 2457, 1409, 2464, 6842, 1405, 352, 5810, 2210, 7541, 4149, 6509, 5809, 5736, 1422], "orig_top_k_doc_id": [1405, 2464, 1409, 6842, 352, 5810, 2210, 2455, 2457, 7541, 4149, 6509, 5809, 5736, 1422]}, {"qid": 1723, "question": "Retrieval at what level performs better, sentence level or paragraph level? in Revealing the Importance of Semantic Retrieval for Machine Reading at Scale", "answer": ["This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval."], "top_k_doc_id": [2455, 2457, 2458, 2459, 490, 7739, 2460, 7736, 2456, 6460, 4304, 5849, 884, 5810, 3095], "orig_top_k_doc_id": [2455, 2458, 2459, 490, 7739, 2460, 2457, 7736, 2456, 6460, 4304, 5849, 884, 5810, 3095]}]}
{"group_id": 501, "group_size": 4, "items": [{"qid": 1726, "question": "how many domains did they experiment with? in Katecheo: A Portable and Modular System for Multi-Topic Question Answering", "answer": ["2"], "top_k_doc_id": [5739, 2464, 2465, 2466, 2467, 5118, 5119, 6272, 5332, 1217, 7832, 2227, 2661, 2234, 2228], "orig_top_k_doc_id": [2466, 2464, 2465, 2467, 5332, 5739, 1217, 5119, 6272, 5118, 7832, 2227, 2661, 2234, 2228]}, {"qid": 1727, "question": "what pretrained models were used? in Katecheo: A Portable and Modular System for Multi-Topic Question Answering", "answer": ["BiDAF, BERT "], "top_k_doc_id": [5739, 2464, 2465, 2466, 2467, 5118, 5119, 6272, 2733, 5737, 2584, 4908, 5257, 308, 5736], "orig_top_k_doc_id": [2466, 2464, 2465, 2467, 6272, 5119, 5118, 2733, 5739, 5737, 2584, 4908, 5257, 308, 5736]}, {"qid": 2837, "question": "what were their performance results? in Extracting clinical concepts from user queries", "answer": [" the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes", "hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes"], "top_k_doc_id": [5739, 4967, 7832, 7837, 4971, 4970, 6317, 7838, 4969, 302, 1190, 4321, 7833, 6207, 5396], "orig_top_k_doc_id": [4971, 4970, 4967, 7832, 7837, 6317, 7838, 4969, 302, 1190, 4321, 5739, 7833, 6207, 5396]}, {"qid": 5031, "question": "What state-of-the-art tagging model did they use? in The Medical Scribe: Corpus Development and Model Performance Analyses", "answer": ["the Span-Attribute Tagging (SAT) model", "Span-Attribute Tagging (SAT) model"], "top_k_doc_id": [5739, 4967, 7832, 7837, 7836, 6144, 6146, 5332, 6658, 1217, 5331, 7287, 7251, 5738, 5404], "orig_top_k_doc_id": [7832, 7837, 7836, 4967, 6144, 6146, 5332, 6658, 1217, 5331, 7287, 5739, 7251, 5738, 5404]}]}
{"group_id": 502, "group_size": 4, "items": [{"qid": 1746, "question": "What language platform does the data come from? in Learning to Automatically Generate Fill-In-The-Blank Quizzes", "answer": ["No"], "top_k_doc_id": [2205, 2498, 198, 492, 493, 2011, 2400, 2499, 4557, 4558, 5910, 5186, 2594, 3126, 5911], "orig_top_k_doc_id": [2498, 2205, 2499, 2011, 2400, 5910, 2594, 4557, 5186, 3126, 4558, 492, 493, 198, 5911]}, {"qid": 1747, "question": "Which two schemes are used? in Learning to Automatically Generate Fill-In-The-Blank Quizzes", "answer": ["sequence classification, sequence labeling"], "top_k_doc_id": [2205, 2498, 198, 492, 493, 2011, 2400, 2499, 4557, 4558, 5910, 5186, 2500, 3835, 7163], "orig_top_k_doc_id": [2498, 2205, 2499, 2011, 492, 5910, 4558, 198, 4557, 5186, 493, 2500, 3835, 2400, 7163]}, {"qid": 1745, "question": "What is the size of the dataset? in Learning to Automatically Generate Fill-In-The-Blank Quizzes", "answer": ["300,000 sentences with 1.5 million single-quiz questions"], "top_k_doc_id": [2205, 2498, 198, 492, 493, 2011, 2400, 2499, 4557, 4558, 5910, 7163, 2210, 2451, 2594], "orig_top_k_doc_id": [2498, 2205, 2499, 2011, 4558, 2400, 493, 4557, 7163, 492, 198, 2210, 2451, 2594, 5910]}, {"qid": 2872, "question": "Is it possible to convert a cloze-style questions to a naturally-looking questions? in Simple and Effective Semi-Supervised Question Answering", "answer": ["No", "No"], "top_k_doc_id": [2205, 2498, 5044, 6676, 5045, 6678, 6823, 355, 4463, 2012, 5367, 5046, 6876, 5047, 352], "orig_top_k_doc_id": [5044, 6676, 2205, 5045, 6678, 6823, 355, 4463, 2012, 5367, 5046, 6876, 5047, 2498, 352]}]}
{"group_id": 503, "group_size": 4, "items": [{"qid": 1781, "question": "What metrics are used for evaluation? in Generating Narrative Text in a Switching Dynamical System", "answer": ["ROUGE BIBREF29 and METEOR BIBREF30"], "top_k_doc_id": [3799, 558, 3803, 2331, 2592, 2593, 2594, 2595, 2596, 3024, 3800, 4976, 820, 491, 4441], "orig_top_k_doc_id": [2592, 2595, 2596, 2594, 3799, 2593, 3803, 3800, 4976, 820, 491, 558, 2331, 4441, 3024]}, {"qid": 1782, "question": "What baselines are used? in Generating Narrative Text in a Switching Dynamical System", "answer": ["a two layer recurrent neural language model with GRU cells of hidden size 512, a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512, a linear dynamical system, semi-supervised SLDS models with varying amount of labelled sentiment tags"], "top_k_doc_id": [3799, 558, 3803, 2331, 2592, 2593, 2594, 2595, 2596, 3024, 6888, 6892, 3798, 3023, 1822], "orig_top_k_doc_id": [2592, 2596, 2595, 2594, 3799, 2593, 3803, 6888, 558, 2331, 6892, 3798, 3023, 3024, 1822]}, {"qid": 2367, "question": "What are the features of used to customize target user interaction?  in Customized Image Narrative Generation via Interactive Visual Question Generation and Answering", "answer": ["image feature, question feature, label vector for the user's answer"], "top_k_doc_id": [3799, 558, 3803, 3798, 3804, 3801, 3802, 3800, 5119, 4744, 6570, 490, 4427, 1940, 2414], "orig_top_k_doc_id": [3798, 3804, 3801, 3802, 3803, 3799, 3800, 558, 5119, 4744, 6570, 490, 4427, 1940, 2414]}, {"qid": 1211, "question": "how are multiple answers from multiple reformulated questions aggregated? in Ask the Right Questions: Active Question Reformulation with Reinforcement Learning", "answer": ["The selection model selects the best answer from the set $\\lbrace a_i\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants."], "top_k_doc_id": [3799, 1637, 4536, 4535, 1642, 1643, 1641, 3789, 1527, 4466, 789, 3794, 794, 7217, 3798], "orig_top_k_doc_id": [1637, 4536, 4535, 1642, 1643, 1641, 3789, 1527, 4466, 789, 3794, 794, 7217, 3798, 3799]}]}
{"group_id": 504, "group_size": 4, "items": [{"qid": 1789, "question": "what state of the accuracy did they obtain? in End-to-End Multi-View Networks for Text Classification", "answer": ["51.5"], "top_k_doc_id": [2661, 3416, 1154, 1217, 2612, 2613, 2917, 7504, 5896, 259, 4247, 4298, 7372, 3162, 2216], "orig_top_k_doc_id": [2661, 1217, 3416, 2917, 7504, 5896, 1154, 2612, 259, 4247, 4298, 2613, 7372, 3162, 2216]}, {"qid": 1791, "question": "which benchmark tasks did they experiment on? in End-to-End Multi-View Networks for Text Classification", "answer": [" They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task."], "top_k_doc_id": [2661, 3416, 1154, 1217, 2612, 2613, 1155, 4428, 7115, 1795, 7688, 4425, 7237, 1987, 575], "orig_top_k_doc_id": [1154, 1217, 1155, 2661, 4428, 7115, 2612, 1795, 7688, 4425, 7237, 2613, 3416, 1987, 575]}, {"qid": 1790, "question": "what models did they compare to? in End-to-End Multi-View Networks for Text Classification", "answer": ["High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM "], "top_k_doc_id": [2661, 3416, 1154, 2917, 1155, 7688, 2116, 6350, 2584, 2649, 7504, 6104, 4522, 424, 2117], "orig_top_k_doc_id": [1154, 2917, 1155, 2661, 7688, 2116, 6350, 2584, 2649, 3416, 7504, 6104, 4522, 424, 2117]}, {"qid": 2393, "question": "What are the strong baselines you have? in Multi-task learning to improve natural language understanding", "answer": ["optimize single task with no synthetic data"], "top_k_doc_id": [2661, 3416, 6651, 4538, 1471, 3972, 6888, 6036, 4901, 6943, 3789, 3868, 7222, 7159, 5698], "orig_top_k_doc_id": [6651, 3416, 4538, 1471, 3972, 6888, 6036, 4901, 6943, 3789, 3868, 7222, 7159, 5698, 2661]}]}
{"group_id": 505, "group_size": 4, "items": [{"qid": 1836, "question": "What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present? in User Generated Data: Achilles' heel of BERT", "answer": ["10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%\n50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%"], "top_k_doc_id": [2676, 2677, 2678, 99, 2818, 4668, 5330, 2899, 803, 2233, 805, 7664, 2904, 100, 6271], "orig_top_k_doc_id": [2677, 2676, 2678, 2818, 4668, 5330, 2899, 803, 2233, 805, 7664, 2904, 99, 100, 6271]}, {"qid": 1837, "question": "Which sentiment analysis data set has a larger performance drop when a 10% error is introduced? in User Generated Data: Achilles' heel of BERT", "answer": ["SST-2 dataset"], "top_k_doc_id": [2676, 2677, 2678, 99, 5354, 5776, 138, 1705, 5995, 5351, 5158, 5255, 7118, 7817, 4277], "orig_top_k_doc_id": [2677, 2676, 5354, 2678, 5776, 138, 1705, 5995, 5351, 5158, 5255, 99, 7118, 7817, 4277]}, {"qid": 1838, "question": "What kind is noise is present in typical industrial data? in User Generated Data: Achilles' heel of BERT", "answer": [" non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages"], "top_k_doc_id": [2676, 2677, 2678, 100, 3370, 5374, 5158, 6362, 4663, 7249, 680, 2730, 4498, 4669, 946], "orig_top_k_doc_id": [2676, 2678, 100, 3370, 5374, 5158, 6362, 4663, 2677, 7249, 680, 2730, 4498, 4669, 946]}, {"qid": 1839, "question": "What is the reason behind the drop in performance using BERT for some popular task? in User Generated Data: Achilles' heel of BERT", "answer": ["Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance."], "top_k_doc_id": [2676, 2677, 2678, 4613, 872, 6821, 2310, 436, 3723, 4900, 533, 1896, 4614, 5099, 5295], "orig_top_k_doc_id": [2677, 2676, 2678, 4613, 872, 6821, 2310, 436, 3723, 4900, 533, 1896, 4614, 5099, 5295]}]}
{"group_id": 506, "group_size": 4, "items": [{"qid": 1845, "question": "What was the baseline? in Video Highlight Prediction Using Audience Chat Reactions", "answer": ["No"], "top_k_doc_id": [145, 146, 998, 1172, 1173, 2689, 2690, 4940, 5549, 379, 1174, 2083, 5666, 4267, 578], "orig_top_k_doc_id": [2690, 2689, 1173, 146, 998, 5549, 145, 1172, 5666, 4940, 379, 2083, 1174, 4267, 578]}, {"qid": 1848, "question": "What were their results? in Video Highlight Prediction Using Audience Chat Reactions", "answer": ["Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set"], "top_k_doc_id": [145, 146, 998, 1172, 1173, 2689, 2690, 4940, 5549, 379, 1174, 2083, 5666, 5102, 2684], "orig_top_k_doc_id": [2690, 2689, 1173, 998, 5549, 146, 1172, 4940, 1174, 145, 2083, 5666, 5102, 2684, 379]}, {"qid": 1847, "question": "How big was the dataset presented? in Video Highlight Prediction Using Audience Chat Reactions", "answer": ["321 videos"], "top_k_doc_id": [145, 146, 998, 1172, 1173, 2689, 2690, 4940, 5549, 379, 1174, 742, 5102, 7125, 5103], "orig_top_k_doc_id": [2690, 2689, 1173, 146, 742, 145, 998, 1172, 5102, 1174, 379, 4940, 7125, 5549, 5103]}, {"qid": 1846, "question": "What is the average length of the recordings? in Video Highlight Prediction Using Audience Chat Reactions", "answer": ["40 minutes"], "top_k_doc_id": [145, 146, 998, 1172, 1173, 2689, 2690, 4940, 5549, 5764, 39, 2083, 5666, 5766, 5015], "orig_top_k_doc_id": [2690, 2689, 998, 5549, 146, 1173, 4940, 5764, 145, 39, 1172, 2083, 5666, 5766, 5015]}]}
{"group_id": 507, "group_size": 4, "items": [{"qid": 1850, "question": "What is the source of the paraphrases of the questions? in Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables", "answer": ["WikiTableQuestions"], "top_k_doc_id": [345, 2378, 4464, 7072, 2691, 2733, 6842, 1155, 2377, 4463, 6847, 4469, 4468, 4467, 1637], "orig_top_k_doc_id": [2691, 4464, 7072, 2377, 4463, 2378, 345, 2733, 6847, 1155, 6842, 4469, 4468, 4467, 1637]}, {"qid": 1851, "question": "Does the dataset they use differ from the one used by Pasupat and Liang, 2015? in Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables", "answer": ["No"], "top_k_doc_id": [345, 2378, 4464, 7072, 2691, 2733, 6842, 1155, 2377, 4463, 7071, 2547, 7734, 7735, 1154], "orig_top_k_doc_id": [7072, 7071, 4464, 2547, 7734, 2733, 2377, 4463, 6842, 7735, 345, 2378, 1154, 2691, 1155]}, {"qid": 1849, "question": "Does a neural scoring function take both the question and the logical form as inputs? in Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables", "answer": ["Yes"], "top_k_doc_id": [345, 2378, 4464, 7072, 2691, 2733, 6842, 2693, 7734, 2737, 1154, 3916, 512, 2379, 1141], "orig_top_k_doc_id": [2691, 4464, 2378, 2693, 7734, 2733, 7072, 345, 2737, 1154, 3916, 512, 2379, 6842, 1141]}, {"qid": 2556, "question": "How is the semi-structured knowledge base created? in Question Answering via Integer Programming over Semi-Structured Knowledge", "answer": ["using a mixture of manual and semi-automatic techniques"], "top_k_doc_id": [345, 2378, 4464, 7072, 4463, 4469, 4496, 145, 146, 3096, 144, 1760, 2023, 4159, 4154], "orig_top_k_doc_id": [4463, 7072, 4464, 345, 4469, 4496, 2378, 145, 146, 3096, 144, 1760, 2023, 4159, 4154]}]}
{"group_id": 508, "group_size": 4, "items": [{"qid": 1853, "question": "Is the model compared against a linear regression baseline? in DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News", "answer": ["No"], "top_k_doc_id": [1848, 1851, 2696, 2697, 2698, 2699, 2700, 3731, 4805, 4806, 4807, 7263, 4798, 6249, 747], "orig_top_k_doc_id": [2696, 2700, 2698, 2697, 2699, 4807, 4806, 7263, 1848, 3731, 1851, 6249, 4805, 747, 4798]}, {"qid": 1854, "question": "What is the prediction accuracy of the model? in DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News", "answer": ["mean prediction accuracy 0.99582651\nS&P 500 Accuracy 0.99582651"], "top_k_doc_id": [1848, 1851, 2696, 2697, 2698, 2699, 2700, 3731, 4805, 4806, 4807, 7263, 1852, 3730, 1850], "orig_top_k_doc_id": [2696, 2700, 2699, 2698, 2697, 4807, 3731, 1851, 4806, 7263, 1848, 3730, 1852, 1850, 4805]}, {"qid": 1855, "question": "What is the dataset used in the paper? in DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News", "answer": ["historical S&P 500 component stocks\n 306242 news articles"], "top_k_doc_id": [1848, 1851, 2696, 2697, 2698, 2699, 2700, 3731, 4805, 4806, 4807, 7263, 1852, 3730, 4800], "orig_top_k_doc_id": [2696, 2698, 2697, 2699, 2700, 4807, 4806, 7263, 3731, 1851, 1848, 4805, 3730, 1852, 4800]}, {"qid": 1856, "question": "How does the differential privacy mechanism work? in DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News", "answer": ["A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$."], "top_k_doc_id": [1848, 1851, 2696, 2697, 2698, 2699, 2700, 3731, 4805, 4806, 4807, 7263, 4798, 1942, 6751], "orig_top_k_doc_id": [2696, 2700, 2698, 2697, 2699, 4807, 4806, 7263, 3731, 1848, 1942, 4805, 6751, 1851, 4798]}]}
{"group_id": 509, "group_size": 4, "items": [{"qid": 1906, "question": "What is the agreement score of their annotated dataset? in Monitoring stance towards vaccination in twitter messages", "answer": [" Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$)."], "top_k_doc_id": [2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 7625, 4113, 7499, 7752, 6742, 7751, 5546], "orig_top_k_doc_id": [2828, 2834, 2827, 2829, 2833, 2831, 2832, 2830, 7752, 7499, 6742, 7751, 4113, 7625, 5546]}, {"qid": 1907, "question": "What is the size of the labelled dataset? in Monitoring stance towards vaccination in twitter messages", "answer": ["27,534 messages "], "top_k_doc_id": [2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 7625, 447, 4113, 4119, 4114, 243, 4118], "orig_top_k_doc_id": [2828, 2827, 2834, 2833, 2831, 2832, 2829, 4113, 2830, 4119, 4114, 7625, 243, 447, 4118]}, {"qid": 1908, "question": "Which features do they use to model Twitter messages? in Monitoring stance towards vaccination in twitter messages", "answer": ["word unigrams, bigrams, and trigrams"], "top_k_doc_id": [2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 7625, 447, 5085, 3542, 6456, 2796, 7307], "orig_top_k_doc_id": [2828, 2827, 2834, 2833, 2831, 2832, 2829, 7625, 447, 5085, 2830, 3542, 6456, 2796, 7307]}, {"qid": 1909, "question": "Do they allow for messages with vaccination-related key terms to be of neutral stance? in Monitoring stance towards vaccination in twitter messages", "answer": ["Yes"], "top_k_doc_id": [2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 7625, 4113, 7499, 2835, 3542, 448, 449], "orig_top_k_doc_id": [2828, 2827, 2834, 2833, 2829, 2831, 2832, 2830, 7625, 4113, 7499, 2835, 3542, 448, 449]}]}
{"group_id": 510, "group_size": 4, "items": [{"qid": 1928, "question": "Which datasets are used? in Language Independent Sequence Labelling for Opinion Target Extraction", "answer": ["ABSA SemEval 2014-2016 datasets\nYelp Academic Dataset\nWikipedia dumps"], "top_k_doc_id": [597, 2873, 2874, 2875, 2878, 2879, 2959, 5512, 6396, 6052, 1170, 2119, 2215, 893, 6183], "orig_top_k_doc_id": [2873, 2874, 2878, 2875, 2879, 597, 6052, 1170, 2959, 5512, 2119, 6396, 2215, 893, 6183]}, {"qid": 1929, "question": "Which six languages are experimented with? in Language Independent Sequence Labelling for Opinion Target Extraction", "answer": ["Dutch, French, Russian, Spanish , Turkish, English "], "top_k_doc_id": [597, 2873, 2874, 2875, 2878, 2879, 2959, 5512, 6396, 2154, 78, 2328, 4511, 1084, 2876], "orig_top_k_doc_id": [2873, 2874, 2878, 2154, 2879, 2959, 2875, 597, 6396, 5512, 78, 2328, 4511, 1084, 2876]}, {"qid": 1927, "question": "What was the baseline? in Language Independent Sequence Labelling for Opinion Target Extraction", "answer": ["the baseline provided by BIBREF8, the baselines provided by the ABSA organizers"], "top_k_doc_id": [597, 2873, 2874, 2875, 2878, 2879, 2959, 6183, 1170, 896, 5422, 897, 5180, 7472, 2448], "orig_top_k_doc_id": [2873, 2874, 2878, 2875, 6183, 1170, 597, 2879, 896, 2959, 5422, 897, 5180, 7472, 2448]}, {"qid": 1930, "question": "What shallow local features are extracted? in Language Independent Sequence Labelling for Opinion Target Extraction", "answer": [" Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context"], "top_k_doc_id": [597, 2873, 2874, 2875, 2878, 2879, 2876, 5406, 2319, 2328, 2983, 897, 6566, 893, 7313], "orig_top_k_doc_id": [2873, 2878, 2874, 2876, 2879, 5406, 2319, 2328, 2875, 597, 2983, 897, 6566, 893, 7313]}]}
{"group_id": 511, "group_size": 4, "items": [{"qid": 1943, "question": "Which evaluation metrics do they use for language modelling? in Exploring Multilingual Syntactic Sentence Representations", "answer": [" functional dissimilarity score, nearest neighbours experiment"], "top_k_doc_id": [247, 2906, 453, 1872, 2875, 2908, 3759, 3817, 5846, 6190, 3183, 404, 4603, 4604, 3184], "orig_top_k_doc_id": [2906, 3183, 2875, 247, 5846, 453, 404, 4603, 3759, 4604, 1872, 3817, 3184, 2908, 6190]}, {"qid": 1946, "question": "Which corpus do they use? in Exploring Multilingual Syntactic Sentence Representations", "answer": ["The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16."], "top_k_doc_id": [247, 2906, 453, 1872, 2875, 2908, 3759, 3817, 5846, 6190, 4447, 6176, 1593, 4591, 6332], "orig_top_k_doc_id": [5846, 2906, 247, 1872, 3817, 4447, 2908, 3759, 6176, 2875, 6190, 1593, 453, 4591, 6332]}, {"qid": 1945, "question": "Do they evaluate on downstream tasks? in Exploring Multilingual Syntactic Sentence Representations", "answer": ["Yes"], "top_k_doc_id": [247, 2906, 453, 1872, 436, 437, 786, 783, 6167, 6060, 6035, 5712, 5184, 3618, 6031], "orig_top_k_doc_id": [2906, 247, 1872, 436, 437, 453, 786, 783, 6167, 6060, 6035, 5712, 5184, 3618, 6031]}, {"qid": 1944, "question": "Do they do quantitative quality analysis of learned embeddings? in Exploring Multilingual Syntactic Sentence Representations", "answer": ["Yes"], "top_k_doc_id": [247, 2906, 2908, 2909, 5314, 5699, 2058, 1255, 2494, 2907, 6853, 6035, 4447, 5183, 4510], "orig_top_k_doc_id": [2906, 2908, 2909, 247, 5314, 5699, 2058, 1255, 2494, 2907, 6853, 6035, 4447, 5183, 4510]}]}
{"group_id": 512, "group_size": 4, "items": [{"qid": 1981, "question": "Do they use skipgram version of word2vec? in Network-Efficient Distributed Word2vec Training System for Large Vocabularies", "answer": ["Yes"], "top_k_doc_id": [2986, 2987, 2988, 2989, 2993, 2994, 4931, 904, 2324, 1447, 1354, 1353, 3927, 48, 1313], "orig_top_k_doc_id": [2988, 2987, 4931, 2986, 1447, 1354, 2989, 1353, 2994, 3927, 2993, 2324, 48, 904, 1313]}, {"qid": 1982, "question": "What domains are considered that have such large vocabularies? in Network-Efficient Distributed Word2vec Training System for Large Vocabularies", "answer": ["relational entities, general text-based attributes, descriptive text of images, nodes in graph structure of networks, queries"], "top_k_doc_id": [2986, 2987, 2988, 2989, 2993, 2994, 4931, 231, 4624, 7736, 7269, 6221, 4387, 1166, 6219], "orig_top_k_doc_id": [2986, 2988, 2993, 2987, 2994, 2989, 231, 4624, 7736, 7269, 6221, 4387, 4931, 1166, 6219]}, {"qid": 1983, "question": "Do they perform any morphological tokenization? in Network-Efficient Distributed Word2vec Training System for Large Vocabularies", "answer": ["No"], "top_k_doc_id": [2986, 2987, 2988, 2989, 2993, 2994, 4931, 231, 1775, 7680, 7687, 1774, 395, 2324, 4850], "orig_top_k_doc_id": [1775, 2988, 7680, 2989, 2986, 2987, 7687, 231, 2994, 1774, 395, 2993, 2324, 4850, 4931]}, {"qid": 1984, "question": "How many nodes does the cluster have? in Network-Efficient Distributed Word2vec Training System for Large Vocabularies", "answer": ["No"], "top_k_doc_id": [2986, 2987, 2988, 2989, 2993, 2994, 4931, 904, 2324, 418, 2964, 241, 2876, 5298, 5823], "orig_top_k_doc_id": [2988, 2986, 2989, 2987, 904, 4931, 2324, 2994, 418, 2993, 2964, 241, 2876, 5298, 5823]}]}
{"group_id": 513, "group_size": 4, "items": [{"qid": 1997, "question": "How is some information lost in the RNN-based generation models? in Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems", "answer": ["the generated sentences often did not include all desired attributes."], "top_k_doc_id": [7584, 3190, 3193, 3507, 5744, 7377, 3012, 5850, 1768, 705, 3014, 2065, 7371, 1771, 2253], "orig_top_k_doc_id": [3012, 5850, 1768, 7584, 705, 3014, 3507, 2065, 7371, 5744, 3190, 1771, 7377, 3193, 2253]}, {"qid": 2098, "question": "what semantically conditioned models did they compare with? in Retrieval-based Goal-Oriented Dialogue Generation", "answer": ["Hierarchical Disentangled Self-Attention"], "top_k_doc_id": [7584, 3190, 3193, 3507, 5744, 7377, 3192, 3194, 3191, 6671, 1471, 2970, 569, 7372, 5431], "orig_top_k_doc_id": [3193, 3190, 3192, 3194, 3191, 6671, 7584, 3507, 1471, 2970, 569, 7372, 7377, 5744, 5431]}, {"qid": 3543, "question": "What is the difference of the proposed model with a standard RNN encoder-decoder? in Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks", "answer": ["Introduce a \"Refinement Adjustment LSTM-based component\" to the decoder"], "top_k_doc_id": [7584, 705, 1674, 1675, 3012, 5850, 6585, 3190, 1683, 1768, 3191, 3358, 6370, 6551, 6371], "orig_top_k_doc_id": [5850, 705, 1675, 3190, 1674, 1683, 1768, 3012, 6585, 3191, 3358, 6370, 7584, 6551, 6371]}, {"qid": 3544, "question": "Does the model evaluated on NLG datasets or dialog datasets? in Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks", "answer": ["NLG datasets", "NLG datasets"], "top_k_doc_id": [7584, 705, 1674, 1675, 3012, 5850, 6585, 1471, 898, 2197, 2065, 1998, 6671, 6320, 6723], "orig_top_k_doc_id": [5850, 705, 1674, 7584, 3012, 1471, 898, 2197, 2065, 1675, 6585, 1998, 6671, 6320, 6723]}]}
{"group_id": 514, "group_size": 4, "items": [{"qid": 2025, "question": "What are the characteristics of the city dialect? in Dialectometric analysis of language variation in Twitter", "answer": ["Lexicon of the cities tend to use most forms of a particular concept"], "top_k_doc_id": [3065, 1286, 1287, 3064, 3066, 6833, 2797, 2798, 4002, 4006, 4007, 6896, 4405, 6897, 7530], "orig_top_k_doc_id": [3064, 3066, 3065, 4002, 1287, 4007, 6833, 6896, 4405, 6897, 7530, 2798, 2797, 4006, 1286]}, {"qid": 2026, "question": "What are the characteristics of the rural dialect? in Dialectometric analysis of language variation in Twitter", "answer": ["It uses particular forms of a concept rather than all of them uniformly"], "top_k_doc_id": [3065, 1286, 1287, 3064, 3066, 6833, 2797, 2798, 4002, 4006, 4007, 6896, 3126, 7307, 2788], "orig_top_k_doc_id": [3064, 3066, 3065, 4002, 1287, 4007, 3126, 6896, 2798, 2797, 4006, 1286, 6833, 7307, 2788]}, {"qid": 2024, "question": "Do the authors mention any possible confounds in their study? in Dialectometric analysis of language variation in Twitter", "answer": ["Yes"], "top_k_doc_id": [3065, 1286, 1287, 3064, 3066, 6833, 6897, 5466, 7029, 7307, 243, 4003, 6902, 1378, 4139], "orig_top_k_doc_id": [3064, 3066, 3065, 6897, 5466, 6833, 7029, 1287, 7307, 1286, 243, 4003, 6902, 1378, 4139]}, {"qid": 2439, "question": "What do the models that they compare predict? in Modeling Global Syntactic Variation in English Using Dialect Classification", "answer": ["national dialects of English"], "top_k_doc_id": [3065, 4002, 4007, 4005, 4006, 4004, 4003, 6828, 6896, 2797, 2798, 2785, 1111, 5909, 4014], "orig_top_k_doc_id": [4002, 4007, 4005, 4006, 4004, 4003, 6828, 6896, 2797, 2798, 2785, 1111, 3065, 5909, 4014]}]}
{"group_id": 515, "group_size": 4, "items": [{"qid": 2045, "question": "Does they focus on any specific product/service domain? in Opinion Recommendation using Neural Memory Model", "answer": ["local businesses (i.e. restaurants)"], "top_k_doc_id": [3101, 3102, 3105, 597, 3106, 893, 2215, 1948, 4124, 1949, 2873, 2216, 3306, 2874, 5105], "orig_top_k_doc_id": [3101, 3105, 597, 3106, 3102, 893, 2215, 1948, 4124, 1949, 2873, 2216, 3306, 2874, 5105]}, {"qid": 2046, "question": "What are the baselines? in Opinion Recommendation using Neural Memory Model", "answer": ["RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att"], "top_k_doc_id": [3101, 3102, 3105, 597, 3106, 3020, 4317, 3019, 4510, 3015, 4511, 6549, 598, 6879, 5831], "orig_top_k_doc_id": [3106, 3102, 3105, 3101, 3020, 4317, 3019, 4510, 3015, 597, 4511, 6549, 598, 6879, 5831]}, {"qid": 4211, "question": "Is an ablation test performed? in Dual Memory Network Model for Biased Product Review Classification", "answer": ["No", "No"], "top_k_doc_id": [3101, 3102, 3105, 2215, 3103, 3104, 6568, 6692, 6693, 6696, 7333, 7335, 7336, 418, 2726], "orig_top_k_doc_id": [6692, 6696, 6693, 3102, 7335, 3103, 3105, 7333, 7336, 3104, 2215, 6568, 418, 3101, 2726]}, {"qid": 4212, "question": "What statistical test is performed? in Dual Memory Network Model for Biased Product Review Classification", "answer": ["t-test", "t-test", "No"], "top_k_doc_id": [3101, 3102, 3105, 2215, 3103, 3104, 6568, 6692, 6693, 6696, 7333, 7335, 7336, 790, 6401], "orig_top_k_doc_id": [6692, 6696, 6693, 3102, 7333, 7335, 3101, 3103, 3105, 7336, 3104, 2215, 790, 6401, 6568]}]}
{"group_id": 516, "group_size": 4, "items": [{"qid": 2057, "question": "What language technologies have been introduced in the past? in Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities", "answer": ["- Font & Keyboard\n- Speech-to-Text\n- Text-to-Speech\n- Text Prediction\n- Spell Checker\n- Grammar Checker\n- Text Search\n- Machine Translation\n- Voice to Text Search\n- Voice to Speech Search"], "top_k_doc_id": [3582, 3125, 3581, 3124, 3126, 3123, 3128, 4864, 3596, 6005, 5956, 4534, 7138, 5764, 3597], "orig_top_k_doc_id": [3124, 3126, 3123, 3125, 3128, 4864, 3596, 3582, 3581, 6005, 5956, 4534, 7138, 5764, 3597]}, {"qid": 2783, "question": "what challenges are identified? in Towards speech-to-text translation without speech recognition", "answer": ["Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage", "low coverage of audio, difficulty in cross-speaker clustering"], "top_k_doc_id": [3582, 3125, 3581, 3007, 7686, 1061, 7687, 6131, 226, 4875, 5667, 5983, 4646, 5915, 6375], "orig_top_k_doc_id": [3007, 3582, 7686, 1061, 7687, 6131, 3581, 226, 3125, 4875, 5667, 5983, 4646, 5915, 6375]}, {"qid": 4546, "question": "Can their method be transferred to other Q&A platforms (in other languages)? in Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform", "answer": ["No", "No"], "top_k_doc_id": [3582, 1120, 2103, 2189, 3591, 3807, 7104, 7106, 4300, 3125, 3124, 3553, 6782, 3586, 3547], "orig_top_k_doc_id": [7104, 7106, 1120, 2189, 3591, 3582, 4300, 3125, 3124, 3553, 6782, 3586, 3807, 3547, 2103]}, {"qid": 4547, "question": "What measures of quality do they use for a Q&A platform? in Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform", "answer": ["Rating scores given by users", "MAE and RMSE "], "top_k_doc_id": [3582, 1120, 2103, 2189, 3591, 3807, 7104, 7106, 6110, 4424, 3581, 3808, 5461, 5102, 7105], "orig_top_k_doc_id": [7104, 7106, 1120, 3807, 3582, 6110, 2103, 4424, 3581, 3591, 3808, 2189, 5461, 5102, 7105]}]}
{"group_id": 517, "group_size": 4, "items": [{"qid": 2065, "question": "How do they quantify alignment between the embeddings of a document and its translation? in Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER", "answer": ["median cosine similarity"], "top_k_doc_id": [247, 248, 249, 3140, 3141, 4590, 4591, 2162, 5621, 5622, 6060, 1040, 4569, 6035, 6031], "orig_top_k_doc_id": [247, 3140, 3141, 249, 4590, 6060, 5621, 248, 1040, 5622, 2162, 4569, 4591, 6035, 6031]}, {"qid": 2067, "question": "Do any of the evaluations show that adversarial learning improves performance in at least two different language families? in Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER", "answer": ["Yes"], "top_k_doc_id": [247, 248, 249, 3140, 3141, 4590, 4591, 2162, 5621, 5622, 633, 397, 4583, 3618, 6034], "orig_top_k_doc_id": [3141, 3140, 249, 247, 248, 4590, 4591, 633, 5622, 397, 5621, 4583, 3618, 2162, 6034]}, {"qid": 2066, "question": "Does adversarial learning have stronger performance gains for text classification, or for NER? in Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER", "answer": ["classification"], "top_k_doc_id": [247, 248, 249, 3140, 3141, 4590, 4591, 4585, 4584, 4583, 6033, 5868, 3566, 5084, 397], "orig_top_k_doc_id": [3140, 3141, 4590, 249, 247, 4591, 4585, 4584, 4583, 6033, 5868, 3566, 5084, 397, 248]}, {"qid": 194, "question": "How they show that mBERT representations can be split into a language-specific component and a language-neutral component? in How Language-Neutral is Multilingual BERT?", "answer": ["We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space."], "top_k_doc_id": [247, 248, 249, 6032, 1777, 3617, 533, 1043, 1778, 3621, 5621, 3620, 534, 6033, 438], "orig_top_k_doc_id": [247, 248, 249, 6032, 1777, 3617, 533, 1043, 1778, 3621, 5621, 3620, 534, 6033, 438]}]}
{"group_id": 518, "group_size": 4, "items": [{"qid": 2103, "question": "What is the optimal trading strategy based on reinforcement learning? in Trading the Twitter Sentiment with Reinforcement Learning", "answer": ["No"], "top_k_doc_id": [3203, 3204, 3206, 4392, 4393, 4798, 4989, 3205, 4394, 4395, 2700, 2698, 1282, 1852, 5177], "orig_top_k_doc_id": [3203, 3206, 3205, 4393, 4392, 3204, 2700, 4798, 4395, 2698, 4394, 1282, 1852, 4989, 5177]}, {"qid": 2104, "question": "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price? in Trading the Twitter Sentiment with Reinforcement Learning", "answer": ["Yes"], "top_k_doc_id": [3203, 3204, 3206, 4392, 4393, 4798, 4989, 2700, 3730, 1852, 1849, 1848, 2081, 1851, 2698], "orig_top_k_doc_id": [3203, 4393, 1852, 3204, 2700, 1849, 1848, 4989, 4798, 3730, 2081, 3206, 1851, 2698, 4392]}, {"qid": 2105, "question": "Which tweets are used to output the daily sentiment signal? in Trading the Twitter Sentiment with Reinforcement Learning", "answer": ["Tesla and Ford are investigated on how Twitter sentiment could impact the stock price"], "top_k_doc_id": [3203, 3204, 3206, 4392, 4393, 4798, 4989, 3205, 4394, 4395, 5179, 1841, 7528, 7307, 5314], "orig_top_k_doc_id": [3203, 4392, 4393, 3204, 3206, 4989, 4395, 3205, 4798, 5179, 4394, 1841, 7528, 7307, 5314]}, {"qid": 2106, "question": "What is the baseline machine learning prediction approach? in Trading the Twitter Sentiment with Reinforcement Learning", "answer": ["linear logistic regression to a set of stock technical signals"], "top_k_doc_id": [3203, 3204, 3206, 4392, 4393, 4798, 4989, 2700, 3730, 3205, 6249, 502, 6250, 100, 3154], "orig_top_k_doc_id": [3206, 3203, 3205, 2700, 3204, 3730, 4392, 4393, 6249, 4798, 4989, 502, 6250, 100, 3154]}]}
{"group_id": 519, "group_size": 4, "items": [{"qid": 2144, "question": "What other datasets are used? in Localization of Fake News Detection via Multitask Transfer Learning", "answer": ["WikiText-TL-39"], "top_k_doc_id": [3273, 3275, 3276, 3277, 3860, 3861, 6056, 6058, 6665, 6666, 3928, 7120, 6663, 6667, 6664], "orig_top_k_doc_id": [3277, 6056, 3860, 6666, 3273, 6058, 3861, 6665, 6667, 3275, 3276, 7120, 6663, 3928, 6664]}, {"qid": 2146, "question": "What is the source of the dataset? in Localization of Fake News Detection via Multitask Transfer Learning", "answer": ["Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera"], "top_k_doc_id": [3273, 3275, 3276, 3277, 3860, 3861, 6056, 6058, 6665, 6666, 3928, 7120, 6663, 1864, 2157], "orig_top_k_doc_id": [3277, 6056, 3860, 3861, 6666, 6058, 3273, 3275, 3928, 7120, 1864, 2157, 6663, 6665, 3276]}, {"qid": 2145, "question": "What is the size of the dataset? in Localization of Fake News Detection via Multitask Transfer Learning", "answer": ["3,206"], "top_k_doc_id": [3273, 3275, 3276, 3277, 3860, 3861, 6056, 6058, 6665, 6666, 3928, 7120, 3274, 3927, 6667], "orig_top_k_doc_id": [3277, 6056, 3275, 6058, 3860, 3861, 6666, 3273, 3274, 6665, 7120, 3276, 3928, 3927, 6667]}, {"qid": 2147, "question": "What were the baselines? in Localization of Fake News Detection via Multitask Transfer Learning", "answer": ["Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations"], "top_k_doc_id": [3273, 3275, 3276, 3277, 3860, 3861, 6056, 6058, 6665, 6666, 1867, 6667, 3274, 3015, 5784], "orig_top_k_doc_id": [3277, 6056, 6058, 6666, 3273, 3860, 1867, 3276, 3275, 3861, 6667, 3274, 3015, 6665, 5784]}]}
{"group_id": 520, "group_size": 4, "items": [{"qid": 2149, "question": "How do they evaluate how their model acquired words? in Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "answer": ["PAR score"], "top_k_doc_id": [3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 5067, 5068, 312, 2948, 6765, 3911], "orig_top_k_doc_id": [3279, 3280, 3278, 3285, 3286, 3282, 3284, 3281, 3283, 5068, 5067, 6765, 312, 3911, 2948]}, {"qid": 2150, "question": "Which method do they use for word segmentation? in Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "answer": ["unsupervised word segmentation method latticelm"], "top_k_doc_id": [3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 5067, 5068, 312, 2948, 6960, 637], "orig_top_k_doc_id": [3279, 3280, 3278, 3285, 3282, 3286, 3284, 3281, 3283, 2948, 6960, 5068, 5067, 312, 637]}, {"qid": 2151, "question": "Does their model start with any prior knowledge of words? in Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "answer": ["No"], "top_k_doc_id": [3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 5067, 5068, 638, 637, 3651, 2317], "orig_top_k_doc_id": [3279, 3278, 3280, 3282, 3285, 3284, 3286, 3281, 3283, 638, 5068, 5067, 637, 3651, 2317]}, {"qid": 2148, "question": "How do they show that acquiring names of places helps self-localization? in Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "answer": ["unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation, Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition"], "top_k_doc_id": [3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 5067, 312, 317, 316, 318, 314], "orig_top_k_doc_id": [3279, 3280, 3278, 3285, 3282, 3284, 3286, 3281, 3283, 312, 5067, 317, 316, 318, 314]}]}
{"group_id": 521, "group_size": 4, "items": [{"qid": 2233, "question": "What dataset did they use? in Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention", "answer": ["weibo-100k, Ontonotes, LCQMC and XNLI"], "top_k_doc_id": [3504, 3505, 3506, 5286, 5789, 5941, 2434, 21, 6386, 2135, 2906, 974, 2917, 5283, 7543], "orig_top_k_doc_id": [3505, 3504, 5789, 6386, 5941, 3506, 21, 2434, 5286, 2135, 2906, 974, 2917, 5283, 7543]}, {"qid": 2234, "question": "What benchmarks did they experiment on? in Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention", "answer": ["Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM), Natural Language Inference (NLI)"], "top_k_doc_id": [3504, 3505, 3506, 5286, 5789, 5941, 2434, 21, 6386, 1640, 2429, 2643, 1117, 1115, 4791], "orig_top_k_doc_id": [3505, 3506, 2434, 3504, 1640, 2429, 5789, 2643, 1117, 6386, 5941, 21, 1115, 5286, 4791]}, {"qid": 2232, "question": "How does the fusion method work? in Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention", "answer": ["ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word, we devise an appropriate aggregation module to fuse the inner-word character attention"], "top_k_doc_id": [3504, 3505, 3506, 5286, 5789, 5941, 2434, 1238, 5790, 1512, 2216, 6555, 7541, 5283, 3180], "orig_top_k_doc_id": [3505, 3504, 3506, 1238, 5790, 1512, 5286, 2216, 2434, 6555, 7541, 5789, 5283, 5941, 3180]}, {"qid": 2231, "question": "What pre-trained models did they compare to? in Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention", "answer": ["BERT, ERNIE, and BERT-wwm"], "top_k_doc_id": [3504, 3505, 3506, 5286, 5789, 5941, 21, 2917, 5622, 2241, 1389, 48, 5285, 5938, 5970], "orig_top_k_doc_id": [3505, 3504, 3506, 5941, 5286, 21, 2917, 5622, 2241, 1389, 5789, 48, 5285, 5938, 5970]}]}
{"group_id": 522, "group_size": 4, "items": [{"qid": 2246, "question": "Do the authors give examples of positive and negative sentiment with regard to the virus? in Word frequency and sentiment analysis of twitter messages during Coronavirus pandemic", "answer": ["No"], "top_k_doc_id": [331, 2402, 2828, 3527, 3528, 3529, 65, 7308, 4501, 448, 4503, 7755, 2832, 4499, 4502], "orig_top_k_doc_id": [3527, 3529, 3528, 2402, 2828, 4501, 65, 448, 4503, 7755, 7308, 331, 2832, 4499, 4502]}, {"qid": 2249, "question": "Do they collect only English data? in Word frequency and sentiment analysis of twitter messages during Coronavirus pandemic", "answer": ["No"], "top_k_doc_id": [331, 2402, 2828, 3527, 3528, 3529, 65, 7308, 7307, 1052, 6873, 4131, 606, 4798, 4690], "orig_top_k_doc_id": [3527, 3529, 3528, 331, 2828, 2402, 65, 7307, 1052, 6873, 7308, 4131, 606, 4798, 4690]}, {"qid": 2248, "question": "Do they specify which countries they collected twitter data from? in Word frequency and sentiment analysis of twitter messages during Coronavirus pandemic", "answer": ["No"], "top_k_doc_id": [331, 2402, 2828, 3527, 3528, 3529, 6873, 2827, 3795, 2533, 6455, 1420, 6833, 4392, 1419], "orig_top_k_doc_id": [3527, 3529, 3528, 2402, 6873, 2828, 331, 2827, 3795, 2533, 6455, 1420, 6833, 4392, 1419]}, {"qid": 2247, "question": "Which word frequencies reflect on the psychology of the twitter users, according to the authors? in Word frequency and sentiment analysis of twitter messages during Coronavirus pandemic", "answer": ["unigram, bigram and trigram"], "top_k_doc_id": [331, 2402, 2828, 3527, 3528, 3529, 6630, 6207, 7307, 3203, 7131, 5191, 6631, 7530, 6629], "orig_top_k_doc_id": [3527, 3529, 3528, 2402, 6630, 331, 6207, 7307, 3203, 7131, 5191, 6631, 2828, 7530, 6629]}]}
{"group_id": 523, "group_size": 4, "items": [{"qid": 2291, "question": "What is their baseline model? in A Multi-cascaded Deep Model for Bilingual SMS Classification", "answer": ["the model proposed in BIBREF3"], "top_k_doc_id": [80, 1040, 3637, 3639, 3640, 1990, 4922, 4923, 285, 1044, 1067, 1112, 282, 1887, 6943], "orig_top_k_doc_id": [3637, 3640, 4922, 3639, 4923, 1990, 1067, 1044, 1112, 282, 80, 1887, 1040, 6943, 285]}, {"qid": 2292, "question": "What is the size of the dataset? in A Multi-cascaded Deep Model for Bilingual SMS Classification", "answer": ["$0.3$ million records"], "top_k_doc_id": [80, 1040, 3637, 3639, 3640, 1990, 4922, 4923, 285, 1044, 1067, 4918, 448, 7658, 6842], "orig_top_k_doc_id": [3637, 3640, 4922, 3639, 4923, 1067, 4918, 285, 80, 1990, 448, 7658, 1040, 1044, 6842]}, {"qid": 2290, "question": "What accuracy score do they obtain? in A Multi-cascaded Deep Model for Bilingual SMS Classification", "answer": ["the best performing model obtained an accuracy of 0.86"], "top_k_doc_id": [80, 1040, 3637, 3639, 3640, 1990, 4922, 4923, 1112, 6842, 247, 5624, 3655, 282, 309], "orig_top_k_doc_id": [3637, 3640, 4922, 3639, 1112, 4923, 6842, 247, 1990, 5624, 3655, 282, 1040, 80, 309]}, {"qid": 2293, "question": "What is the 12 class bilingual text? in A Multi-cascaded Deep Model for Bilingual SMS Classification", "answer": ["Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant"], "top_k_doc_id": [80, 1040, 3637, 3639, 3640, 1045, 5715, 1067, 1048, 1053, 6032, 1044, 5624, 1457, 1886], "orig_top_k_doc_id": [3637, 3640, 3639, 80, 1045, 1040, 5715, 1067, 1048, 1053, 6032, 1044, 5624, 1457, 1886]}]}
{"group_id": 524, "group_size": 4, "items": [{"qid": 2300, "question": "Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation? in Transformer-based Cascaded Multimodal Speech Translation", "answer": ["BLEU scores"], "top_k_doc_id": [4918, 4922, 4923, 285, 3655, 3657, 3658, 3736, 6110, 7144, 7661, 3737, 115, 7143, 4744], "orig_top_k_doc_id": [3655, 3658, 7661, 4923, 4922, 7144, 4918, 6110, 3657, 3737, 115, 3736, 285, 7143, 4744]}, {"qid": 2301, "question": "What dataset was used in this work? in Transformer-based Cascaded Multimodal Speech Translation", "answer": ["How2"], "top_k_doc_id": [4918, 4922, 4923, 285, 3655, 3657, 3658, 3736, 6110, 7144, 7661, 1112, 2922, 5618, 1113], "orig_top_k_doc_id": [3655, 3658, 4923, 7661, 4922, 3657, 4918, 1112, 7144, 2922, 6110, 5618, 285, 3736, 1113]}, {"qid": 2809, "question": "What are the baselines? in Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation", "answer": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "top_k_doc_id": [4918, 4922, 4923, 1742, 2887, 4919, 6313, 7463, 6063, 1639, 6039, 6032, 6310, 4855, 7462], "orig_top_k_doc_id": [4922, 4923, 4918, 1742, 4919, 7463, 6063, 2887, 1639, 6039, 6032, 6310, 4855, 7462, 6313]}, {"qid": 2810, "question": "What is the attention module pretrained on? in Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation", "answer": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "top_k_doc_id": [4918, 4922, 4923, 1742, 2887, 4919, 6313, 6270, 4369, 3742, 6351, 5276, 7655, 2886, 6269], "orig_top_k_doc_id": [4918, 4922, 4923, 4919, 1742, 6270, 4369, 3742, 6313, 2887, 6351, 5276, 7655, 2886, 6269]}]}
{"group_id": 525, "group_size": 4, "items": [{"qid": 2320, "question": "Which other unsupervised models are used for comparison? in Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "answer": ["Sequential (Denoising) Autoencoder, TF-IDF BOW, SkipThought, FastSent, Siamese C-BOW, C-BOW, C-PHRASE, ParagraphVector"], "top_k_doc_id": [3703, 3704, 7686, 48, 2619, 3701, 2371, 5286, 6532, 3699, 3700, 7769, 690, 7687, 1880], "orig_top_k_doc_id": [3704, 3703, 3701, 5286, 7686, 2371, 6532, 48, 3699, 3700, 7769, 2619, 690, 7687, 1880]}, {"qid": 2321, "question": "What metric is used to measure performance? in Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "answer": ["Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks"], "top_k_doc_id": [3703, 3704, 7686, 48, 2619, 3701, 2371, 5286, 135, 3719, 3200, 1658, 3807, 5660, 1077], "orig_top_k_doc_id": [3704, 2371, 3701, 3703, 135, 48, 3719, 7686, 3200, 1658, 2619, 3807, 5660, 5286, 1077]}, {"qid": 2319, "question": "Do they report results only on English data? in Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "answer": ["Yes"], "top_k_doc_id": [3703, 3704, 7686, 48, 2619, 3701, 3700, 6072, 7687, 6148, 1077, 3633, 5661, 4049, 2324], "orig_top_k_doc_id": [3703, 3704, 3700, 6072, 3701, 2619, 7687, 48, 6148, 1077, 7686, 3633, 5661, 4049, 2324]}, {"qid": 2322, "question": "How do the n-gram features incorporate compositionality? in Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "answer": ["by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words"], "top_k_doc_id": [3703, 3704, 7686, 1077, 5516, 5286, 5302, 7687, 3957, 5515, 3956, 3700, 7688, 2615, 2054], "orig_top_k_doc_id": [3704, 1077, 5516, 5286, 7686, 5302, 7687, 3957, 3703, 5515, 3956, 3700, 7688, 2615, 2054]}]}
{"group_id": 526, "group_size": 4, "items": [{"qid": 2354, "question": "Which of the two ensembles yields the best performance? in BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA", "answer": ["Answer with content missing: (Table 2) CONCAT ensemble"], "top_k_doc_id": [3776, 3777, 3778, 4900, 2369, 2370, 3855, 7518, 3485, 949, 692, 5473, 693, 7830, 5019], "orig_top_k_doc_id": [3776, 3777, 3778, 7518, 4900, 2369, 949, 692, 5473, 693, 3485, 3855, 2370, 7830, 5019]}, {"qid": 2356, "question": "How is it determined that a fact is easy-to-guess? in BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA", "answer": [" filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch), person name filter uses cloze-style questions to elicit name associations inherent in BERT, and deletes KB triples that correlate with them"], "top_k_doc_id": [3776, 3777, 3778, 4900, 2369, 2370, 3855, 7518, 3485, 4277, 2663, 4994, 7591, 4637, 5047], "orig_top_k_doc_id": [3776, 3778, 3777, 4900, 7518, 4277, 2663, 2369, 4994, 7591, 3855, 3485, 2370, 4637, 5047]}, {"qid": 2355, "question": "What are the two ways of ensembling BERT and E-BERT? in BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA", "answer": ["mean-pooling their outputs (AVG), concatenating the entity and its name with a slash symbol (CONCAT)"], "top_k_doc_id": [3776, 3777, 3778, 4900, 2369, 2370, 3855, 7518, 946, 945, 4277, 7830, 105, 3856, 1145], "orig_top_k_doc_id": [3776, 3778, 3777, 7518, 4900, 946, 945, 4277, 3855, 7830, 105, 3856, 2370, 2369, 1145]}, {"qid": 2793, "question": "How did they extend LAMA evaluation framework to focus on negation? in Negated LAMA: Birds cannot fly", "answer": ["To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statement", "Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions."], "top_k_doc_id": [3776, 3777, 3778, 4900, 4901, 87, 1319, 6752, 2087, 450, 2695, 731, 5900, 6026, 1122], "orig_top_k_doc_id": [4900, 4901, 3776, 3777, 3778, 87, 1319, 6752, 2087, 450, 2695, 731, 5900, 6026, 1122]}]}
{"group_id": 527, "group_size": 4, "items": [{"qid": 2402, "question": "Which conventional alignment models do they use as guidance? in Neural Machine Translation with Supervised Attention", "answer": ["GIZA++ BIBREF3 or fast_align BIBREF4 "], "top_k_doc_id": [4766, 2491, 3917, 3920, 4767, 6595, 3918, 3919, 896, 4212, 4396, 799, 7669, 3687, 1887], "orig_top_k_doc_id": [3920, 3917, 3918, 4766, 3919, 896, 4212, 4396, 799, 6595, 4767, 7669, 3687, 1887, 2491]}, {"qid": 2727, "question": "What useful information does attention capture? in What does Attention in Neural Machine Translation Pay Attention to?", "answer": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "top_k_doc_id": [4766, 2040, 2636, 2705, 3198, 4769, 6256, 365, 6943, 4178, 1013, 2135, 7579, 2837, 7694], "orig_top_k_doc_id": [4769, 4766, 2040, 365, 2705, 6943, 4178, 6256, 3198, 1013, 2135, 7579, 2837, 7694, 2636]}, {"qid": 2728, "question": "What datasets are used? in What does Attention in Neural Machine Translation Pay Attention to?", "answer": ["WMT15 German-to-English, RWTH German-English dataset", "RWTH German-English dataset"], "top_k_doc_id": [4766, 2040, 2636, 2705, 3198, 4769, 6256, 3416, 2044, 5835, 34, 5345, 2839, 7246, 4906], "orig_top_k_doc_id": [4769, 4766, 3198, 3416, 2044, 5835, 34, 5345, 6256, 2839, 7246, 2705, 2040, 2636, 4906]}, {"qid": 2729, "question": "In what cases is attention different from alignment? in What does Attention in Neural Machine Translation Pay Attention to?", "answer": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "top_k_doc_id": [4766, 2491, 3917, 3920, 4767, 6595, 4769, 4692, 5835, 1013, 7579, 4693, 5790, 7686, 3198], "orig_top_k_doc_id": [4766, 4769, 2491, 4692, 4767, 5835, 1013, 3920, 7579, 4693, 5790, 3917, 6595, 7686, 3198]}]}
{"group_id": 528, "group_size": 4, "items": [{"qid": 2417, "question": "How big is ICSI meeting corpus? in Combining Acoustics, Content and Interaction Features to Find Hot Spots in Meetings", "answer": [" 75 meetings and about 70 hours of real-time audio duration"], "top_k_doc_id": [3949, 3950, 3951, 3952, 6858, 1132, 3924, 3925, 5124, 7794, 4295, 6236, 7793, 7799, 3923], "orig_top_k_doc_id": [3949, 3952, 3950, 3951, 3924, 4295, 3925, 6236, 5124, 6858, 1132, 7794, 7793, 7799, 3923]}, {"qid": 2418, "question": "What annotations are available in ICSI meeting corpus? in Combining Acoustics, Content and Interaction Features to Find Hot Spots in Meetings", "answer": ["8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator"], "top_k_doc_id": [3949, 3950, 3951, 3952, 6858, 1132, 3924, 3925, 5124, 7794, 4295, 6236, 7793, 7799, 3099], "orig_top_k_doc_id": [3949, 3952, 3950, 3951, 3924, 3925, 4295, 6236, 6858, 5124, 1132, 7793, 7794, 7799, 3099]}, {"qid": 2415, "question": "What they use as a metric of finding hot spots in meeting? in Combining Acoustics, Content and Interaction Features to Find Hot Spots in Meetings", "answer": ["unweighted average recall (UAR) metric"], "top_k_doc_id": [3949, 3950, 3951, 3952, 6858, 1132, 3924, 3925, 5124, 7794, 3099, 3195, 1284, 5925, 3923], "orig_top_k_doc_id": [3949, 3952, 3950, 3951, 3099, 5124, 3195, 6858, 3924, 7794, 1284, 3925, 1132, 5925, 3923]}, {"qid": 2416, "question": "Is this approach compared to some baseline? in Combining Acoustics, Content and Interaction Features to Find Hot Spots in Meetings", "answer": ["No"], "top_k_doc_id": [3949, 3950, 3951, 3952, 6858, 226, 230, 6335, 5264, 6336, 1516, 6846, 7379, 3195, 6589], "orig_top_k_doc_id": [3949, 3952, 3951, 3950, 226, 230, 6335, 5264, 6336, 1516, 6846, 6858, 7379, 3195, 6589]}]}
{"group_id": 529, "group_size": 4, "items": [{"qid": 2442, "question": "How big PIE datasets are obtained from dictionaries? in Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions", "answer": ["46 documents makes up our base corpus"], "top_k_doc_id": [4011, 4017, 4014, 4012, 4024, 4018, 4025, 4013, 4016, 4015, 4022, 4019, 4020, 4023, 4021], "orig_top_k_doc_id": [4011, 4017, 4014, 4012, 4024, 4018, 4025, 4013, 4016, 4015, 4022, 4019, 4020, 4023, 4021]}, {"qid": 2443, "question": "What compleentary PIE extraction methods are used to increase reliability further? in Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions", "answer": ["exact string matching, inflectional string matching"], "top_k_doc_id": [4011, 4017, 4014, 4012, 4024, 4018, 4025, 4013, 4016, 4015, 4022, 4019, 4020, 4023, 4021], "orig_top_k_doc_id": [4011, 4017, 4014, 4012, 4024, 4022, 4018, 4019, 4016, 4013, 4020, 4025, 4023, 4015, 4021]}, {"qid": 2444, "question": "Are PIEs extracted automatically subjected to human evaluation? in Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions", "answer": ["Yes"], "top_k_doc_id": [4011, 4017, 4014, 4012, 4024, 4018, 4025, 4013, 4016, 4015, 4022, 4019, 4020, 4023, 4021], "orig_top_k_doc_id": [4011, 4017, 4012, 4014, 4018, 4024, 4022, 4020, 4025, 4015, 4023, 4013, 4016, 4019, 4021]}, {"qid": 2445, "question": "What dictionaries are used for automatic extraction of PIEs? in Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions", "answer": ["Wiktionary, Oxford Dictionary of English Idioms, UsingEnglish.com (UE), Sporleder corpus, VNC dataset, SemEval-2013 Task 5 dataset"], "top_k_doc_id": [4011, 4017, 4014, 4012, 4024, 4018, 4025, 4013, 4016, 4015, 4022, 4019, 4020, 4023, 4021], "orig_top_k_doc_id": [4011, 4017, 4014, 4024, 4012, 4025, 4018, 4016, 4015, 4022, 4020, 4013, 4019, 4023, 4021]}]}
{"group_id": 530, "group_size": 4, "items": [{"qid": 2481, "question": "What experiments they perform to demonstrate that their approach leads more accurate region based representations? in Modelling Semantic Categories using Conceptual Neighborhood", "answer": [" To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing."], "top_k_doc_id": [4222, 3208, 4082, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 6756, 6251, 7439, 3050, 4268], "orig_top_k_doc_id": [4172, 4171, 4177, 4176, 4173, 4175, 4174, 4082, 3208, 6756, 4222, 6251, 7439, 3050, 4268]}, {"qid": 2482, "question": "How they indentify conceptual neighbours? in Modelling Semantic Categories using Conceptual Neighborhood", "answer": ["Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known."], "top_k_doc_id": [4222, 3208, 4082, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 5003, 234, 4084, 4083, 4221], "orig_top_k_doc_id": [4172, 4176, 4177, 4173, 4175, 4171, 4174, 4082, 4222, 5003, 3208, 234, 4084, 4083, 4221]}, {"qid": 2497, "question": "What other models are compared to the Blending Game? in On the emergence of syntactic structures: quantifying and modelling duality of patterning", "answer": ["No"], "top_k_doc_id": [4222, 1540, 1798, 4221, 4257, 4258, 4506, 4642, 6341, 826, 4619, 3944, 1014, 562, 6251], "orig_top_k_doc_id": [4221, 4222, 1540, 4258, 4257, 4642, 826, 4619, 3944, 6341, 1014, 562, 4506, 1798, 6251]}, {"qid": 2498, "question": "What empirical data are the Blending Game predictions compared to? in On the emergence of syntactic structures: quantifying and modelling duality of patterning", "answer": ["words length distribution, the frequency of use of the different forms and a measure for the combinatoriality"], "top_k_doc_id": [4222, 1540, 1798, 4221, 4257, 4258, 4506, 4642, 6341, 3385, 4186, 3793, 2312, 1283, 3399], "orig_top_k_doc_id": [4221, 4222, 1540, 3385, 1798, 4642, 4258, 4257, 4186, 4506, 3793, 2312, 1283, 6341, 3399]}]}
{"group_id": 531, "group_size": 4, "items": [{"qid": 2545, "question": "What models are included in the toolkit? in LeafNATS: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization", "answer": [" recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS"], "top_k_doc_id": [4825, 5541, 4478, 5540, 7762, 4424, 4425, 4426, 5542, 4384, 7136, 6492, 5554, 2334, 34], "orig_top_k_doc_id": [4424, 4425, 4426, 5542, 4384, 7136, 7762, 6492, 4825, 5541, 5540, 5554, 2334, 4478, 34]}, {"qid": 3655, "question": "What domain of text are they working with? in Neural Summarization by Extracting Sentences and Words", "answer": ["news articles", "news", "news articles", "news"], "top_k_doc_id": [4825, 5541, 2334, 2335, 5138, 5997, 6839, 6931, 3198, 4646, 4334, 7472, 1141, 5540, 4380], "orig_top_k_doc_id": [2335, 2334, 5138, 3198, 4825, 4646, 4334, 7472, 6839, 5541, 6931, 1141, 5540, 4380, 5997]}, {"qid": 3656, "question": "What dataset do they use? in Neural Summarization by Extracting Sentences and Words", "answer": ["DUC 2002 document summarization corpus, our own DailyMail news highlights corpus", "DUC 2002, our own Dailymail news highlights corpus", "the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus", "DailyMail news articles"], "top_k_doc_id": [4825, 5541, 2334, 2335, 5138, 5997, 6839, 6931, 730, 543, 6716, 2083, 1255, 3201, 6955], "orig_top_k_doc_id": [5138, 4825, 2335, 2334, 730, 6839, 543, 6931, 5997, 5541, 6716, 2083, 1255, 3201, 6955]}, {"qid": 3657, "question": "Do they compare to abstractive summarization methods? in Neural Summarization by Extracting Sentences and Words", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [4825, 5541, 4478, 5540, 7762, 730, 5138, 1253, 6716, 5142, 5143, 1254, 2419, 3715, 5396], "orig_top_k_doc_id": [4825, 730, 5138, 1253, 6716, 5142, 5143, 4478, 1254, 2419, 3715, 5396, 5541, 7762, 5540]}]}
{"group_id": 532, "group_size": 4, "items": [{"qid": 2575, "question": "What is the baseline? in Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies", "answer": ["The baseline is a multi-task architecture inspired by another paper."], "top_k_doc_id": [2, 1061, 2709, 4538, 4539, 4727, 5044, 2873, 6294, 2796, 6782, 6295, 599, 5387, 2429], "orig_top_k_doc_id": [4538, 5044, 4727, 2709, 4539, 2, 1061, 2873, 6294, 6782, 2796, 6295, 599, 5387, 2429]}, {"qid": 2577, "question": "How many supervised tasks are used? in Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies", "answer": ["two"], "top_k_doc_id": [2, 1061, 2709, 4538, 4539, 4727, 5044, 2873, 6294, 2796, 165, 4728, 6291, 85, 5655], "orig_top_k_doc_id": [4538, 4727, 2709, 4539, 5044, 2796, 165, 1061, 4728, 6291, 85, 5655, 2, 6294, 2873]}, {"qid": 2576, "question": "What is the unsupervised task in the final layer? in Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies", "answer": ["Language Modeling"], "top_k_doc_id": [2, 1061, 2709, 4538, 4539, 4727, 5044, 2873, 6294, 6295, 6291, 2713, 4209, 6293, 4531], "orig_top_k_doc_id": [4538, 2709, 4727, 1061, 6294, 6295, 2873, 2, 6291, 2713, 4539, 5044, 4209, 6293, 4531]}, {"qid": 2578, "question": "What is the network architecture? in Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies", "answer": ["The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers."], "top_k_doc_id": [2, 1061, 2709, 4538, 4539, 4727, 5044, 6291, 2874, 7420, 599, 5283, 7422, 4209, 2796], "orig_top_k_doc_id": [4538, 4727, 5044, 4539, 2709, 6291, 1061, 2874, 7420, 599, 2, 5283, 7422, 4209, 2796]}]}
{"group_id": 533, "group_size": 4, "items": [{"qid": 2609, "question": "On which benchmarks they achieve the state of the art? in Improved Neural Relation Detection for Knowledge Base Question Answering", "answer": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "top_k_doc_id": [4597, 1422, 2306, 4841, 7351, 1423, 2578, 3409, 1217, 1120, 3617, 4698, 2519, 2234, 4074], "orig_top_k_doc_id": [1422, 4597, 2306, 1423, 2578, 7351, 3409, 1217, 1120, 3617, 4698, 4841, 2519, 2234, 4074]}, {"qid": 2610, "question": "What they use in their propsoed framework? in Improved Neural Relation Detection for Knowledge Base Question Answering", "answer": ["break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method", "break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection"], "top_k_doc_id": [4597, 1422, 2306, 4841, 7351, 946, 491, 4257, 4075, 7520, 3490, 2023, 490, 2893, 4719], "orig_top_k_doc_id": [7351, 1422, 4597, 946, 491, 4257, 2306, 4075, 7520, 4841, 3490, 2023, 490, 2893, 4719]}, {"qid": 2611, "question": "What does KBQA abbreviate for in Improved Neural Relation Detection for Knowledge Base Question Answering", "answer": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "top_k_doc_id": [4597, 490, 491, 826, 2821, 4599, 4600, 4601, 4602, 4640, 6947, 7677, 2363, 2362, 6951], "orig_top_k_doc_id": [4597, 4640, 4599, 4601, 2821, 4600, 4602, 7677, 491, 2363, 490, 826, 2362, 6951, 6947]}, {"qid": 2612, "question": "What is te core component for KBQA? in Improved Neural Relation Detection for Knowledge Base Question Answering", "answer": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "top_k_doc_id": [4597, 490, 491, 826, 2821, 4599, 4600, 4601, 4602, 4640, 6947, 7677, 946, 6949, 4075], "orig_top_k_doc_id": [4597, 4600, 4602, 7677, 4601, 4640, 2821, 4599, 826, 490, 491, 6947, 946, 6949, 4075]}]}
{"group_id": 534, "group_size": 4, "items": [{"qid": 2669, "question": "Does their framework automatically optimize for hyperparameters? in Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "answer": ["No", "No"], "top_k_doc_id": [1874, 4692, 4693, 4694, 4695, 4697, 5835, 7339, 4590, 6060, 7340, 274, 2906, 398, 5412], "orig_top_k_doc_id": [4693, 4692, 4697, 7339, 4694, 4695, 274, 4590, 1874, 2906, 398, 6060, 7340, 5412, 5835]}, {"qid": 2670, "question": "Does their framework always generate purely attention-based models? in Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "answer": ["Yes", "No"], "top_k_doc_id": [1874, 4692, 4693, 4694, 4695, 4697, 5835, 7339, 4590, 6060, 7340, 2998, 4184, 4568, 3817], "orig_top_k_doc_id": [4693, 4692, 4697, 7339, 7340, 4694, 2998, 5835, 4184, 4695, 6060, 4590, 4568, 3817, 1874]}, {"qid": 2672, "question": "Which languages do they test on for the under-resourced scenario? in Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "answer": ["English, German", "small portion of the large parallel corpus for English-German is used as a simulation"], "top_k_doc_id": [1874, 4692, 4693, 4694, 4695, 4697, 5835, 7339, 4590, 4696, 5838, 4568, 398, 5846, 51], "orig_top_k_doc_id": [4692, 4695, 4697, 4693, 5835, 4694, 7339, 4696, 4590, 5838, 4568, 1874, 398, 5846, 51]}, {"qid": 2671, "question": "Do they test their framework performance on commonly used language pairs, such as English-to-German? in Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "answer": ["Yes", "Yes"], "top_k_doc_id": [1874, 4692, 4693, 4694, 4695, 4697, 5835, 7339, 6853, 398, 2760, 4568, 4030, 4027, 6854], "orig_top_k_doc_id": [4695, 4693, 4694, 5835, 6853, 1874, 398, 7339, 2760, 4568, 4030, 4027, 4692, 4697, 6854]}]}
{"group_id": 535, "group_size": 4, "items": [{"qid": 2731, "question": "How much data samples do they start with before obtaining the initial model labels? in Active Learning for Speech Recognition: the Power of Gradients", "answer": ["1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset", "INLINEFORM2 is queried for the \u201cmost informative\u201d instance(s) INLINEFORM3"], "top_k_doc_id": [4771, 4772, 5476, 7793, 1059, 2577, 2607, 2803, 4690, 3753, 1593, 1365, 4286, 3978, 5262], "orig_top_k_doc_id": [4771, 4772, 5476, 4690, 3753, 1059, 1593, 7793, 2577, 1365, 2803, 4286, 3978, 5262, 2607]}, {"qid": 2733, "question": "Which dataset do they use? in Active Learning for Speech Recognition: the Power of Gradients", "answer": ["190 hours ( INLINEFORM1 100K instances)", "trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset"], "top_k_doc_id": [4771, 4772, 5476, 7793, 1059, 2577, 2607, 2803, 483, 2576, 3976, 381, 1665, 482, 2575], "orig_top_k_doc_id": [5476, 4771, 4772, 2607, 483, 7793, 2576, 2577, 1059, 3976, 381, 1665, 482, 2575, 2803]}, {"qid": 2732, "question": "Which model do they use for end-to-end speech recognition? in Active Learning for Speech Recognition: the Power of Gradients", "answer": ["RNN", " Recurrent Neural Network (RNN)"], "top_k_doc_id": [4771, 4772, 5476, 7793, 381, 3834, 2450, 3976, 373, 3835, 6968, 5987, 4862, 1618, 4863], "orig_top_k_doc_id": [4771, 4772, 7793, 5476, 381, 3834, 2450, 3976, 373, 3835, 6968, 5987, 4862, 1618, 4863]}, {"qid": 2730, "question": "How do they calculate variance from the model outputs? in Active Learning for Speech Recognition: the Power of Gradients", "answer": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "top_k_doc_id": [4771, 4772, 5476, 7150, 5865, 4370, 6480, 5013, 2607, 973, 1638, 3092, 3976, 7151, 4194], "orig_top_k_doc_id": [4771, 4772, 7150, 5476, 5865, 4370, 6480, 5013, 2607, 973, 1638, 3092, 3976, 7151, 4194]}]}
{"group_id": 536, "group_size": 4, "items": [{"qid": 2765, "question": "How many humans participated? in Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors", "answer": ["No", "16"], "top_k_doc_id": [4838, 4839, 4191, 4192, 4835, 4840, 5708, 5716, 6254, 5150, 3184, 6527, 1969, 107, 3208], "orig_top_k_doc_id": [4839, 4840, 4838, 4835, 6254, 5716, 4192, 5150, 3184, 5708, 6527, 1969, 107, 3208, 4191]}, {"qid": 2766, "question": "What embedding techniques are explored in the paper? in Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors", "answer": ["Skip\u2013gram, CBOW", "integrated vector-res, vector-faith, Skip\u2013gram, CBOW"], "top_k_doc_id": [4838, 4839, 4191, 4192, 4835, 4840, 5708, 5716, 6254, 3612, 6025, 5152, 5709, 6252, 6251], "orig_top_k_doc_id": [4839, 4840, 4838, 4835, 5716, 4192, 3612, 6025, 5152, 5709, 6252, 6251, 5708, 6254, 4191]}, {"qid": 2764, "question": "What is a second order co-ocurrence matrix? in Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors", "answer": ["frequencies of the other words which occur with both of them (i.e., second order co\u2013occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "top_k_doc_id": [4838, 4839, 4191, 4192, 4835, 4840, 5708, 5716, 4837, 2406, 5701, 6126, 6127, 6025, 6573], "orig_top_k_doc_id": [4839, 4840, 4838, 4835, 4837, 5708, 4192, 2406, 5701, 6126, 5716, 6127, 4191, 6025, 6573]}, {"qid": 2137, "question": "What datasets was the method evaluated on? in Tagged Back-Translation", "answer": ["WMT18 EnDe bitext, WMT16 EnRo bitext, WMT15 EnFr bitext, We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs."], "top_k_doc_id": [4838, 4839, 3261, 3263, 3260, 3564, 7173, 7024, 5868, 5845, 4619, 6144, 6591, 2836, 2741], "orig_top_k_doc_id": [4839, 3261, 3263, 3260, 3564, 7173, 7024, 5868, 5845, 4619, 6144, 6591, 2836, 2741, 4838]}]}
{"group_id": 537, "group_size": 4, "items": [{"qid": 2804, "question": "What features are absent from MRC gold standards that can result in potential lexical ambiguity? in A Framework for Evaluation of Machine Reading Comprehension Gold Standards", "answer": ["Restrictivity , Factivity , Coreference ", "semantics-altering grammatical modifiers"], "top_k_doc_id": [1147, 4910, 4911, 4912, 4913, 4914, 4915, 5037, 7572, 7573, 3416, 2327, 3973, 4518, 1512], "orig_top_k_doc_id": [4915, 4910, 4912, 4914, 4913, 4911, 1147, 7572, 2327, 7573, 3416, 3973, 5037, 4518, 1512]}, {"qid": 2805, "question": "What modern MRC gold standards are analyzed? in A Framework for Evaluation of Machine Reading Comprehension Gold Standards", "answer": ["fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\ year) \\times 20$ citations", "MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP."], "top_k_doc_id": [1147, 4910, 4911, 4912, 4913, 4914, 4915, 5037, 7572, 7573, 3416, 566, 4637, 2339, 3972], "orig_top_k_doc_id": [4915, 4910, 4911, 4912, 4914, 4913, 566, 7572, 3416, 5037, 7573, 4637, 2339, 3972, 1147]}, {"qid": 2803, "question": "Have they made any attempt to correct MRC gold standards according to their findings?  in A Framework for Evaluation of Machine Reading Comprehension Gold Standards", "answer": ["Yes", "No"], "top_k_doc_id": [1147, 4910, 4911, 4912, 4913, 4914, 4915, 5037, 7572, 7573, 2836, 2388, 1357, 2442, 2011], "orig_top_k_doc_id": [4910, 4915, 4912, 4911, 4913, 4914, 2836, 2388, 5037, 1147, 7572, 1357, 7573, 2442, 2011]}, {"qid": 2806, "question": "How does proposed qualitative annotation schema looks like? in A Framework for Evaluation of Machine Reading Comprehension Gold Standards", "answer": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "top_k_doc_id": [1147, 4910, 4911, 4912, 4913, 4914, 4915, 4637, 4415, 4074, 2395, 5966, 566, 4257, 2387], "orig_top_k_doc_id": [4915, 4910, 4912, 4911, 4914, 4913, 4637, 4415, 4074, 2395, 5966, 566, 4257, 2387, 1147]}]}
{"group_id": 538, "group_size": 4, "items": [{"qid": 2842, "question": "What model is used to encode the images? in Contextualize, Show and Tell: A Neural Visual Storyteller", "answer": ["a Convolutional Neural Network (CNN)", "LSTM"], "top_k_doc_id": [4746, 4977, 81, 83, 84, 6477, 2418, 5589, 4630, 4629, 280, 160, 7357, 4428, 2733], "orig_top_k_doc_id": [4977, 81, 4746, 2418, 84, 6477, 5589, 4630, 4629, 280, 83, 160, 7357, 4428, 2733]}, {"qid": 2844, "question": "Is the position in the sequence part of the input? in Contextualize, Show and Tell: A Neural Visual Storyteller", "answer": ["No", "Yes"], "top_k_doc_id": [4746, 4977, 81, 83, 84, 6477, 1705, 21, 3169, 117, 4976, 3148, 4755, 2925, 6439], "orig_top_k_doc_id": [4977, 4746, 84, 83, 6477, 1705, 81, 21, 3169, 117, 4976, 3148, 4755, 2925, 6439]}, {"qid": 2845, "question": "Do the decoder LSTMs all have the same weights? in Contextualize, Show and Tell: A Neural Visual Storyteller", "answer": ["No", "No"], "top_k_doc_id": [4746, 4977, 81, 83, 1705, 3116, 6529, 7333, 4755, 1793, 7363, 3980, 1794, 901, 1518], "orig_top_k_doc_id": [4977, 81, 4746, 1705, 3116, 6529, 7333, 4755, 1793, 7363, 3980, 1794, 901, 83, 1518]}, {"qid": 2843, "question": "How is the sequential nature of the story captured? in Contextualize, Show and Tell: A Neural Visual Storyteller", "answer": ["we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story", "The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\\lbrace p_1,...,p_{n}\\rbrace $ for each image in the sequence. "], "top_k_doc_id": [4746, 4977, 285, 1518, 4976, 3530, 5589, 5197, 6477, 5730, 6441, 5594, 6529, 7363, 84], "orig_top_k_doc_id": [4977, 285, 1518, 4976, 3530, 5589, 5197, 6477, 4746, 5730, 6441, 5594, 6529, 7363, 84]}]}
{"group_id": 539, "group_size": 4, "items": [{"qid": 2861, "question": "Is the model tested against any baseline? in VAIS ASR: Building a conversational speech recognition system using language model combination", "answer": ["No", "No"], "top_k_doc_id": [381, 1161, 5017, 1889, 5987, 6479, 6777, 380, 6480, 4373, 7403, 1786, 2299, 3562, 1787], "orig_top_k_doc_id": [381, 1161, 6777, 5017, 1889, 4373, 7403, 1786, 6480, 2299, 380, 3562, 5987, 6479, 1787]}, {"qid": 2862, "question": "What is the language model combination technique used in the paper? in VAIS ASR: Building a conversational speech recognition system using language model combination", "answer": ["system combination on the decoding lattice level, combination weights", "system combination on the decoding lattice level"], "top_k_doc_id": [381, 1161, 5017, 1889, 5987, 6479, 6777, 380, 6480, 5018, 1061, 1266, 4376, 4444, 1268], "orig_top_k_doc_id": [5017, 1161, 381, 6480, 5018, 5987, 6777, 1061, 6479, 1889, 1266, 4376, 380, 4444, 1268]}, {"qid": 2863, "question": "What are the deep learning architectures used in the task? in VAIS ASR: Building a conversational speech recognition system using language model combination", "answer": ["DNN-based acoustic model BIBREF0", "No"], "top_k_doc_id": [381, 1161, 5017, 1889, 5987, 6479, 6777, 5481, 4972, 1061, 1296, 1228, 373, 1987, 3772], "orig_top_k_doc_id": [1161, 381, 5481, 6479, 5987, 4972, 1061, 1296, 1228, 5017, 6777, 373, 1987, 3772, 1889]}, {"qid": 2860, "question": "What is the performance reported for the best models in the VLSP 2018 and VLSP 2019 challenges? in VAIS ASR: Building a conversational speech recognition system using language model combination", "answer": ["No", "No"], "top_k_doc_id": [381, 1161, 5017, 5018, 876, 4517, 1061, 4515, 4516, 2003, 3430, 4646, 5391, 1266, 1296], "orig_top_k_doc_id": [5017, 5018, 876, 4517, 1061, 4515, 4516, 1161, 2003, 3430, 4646, 5391, 1266, 381, 1296]}]}
{"group_id": 540, "group_size": 4, "items": [{"qid": 2871, "question": "How do they compare lexicons? in Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon", "answer": ["Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.", "1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)"], "top_k_doc_id": [451, 4500, 1040, 5037, 5105, 5422, 6752, 5043, 5038, 5042, 6856, 447, 448, 6855, 5], "orig_top_k_doc_id": [5037, 5043, 5038, 4500, 5105, 451, 5042, 6856, 6752, 447, 5422, 448, 6855, 1040, 5]}, {"qid": 4344, "question": "Do they compare against manually-created lexicons? in Building a robust sentiment lexicon with (almost) no resource", "answer": ["Yes", "Yes"], "top_k_doc_id": [451, 4500, 1040, 5037, 5105, 5422, 6752, 67, 5107, 4501, 6918, 5108, 4916, 5106, 7173], "orig_top_k_doc_id": [67, 451, 5107, 4501, 6752, 5037, 4500, 6918, 5105, 5108, 4916, 5422, 1040, 5106, 7173]}, {"qid": 4345, "question": "Do they compare to non-lexicon methods? in Building a robust sentiment lexicon with (almost) no resource", "answer": ["Yes", "Yes"], "top_k_doc_id": [451, 4500, 67, 80, 1044, 2139, 2162, 4344, 5107, 5108, 5038, 4863, 6752, 1040, 1784], "orig_top_k_doc_id": [5107, 2139, 67, 451, 5038, 80, 2162, 4500, 4863, 1044, 4344, 5108, 6752, 1040, 1784]}, {"qid": 4346, "question": "What language pairs are considered? in Building a robust sentiment lexicon with (almost) no resource", "answer": ["English-French, English-Italian, English-Spanish, English-German.", "French, Italian, Spanish and German, Existing English sentiment lexicons are translated to the target languages"], "top_k_doc_id": [451, 4500, 67, 80, 1044, 2139, 2162, 4344, 5107, 5108, 0, 7173, 502, 5714, 2165], "orig_top_k_doc_id": [5107, 2162, 1044, 2139, 5108, 451, 4344, 80, 0, 7173, 502, 5714, 2165, 4500, 67]}]}
{"group_id": 541, "group_size": 4, "items": [{"qid": 2879, "question": "How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications? in Natural Language Processing via LDA Topic Model in Recommendation Systems", "answer": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "top_k_doc_id": [5066, 5065, 5067, 5069, 5068, 5070, 1870, 1937, 1869, 6909, 4417, 5551, 6802, 3795, 5552], "orig_top_k_doc_id": [5069, 5068, 5065, 5070, 5066, 5067, 1870, 1937, 1869, 6909, 4417, 5551, 6802, 3795, 5552]}, {"qid": 3037, "question": "What alternative to Gibbs sampling is used? in Open Event Extraction from Online Text using a Generative Adversarial Network", "answer": ["generator network to capture the event-related patterns"], "top_k_doc_id": [5066, 5065, 5067, 5069, 5186, 5187, 5190, 4359, 5559, 5649, 2595, 6759, 2593, 5558, 3235], "orig_top_k_doc_id": [5186, 5187, 5190, 5065, 4359, 5066, 5067, 5559, 5649, 2595, 6759, 5069, 2593, 5558, 3235]}, {"qid": 1361, "question": "What are the measures of \"performance\" used in this paper? in Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling", "answer": ["test-set perplexity, likelihood convergence and clustering measures, visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task"], "top_k_doc_id": [5066, 1870, 1869, 1377, 7232, 4359, 1378, 3300, 1758, 5068, 1935, 879, 2402, 441, 521], "orig_top_k_doc_id": [1870, 1869, 1377, 7232, 4359, 1378, 3300, 5066, 1758, 5068, 1935, 879, 2402, 441, 521]}, {"qid": 2268, "question": "What topic clusters are identified by LDA? in Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter", "answer": ["Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club"], "top_k_doc_id": [5066, 3575, 3577, 2794, 599, 3796, 1401, 5003, 3795, 3574, 5068, 4515, 5288, 1869, 2404], "orig_top_k_doc_id": [3575, 3577, 2794, 599, 5066, 3796, 1401, 5003, 3795, 3574, 5068, 4515, 5288, 1869, 2404]}]}
{"group_id": 542, "group_size": 4, "items": [{"qid": 3058, "question": "What background knowledge do they leverage? in Robustly Leveraging Prior Knowledge in Text Classification", "answer": ["labeled features", "labelled features, which are words whose presence strongly indicates a specific class or topic"], "top_k_doc_id": [3058, 3094, 5207, 5208, 7111, 1625, 3489, 3490, 4154, 638, 5204, 6943, 4716, 4159, 6368], "orig_top_k_doc_id": [5207, 3489, 3490, 5208, 3058, 3094, 4154, 638, 5204, 1625, 7111, 6943, 4716, 4159, 6368]}, {"qid": 3059, "question": "What are the three regularization terms? in Robustly Leveraging Prior Knowledge in Text Classification", "answer": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "top_k_doc_id": [3058, 3094, 5207, 5208, 7111, 105, 5204, 6339, 390, 5395, 2308, 3416, 4405, 24, 3234], "orig_top_k_doc_id": [5207, 5208, 5204, 390, 5395, 3094, 7111, 3058, 2308, 105, 3416, 4405, 24, 3234, 6339]}, {"qid": 3060, "question": "What NLP tasks do they consider? in Robustly Leveraging Prior Knowledge in Text Classification", "answer": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "top_k_doc_id": [3058, 3094, 5207, 5208, 7111, 1625, 3489, 2074, 390, 4216, 6606, 5217, 4859, 3985, 6692], "orig_top_k_doc_id": [5207, 3094, 2074, 3489, 5208, 3058, 1625, 7111, 390, 4216, 6606, 5217, 4859, 3985, 6692]}, {"qid": 3061, "question": "How do they define robustness of a model? in Robustly Leveraging Prior Knowledge in Text Classification", "answer": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "top_k_doc_id": [3058, 3094, 5207, 5208, 7111, 105, 5204, 6339, 4216, 861, 1625, 1029, 2162, 5217, 1597], "orig_top_k_doc_id": [5207, 5208, 3058, 4216, 5204, 105, 861, 1625, 7111, 1029, 2162, 3094, 5217, 6339, 1597]}]}
{"group_id": 543, "group_size": 4, "items": [{"qid": 3067, "question": "What neural models are used to encode the text? in Knowledge Graph Representation with Jointly Structural and Textual Encoding", "answer": ["NBOW, LSTM, attentive LSTM", "neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder"], "top_k_doc_id": [337, 338, 2004, 5214, 5215, 3694, 7626, 2917, 5966, 7625, 5212, 1760, 4274, 3020, 2869], "orig_top_k_doc_id": [5215, 7626, 5214, 5212, 2004, 2917, 3694, 1760, 337, 338, 7625, 4274, 3020, 5966, 2869]}, {"qid": 3069, "question": "What datasets are used to evaluate this paper? in Knowledge Graph Representation with Jointly Structural and Textual Encoding", "answer": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "top_k_doc_id": [337, 338, 2004, 5214, 5215, 3694, 7626, 2917, 5966, 7625, 3412, 123, 5898, 558, 236], "orig_top_k_doc_id": [5215, 5214, 5966, 3412, 3694, 337, 2917, 338, 123, 7626, 5898, 7625, 2004, 558, 236]}, {"qid": 3068, "question": "What baselines are used for comparison? in Knowledge Graph Representation with Jointly Structural and Textual Encoding", "answer": ["TransE", "TransE"], "top_k_doc_id": [337, 338, 2004, 5214, 5215, 3694, 7626, 3020, 4718, 236, 7628, 2008, 341, 6888, 1252], "orig_top_k_doc_id": [5215, 5214, 3020, 4718, 2004, 3694, 7626, 236, 7628, 337, 2008, 341, 6888, 338, 1252]}, {"qid": 3066, "question": "How large are the textual descriptions of entities? in Knowledge Graph Representation with Jointly Structural and Textual Encoding", "answer": ["No", "No"], "top_k_doc_id": [337, 338, 2004, 5214, 5215, 5212, 558, 4493, 4490, 2869, 3409, 4492, 4103, 3412, 131], "orig_top_k_doc_id": [5215, 5214, 5212, 338, 558, 4493, 2004, 4490, 2869, 3409, 4492, 4103, 3412, 131, 337]}]}
{"group_id": 544, "group_size": 4, "items": [{"qid": 3075, "question": "What baselines do they compare with? in Joint Detection and Location of English Puns", "answer": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "top_k_doc_id": [1066, 1067, 1068, 1069, 5222, 5223, 5224, 5225, 2083, 5189, 2084, 6850, 4602, 4610, 5015], "orig_top_k_doc_id": [5224, 5222, 5225, 1068, 5223, 1067, 1066, 2084, 1069, 5189, 2083, 6850, 4602, 4610, 5015]}, {"qid": 3076, "question": "What datasets are used in evaluation? in Joint Detection and Location of English Puns", "answer": ["The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.", "A homographic and heterographic benchmark datasets by BIBREF9."], "top_k_doc_id": [1066, 1067, 1068, 1069, 5222, 5223, 5224, 5225, 2083, 5189, 2084, 3007, 2874, 4790, 6207], "orig_top_k_doc_id": [5222, 5224, 5225, 1068, 1067, 5223, 1069, 1066, 2083, 3007, 5189, 2874, 4790, 6207, 2084]}, {"qid": 3077, "question": "What is the tagging scheme employed? in Joint Detection and Location of English Puns", "answer": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "top_k_doc_id": [1066, 1067, 1068, 1069, 5222, 5223, 5224, 5225, 2083, 5189, 626, 295, 7320, 5186, 4790], "orig_top_k_doc_id": [5224, 5222, 5225, 5223, 1068, 1067, 2083, 5189, 626, 1066, 1069, 295, 7320, 5186, 4790]}, {"qid": 3074, "question": "What state-of-the-art results are achieved? in Joint Detection and Location of English Puns", "answer": ["F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.", "for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection"], "top_k_doc_id": [1066, 1067, 1068, 1069, 5222, 5223, 5224, 5225, 3578, 1434, 1329, 1581, 2330, 4268, 767], "orig_top_k_doc_id": [5224, 5222, 5225, 5223, 1068, 1067, 1069, 1066, 3578, 1434, 1329, 1581, 2330, 4268, 767]}]}
{"group_id": 545, "group_size": 4, "items": [{"qid": 3120, "question": "What are the baselines used in the paper? in Using General Adversarial Networks for Marketing: A Case Study of Airbnb", "answer": ["GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling", "No"], "top_k_doc_id": [5279, 5281, 5282, 6157, 7472, 1597, 5280, 447, 943, 4206, 4207, 4901, 5270, 6846, 5970], "orig_top_k_doc_id": [5279, 5282, 5281, 1597, 5270, 4901, 7472, 943, 4207, 447, 6846, 6157, 4206, 5970, 5280]}, {"qid": 3121, "question": "What is the size of the Airbnb? in Using General Adversarial Networks for Marketing: A Case Study of Airbnb", "answer": ["roughly 40,000 Manhattan listings"], "top_k_doc_id": [5279, 5281, 5282, 6157, 7472, 1597, 5280, 447, 943, 4206, 4207, 4901, 942, 941, 7474], "orig_top_k_doc_id": [5279, 5282, 5281, 943, 5280, 7472, 942, 1597, 447, 4207, 941, 6157, 4901, 7474, 4206]}, {"qid": 3118, "question": "What are the user-defined keywords? in Using General Adversarial Networks for Marketing: A Case Study of Airbnb", "answer": ["Words that a user wants them to appear in the generated output.", "terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'"], "top_k_doc_id": [5279, 5281, 5282, 6157, 7472, 1597, 5280, 7449, 1296, 4202, 7744, 6193, 3587, 2396, 495], "orig_top_k_doc_id": [5279, 5281, 5282, 5280, 1597, 6157, 7472, 7449, 1296, 4202, 7744, 6193, 3587, 2396, 495]}, {"qid": 3119, "question": "Does the method achieve sota performance on this dataset? in Using General Adversarial Networks for Marketing: A Case Study of Airbnb", "answer": ["No", "No"], "top_k_doc_id": [5279, 5281, 5282, 6157, 7472, 3348, 4206, 4207, 3347, 4214, 7546, 3344, 6095, 3566, 5561], "orig_top_k_doc_id": [5279, 5282, 3348, 5281, 4206, 4207, 3347, 4214, 7546, 3344, 6095, 3566, 7472, 5561, 6157]}]}
{"group_id": 546, "group_size": 4, "items": [{"qid": 3124, "question": "What is meant by closed test setting? in Attention Is All You Need for Chinese Word Segmentation", "answer": ["closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation", "closed test limits all the data for learning should not be beyond the given training set"], "top_k_doc_id": [5283, 5790, 2273, 2590, 3505, 6097, 7293, 5286, 1691, 1692, 3231, 1117, 2920, 2452, 2919], "orig_top_k_doc_id": [5286, 5283, 1691, 1692, 7293, 2590, 6097, 2273, 3231, 3505, 1117, 2920, 5790, 2452, 2919]}, {"qid": 3125, "question": "What are strong baselines model is compared to? in Attention Is All You Need for Chinese Word Segmentation", "answer": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "top_k_doc_id": [5283, 5790, 2273, 2590, 3505, 6097, 7293, 829, 3647, 650, 3646, 2948, 649, 3504, 6029], "orig_top_k_doc_id": [7293, 3505, 829, 5283, 3647, 650, 3646, 2948, 649, 6097, 3504, 2273, 2590, 5790, 6029]}, {"qid": 3122, "question": "How better is performance compared to previous state-of-the-art models? in Attention Is All You Need for Chinese Word Segmentation", "answer": ["F1 score of 97.5 on MSR and 95.7 on AS", "MSR: 97.7 compared to 97.5 of baseline\nAS: 95.7 compared to 95.6 of baseline"], "top_k_doc_id": [5283, 5790, 5286, 6029, 3258, 2948, 2433, 5287, 829, 4749, 3504, 2429, 538, 4750, 1691], "orig_top_k_doc_id": [5286, 6029, 3258, 2948, 2433, 5287, 829, 5283, 4749, 3504, 2429, 538, 5790, 4750, 1691]}, {"qid": 3123, "question": "How does Gaussian-masked directional multi-head attention works? in Attention Is All You Need for Chinese Word Segmentation", "answer": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "top_k_doc_id": [5283, 5790, 5287, 5284, 5286, 5285, 4642, 1770, 3505, 1819, 7117, 1769, 3504, 5259, 2257], "orig_top_k_doc_id": [5287, 5283, 5284, 5286, 5285, 5790, 4642, 1770, 3505, 1819, 7117, 1769, 3504, 5259, 2257]}]}
{"group_id": 547, "group_size": 4, "items": [{"qid": 3126, "question": "Does the dataset feature only English language data? in Comparative Studies of Detecting Abusive Language on Twitter", "answer": ["Yes", "No"], "top_k_doc_id": [1735, 5085, 5144, 5288, 6176, 7257, 1726, 1734, 1876, 5289, 5290, 3585, 3586, 3988, 1788], "orig_top_k_doc_id": [5288, 6176, 7257, 1734, 1735, 1726, 5289, 1876, 3585, 1788, 5290, 3586, 5085, 3988, 5144]}, {"qid": 3128, "question": "What learning models are used on the dataset? in Comparative Studies of Detecting Abusive Language on Twitter", "answer": ["Na\u00efve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Na\u00efve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "top_k_doc_id": [1735, 5085, 5144, 5288, 6176, 7257, 1726, 1734, 1876, 5289, 5290, 3585, 3586, 3988, 3582], "orig_top_k_doc_id": [5288, 6176, 5085, 3586, 1735, 1876, 5290, 3988, 5289, 1726, 5144, 7257, 3585, 1734, 3582]}, {"qid": 3127, "question": "What additional features and context are proposed? in Comparative Studies of Detecting Abusive Language on Twitter", "answer": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "top_k_doc_id": [1735, 5085, 5144, 5288, 6176, 7257, 1726, 1734, 1876, 5289, 5290, 7807, 6074, 5291, 770], "orig_top_k_doc_id": [5288, 5289, 5290, 1735, 6176, 5085, 1876, 7807, 6074, 1726, 1734, 5144, 5291, 770, 7257]}, {"qid": 3129, "question": "What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give? in Comparative Studies of Detecting Abusive Language on Twitter", "answer": ["detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset"], "top_k_doc_id": [1735, 5085, 5144, 5288, 6176, 7257, 6074, 4948, 3584, 7807, 6629, 5291, 3585, 3588, 33], "orig_top_k_doc_id": [6074, 5144, 6176, 5288, 4948, 3584, 7257, 7807, 1735, 5085, 6629, 5291, 3585, 3588, 33]}]}
{"group_id": 548, "group_size": 4, "items": [{"qid": 3150, "question": "Which dataset do they use? in Improving Distributed Representations of Tweets - Present and Future", "answer": ["No", " Paraphrase Database (PPDB) ,  book corpus", "No"], "top_k_doc_id": [5582, 4300, 4301, 4322, 5290, 5314, 5488, 3544, 6529, 6833, 3589, 413, 4991, 6075, 5785], "orig_top_k_doc_id": [5290, 4322, 5582, 3544, 6529, 6833, 5488, 4301, 3589, 413, 4991, 6075, 5314, 5785, 4300]}, {"qid": 3151, "question": "Do they evaluate their learned representations on downstream tasks? in Improving Distributed Representations of Tweets - Present and Future", "answer": ["No", "No"], "top_k_doc_id": [5582, 1743, 5313, 5315, 5314, 6697, 945, 2709, 6519, 986, 4331, 293, 4931, 1872, 2223], "orig_top_k_doc_id": [5314, 6697, 945, 5313, 2709, 5582, 6519, 986, 4331, 5315, 1743, 293, 4931, 1872, 2223]}, {"qid": 3152, "question": "Which representation learning architecture do they adopt? in Improving Distributed Representations of Tweets - Present and Future", "answer": ["No"], "top_k_doc_id": [5582, 4300, 4301, 4322, 5290, 5314, 5488, 5313, 891, 5485, 250, 527, 3300, 6057, 6058], "orig_top_k_doc_id": [5582, 5314, 4301, 5313, 4300, 4322, 891, 5485, 250, 5488, 527, 3300, 6057, 5290, 6058]}, {"qid": 3153, "question": "How do they encourage understanding of literature as part of their objective function? in Improving Distributed Representations of Tweets - Present and Future", "answer": ["They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision."], "top_k_doc_id": [5582, 1743, 5313, 5315, 5305, 5306, 3589, 4954, 1735, 1673, 4878, 1926, 5716, 6833, 3617], "orig_top_k_doc_id": [5313, 5305, 5315, 5306, 3589, 4954, 5582, 1735, 1673, 4878, 1926, 5716, 1743, 6833, 3617]}]}
{"group_id": 549, "group_size": 4, "items": [{"qid": 3167, "question": "How effective is their NCEL approach overall? in Neural Collective Entity Linking", "answer": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "top_k_doc_id": [3633, 5335, 5336, 5338, 5339, 6046, 6048, 1240, 5337, 1294, 209, 6049, 4158, 1293, 5123], "orig_top_k_doc_id": [5335, 5339, 5336, 5338, 5337, 1294, 209, 6049, 3633, 6048, 6046, 1240, 4158, 1293, 5123]}, {"qid": 3169, "question": "Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)? in Neural Collective Entity Linking", "answer": ["NCEL considers only adjacent mentions.", "More than that in some cases (next to adjacent) "], "top_k_doc_id": [3633, 5335, 5336, 5338, 5339, 6046, 6048, 1240, 5337, 214, 210, 4944, 3784, 4749, 4353], "orig_top_k_doc_id": [5335, 5336, 5339, 5338, 5337, 214, 1240, 210, 4944, 6046, 3784, 3633, 4749, 4353, 6048]}, {"qid": 3166, "question": "Which datasets do they use? in Neural Collective Entity Linking", "answer": ["CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW", "CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW"], "top_k_doc_id": [3633, 5335, 5336, 5338, 5339, 6046, 6048, 3784, 5923, 3915, 4859, 5925, 1262, 5926, 5924], "orig_top_k_doc_id": [5335, 5339, 5336, 3633, 6046, 5338, 6048, 3784, 5923, 3915, 4859, 5925, 1262, 5926, 5924]}, {"qid": 3168, "question": "How do they verify generalization ability? in Neural Collective Entity Linking", "answer": ["By calculating Macro F1 metric at the document level.", "by evaluating their model on five different benchmarks"], "top_k_doc_id": [3633, 5335, 5336, 5338, 5339, 4859, 2578, 6948, 214, 4719, 6870, 5128, 7280, 4944, 4597], "orig_top_k_doc_id": [5335, 5339, 5338, 5336, 4859, 2578, 6948, 214, 4719, 6870, 3633, 5128, 7280, 4944, 4597]}]}
{"group_id": 550, "group_size": 4, "items": [{"qid": 3170, "question": "Do the authors mention any downside of lemmatizing input before training ELMo? in To lemmatize or not to lemmatize: how word normalisation affects ELMo performance in word sense disambiguation", "answer": ["Yes", "Yes"], "top_k_doc_id": [1440, 5084, 5341, 5344, 397, 5757, 6768, 6769, 3370, 5758, 5759, 1441, 1330, 5051, 4859], "orig_top_k_doc_id": [5341, 1440, 5344, 5084, 5757, 1441, 1330, 3370, 6769, 5759, 6768, 5051, 5758, 397, 4859]}, {"qid": 3173, "question": "How big was the corpora they trained ELMo on? in To lemmatize or not to lemmatize: how word normalisation affects ELMo performance in word sense disambiguation", "answer": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "top_k_doc_id": [1440, 5084, 5341, 5344, 397, 5757, 6768, 6769, 3370, 5758, 5759, 5343, 7318, 4603, 5342], "orig_top_k_doc_id": [5341, 5344, 1440, 5759, 5757, 5084, 5343, 6768, 6769, 7318, 5758, 397, 4603, 3370, 5342]}, {"qid": 3172, "question": "Why is lemmatization not necessary in English? in To lemmatize or not to lemmatize: how word normalisation affects ELMo performance in word sense disambiguation", "answer": ["Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations."], "top_k_doc_id": [1440, 5084, 5341, 5344, 397, 5757, 6768, 6769, 7318, 1442, 6594, 395, 5343, 1441, 1985], "orig_top_k_doc_id": [5341, 5344, 7318, 1440, 397, 1442, 6594, 395, 5343, 5084, 1441, 6769, 1985, 6768, 5757]}, {"qid": 3171, "question": "What other examples of morphologically-rich languages do the authors give? in To lemmatize or not to lemmatize: how word normalisation affects ELMo performance in word sense disambiguation", "answer": ["Russian", "Russian"], "top_k_doc_id": [1440, 5084, 5341, 5344, 7318, 395, 6594, 2448, 68, 5343, 6791, 3370, 7687, 5342, 982], "orig_top_k_doc_id": [5341, 5344, 7318, 5084, 395, 1440, 6594, 2448, 68, 5343, 6791, 3370, 7687, 5342, 982]}]}
{"group_id": 551, "group_size": 4, "items": [{"qid": 3241, "question": "How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)? in ARAML: A Stable Adversarial Training Framework for Text Generation", "answer": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "top_k_doc_id": [5442, 5444, 3235, 3328, 5443, 5445, 5446, 1322, 4071, 5505, 5691, 6868, 1323, 3566, 4206], "orig_top_k_doc_id": [5442, 5444, 5445, 5446, 5443, 1322, 3235, 4071, 1323, 3566, 6868, 3328, 4206, 5691, 5505]}, {"qid": 3242, "question": "Is the discriminator's reward made available at each step to the generator? in ARAML: A Stable Adversarial Training Framework for Text Generation", "answer": ["No"], "top_k_doc_id": [5442, 5444, 3235, 3328, 5443, 5445, 5446, 1322, 4071, 5505, 5691, 6868, 5504, 278, 6589], "orig_top_k_doc_id": [5444, 5442, 5443, 5445, 5446, 5504, 5505, 1322, 4071, 278, 3235, 5691, 6589, 6868, 3328]}, {"qid": 3240, "question": "What GAN models were used as baselines to compare against? in ARAML: A Stable Adversarial Training Framework for Text Generation", "answer": ["MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN", "SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN"], "top_k_doc_id": [5442, 5444, 3235, 3328, 5443, 5445, 5446, 5504, 281, 94, 279, 5186, 276, 287, 7145], "orig_top_k_doc_id": [5444, 5445, 5446, 5442, 5443, 3328, 5504, 281, 94, 279, 5186, 276, 287, 7145, 3235]}, {"qid": 3020, "question": "What is the combination of rewards for reinforcement learning? in A Neural Approach to Irony Generation", "answer": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "top_k_doc_id": [5442, 5444, 5179, 5175, 5178, 5176, 5177, 1322, 5174, 3475, 4482, 4071, 1673, 6588, 6288], "orig_top_k_doc_id": [5179, 5175, 5178, 5176, 5177, 1322, 5442, 5174, 3475, 4482, 4071, 1673, 5444, 6588, 6288]}]}
{"group_id": 552, "group_size": 4, "items": [{"qid": 3389, "question": "Do they report results only on English data? in When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums", "answer": ["No", "No"], "top_k_doc_id": [2335, 5613, 5614, 5615, 5616, 5617, 3362, 3363, 3789, 5430, 7478, 7668, 643, 2111, 1147], "orig_top_k_doc_id": [5613, 5614, 5615, 5616, 5617, 3362, 3363, 2335, 643, 2111, 5430, 7478, 1147, 3789, 7668]}, {"qid": 3390, "question": "What aspects of discussion are relevant to instructor intervention, according to the attention mechanism? in When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums", "answer": ["context inference"], "top_k_doc_id": [2335, 5613, 5614, 5615, 5616, 5617, 1147, 1653, 1654, 3942, 6963, 3362, 7668, 5647, 4619], "orig_top_k_doc_id": [5613, 5614, 5615, 5617, 5616, 2335, 1653, 1654, 3362, 1147, 7668, 5647, 3942, 6963, 4619]}, {"qid": 3391, "question": "What was the previous state of the art for this task? in When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums", "answer": ["hLSTM", "hLSTM"], "top_k_doc_id": [2335, 5613, 5614, 5615, 5616, 5617, 3362, 3363, 3789, 5430, 7478, 7668, 5651, 5647, 5426], "orig_top_k_doc_id": [5613, 5614, 5615, 5616, 5617, 2335, 3789, 5430, 5651, 7668, 3363, 3362, 5647, 5426, 7478]}, {"qid": 3392, "question": "What type of latent context is used to predict instructor intervention? in When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums", "answer": ["the series of posts that trigger an intervention"], "top_k_doc_id": [2335, 5613, 5614, 5615, 5616, 5617, 1147, 1653, 1654, 3942, 6963, 6533, 3789, 5651, 5430], "orig_top_k_doc_id": [5613, 5614, 5615, 5616, 5617, 3942, 1147, 6963, 6533, 1654, 3789, 2335, 1653, 5651, 5430]}]}
{"group_id": 553, "group_size": 4, "items": [{"qid": 3423, "question": "how many humans evaluated the results? in Context-Aware Monolingual Repair for Neural Machine Translation", "answer": ["No", "No"], "top_k_doc_id": [5669, 5670, 5671, 5672, 5673, 6424, 6482, 1244, 1303, 7659, 28, 4712, 7662, 5868, 1720], "orig_top_k_doc_id": [5672, 5670, 1244, 5673, 5671, 5669, 6482, 28, 6424, 1303, 4712, 7662, 7659, 5868, 1720]}, {"qid": 3424, "question": "what was the baseline? in Context-Aware Monolingual Repair for Neural Machine Translation", "answer": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "top_k_doc_id": [5669, 5670, 5671, 5672, 5673, 6424, 6482, 1244, 1303, 7659, 6041, 3260, 1048, 2761, 6480], "orig_top_k_doc_id": [5672, 5670, 5671, 5673, 5669, 6482, 7659, 1303, 6424, 6041, 3260, 1048, 2761, 1244, 6480]}, {"qid": 3425, "question": "what phenomena do they mention is hard to capture? in Context-Aware Monolingual Repair for Neural Machine Translation", "answer": ["Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection."], "top_k_doc_id": [5669, 5670, 5671, 5672, 5673, 6424, 6482, 1348, 6042, 2331, 6041, 3260, 6656, 4615, 3746], "orig_top_k_doc_id": [5669, 5673, 5672, 5671, 5670, 1348, 6042, 2331, 6041, 6482, 3260, 6424, 6656, 4615, 3746]}, {"qid": 3426, "question": "by how much did the BLEU score improve? in Context-Aware Monolingual Repair for Neural Machine Translation", "answer": ["On average 0.64 "], "top_k_doc_id": [5669, 5670, 5671, 5672, 5673, 2762, 7659, 4695, 7660, 7827, 379, 6426, 1458, 3260, 5455], "orig_top_k_doc_id": [5672, 5670, 5669, 5671, 5673, 2762, 7659, 4695, 7660, 7827, 379, 6426, 1458, 3260, 5455]}]}
{"group_id": 554, "group_size": 4, "items": [{"qid": 3457, "question": "Is the LSTM baseline a sub-word model? in Finnish Language Modeling with Deep Transformer Models", "answer": ["Yes", "Yes"], "top_k_doc_id": [5724, 7285, 1456, 1457, 1691, 3560, 5725, 5726, 7541, 5710, 1741, 3274, 3273, 3643, 1318], "orig_top_k_doc_id": [5724, 5725, 5726, 1691, 7285, 7541, 1457, 1456, 5710, 1741, 3274, 3273, 3643, 1318, 3560]}, {"qid": 3458, "question": "How is pseudo-perplexity defined? in Finnish Language Modeling with Deep Transformer Models", "answer": ["Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated."], "top_k_doc_id": [5724, 7285, 1456, 1457, 1691, 3560, 5725, 5726, 7791, 3559, 6794, 7290, 3706, 1061, 7497], "orig_top_k_doc_id": [5724, 5725, 5726, 1457, 1456, 7791, 1691, 7285, 3559, 6794, 7290, 3706, 3560, 1061, 7497]}, {"qid": 4667, "question": "What previous proposed methods did they explore? in Multilingual is not enough: BERT for Finnish", "answer": ["ELMo , ULMFit , BERT", "che2018towards, lim2018sex, FiNER-tagger BIBREF32, gungor2018, HIT-SCIR BIBREF22, BIBREF33"], "top_k_doc_id": [5724, 7285, 5712, 7287, 7288, 7289, 7290, 7291, 4510, 5711, 6661, 5710, 2906, 6870, 3130], "orig_top_k_doc_id": [7285, 7290, 7291, 7289, 4510, 5711, 6661, 7287, 5710, 5712, 5724, 2906, 7288, 6870, 3130]}, {"qid": 4668, "question": "What was the new Finnish model trained on? in Multilingual is not enough: BERT for Finnish", "answer": ["Yle corpus, STT corpus, Suomi24 corpus (version 2017H2), luotolahti2015towards, Common Crawl, Finnish Wikipedia", "news, online discussion, and an internet crawl"], "top_k_doc_id": [5724, 7285, 5712, 7287, 7288, 7289, 7290, 7291, 7286, 5725, 1457, 5869, 2971, 1456, 5049], "orig_top_k_doc_id": [7285, 7291, 7290, 7289, 7288, 7286, 7287, 5724, 5725, 1457, 5712, 5869, 2971, 1456, 5049]}]}
{"group_id": 555, "group_size": 4, "items": [{"qid": 3488, "question": "Which hyperparameters were varied in the experiments on the four tasks? in On the effectiveness of feature set augmentation using clusters of word embeddings", "answer": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "top_k_doc_id": [5775, 5776, 5777, 4116, 5778, 2978, 2982, 3005, 5423, 6559, 6917, 5853, 1179, 3543, 942], "orig_top_k_doc_id": [5777, 4116, 5423, 6559, 5776, 3005, 2982, 5778, 6917, 5775, 5853, 1179, 3543, 2978, 942]}, {"qid": 3489, "question": "Which other hyperparameters, other than number of clusters are typically evaluated in this type of research? in On the effectiveness of feature set augmentation using clusters of word embeddings", "answer": ["selection of word vectors"], "top_k_doc_id": [5775, 5776, 5777, 4116, 5778, 2978, 2982, 3005, 4685, 3303, 3006, 1938, 4049, 1996, 3370], "orig_top_k_doc_id": [5777, 4116, 5776, 5775, 2982, 4685, 3005, 3303, 3006, 1938, 4049, 2978, 1996, 5778, 3370]}, {"qid": 3490, "question": "How were the cluster extracted?  in On the effectiveness of feature set augmentation using clusters of word embeddings", "answer": ["Word clusters are extracted using k-means on word embeddings"], "top_k_doc_id": [5775, 5776, 5777, 4116, 5778, 3303, 6925, 2876, 4476, 2794, 4475, 73, 3077, 6753, 3006], "orig_top_k_doc_id": [5777, 5778, 3303, 5776, 5775, 6925, 2876, 4476, 2794, 4116, 4475, 73, 3077, 6753, 3006]}, {"qid": 3487, "question": "Do they report results only on English datasets? in On the effectiveness of feature set augmentation using clusters of word embeddings", "answer": ["Yes", "Yes"], "top_k_doc_id": [5775, 5776, 5777, 2321, 4686, 4049, 2324, 2875, 2876, 6926, 3943, 2877, 6799, 2874, 5500], "orig_top_k_doc_id": [2321, 5777, 4686, 4049, 5775, 2324, 2875, 2876, 6926, 5776, 3943, 2877, 6799, 2874, 5500]}]}
{"group_id": 556, "group_size": 4, "items": [{"qid": 3505, "question": "How many different characters were in dataset? in Follow Alice into the Rabbit Hole: Giving Dialogue Agents Understanding of Human Level Attributes.", "answer": ["45,821 characters", "45,821 characters"], "top_k_doc_id": [1711, 1712, 1817, 2967, 3475, 4523, 5793, 3680, 1443, 3451, 1444, 899, 6590, 5882, 6586], "orig_top_k_doc_id": [5793, 2967, 1443, 1711, 1712, 4523, 3451, 1444, 3475, 899, 1817, 3680, 6590, 5882, 6586]}, {"qid": 3506, "question": "How does dataset model character's profiles? in Follow Alice into the Rabbit Hole: Giving Dialogue Agents Understanding of Human Level Attributes.", "answer": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "top_k_doc_id": [1711, 1712, 1817, 2967, 3475, 4523, 5793, 3680, 1443, 3451, 5798, 4124, 7218, 576, 312], "orig_top_k_doc_id": [5793, 5798, 2967, 1443, 1711, 4523, 4124, 1817, 1712, 3451, 3680, 3475, 7218, 576, 312]}, {"qid": 3508, "question": "What baseline models are used? in Follow Alice into the Rabbit Hole: Giving Dialogue Agents Understanding of Human Level Attributes.", "answer": ["the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker", "Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker"], "top_k_doc_id": [1711, 1712, 1817, 2967, 3475, 4523, 5793, 3680, 6586, 576, 6723, 575, 7217, 6590, 314], "orig_top_k_doc_id": [2967, 5793, 1711, 6586, 4523, 576, 6723, 575, 1817, 1712, 7217, 6590, 314, 3475, 3680]}, {"qid": 3507, "question": "How big is the difference in performance between proposed model and baselines? in Follow Alice into the Rabbit Hole: Giving Dialogue Agents Understanding of Human Level Attributes.", "answer": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "top_k_doc_id": [1711, 1712, 1817, 2967, 3475, 4523, 5793, 6586, 6036, 6589, 5798, 3633, 312, 1718, 6723], "orig_top_k_doc_id": [5793, 2967, 3475, 6586, 6036, 1817, 6589, 1712, 1711, 5798, 3633, 4523, 312, 1718, 6723]}]}
{"group_id": 557, "group_size": 4, "items": [{"qid": 3517, "question": "Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia? in Analysis of Wikipedia-based Corpora for Question Answering", "answer": ["No"], "top_k_doc_id": [5809, 5810, 1547, 2210, 5736, 1092, 2661, 7071, 7072, 7605, 2264, 5044, 5470, 5811, 2265], "orig_top_k_doc_id": [1092, 5810, 5809, 2210, 1547, 7072, 2264, 5044, 7605, 2661, 7071, 5470, 5811, 5736, 2265]}, {"qid": 3518, "question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset? in Analysis of Wikipedia-based Corpora for Question Answering", "answer": ["Yes", "No"], "top_k_doc_id": [5809, 5810, 1547, 2210, 5736, 1092, 2661, 7071, 7072, 7605, 6244, 4640, 6188, 3806, 1091], "orig_top_k_doc_id": [1092, 5809, 5810, 6244, 7072, 7605, 1547, 2210, 4640, 6188, 7071, 3806, 1091, 2661, 5736]}, {"qid": 3519, "question": "How many question types do they find in the datasets analyzed? in Analysis of Wikipedia-based Corpora for Question Answering", "answer": ["seven ", "7"], "top_k_doc_id": [5809, 5810, 1547, 2210, 5736, 7615, 4640, 5580, 1091, 4278, 1098, 787, 6676, 2377, 5811], "orig_top_k_doc_id": [5810, 5809, 7615, 4640, 1547, 5580, 1091, 2210, 4278, 5736, 1098, 787, 6676, 2377, 5811]}, {"qid": 3520, "question": "How do they analyze contextual similaries across datasets? in Analysis of Wikipedia-based Corpora for Question Answering", "answer": ["They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."], "top_k_doc_id": [5809, 5810, 5048, 3467, 3487, 511, 4604, 510, 5044, 2096, 5580, 720, 7572, 7071, 2306], "orig_top_k_doc_id": [5809, 5810, 5048, 3467, 3487, 511, 4604, 510, 5044, 2096, 5580, 720, 7572, 7071, 2306]}]}
{"group_id": 558, "group_size": 4, "items": [{"qid": 3621, "question": "How do they discover coherent word clusters? in Automatically Inferring Gender Associations from Language", "answer": ["First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.", "First, we trained domain-specific word embeddings, Then, we used k-means clustering to cluster the embeddings of the gender-associated words", "First, they  trained domain-specific word embeddings using the Word2Vec  model, then used k-means clustering to cluster the embeddings of the gender-associated words.", "The authors first generated a set of words which are associated with each gender, then built domain-specific word embeddings and used k-means clustering to cluster the gendered word associations together. "], "top_k_doc_id": [5949, 5950, 5951, 5952, 521, 3941, 6962, 6965, 3354, 1481, 6490, 2948, 6964, 4620, 7060], "orig_top_k_doc_id": [5951, 5949, 5950, 5952, 3941, 6965, 521, 3354, 6962, 1481, 6490, 2948, 6964, 4620, 7060]}, {"qid": 3622, "question": "How big are two introduced datasets? in Automatically Inferring Gender Associations from Language", "answer": ["300K sentences in each dataset", "each consisting of over 300K sentences", "Celeb dataset: 15917 texts and 342645 sentences\nProfessor dataset: 283973 texts and 976677 sentences", "Celebrity Dataset has 15,917 texts, 342,645 sentences, and the Female Male Proportions are  0.67/ 0.33. \nProfessor Dataset has 283,973 texts, 976, 667 sentences, and the Femal Male Proportions are 0.28./ 0,72"], "top_k_doc_id": [5949, 5950, 5951, 5952, 521, 3941, 6962, 6965, 3011, 520, 7454, 2037, 5525, 7358, 7455], "orig_top_k_doc_id": [5949, 5952, 3941, 521, 5950, 3011, 5951, 520, 7454, 2037, 5525, 7358, 6965, 7455, 6962]}, {"qid": 3623, "question": "What are strong baselines authors used? in Automatically Inferring Gender Associations from Language", "answer": ["The authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.", "No", "the top 4 predicted labels and the centroid of the cluster", "the top 4 predicted labels and the centroid of the cluster as a strong baseline label"], "top_k_doc_id": [5949, 5950, 5951, 5952, 521, 2037, 3011, 1432, 7064, 53, 3354, 85, 1495, 696, 2039], "orig_top_k_doc_id": [5952, 2037, 3011, 5950, 1432, 5949, 7064, 5951, 53, 3354, 85, 1495, 696, 521, 2039]}, {"qid": 3620, "question": "How do they decide what is the semantic concept label of particular cluster? in Automatically Inferring Gender Associations from Language", "answer": ["Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.", "Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.", "They automatically  label the cluster using WordNet and context-sensitive strengths of domain-specific word embeddings", "Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering"], "top_k_doc_id": [5949, 5950, 5951, 5952, 3941, 5727, 5910, 5529, 6965, 7725, 5191, 2335, 1432, 3008, 5528], "orig_top_k_doc_id": [5951, 5952, 5949, 3941, 5727, 5950, 5910, 5529, 6965, 7725, 5191, 2335, 1432, 3008, 5528]}]}
{"group_id": 559, "group_size": 4, "items": [{"qid": 3680, "question": "How do they incorporate expert knowledge into their topic model? in Toward Interpretable Topic Discovery via Anchored Correlation Explanation", "answer": ["The experts define anchors and the model learns correlations between the anchors and latent topics.", "anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors", "They use an anchored information theoretic topic modeling using Correlation Explanation and  information bottleneck."], "top_k_doc_id": [6025, 252, 882, 6026, 6027, 253, 2191, 2730, 5913, 6126, 5091, 1255, 6127, 6129, 4258], "orig_top_k_doc_id": [6025, 6027, 882, 6026, 2191, 252, 2730, 5913, 6126, 253, 5091, 1255, 6127, 6129, 4258]}, {"qid": 3681, "question": "On which corpora do they evaluate on? in Toward Interpretable Topic Discovery via Anchored Correlation Explanation", "answer": ["20 Newsgroups, i2b2 2008 Obesity Challenge BIBREF22 data set", "20 Newsgroups , i2b2 2008 Obesity Challenge", "20 Newsgroups, i2b2 2008 Obesity Challenge BIBREF22 data set", " i2b2 2008 Obesity Challenge BIBREF22, 20 Newsgroups"], "top_k_doc_id": [6025, 252, 882, 6026, 6027, 253, 2191, 1935, 557, 3530, 2181, 4780, 2405, 1934, 1906], "orig_top_k_doc_id": [6025, 6027, 252, 6026, 253, 882, 2191, 1935, 557, 3530, 2181, 4780, 2405, 1934, 1906]}, {"qid": 3682, "question": "Do they compare against popular topic models, such as LDA? in Toward Interpretable Topic Discovery via Anchored Correlation Explanation", "answer": ["No", "No", "No"], "top_k_doc_id": [6025, 252, 882, 6026, 6027, 1935, 1191, 5913, 3795, 881, 5065, 121, 1190, 128, 1869], "orig_top_k_doc_id": [6025, 882, 6027, 252, 1935, 6026, 1191, 5913, 3795, 881, 5065, 121, 1190, 128, 1869]}, {"qid": 2544, "question": "How is performance measured? in A framework for streamlined statistical prediction using topic models", "answer": ["they use ROC curves and cross-validation"], "top_k_doc_id": [6025, 4423, 4422, 1193, 2404, 4417, 1395, 2405, 1934, 7166, 1306, 4382, 1303, 5913, 1935], "orig_top_k_doc_id": [4423, 4422, 1193, 2404, 4417, 1395, 2405, 1934, 7166, 1306, 4382, 1303, 6025, 5913, 1935]}]}
{"group_id": 560, "group_size": 4, "items": [{"qid": 3691, "question": "What kind of evaluations do use to evaluate dialogue? in XPersona: Evaluating Multilingual Personalized Chatbot", "answer": ["They use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.", "No", "perplexity (ppl.) and BLEU, which of the two dialogues is better in terms of engagingness, interestingness, and humanness", "perplexity, BLEU, ACUTE-EVA"], "top_k_doc_id": [6036, 6037, 6038, 6039, 6040, 2969, 2970, 823, 1817, 7222, 6590, 3359, 3475, 3190, 824], "orig_top_k_doc_id": [6036, 6040, 6039, 6038, 2970, 823, 2969, 1817, 7222, 6590, 6037, 3359, 3475, 3190, 824]}, {"qid": 3692, "question": "By how much do their cross-lingual models lag behind other models? in XPersona: Evaluating Multilingual Personalized Chatbot", "answer": ["significant gap between the cross-lingual model and other models, Table TABREF20", "BLUE score is lower by 4 times than that of the best multilingual model."], "top_k_doc_id": [6036, 6037, 6038, 6039, 6040, 3621, 1048, 5714, 3547, 5872, 3617, 5554, 5716, 6060, 5713], "orig_top_k_doc_id": [6036, 6040, 6039, 6038, 1048, 5714, 3547, 5872, 3621, 6037, 3617, 5554, 5716, 6060, 5713]}, {"qid": 3693, "question": "Which translation pipelines do they use to compare against? in XPersona: Evaluating Multilingual Personalized Chatbot", "answer": ["Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.", "M-Bert2Bert, M-CausalBert, Bert2Bert, CausalBert, Poly-encoder BIBREF75, XNLG", "Google Translate API"], "top_k_doc_id": [6036, 6037, 6038, 6039, 6040, 2969, 2970, 6575, 6577, 6576, 3621, 4030, 2972, 3357, 126], "orig_top_k_doc_id": [6039, 6036, 6040, 6038, 6575, 6577, 2969, 6576, 6037, 3621, 4030, 2972, 2970, 3357, 126]}, {"qid": 3694, "question": "Which languages does their newly created dataset contain? in XPersona: Evaluating Multilingual Personalized Chatbot", "answer": ["Chinese, French, Indonesian, Italian, Korean, Japanese", "English, Chinese, French, Indonesian, Italian, Korean, Japanese", "Chinese, French, Indonesian, Italian, Korean, Japanese", "Chinese, French, Indonesian, Italian, Korean, and Japanese"], "top_k_doc_id": [6036, 6037, 6038, 6039, 6040, 3621, 2906, 2907, 3007, 2285, 5868, 5429, 4787, 5489, 436], "orig_top_k_doc_id": [6036, 6040, 6039, 6038, 3621, 2906, 2907, 3007, 2285, 5868, 5429, 6037, 4787, 5489, 436]}]}
{"group_id": 561, "group_size": 4, "items": [{"qid": 3720, "question": "What is the architecture of the decoder? in Cross-Lingual Natural Language Generation via Pre-Training", "answer": ["pre-trained Xnlg, 6-layer decoder", "6 transformer layers, each layer containing 1024 hidden units, 8 attention heads, and GELU activations.", "denoising auto-encoding (DAE) objective BIBREF24"], "top_k_doc_id": [6034, 6060, 6061, 4029, 6036, 6039, 6062, 6063, 6064, 1453, 1454, 2149, 6852, 6037, 3264], "orig_top_k_doc_id": [6060, 6061, 4029, 6036, 6039, 6852, 6063, 1453, 6062, 2149, 6034, 1454, 6037, 6064, 3264]}, {"qid": 3721, "question": "What is the architecture of the encoder? in Cross-Lingual Natural Language Generation via Pre-Training", "answer": ["pre-trained Xnlg with a 10-layer encoder", "denoising auto-encoding (DAE) objective BIBREF24", "10 transformer layers, each layer containing 1024 hidden units, 8 attentions heads, and GELU activations."], "top_k_doc_id": [6034, 6060, 6061, 4029, 6036, 6039, 6062, 6063, 6064, 1453, 1454, 2149, 6852, 3617, 4027], "orig_top_k_doc_id": [6060, 6061, 2149, 6034, 4029, 6036, 6063, 6852, 6039, 1453, 6062, 6064, 3617, 1454, 4027]}, {"qid": 3722, "question": "What is their baseline? in Cross-Lingual Natural Language Generation via Pre-Training", "answer": ["CorefNqg BIBREF33, Mp-Gsn BIBREF31, Xlm BIBREF5, Xlm Fine-tuning, Pipeline (Xlm), Pipeline (Xlm) with Google Translator", "CorefNqg, Mp-Gsn, Xlm, Pipeline (Xlm), Pipeline (Xlm) with Google Translator", "CorefNqg BIBREF33 , Mp-Gsn BIBREF31, Xlm BIBREF5"], "top_k_doc_id": [6034, 6060, 6061, 4029, 6036, 6039, 6062, 6063, 6064, 2840, 3617, 2811, 247, 5621, 4572], "orig_top_k_doc_id": [6060, 6036, 6063, 6064, 2840, 6039, 6061, 3617, 6034, 2811, 6062, 247, 4029, 5621, 4572]}, {"qid": 3719, "question": "What languages do they use during pretraining? in Cross-Lingual Natural Language Generation via Pre-Training", "answer": ["English, French, Chinese", "English, Chinese, French", "English/French/Chinese"], "top_k_doc_id": [6034, 6060, 6061, 3617, 4572, 4570, 4415, 3618, 4571, 6870, 3140, 3141, 5713, 3621, 5709], "orig_top_k_doc_id": [3617, 4572, 4570, 4415, 3618, 6034, 4571, 6060, 6870, 3140, 3141, 5713, 3621, 5709, 6061]}]}
{"group_id": 562, "group_size": 4, "items": [{"qid": 3791, "question": "Is the morphology detection task evaluated? in Design and implementation of an open source Greek POS Tagger and Entity Recognizer using spaCy", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [4575, 5189, 6144, 6151, 6152, 6153, 6154, 6177, 6178, 6179, 7287, 4612, 6467, 4359, 7113], "orig_top_k_doc_id": [6151, 6153, 6154, 6178, 6152, 6179, 6467, 4575, 4612, 6177, 7287, 5189, 4359, 6144, 7113]}, {"qid": 3792, "question": "How does the model proposed extend ENAMEX? in Design and implementation of an open source Greek POS Tagger and Entity Recognizer using spaCy", "answer": ["Extended with facility (FAC) type.", "The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC)", "SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based models"], "top_k_doc_id": [4575, 5189, 6144, 6151, 6152, 6153, 6154, 6177, 6178, 6179, 7287, 4612, 6811, 857, 6810], "orig_top_k_doc_id": [6151, 6153, 6154, 6811, 6178, 6152, 7287, 6179, 6144, 4575, 6177, 5189, 4612, 857, 6810]}, {"qid": 3790, "question": "What are the issues identified for out-of-vocabulary words? in Design and implementation of an open source Greek POS Tagger and Entity Recognizer using spaCy", "answer": ["model did not have a flexibility in OOV words, One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, It was noticed that the model performed better when using the vectors from different FastText models", "for unknown words the model assigned a zero vector", "Also, the model with the dataset vectors did not have the flexibility to classify unknown words., the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results"], "top_k_doc_id": [4575, 5189, 6144, 6151, 6152, 6153, 6154, 6177, 6178, 6179, 7287, 2622, 6677, 4359, 6811], "orig_top_k_doc_id": [6153, 6151, 6154, 5189, 6152, 4575, 6178, 6179, 2622, 6677, 7287, 6144, 6177, 4359, 6811]}, {"qid": 3793, "question": "Which morphological features are extracted? in Design and implementation of an open source Greek POS Tagger and Entity Recognizer using spaCy", "answer": ["like the gender, the number, and the case", "Information about the dataset includes the tokens of a set of articles and their position in a sentence, the lemma and the part of speech of every token, The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case", "The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers"], "top_k_doc_id": [4575, 5189, 6144, 6151, 6152, 6153, 6154, 6177, 6178, 6179, 4147, 4146, 7097, 4790, 4359], "orig_top_k_doc_id": [6151, 6153, 6152, 6154, 6178, 4575, 4147, 4146, 6179, 6177, 5189, 6144, 7097, 4790, 4359]}]}
{"group_id": 563, "group_size": 4, "items": [{"qid": 3851, "question": "How long is their dataset? in One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data", "answer": ["No", "No", "No"], "top_k_doc_id": [931, 2238, 4945, 4946, 5341, 5757, 6229, 6232, 6331, 37, 3147, 6122, 7490, 4476, 4475], "orig_top_k_doc_id": [6229, 5757, 6331, 2238, 5341, 3147, 37, 4945, 4946, 6232, 7490, 931, 6122, 4476, 4475]}, {"qid": 3853, "question": "How many layers does their model have? in One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data", "answer": ["6", "6", "6 layers"], "top_k_doc_id": [931, 2238, 4945, 4946, 5341, 5757, 6229, 6232, 6331, 37, 3147, 6122, 6697, 5475, 485], "orig_top_k_doc_id": [6229, 6331, 5757, 5341, 6697, 6232, 4945, 5475, 485, 2238, 37, 931, 4946, 3147, 6122]}, {"qid": 3854, "question": "What metrics do they use? in One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data", "answer": ["F-measure", "F-measure", "F-measure"], "top_k_doc_id": [931, 2238, 4945, 4946, 5341, 5757, 6229, 6232, 6331, 37, 3147, 4947, 68, 4475, 1517], "orig_top_k_doc_id": [6331, 5757, 6229, 2238, 4946, 4945, 37, 931, 4947, 68, 4475, 5341, 3147, 1517, 6232]}, {"qid": 3852, "question": "Do they use pretrained word embeddings? in One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data", "answer": ["No", "Yes", "Yes"], "top_k_doc_id": [931, 2238, 4945, 4946, 5341, 5757, 6229, 6232, 6331, 3836, 68, 2429, 350, 1774, 4296], "orig_top_k_doc_id": [6331, 4946, 6229, 5757, 2238, 931, 3836, 68, 4945, 2429, 6232, 350, 1774, 5341, 4296]}]}
{"group_id": 564, "group_size": 4, "items": [{"qid": 3897, "question": "Which sentence compression technique works best? in Explicit Sentence Compression for Neural Machine Translation", "answer": ["ESC model", "semi-supervised ESC model ", "ESC model "], "top_k_doc_id": [6291, 6292, 6294, 6295, 111, 3559, 7617, 110, 5997, 7619, 5991, 5992, 6296, 4194, 7042], "orig_top_k_doc_id": [6295, 6291, 111, 6292, 6294, 3559, 5991, 5992, 6296, 110, 7619, 4194, 5997, 7042, 7617]}, {"qid": 3898, "question": "Do they compare performance against state of the art systems? in Explicit Sentence Compression for Neural Machine Translation", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6291, 6292, 6294, 6295, 111, 3559, 7617, 110, 5997, 7619, 7600, 1251, 1374, 2760, 3920], "orig_top_k_doc_id": [6295, 111, 6291, 7600, 6294, 6292, 1251, 7619, 5997, 1374, 7617, 2760, 3920, 3559, 110]}, {"qid": 3896, "question": "Which baselines to they compare to? in Explicit Sentence Compression for Neural Machine Translation", "answer": ["AllText, F8W", "AllText, F8W", "AllText, F8W"], "top_k_doc_id": [6291, 6292, 6294, 6295, 111, 3559, 7617, 1251, 7580, 1255, 7581, 3817, 7352, 7023, 4307], "orig_top_k_doc_id": [6295, 6291, 111, 6292, 6294, 1251, 7580, 1255, 7581, 3559, 3817, 7617, 7352, 7023, 4307]}, {"qid": 2629, "question": "What state-of-the-art compression techniques were used in the comparison? in Extreme Language Model Compression with Optimal Subwords and Shared Projections", "answer": ["baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)", "NoKD, PKD, BERTBASE teacher model"], "top_k_doc_id": [6291, 6292, 6294, 6295, 4624, 4627, 5992, 6293, 2783, 5991, 3558, 3650, 2784, 5994, 7042], "orig_top_k_doc_id": [4624, 4627, 6294, 5992, 6293, 2783, 6292, 5991, 6295, 3558, 3650, 2784, 5994, 7042, 6291]}]}
{"group_id": 565, "group_size": 4, "items": [{"qid": 3920, "question": "what are the performance metrics? in Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization", "answer": ["Rouge-1, Rouge-2 and Rouge-4 recall", "Rouge-1 recall, Rouge-2 recall, Rouge-4 recall", "Rouge-1, Rouge-2 and Rouge-4 recall"], "top_k_doc_id": [2339, 6325, 6326, 6327, 6571, 6925, 3137, 7281, 459, 6436, 7280, 1684, 7129, 6928, 3200], "orig_top_k_doc_id": [6327, 6325, 6326, 2339, 459, 6571, 6925, 7281, 6436, 7280, 1684, 7129, 6928, 3200, 3137]}, {"qid": 3921, "question": "what is the original model they refer to? in Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization", "answer": ["BIBREF0 , BIBREF6", "Original centroid-based model by BIBREF5", "it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection"], "top_k_doc_id": [2339, 6325, 6326, 6327, 6571, 6925, 3137, 7281, 4760, 1683, 5140, 730, 6495, 6496, 1682], "orig_top_k_doc_id": [6327, 6325, 6326, 2339, 4760, 6571, 7281, 1683, 5140, 730, 6925, 6495, 6496, 1682, 3137]}, {"qid": 3919, "question": "what state of the art methods are compared to? in Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization", "answer": ["CLASSY04, ICSI, Submodular, DPP, RegSum", "CLASSY04, ICSI, Submodular, DPP and RegSum.", "CLASSY04, ICSI, Submodular, DPP, RegSum"], "top_k_doc_id": [2339, 6325, 6326, 6327, 6571, 6925, 5140, 459, 4760, 7242, 1132, 1568, 3157, 6436, 1567], "orig_top_k_doc_id": [6327, 6325, 6326, 2339, 6571, 6925, 5140, 459, 4760, 7242, 1132, 1568, 3157, 6436, 1567]}, {"qid": 3922, "question": "how are sentences selected prior to making the summary? in Revisiting the Centroid-based Method: A Strong Baseline for Multi-Document Summarization", "answer": ["Using three algorithms: N-first, N-best and New-TF-IDF.", "Sentences are selected using 3 different greedy selection algorithms.", "All words in the vocabulary are ranked by their value in the centroid vector. Then the ranked list of sentences is de-queued in decreasing order."], "top_k_doc_id": [2339, 6325, 6326, 6327, 6571, 6925, 7281, 5396, 1255, 5140, 5143, 4382, 1254, 7280, 456], "orig_top_k_doc_id": [6325, 6326, 6327, 6571, 2339, 7281, 5396, 1255, 5140, 6925, 5143, 4382, 1254, 7280, 456]}]}
{"group_id": 566, "group_size": 4, "items": [{"qid": 3928, "question": "what datasets were used? in Polyglot Semantic Role Labeling", "answer": ["semantic role labeling portion of the CoNLL-2009 shared task BIBREF0", "CoNLL 2009 dataset", "semantic role labeling portion of the CoNLL-2009 shared task"], "top_k_doc_id": [3122, 6331, 6332, 6333, 3121, 3604, 4882, 2917, 4883, 5816, 6153, 7101, 3041, 3622, 780], "orig_top_k_doc_id": [6331, 6332, 6333, 3122, 4882, 3121, 6153, 4883, 3041, 2917, 3622, 780, 3604, 7101, 5816]}, {"qid": 3930, "question": "what languages are explored in this paper? in Polyglot Semantic Role Labeling", "answer": ["Catalan, Chinese, Czech, English, German, Japanese, Spanish", "Catalan, Chinese, Czech, English, German, Japanese, Spanish", " Catalan, Chinese, Czech, English, German, Japanese and Spanish"], "top_k_doc_id": [3122, 6331, 6332, 6333, 3121, 3604, 4882, 2917, 4883, 5816, 6153, 7101, 6036, 6881, 5965], "orig_top_k_doc_id": [6331, 6333, 6332, 3122, 2917, 7101, 6153, 6036, 3121, 5816, 6881, 4882, 5965, 4883, 3604]}, {"qid": 3929, "question": "what is the monolingual baseline? in Polyglot Semantic Role Labeling", "answer": ["For each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.", " basic model adapts the span-based dependency SRL model of He2017-deepsrl", "biLSTM with pre-trained GloVe embeddings."], "top_k_doc_id": [3122, 6331, 6332, 6333, 3121, 3604, 4882, 5357, 5360, 3603, 5359, 5358, 1782, 5361, 3622], "orig_top_k_doc_id": [6331, 6332, 6333, 5357, 3604, 5360, 3603, 5359, 5358, 4882, 3121, 3122, 1782, 5361, 3622]}, {"qid": 3927, "question": "what resources are combined to build the labeler? in Polyglot Semantic Role Labeling", "answer": ["multilingual word vectors, training data across languages", "a sequence of pretrained embeddings for the surface forms of the sentence tokens, annotations for a single predicate, CoNLL 2009 dataset", "multilingual word vectors, concatenate a language ID vector to each multilingual word embedding"], "top_k_doc_id": [3122, 6331, 6332, 6333, 3042, 2109, 5388, 4944, 7835, 2917, 5817, 7832, 3043, 3622, 7101], "orig_top_k_doc_id": [6331, 6332, 3042, 2109, 5388, 4944, 3122, 7835, 2917, 6333, 5817, 7832, 3043, 3622, 7101]}]}
{"group_id": 567, "group_size": 4, "items": [{"qid": 3934, "question": "Was the filtering based on fluency and domain relevance done automatically? in Question Generation from a Knowledge Base with Web Exploration", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [1091, 6338, 6339, 4637, 130, 5122, 6842, 107, 1887, 4858, 1123, 1323, 4841, 145, 1973], "orig_top_k_doc_id": [6338, 6339, 107, 4637, 130, 1091, 5122, 1887, 4858, 1123, 1323, 4841, 145, 6842, 1973]}, {"qid": 3935, "question": "How was domain relevance estimated? in Question Generation from a Knowledge Base with Web Exploration", "answer": ["For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.", "the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document", "we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$"], "top_k_doc_id": [1091, 6338, 6339, 4637, 130, 5122, 6842, 1121, 7315, 820, 7351, 6340, 4639, 1120, 7316], "orig_top_k_doc_id": [6338, 6339, 4637, 6842, 130, 1091, 1121, 7315, 820, 5122, 7351, 6340, 4639, 1120, 7316]}, {"qid": 3937, "question": "How was the fluency measured? in Question Generation from a Knowledge Base with Web Exploration", "answer": ["For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.", "$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count", "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count"], "top_k_doc_id": [1091, 6338, 6339, 4637, 1121, 7541, 1120, 1123, 1122, 491, 7180, 481, 1159, 107, 2262], "orig_top_k_doc_id": [6338, 1121, 6339, 7541, 1120, 1091, 1123, 1122, 4637, 491, 7180, 481, 1159, 107, 2262]}, {"qid": 3936, "question": "How many hand-crafted templates did they have to make? in Question Generation from a Knowledge Base with Web Exploration", "answer": ["269.", "269", "106, 163"], "top_k_doc_id": [1091, 6338, 6339, 2498, 5122, 815, 7351, 4656, 4077, 4658, 1122, 1120, 490, 7182, 1170], "orig_top_k_doc_id": [6338, 2498, 6339, 5122, 815, 1091, 7351, 4656, 4077, 4658, 1122, 1120, 490, 7182, 1170]}]}
{"group_id": 568, "group_size": 4, "items": [{"qid": 3963, "question": "What is their baseline? in Automated email Generation for Targeted Attacks using Natural Language", "answer": ["synthetic emails generated by Dada engine", "Dada engine BIBREF6", "Dada engine"], "top_k_doc_id": [4199, 34, 3596, 3616, 6410, 6411, 1707, 3446, 6863, 6864, 5558, 6412, 5351, 4739, 6416], "orig_top_k_doc_id": [6410, 6411, 34, 4199, 5558, 6863, 3616, 6412, 5351, 1707, 3596, 6864, 3446, 4739, 6416]}, {"qid": 3965, "question": "Do they compare to previous work? in Automated email Generation for Targeted Attacks using Natural Language", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [4199, 34, 3596, 3616, 6410, 6411, 1707, 3446, 6863, 6864, 5646, 3445, 1726, 4130, 1597], "orig_top_k_doc_id": [6410, 6411, 6863, 6864, 4199, 3446, 5646, 34, 3445, 1707, 3596, 1726, 3616, 4130, 1597]}, {"qid": 3964, "question": "Is human evaluation of the malicious content performed? in Automated email Generation for Targeted Attacks using Natural Language", "answer": ["No", "No", "No"], "top_k_doc_id": [4199, 34, 3596, 3616, 6410, 6411, 6415, 6413, 6414, 6416, 6412, 1726, 3594, 1706, 6770], "orig_top_k_doc_id": [6410, 6411, 6415, 6413, 6414, 3596, 6416, 6412, 4199, 1726, 34, 3616, 3594, 1706, 6770]}, {"qid": 2493, "question": "Which strategies show the most promise in deterring these attacks? in Towards a Robust Deep Neural Network in Text Domain A Survey", "answer": ["At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples."], "top_k_doc_id": [4199, 5351, 4208, 6590, 5646, 5291, 280, 6863, 227, 6256, 1957, 2700, 3566, 4131, 1053], "orig_top_k_doc_id": [5351, 4208, 4199, 6590, 5646, 5291, 280, 6863, 227, 6256, 1957, 2700, 3566, 4131, 1053]}]}
{"group_id": 569, "group_size": 4, "items": [{"qid": 3991, "question": "What is the BM25 baseline? in Universal Text Representation from BERT: An Empirical Study", "answer": ["No", "No", "No"], "top_k_doc_id": [5100, 6448, 6449, 6450, 2216, 7630, 1798, 6847, 6846, 6842, 7129, 3167, 7315, 7551, 3968], "orig_top_k_doc_id": [6450, 6449, 6448, 1798, 6847, 6846, 6842, 7129, 3167, 5100, 7315, 2216, 7551, 7630, 3968]}, {"qid": 3994, "question": "Which four QA datasets are examined? in Universal Text Representation from BERT: An Empirical Study", "answer": ["(1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, (4) SearchQA BIBREF16", "WikiPassageQA, InsuranceQA, Quasar-t, SearchQA", "WikiPassageQA, InsuranceQA , Quasar-t , SearchQA"], "top_k_doc_id": [5100, 6448, 6449, 6450, 2216, 7630, 1798, 6166, 4277, 4698, 3579, 1141, 1908, 6167, 5377], "orig_top_k_doc_id": [6448, 6450, 6449, 1798, 6166, 7630, 2216, 4277, 4698, 3579, 5100, 1141, 1908, 6167, 5377]}, {"qid": 3992, "question": "Which BERT layers were combined to boost performance? in Universal Text Representation from BERT: An Empirical Study", "answer": ["Top and bottom layers", " the top and bottom layer of the BERT fine-tuned on SNLI dataset", "combining the top and bottom layer embeddings"], "top_k_doc_id": [5100, 6448, 6449, 6450, 2216, 7630, 4833, 6401, 3276, 7005, 2048, 7141, 5292, 4603, 4289], "orig_top_k_doc_id": [6450, 6448, 6449, 4833, 7630, 6401, 3276, 7005, 5100, 2216, 2048, 7141, 5292, 4603, 4289]}, {"qid": 3990, "question": "Do they report results only on English data? in Universal Text Representation from BERT: An Empirical Study", "answer": ["No", "No", "No"], "top_k_doc_id": [5100, 6448, 6449, 6450, 1798, 2806, 5621, 6870, 3713, 5710, 6166, 3968, 6853, 4790, 2679], "orig_top_k_doc_id": [6448, 1798, 2806, 6450, 5100, 5621, 6870, 3713, 5710, 6166, 3968, 6853, 4790, 2679, 6449]}]}
{"group_id": 570, "group_size": 4, "items": [{"qid": 4016, "question": "what was their accuracy result? in Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis", "answer": ["No", "No"], "top_k_doc_id": [5374, 5375, 6181, 6471, 6472, 6473, 1656, 1657, 3129, 5376, 6053, 2109, 2874, 1905, 3578], "orig_top_k_doc_id": [6472, 6473, 6471, 1905, 6053, 5375, 6181, 5374, 2874, 3578, 1656, 1657, 3129, 5376, 2109]}, {"qid": 4019, "question": "what dataset was used? in Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis", "answer": ["SNAP Amazon Dataset , Bing Liu's dataset", "Bing Liu's dataset", "SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20"], "top_k_doc_id": [5374, 5375, 6181, 6471, 6472, 6473, 1656, 1657, 3129, 5376, 6053, 2109, 2874, 1659, 6640], "orig_top_k_doc_id": [6472, 6471, 6473, 5375, 6053, 5374, 5376, 6181, 1656, 3129, 1659, 1657, 6640, 2874, 2109]}, {"qid": 4018, "question": "what was the baseline? in Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis", "answer": ["No", "No", "No"], "top_k_doc_id": [5374, 5375, 6181, 6471, 6472, 6473, 1656, 1657, 3129, 5376, 6053, 6183, 1905, 5390, 1053], "orig_top_k_doc_id": [6472, 6471, 6473, 6053, 5375, 1656, 5374, 6183, 3129, 6181, 5376, 1905, 1657, 5390, 1053]}, {"qid": 4017, "question": "what domain do the opinions fall under? in Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis", "answer": ["computers, wireless routers, speakers", "computers, wireless routers, speakers ", "computers, wireless routers, and speakers"], "top_k_doc_id": [5374, 5375, 6181, 6471, 6472, 6473, 2215, 1039, 6640, 447, 6183, 4579, 2874, 597, 3578], "orig_top_k_doc_id": [6472, 6471, 2215, 1039, 6640, 6181, 6473, 5374, 447, 6183, 4579, 2874, 5375, 597, 3578]}]}
{"group_id": 571, "group_size": 4, "items": [{"qid": 4188, "question": "What is the established approach used for comparison? in Unsupervised Domain Clusters in Pretrained Language Models", "answer": ["method of BIBREF4", "established method for data selection was proposed by BIBREF4"], "top_k_doc_id": [678, 6656, 6657, 6659, 6660, 6661, 3005, 4620, 4049, 5711, 2315, 2165, 3069, 2226, 3651], "orig_top_k_doc_id": [6656, 6661, 6659, 6657, 6660, 678, 5711, 2315, 4620, 2165, 3005, 4049, 3069, 2226, 3651]}, {"qid": 4190, "question": "Which pre-trained language models are used? in Unsupervised Domain Clusters in Pretrained Language Models", "answer": ["BERT, DistilBERT, RoBERTa", "BERT, DistilBERT, RoBERTa, GPT-2, XLNet", "BERT, DistilBERT, RoBERTa, GPT-2, XLNet"], "top_k_doc_id": [678, 6656, 6657, 6659, 6660, 6661, 3005, 4620, 4049, 7853, 2162, 4678, 2324, 2876, 3617], "orig_top_k_doc_id": [6656, 6661, 6657, 6659, 3005, 6660, 678, 4049, 7853, 2162, 4678, 4620, 2324, 2876, 3617]}, {"qid": 4189, "question": "What are the five domains? in Unsupervised Domain Clusters in Pretrained Language Models", "answer": ["subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software)", "subtitles, medical, legal, Koran, IT", "subtitles, medical text, legal text, translations of the Koran, IT-related text"], "top_k_doc_id": [678, 6656, 6657, 6659, 6660, 6661, 3005, 4620, 6658, 2951, 3160, 7282, 322, 5711, 2318], "orig_top_k_doc_id": [6656, 6657, 6661, 678, 6659, 6658, 4620, 3005, 2951, 3160, 6660, 7282, 322, 5711, 2318]}, {"qid": 4187, "question": "How much improvement is there in the BLEU score? in Unsupervised Domain Clusters in Pretrained Language Models", "answer": ["Average SacreBLEU score accross all domains is improved from 40.88 to 41.26.", "On average the three selection methods had better BLEU scores than Random and Oracle methods. \nThe proposed method Domain-Finetune-Top-500k had better BLEU score than random by 4.34, better than Moore-Lewis by 0.38, better than Oracle by 0.92, and better than All method by 1.4"], "top_k_doc_id": [678, 6656, 6657, 6659, 6660, 6661, 6312, 7852, 3809, 7853, 3002, 2226, 6294, 2794, 5775], "orig_top_k_doc_id": [6659, 6661, 6657, 6656, 6312, 7852, 3809, 7853, 3002, 2226, 6660, 678, 6294, 2794, 5775]}]}
{"group_id": 572, "group_size": 4, "items": [{"qid": 4191, "question": "Do they report results only on English data? in Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection", "answer": ["Yes", "No", "No"], "top_k_doc_id": [3927, 5321, 5325, 6663, 6664, 6665, 6666, 6667, 5326, 6107, 3860, 2157, 2158, 5783, 3015], "orig_top_k_doc_id": [6667, 6663, 6666, 6664, 6665, 5321, 6107, 5325, 3860, 2157, 2158, 3927, 5783, 3015, 5326]}, {"qid": 4192, "question": "What are the hyperparameter setting of the MTL model? in Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection", "answer": ["size of word embeddings is 200, size of position embedding is 100, the number of attention heads in transformer block is 6, the number of attention block Is 2, dropout of multi-head attention is 0.7, minibatch size is 64, the initiall learning rate is .001. In fake news detection, the dropout rate is 0.3 and lambda is 0.6.", "The sizes of word embeddings and position embeddings are set to 200 and 100, attention heads and blocks are set to 6 and 2 respectively, and the dropout of multi-head attention is set to 0.7,  the minibatch size is 64, the initial learning rate is set to 0.001, the dropout rate to 0.3, and $\\lambda $ to 0.6 for fake news detection.", "Size of word embeddings is 200, size of position embeddings is 100, 6 attention heads and 2 blocks in encoder, dropout in multi-head attention is 0.7, minibatch size is 64, initial learning rate is 0.001, dropout rate is 0.3, lambda is 0.6."], "top_k_doc_id": [3927, 5321, 5325, 6663, 6664, 6665, 6666, 6667, 1318, 2556, 3416, 3344, 7441, 7439, 1320], "orig_top_k_doc_id": [6666, 6667, 6663, 6664, 1318, 6665, 5321, 3416, 3344, 2556, 7441, 7439, 5325, 3927, 1320]}, {"qid": 4193, "question": "What architecture does the rest of the multi-task learning setup use? in Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection", "answer": ["shared features in the shared layer are equally sent to their respective tasks without filtering", "transformer"], "top_k_doc_id": [3927, 5321, 5325, 6663, 6664, 6665, 6666, 6667, 1318, 2556, 6058, 3860, 5611, 3015, 7120], "orig_top_k_doc_id": [6666, 6667, 6663, 6664, 6665, 1318, 5321, 2556, 3927, 5325, 6058, 3860, 5611, 3015, 7120]}, {"qid": 4194, "question": "How is the selected sharing layer trained? in Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection", "answer": ["The selected sharing layer is trained jointly on the tasks of stance detection and fake news detection", "By jointly training the tasks of stance and fake news detection."], "top_k_doc_id": [3927, 5321, 5325, 6663, 6664, 6665, 6666, 6667, 5326, 32, 2556, 6455, 1880, 2557, 3273], "orig_top_k_doc_id": [6667, 6663, 6666, 6664, 6665, 5325, 5321, 5326, 32, 2556, 3927, 6455, 1880, 2557, 3273]}]}
{"group_id": 573, "group_size": 4, "items": [{"qid": 4199, "question": "What baseline did they use? in Unsupervised Question Answering for Fact-Checking", "answer": ["we compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF", "HexaF", "HexaF - UCL "], "top_k_doc_id": [1217, 3483, 3487, 6676, 6677, 6678, 2661, 3485, 3019, 3853, 4900, 7675, 2900, 3486, 7572], "orig_top_k_doc_id": [6676, 2661, 6678, 3485, 3487, 6677, 1217, 3019, 2900, 3483, 3486, 3853, 4900, 7572, 7675]}, {"qid": 4200, "question": "What is the threshold? in Unsupervised Question Answering for Fact-Checking", "answer": ["0.76, 0.67", "0.76 suggests that at least 3 out of the 4 questions have to be answered correctly, 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly"], "top_k_doc_id": [1217, 3483, 3487, 6676, 6677, 6678, 2661, 3485, 3019, 3853, 4900, 7675, 322, 7402, 1637], "orig_top_k_doc_id": [6676, 6677, 6678, 3487, 3483, 3853, 2661, 4900, 7675, 322, 3485, 1217, 7402, 1637, 3019]}, {"qid": 4201, "question": "How was the masking done? in Unsupervised Question Answering for Fact-Checking", "answer": ["The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "No", "similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank"], "top_k_doc_id": [1217, 3483, 3487, 6676, 6677, 6678, 2661, 3485, 2900, 7002, 559, 3486, 533, 2455, 1973], "orig_top_k_doc_id": [6678, 6676, 6677, 3487, 2900, 7002, 559, 3483, 3486, 1217, 533, 3485, 2455, 2661, 1973]}, {"qid": 4202, "question": "How large is the FEVER dataset? in Unsupervised Question Answering for Fact-Checking", "answer": ["around 185k claims from the corpus of 5.4M Wikipedia articles", "185k claims"], "top_k_doc_id": [1217, 3483, 3487, 6676, 6677, 6678, 2457, 2455, 6740, 6743, 3287, 6745, 6741, 2459, 2900], "orig_top_k_doc_id": [6676, 6677, 6678, 2457, 2455, 6740, 6743, 3287, 6745, 6741, 2459, 3487, 1217, 3483, 2900]}]}
{"group_id": 574, "group_size": 4, "items": [{"qid": 4213, "question": "Which downstream tasks are used for evaluation in this paper? in Alternative Weighting Schemes for ELMo Embeddings", "answer": ["Argument component detection, ACE Entities/Events, POS, Chunking, WNUT16, CoNLL 2003 shared task on named entity recognition, GENIA NER", "Various sequence tagging tasks: Argument detection, ACE entity and event detection, part-of-speech tagging, CoNLL chunking, CoNLL named entity recognition, GENIA bio-entity recognition, WNUT named entity recognition. They also evaluate on Stanford Sentiment Treebank, Penn TreeBank constituency parsing, and Stanford Natural Language Inference."], "top_k_doc_id": [5051, 2241, 4514, 5183, 6697, 6698, 6699, 5184, 5185, 2850, 5048, 2682, 5693, 4968, 4780], "orig_top_k_doc_id": [6697, 6699, 6698, 5183, 5184, 5185, 2241, 2850, 5048, 2682, 4514, 5693, 4968, 5051, 4780]}, {"qid": 4214, "question": "Which datasets are used for evaluation? in Alternative Weighting Schemes for ELMo Embeddings", "answer": ["Argument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus", "Arguments, ACE 2005 dataset, part-of-speech tags from Universal Dependencies v. 1.3 for English, CoNLL 2000 shared task dataset on chunking, CoNLL 2003 shared task on named entity recognition, GENIA NER, WNUT16", "For the first experiment, the datasets used were: argument component detection persuasive essays, ACE 2005 dataset of entities/essays, POS tags from Universal Dependencies, CoNLL 2000 shared task on chunking, CoNLL 2003\nshared task on named entity recognition, the Bio-Entity Recognition Task dataset, WNUT 16 dataset on NER over tweets. For the second experiment, they used the CoNLL 2003 NER\ndataset, the Stanford Sentiment Treebank (SST5) dataset, the constituency parsing model for the\nPenn TreeBank as dataset, and the Stanford Natural Language Inference Corpus (SNLI) dataset."], "top_k_doc_id": [5051, 2241, 4514, 5183, 6697, 6698, 6699, 3416, 4969, 3610, 5695, 3420, 3417, 5690, 2657], "orig_top_k_doc_id": [6697, 6699, 6698, 2241, 4514, 3416, 4969, 3610, 5695, 5183, 3420, 5051, 3417, 5690, 2657]}, {"qid": 4270, "question": "What languages are evaluated? in Subword ELMo", "answer": ["No", "No", "No"], "top_k_doc_id": [5051, 710, 712, 713, 6767, 6768, 6769, 1064, 7042, 709, 1063, 5344, 6785, 1061, 5343], "orig_top_k_doc_id": [6768, 6767, 710, 6769, 712, 1064, 7042, 5051, 709, 1063, 5344, 6785, 713, 1061, 5343]}, {"qid": 4271, "question": "Does the training of ESuLMo take longer compared to ELMo? in Subword ELMo", "answer": ["No", "No"], "top_k_doc_id": [5051, 710, 712, 713, 6767, 6768, 6769, 3499, 6699, 4649, 5050, 5185, 6698, 6697, 3420], "orig_top_k_doc_id": [6768, 6769, 6767, 710, 713, 5051, 712, 3499, 6699, 4649, 5050, 5185, 6698, 6697, 3420]}]}
{"group_id": 575, "group_size": 4, "items": [{"qid": 4215, "question": "What does the human-in-the-loop do to help their system? in Generating Clues for Gender based Occupation De-biasing in Text", "answer": ["identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it", "appropriately modify the text to create an unbiased version", "modify the text to create an unbiased version"], "top_k_doc_id": [717, 1443, 1445, 1446, 6700, 6702, 521, 5191, 7065, 522, 6701, 718, 7061, 6438, 3549], "orig_top_k_doc_id": [6700, 1446, 6702, 717, 1445, 6701, 522, 1443, 718, 5191, 7061, 6438, 521, 7065, 3549]}, {"qid": 4216, "question": "Which dataset do they use to train their model? in Generating Clues for Gender based Occupation De-biasing in Text", "answer": ["A dataset they created that contains occupation and names data.", "1) Occupation Data, 2) Names Data"], "top_k_doc_id": [717, 1443, 1445, 1446, 6700, 6702, 521, 5191, 7065, 522, 6701, 3414, 5154, 5349, 5194], "orig_top_k_doc_id": [6700, 6702, 1446, 1445, 1443, 717, 6701, 3414, 522, 5154, 521, 7065, 5191, 5349, 5194]}, {"qid": 4218, "question": "How do they evaluate their de-biasing approach? in Generating Clues for Gender based Occupation De-biasing in Text", "answer": ["No", "No", "No"], "top_k_doc_id": [717, 1443, 1445, 1446, 6700, 6702, 521, 5191, 7065, 1419, 5169, 7772, 5154, 293, 7271], "orig_top_k_doc_id": [6700, 1446, 6702, 1443, 1445, 1419, 717, 7065, 5169, 7772, 5191, 5154, 521, 293, 7271]}, {"qid": 4217, "question": "Can their approach be extended to eliminate racial or ethnic biases? in Generating Clues for Gender based Occupation De-biasing in Text", "answer": ["No", "No"], "top_k_doc_id": [717, 1443, 1445, 1446, 6700, 6702, 5155, 718, 3007, 3988, 6560, 5154, 5525, 3045, 7690], "orig_top_k_doc_id": [6700, 6702, 5155, 1445, 717, 1443, 1446, 718, 3007, 3988, 6560, 5154, 5525, 3045, 7690]}]}
{"group_id": 576, "group_size": 4, "items": [{"qid": 4229, "question": "What sources of less sensitive data are available? in A Short Review of Ethical Challenges in Clinical Natural Language Processing", "answer": ["MIMICII(I), THYME, results from i2b2 and ShARe/CLEF shared task, MiPACQ, Blulab, EMC Dutch Clinical Corpus, 2010 i2b2/VA, VetCompass", "deceased persons, surrogate data, derived data, veterinary texts", "personal health information of deceased persons, surrogate data, derived data. Data that can not be used to reconstruct the original text, veterinary texts"], "top_k_doc_id": [3752, 6711, 3431, 3581, 3582, 4609, 5906, 6209, 6712, 6713, 6714, 6207, 19, 2182, 5907], "orig_top_k_doc_id": [6711, 6713, 6712, 6209, 3582, 4609, 6207, 6714, 3581, 5906, 3431, 19, 2182, 3752, 5907]}, {"qid": 4230, "question": "Other than privacy, what are the other major ethical challenges in clinical data? in A Short Review of Ethical Challenges in Clinical Natural Language Processing", "answer": ["Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models., Clinical texts may include bias coming from both patient's and clinician's reporting., prejudices held by healthcare practitioners which may impact patients' perceptions, communication difficulties in the case of ethnic differences, Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports, Dual use", "sampling bias, unfair treatment due to biased data,  incomplete clinical stories, and reflection of health disparities."], "top_k_doc_id": [3752, 6711, 3431, 3581, 3582, 4609, 5906, 6209, 6712, 6713, 6714, 3591, 7261, 4646, 7832], "orig_top_k_doc_id": [6711, 6209, 6713, 6712, 4609, 3582, 6714, 3581, 3591, 5906, 3752, 7261, 4646, 3431, 7832]}, {"qid": 3428, "question": "Does the paper explore extraction from electronic health records? in A Biomedical Information Extraction Primer for NLP Researchers", "answer": ["Yes"], "top_k_doc_id": [3752, 6711, 3431, 5674, 7250, 3076, 5396, 4646, 2127, 6194, 19, 6985, 4867, 7832, 5489], "orig_top_k_doc_id": [5674, 3752, 6711, 7250, 3076, 5396, 4646, 2127, 6194, 19, 6985, 4867, 7832, 5489, 3431]}, {"qid": 2347, "question": "What are the categories being extracted? in An Interactive Tool for Natural Language Processing on Clinical Text", "answer": ["No"], "top_k_doc_id": [3752, 6711, 19, 3758, 3756, 7096, 2127, 88, 5257, 7097, 6317, 5818, 5131, 3669, 7091], "orig_top_k_doc_id": [3752, 19, 3758, 3756, 7096, 2127, 6711, 88, 5257, 7097, 6317, 5818, 5131, 3669, 7091]}]}
{"group_id": 577, "group_size": 4, "items": [{"qid": 4235, "question": "How many layers does the neural network have? in Experiments in Detecting Persuasion Techniques in the News", "answer": ["No", "No"], "top_k_doc_id": [808, 6729, 715, 27, 2157, 3926, 6555, 7232, 1497, 714, 1501, 5168, 5326, 3927, 5321], "orig_top_k_doc_id": [6729, 3926, 27, 7232, 808, 6555, 1497, 714, 1501, 715, 5168, 5326, 3927, 5321, 2157]}, {"qid": 4238, "question": "Do they look at various languages? in Experiments in Detecting Persuasion Techniques in the News", "answer": ["No", "No"], "top_k_doc_id": [808, 6729, 715, 27, 2157, 3926, 6555, 105, 5378, 1280, 1725, 3878, 6971, 3277, 4942], "orig_top_k_doc_id": [6729, 808, 105, 27, 2157, 5378, 1280, 1725, 3878, 3926, 6971, 3277, 6555, 715, 4942]}, {"qid": 4236, "question": "Which BERT-based baselines do they compare to? in Experiments in Detecting Persuasion Techniques in the News", "answer": ["BERT. We add a linear layer on top of BERT and we fine-tune it, BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b)., BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC", "BERT, BERT-Joint, BERT-Granularity", "BERT with one separately trained linear layer for each of the two tasks, BERT-Joint, which trains a layer for both tasks jointly, BERT-Granularity,  a modification of BERT-Joint which transfers information from the less granular task to the more granular task. "], "top_k_doc_id": [808, 6729, 715, 105, 3446, 4942, 6731, 1497, 2241, 5611, 3273, 4790, 3277, 1494, 5930], "orig_top_k_doc_id": [6729, 105, 808, 3446, 4942, 6731, 1497, 2241, 5611, 3273, 4790, 3277, 715, 1494, 5930]}, {"qid": 4239, "question": "What datasets did they use in their experiment? in Experiments in Detecting Persuasion Techniques in the News", "answer": ["retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques", "A dataset of news articles from different news outlets collected by the authors.", "451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4"], "top_k_doc_id": [808, 6729, 3926, 5198, 105, 3878, 1499, 5374, 6633, 7259, 3784, 5547, 873, 1278, 1500], "orig_top_k_doc_id": [3926, 808, 5198, 105, 6729, 3878, 1499, 5374, 6633, 7259, 3784, 5547, 873, 1278, 1500]}]}
{"group_id": 578, "group_size": 4, "items": [{"qid": 4279, "question": "How is the model evaluated against the original recursive training algorithm? in Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning", "answer": ["The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.", "We perform an error analysis, with the purpose of gaining more insight into the ability of the methods to model particular aspects of morphology.", "boundary precision, boundary recall,  boundary $F_{1}$-score", "Morfessor EM+Prune configuration significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi. Morfessor EM+Prune is less responsive to tuning than Morfessor Baseline."], "top_k_doc_id": [2680, 6782, 6783, 6786, 6767, 6768, 6784, 6785, 775, 776, 1553, 6890, 5303, 5302, 923], "orig_top_k_doc_id": [6782, 6783, 6786, 6785, 6784, 6767, 6768, 6890, 5303, 2680, 1553, 5302, 923, 775, 776]}, {"qid": 4281, "question": "What is the improvement in performance brought by lexicon pruning on a simple EM algorithm? in Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning", "answer": ["No"], "top_k_doc_id": [2680, 6782, 6783, 6786, 6767, 6768, 6784, 6785, 775, 776, 1553, 6890, 22, 2681, 23], "orig_top_k_doc_id": [6782, 6783, 6786, 6784, 6785, 2680, 6767, 6768, 22, 2681, 776, 23, 775, 6890, 1553]}, {"qid": 4280, "question": "What is the improvement in performance compared to the linguistic gold standard? in Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning", "answer": ["Proposed approach is best in:\n- Recall English: +3.47 (70.84 compared to next best 67.37)\n- Precision Finnish: +6.16 (68.18 compared to 62.02)\n- Recall NorthSami: +1.44 (62.84 compared to 61.40)", " For English and North S\u00e1mi, EM+Prune results in less under-segmentation but worse over-segmentation.  For Finnish these results are reversed."], "top_k_doc_id": [2680, 6782, 6783, 6786, 6767, 6768, 6784, 6785, 650, 5303, 5302, 7044, 6253, 5963, 2681], "orig_top_k_doc_id": [6782, 6786, 6784, 6783, 6785, 6767, 6768, 650, 2680, 5303, 5302, 7044, 6253, 5963, 2681]}, {"qid": 4967, "question": "what pruning did they perform? in It was the training data pruning too!", "answer": ["eliminate spurious training data entries", "separate algorithm for pruning out spurious logical forms using fictitious tables"], "top_k_doc_id": [2680, 6782, 6783, 6786, 3042, 2682, 7735, 2681, 7734, 2199, 7603, 4309, 4274, 2679, 2198], "orig_top_k_doc_id": [3042, 2682, 7735, 2680, 2681, 7734, 2199, 7603, 4309, 4274, 6783, 2679, 6786, 2198, 6782]}]}
{"group_id": 579, "group_size": 4, "items": [{"qid": 4303, "question": "what dataset was used? in PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese", "answer": ["CoNLL_X bosque data, News data by Lusa agency, Sports news data", "News, Sports news", "News, Sports news"], "top_k_doc_id": [2732, 5816, 6810, 6811, 6812, 6813, 6814, 2731, 5879, 4967, 6151, 6153, 6050, 7287, 1773], "orig_top_k_doc_id": [6811, 6810, 6814, 6813, 6812, 5816, 2732, 6151, 6050, 6153, 5879, 7287, 4967, 1773, 2731]}, {"qid": 4306, "question": "how many rules did they use? in PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese", "answer": ["No"], "top_k_doc_id": [2732, 5816, 6810, 6811, 6812, 6813, 6814, 2731, 5879, 4967, 6151, 6153, 6144, 1774, 2321], "orig_top_k_doc_id": [6810, 6811, 6814, 6812, 6813, 5816, 6151, 2732, 6153, 6144, 2731, 5879, 1774, 2321, 4967]}, {"qid": 4304, "question": "by how much did their model improve over current alternatives? in PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese", "answer": ["On average, it had better Recall by 0.481 in case of news dataset and by 0.372 in case of sports news dataset. \nOn average, it had better Precision by 0.086 in case of news dataset and by 0.37 in case of sports news dataset. \nOn average, it had better F1 by 0.381 in case of news dataset and by 0.616 in case of sports news dataset. ", "Pampo had F1 score of 0.932 and 0.971 compared to best alternative result of 0.608 and 0.794 on News and Sport news dataset respectively."], "top_k_doc_id": [2732, 5816, 6810, 6811, 6812, 6813, 6814, 2731, 5879, 6777, 2321, 6144, 1774, 4790, 633], "orig_top_k_doc_id": [6811, 6810, 6814, 6813, 6812, 5816, 2732, 2731, 6777, 2321, 6144, 1774, 4790, 5879, 633]}, {"qid": 4305, "question": "did they experiment with other languages besides portuguese? in PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese", "answer": ["No", "No", "No"], "top_k_doc_id": [2732, 5816, 6810, 6811, 6812, 6813, 6814, 633, 5210, 6153, 5209, 5985, 6280, 5984, 6151], "orig_top_k_doc_id": [6811, 6810, 6814, 6812, 6813, 5816, 633, 5210, 6153, 5209, 5985, 6280, 2732, 5984, 6151]}]}
{"group_id": 580, "group_size": 4, "items": [{"qid": 4312, "question": "What is the performance of their model? in Text Understanding with the Attention Sum Reader Network", "answer": ["CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%, In named entity prediction our best single model with accuracy of 68.6%", "The different AS Reader models had average test accuracy of 71,35% and AS Reader (avg ensemble) had the highest test accuracy between all tested models with 75.4%\n\nIn case of Daily Mail average was  75.55% and greedy assemble had the highest value with 77.7%\nCBT NE average was 69.65% and greedy ensemble had the highest value of 71% \n\nCBT CN had average of 65.5% and avg assemble had the highest value of 68.9%\n"], "top_k_doc_id": [1357, 1373, 2752, 6824, 1372, 2612, 3417, 6825, 1500, 5609, 5998, 6879, 1374, 6460, 1501], "orig_top_k_doc_id": [1373, 6824, 3417, 1500, 1374, 1357, 2612, 6825, 2752, 6879, 5609, 6460, 1372, 5998, 1501]}, {"qid": 4314, "question": "What datasets is the model evaluated on? in Text Understanding with the Attention Sum Reader Network", "answer": ["CNN , Daily Mail,  CBT CN and NE", "CNN, Daily Mail and CBT", "CNN, Daily Mail, Children's Book Test"], "top_k_doc_id": [1357, 1373, 2752, 6824, 1372, 2612, 3417, 6825, 1500, 5609, 5998, 6879, 5369, 2012, 2757], "orig_top_k_doc_id": [6825, 1373, 1500, 5369, 6824, 2752, 1357, 3417, 5998, 5609, 2012, 6879, 2757, 1372, 2612]}, {"qid": 4311, "question": "Which datasets did they use to train the model? in Text Understanding with the Attention Sum Reader Network", "answer": ["CNN, Daily Mail, Children's Book Test", "CNN , Daily Mail, CBT CN and NE"], "top_k_doc_id": [1357, 1373, 2752, 6824, 1372, 2612, 3417, 6825, 2012, 2757, 3416, 5369, 1374, 4740, 1640], "orig_top_k_doc_id": [6825, 1373, 2752, 6824, 3417, 2012, 2757, 3416, 2612, 5369, 1374, 1372, 1357, 4740, 1640]}, {"qid": 4313, "question": "What baseline do they compare against? in Text Understanding with the Attention Sum Reader Network", "answer": ["Attentive and Impatient Readers , Chen et al. 2016\n, MenNN, Dynamic Entity Representation , LSTM ", "No"], "top_k_doc_id": [1357, 1373, 2752, 6824, 5609, 1640, 1374, 37, 4267, 1494, 2649, 4740, 1257, 7823, 6853], "orig_top_k_doc_id": [6824, 1373, 5609, 1640, 2752, 1374, 1357, 37, 4267, 1494, 2649, 4740, 1257, 7823, 6853]}]}
{"group_id": 581, "group_size": 4, "items": [{"qid": 4319, "question": "Do they authors offer a hypothesis for why Twitter data makes better predictions about the inventory of languages used in each country? in Mapping Languages and Demographics with Georeferenced Corpora", "answer": ["Yes", "Twitter data has fewer missing languages than what census-based data contains because it matches populations better when they are weighting by GDP"], "top_k_doc_id": [6833, 6834, 6835, 6836, 1419, 2534, 4003, 6837, 4004, 3135, 5871, 5272, 3007, 68, 7669], "orig_top_k_doc_id": [6835, 6833, 6834, 6836, 6837, 4004, 3135, 5871, 2534, 5272, 4003, 3007, 1419, 68, 7669]}, {"qid": 4322, "question": "What countries and languages are represented in the datasets? in Mapping Languages and Demographics with Georeferenced Corpora", "answer": ["No", "English, Spanish, Russian, Serbo-Croatian, Mandarin, German, French, Slovenian, Portuguese, Finnish, Bulgarian, Arabic, Indonesian, Latvian, Estonian, Slovak, Azerbaijani, Romanina, Icelandic, Italian, among others."], "top_k_doc_id": [6833, 6834, 6835, 6836, 1419, 2534, 4003, 6837, 2798, 3586, 1901, 6838, 2797, 6855, 2533], "orig_top_k_doc_id": [6835, 6833, 6834, 6836, 2798, 6837, 2534, 3586, 1901, 6838, 2797, 1419, 6855, 4003, 2533]}, {"qid": 4320, "question": "What social media platforms are represented? in Mapping Languages and Demographics with Georeferenced Corpora", "answer": ["Twitter", "Twitter ", "Twitter"], "top_k_doc_id": [6833, 6834, 6835, 6836, 1901, 5907, 3586, 2798, 520, 3131, 6896, 3135, 3622, 7028, 243], "orig_top_k_doc_id": [6835, 6833, 6834, 1901, 5907, 3586, 6836, 2798, 520, 3131, 6896, 3135, 3622, 7028, 243]}, {"qid": 4321, "question": "Which websites were used in the web crawl? in Mapping Languages and Demographics with Georeferenced Corpora", "answer": ["81.5 billion web pages covered in Common Crawl dataset", "web-crawled data from the Common Crawl"], "top_k_doc_id": [6833, 6834, 6835, 6836, 2447, 3748, 5049, 6402, 7241, 2793, 4003, 1900, 6401, 3664, 4004], "orig_top_k_doc_id": [6833, 6835, 6834, 6836, 2447, 3748, 5049, 6402, 7241, 2793, 4003, 1900, 6401, 3664, 4004]}]}
{"group_id": 582, "group_size": 4, "items": [{"qid": 4347, "question": "How many abstractive summarizations exist for each dialogue? in SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization", "answer": ["No", "Each dialogue contains only one reference summary."], "top_k_doc_id": [6858, 6860, 6862, 1132, 3158, 2435, 3715, 4619, 6861, 7241, 6036, 1138, 2420, 4380, 1971], "orig_top_k_doc_id": [6858, 6862, 6860, 2435, 1132, 7241, 3715, 4619, 6861, 6036, 3158, 1138, 2420, 4380, 1971]}, {"qid": 4349, "question": "What models have been evaluated? in SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization", "answer": ["MIDDLE-n, LONGEST-n, LONGER-THAN-n and MOST-ACTIVE-PERSON are the baselines, and experiments also carried out on Pointer generator networks, Transformers, Fast Abs RL, Fast Abs RL Enhanced, LightConv and DynamicConv ", "Pointer generator network, Transformer, Fast Abs RL, Fast Abs RL Enhanced, LightConv and DynamicConv"], "top_k_doc_id": [6858, 6860, 6862, 1132, 3158, 2435, 3715, 4619, 6861, 111, 3718, 5542, 3157, 2339, 7243], "orig_top_k_doc_id": [6858, 6862, 6860, 6861, 1132, 111, 3718, 3158, 4619, 5542, 3157, 2339, 3715, 7243, 2435]}, {"qid": 4351, "question": "How big is SAMSum Corpus? in SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization", "answer": ["16369 conversations", "contains over 16k chat dialogues with manually annotated summaries"], "top_k_doc_id": [6858, 6860, 6862, 1132, 3158, 1135, 1694, 3157, 4380, 7243, 7241, 3718, 1138, 6492, 5142], "orig_top_k_doc_id": [6858, 6862, 6860, 1132, 1135, 1694, 3158, 3157, 4380, 7243, 7241, 3718, 1138, 6492, 5142]}, {"qid": 4350, "question": "Do authors propose some better metric than ROUGE for measurement of abstractive dialogue summarization? in SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization", "answer": ["No", "No"], "top_k_doc_id": [6858, 6860, 6862, 6861, 111, 1135, 3718, 3715, 4829, 4478, 3716, 7243, 1138, 4482, 3717], "orig_top_k_doc_id": [6858, 6862, 6860, 6861, 111, 1135, 3718, 3715, 4829, 4478, 3716, 7243, 1138, 4482, 3717]}]}
{"group_id": 583, "group_size": 4, "items": [{"qid": 4366, "question": "How well does their model perform on the recommendation task? in Iterative Multi-document Neural Attention for Multiple Answer Prediction", "answer": ["Their model achieves 30.0 HITS@100 on the recommendation task, more than any other baseline", "Proposed model achieves HITS@100 of 30.0 compared to best baseline model result of 29.2 on recommendation task."], "top_k_doc_id": [2759, 6876, 6878, 6879, 6933, 4518, 4521, 4522, 6877, 3102, 1357, 1949, 1361, 5827, 4637], "orig_top_k_doc_id": [6876, 6879, 2759, 4521, 4518, 3102, 1357, 1949, 4522, 6878, 6933, 6877, 1361, 5827, 4637]}, {"qid": 4368, "question": "Which neural network architecture do they use? in Iterative Multi-document Neural Attention for Multiple Answer Prediction", "answer": ["bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU), additional recurrent neural network with GRU units", "Gated Recurrent Units"], "top_k_doc_id": [2759, 6876, 6878, 6879, 6933, 4518, 4521, 4522, 6877, 7728, 6257, 2101, 7803, 6934, 2801], "orig_top_k_doc_id": [2759, 6879, 4521, 6933, 6878, 7728, 4522, 6257, 2101, 6877, 6876, 7803, 6934, 4518, 2801]}, {"qid": 4367, "question": "Which knowledge base do they use to retrieve facts? in Iterative Multi-document Neural Attention for Multiple Answer Prediction", "answer": ["bAbI Movie Dialog dataset", "No"], "top_k_doc_id": [2759, 6876, 6878, 6879, 6933, 7801, 4640, 4637, 6932, 2370, 7803, 4463, 7351, 2457, 4216], "orig_top_k_doc_id": [7801, 6878, 6876, 4640, 4637, 6879, 6933, 6932, 2370, 2759, 7803, 4463, 7351, 2457, 4216]}, {"qid": 1420, "question": "Why MS-MARCO is different from SQuAD? in S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension", "answer": ["there are several related passages for each question in the MS-MARCO dataset., MS-MARCO also annotates which passage is correct"], "top_k_doc_id": [2759, 1961, 1964, 1966, 7728, 7727, 4522, 968, 4521, 5266, 1965, 2755, 4518, 2664, 1963], "orig_top_k_doc_id": [1961, 1964, 1966, 7728, 7727, 4522, 968, 4521, 5266, 1965, 2755, 2759, 4518, 2664, 1963]}]}
{"group_id": 584, "group_size": 4, "items": [{"qid": 4383, "question": "What sociolinguistic variables (phonetic spellings) did they analyze?  in The Social Dynamics of Language Change in Online Networks", "answer": ["variation and change in the use of words characteristic from eight US cities that have non-standard spellings", "phonetic spelling, abbreviation, lexical words"], "top_k_doc_id": [4140, 6896, 6897, 6898, 6902, 234, 7028, 7029, 7530, 236, 2076, 7036, 2121, 5468, 6804], "orig_top_k_doc_id": [6896, 6902, 7028, 6898, 7029, 7530, 236, 2076, 7036, 6897, 2121, 4140, 234, 5468, 6804]}, {"qid": 4384, "question": "What older dialect markers did they explore? in The Social Dynamics of Language Change in Online Networks", "answer": ["No"], "top_k_doc_id": [4140, 6896, 6897, 6898, 6902, 234, 7028, 18, 3550, 242, 7016, 4002, 523, 12, 1899], "orig_top_k_doc_id": [6896, 18, 6897, 7028, 3550, 242, 6902, 7016, 6898, 4002, 523, 234, 12, 4140, 1899]}, {"qid": 4382, "question": "Did they represent tie strength only as number of social ties in a networks?  in The Social Dynamics of Language Change in Online Networks", "answer": ["Yes", "Yes, a normalized mutual friends metric", "No"], "top_k_doc_id": [4140, 6896, 6897, 6898, 6902, 6899, 6075, 7034, 6901, 5468, 6074, 5323, 236, 7807, 7625], "orig_top_k_doc_id": [6902, 6898, 6899, 6075, 6896, 7034, 6901, 6897, 5468, 4140, 6074, 5323, 236, 7807, 7625]}, {"qid": 4381, "question": "Does the paper discuss limitations of considering only data from Twitter? in The Social Dynamics of Language Change in Online Networks", "answer": ["No", "No"], "top_k_doc_id": [4140, 6896, 7016, 1735, 7530, 446, 330, 4139, 3582, 331, 234, 1378, 2076, 4404, 1734], "orig_top_k_doc_id": [4140, 7016, 1735, 7530, 446, 330, 4139, 3582, 331, 234, 6896, 1378, 2076, 4404, 1734]}]}
{"group_id": 585, "group_size": 4, "items": [{"qid": 4385, "question": "How many domains do they create ontologies for? in Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling", "answer": ["4", "four domains"], "top_k_doc_id": [689, 6904, 6905, 6907, 6908, 6909, 4462, 7176, 7179, 6002, 7688, 4461, 4345, 4444, 3300], "orig_top_k_doc_id": [6904, 6909, 6905, 6908, 7179, 6907, 6002, 7688, 7176, 4461, 689, 4345, 4462, 4444, 3300]}, {"qid": 4386, "question": "Do they separately extract topic relations and topic hierarchies in their model? in Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling", "answer": ["No", "No"], "top_k_doc_id": [689, 6904, 6905, 6907, 6908, 6909, 3300, 3467, 3468, 6906, 688, 6026, 3088, 6758, 6126], "orig_top_k_doc_id": [6904, 6905, 6909, 6907, 6908, 689, 3468, 6906, 688, 3467, 3300, 6026, 3088, 6758, 6126]}, {"qid": 4387, "question": "How do they measure the usefulness of obtained ontologies compared to domain expert ones? in Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling", "answer": ["precision, recall, F-measure", "We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. "], "top_k_doc_id": [689, 6904, 6905, 6907, 6908, 6909, 4462, 7176, 7179, 3052, 2014, 7700, 7721, 7835, 6227], "orig_top_k_doc_id": [6904, 6909, 7176, 6908, 6905, 6907, 689, 7179, 3052, 2014, 4462, 7700, 7721, 7835, 6227]}, {"qid": 4388, "question": "How do they obtain syntax from raw documents in hrLDA? in Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling", "answer": ["By extracting syntactically related noun phrases and their connections using a language parser.", " syntax and document structures such as paragraph indentations and item lists, assigns multiple topic paths for every document, and allows topic trees to grow vertically and horizontally., . By contrast, a complex sentence can be subdivided into multiple atomic sentences. Given that the syntactic verb in a relation triplet is determined by the subject and the object, a document INLINEFORM4 in a corpus INLINEFORM5 can be ultimately reduced to INLINEFORM6 subject phrases (we convert objects to subjects using passive voice) associated with INLINEFORM7 relation triplets INLINEFORM8,  The idea is to find all syntactically related noun phrases and their connections using a language parser such as the Stanford NLP parser BIBREF24 and Ollie BIBREF25 . "], "top_k_doc_id": [689, 6904, 6905, 6907, 6908, 6909, 3300, 3467, 3468, 6906, 1389, 7688, 118, 5896, 121], "orig_top_k_doc_id": [6904, 6909, 6905, 6907, 6908, 6906, 3300, 3467, 1389, 3468, 7688, 118, 5896, 689, 121]}]}
{"group_id": 586, "group_size": 4, "items": [{"qid": 4408, "question": "Which labeling scheme do they use? in Sequence Labeling Parsing by Learning Across Representations", "answer": ["No", "No"], "top_k_doc_id": [7687, 1342, 1345, 4584, 893, 1591, 1743, 6937, 6938, 3043, 4343, 5223, 5132, 3912, 3858], "orig_top_k_doc_id": [6938, 1345, 1591, 6937, 3043, 7687, 1342, 1743, 4343, 893, 5223, 5132, 4584, 3912, 3858]}, {"qid": 4410, "question": "Which dataset do they use? in Sequence Labeling Parsing by Learning Across Representations", "answer": ["English Penn Treebank, spmrl datasets", " English Penn Treebank, spmrl datasets"], "top_k_doc_id": [7687, 1342, 1345, 4584, 893, 1591, 1743, 6937, 6938, 4825, 1592, 897, 1744, 1745, 4362], "orig_top_k_doc_id": [6938, 6937, 1591, 1743, 7687, 4825, 1592, 893, 1342, 897, 1345, 1744, 1745, 4584, 4362]}, {"qid": 1025, "question": "How does this compare to simple interpolation between a word-level and a character-level language model? in Attending to Characters in Neural Sequence Labeling Models", "answer": ["No"], "top_k_doc_id": [7687, 1342, 1345, 4584, 7686, 7688, 4844, 2621, 2590, 5938, 5962, 4755, 6119, 831, 2995], "orig_top_k_doc_id": [1345, 7686, 7688, 4844, 7687, 2621, 2590, 4584, 5938, 5962, 4755, 6119, 1342, 831, 2995]}, {"qid": 2038, "question": "What other non-neural baselines do the authors compare to?  in Character-Based Text Classification using Top Down Semantic Model for Sentence Representation", "answer": ["bag of words, tf-idf, bag-of-means"], "top_k_doc_id": [7687, 3087, 4296, 3020, 6532, 1434, 6119, 7688, 6387, 6523, 4652, 5970, 1389, 1327, 2803], "orig_top_k_doc_id": [3087, 4296, 3020, 7687, 6532, 1434, 6119, 7688, 6387, 6523, 4652, 5970, 1389, 1327, 2803]}]}
{"group_id": 587, "group_size": 4, "items": [{"qid": 4465, "question": "How long is their sentiment analysis dataset? in Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference", "answer": ["Three datasets had total of 14.5k samples.", "2900, 4700, 6900"], "top_k_doc_id": [1560, 2306, 6136, 7005, 7006, 7632, 2217, 7472, 728, 2215, 3152, 1325, 2307, 725, 7475], "orig_top_k_doc_id": [7005, 7006, 2306, 1560, 7472, 2217, 2215, 6136, 1325, 7632, 2307, 3152, 725, 728, 7475]}, {"qid": 4467, "question": "What aspects are considered? in Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference", "answer": ["No", "dot-product attention module to dynamically combine all intermediates"], "top_k_doc_id": [1560, 2306, 6136, 7005, 7006, 7632, 2217, 7472, 728, 2215, 3152, 2216, 3578, 6472, 729], "orig_top_k_doc_id": [7005, 7006, 2217, 2216, 7632, 7472, 2306, 3578, 1560, 728, 2215, 6472, 729, 3152, 6136]}, {"qid": 4468, "question": "What layer gave the better results? in Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference", "answer": ["12", "BERT-Attention and BERT-LSTM perform better than vanilla BERT$_{\\tiny \\textsc {BASE}}$"], "top_k_doc_id": [1560, 2306, 6136, 7005, 7006, 7632, 2217, 7472, 6401, 5994, 5992, 4624, 7630, 1325, 6093], "orig_top_k_doc_id": [7005, 7006, 1560, 7632, 6136, 6401, 5994, 2217, 5992, 2306, 4624, 7472, 7630, 1325, 6093]}, {"qid": 4466, "question": "What NLI dataset was used? in Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference", "answer": ["Stanford Natural Language Inference BIBREF7", "SNLI"], "top_k_doc_id": [1560, 2306, 6136, 7005, 7006, 7632, 256, 1144, 3580, 6135, 3578, 5597, 3579, 873, 2679], "orig_top_k_doc_id": [7005, 7006, 6136, 1560, 7632, 256, 1144, 3580, 6135, 3578, 5597, 3579, 873, 2306, 2679]}]}
{"group_id": 588, "group_size": 4, "items": [{"qid": 4470, "question": "How is data collected? in Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment Classification", "answer": ["original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner)", "No"], "top_k_doc_id": [5417, 7007, 7669, 7746, 6752, 2285, 6208, 7308, 2825, 5266, 5038, 7009, 3585, 7752, 4948], "orig_top_k_doc_id": [7007, 7669, 2825, 5266, 7308, 5038, 5417, 7009, 2285, 6752, 3585, 7746, 7752, 4948, 6208]}, {"qid": 4472, "question": "What full English language based sentiment analysis models are tried? in Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment Classification", "answer": ["the original VADER English lexicon.", "No"], "top_k_doc_id": [5417, 7007, 7669, 7746, 6752, 2285, 6208, 7308, 4931, 6971, 447, 7115, 5421, 1051, 1043], "orig_top_k_doc_id": [7007, 7669, 5417, 6208, 6752, 4931, 6971, 447, 2285, 7115, 5421, 1051, 7746, 7308, 1043]}, {"qid": 4471, "question": "How much better is performance of Nigerian Pitdgin English sentiment classification of models that use additional Nigerian English data compared to orginal English-only models? in Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment Classification", "answer": ["No", "No"], "top_k_doc_id": [5417, 7007, 7669, 7746, 6752, 2825, 3313, 5421, 7752, 5980, 7751, 1048, 309, 1049, 1043], "orig_top_k_doc_id": [7007, 7669, 2825, 3313, 5417, 5421, 7752, 5980, 7751, 1048, 309, 1049, 7746, 1043, 6752]}, {"qid": 4469, "question": "How many annotators were used for sentiment labeling? in Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment Classification", "answer": ["Each labelled Data point was verified by at least one other person after initial labelling.", "Three people"], "top_k_doc_id": [5417, 7007, 7669, 7746, 2285, 7752, 7751, 6183, 7308, 7309, 7747, 2139, 2140, 2391, 5979], "orig_top_k_doc_id": [7007, 7669, 7746, 2285, 7752, 7751, 6183, 7308, 7309, 7747, 5417, 2139, 2140, 2391, 5979]}]}
{"group_id": 589, "group_size": 4, "items": [{"qid": 4492, "question": "what existing databases were used? in Information Extraction with Character-level Neural Networks and Free Noisy Supervision", "answer": ["database containing historical time series data", "a database containing historical time series data"], "top_k_doc_id": [4399, 7026, 4755, 7027, 7686, 2733, 7367, 5674, 2127, 5675, 3539, 2300, 931, 2294, 2023], "orig_top_k_doc_id": [7026, 7027, 4399, 7686, 5674, 2127, 5675, 3539, 2300, 4755, 2733, 7367, 931, 2294, 2023]}, {"qid": 4493, "question": "what existing parser is used? in Information Extraction with Character-level Neural Networks and Free Noisy Supervision", "answer": ["No", "candidate-generating parser "], "top_k_doc_id": [4399, 7026, 4755, 7027, 7686, 2733, 7367, 1389, 3490, 5582, 1853, 559, 3916, 1738, 7687], "orig_top_k_doc_id": [7026, 7027, 7686, 1389, 3490, 4399, 5582, 4755, 2733, 1853, 559, 3916, 1738, 7687, 7367]}, {"qid": 4491, "question": "by how much did the system improve? in Information Extraction with Character-level Neural Networks and Free Noisy Supervision", "answer": ["By more than 90%", "false positives improved by 90% and recall improved by 1%"], "top_k_doc_id": [4399, 7026, 4755, 7027, 7686, 6603, 7688, 2621, 3541, 6484, 1738, 3490, 3844, 6503, 1625], "orig_top_k_doc_id": [7026, 7027, 7686, 6603, 7688, 2621, 4755, 3541, 6484, 4399, 1738, 3490, 3844, 6503, 1625]}, {"qid": 1786, "question": "Do they assume sentence-level supervision? in End-to-End Information Extraction without Token-Level Supervision", "answer": ["No"], "top_k_doc_id": [4399, 7026, 2605, 1625, 5675, 1237, 2622, 1533, 4075, 6503, 2294, 2621, 4479, 7817, 1154], "orig_top_k_doc_id": [2605, 1625, 5675, 1237, 7026, 2622, 1533, 4075, 6503, 2294, 2621, 4479, 7817, 1154, 4399]}]}
{"group_id": 590, "group_size": 4, "items": [{"qid": 4538, "question": "What are the baseline models? in Reinforcing an Image Caption Generator Using Off-Line Human Feedback", "answer": [" MLE model, Baseline$+(t)$", "MLE model"], "top_k_doc_id": [6477, 6478, 7085, 7086, 7087, 7088, 2258, 2413, 5681, 2417, 7089, 3428, 4071, 6723, 2416], "orig_top_k_doc_id": [7085, 7086, 7088, 7087, 6478, 7089, 6477, 3428, 2258, 4071, 2417, 5681, 6723, 2413, 2416]}, {"qid": 4541, "question": "How big is the human ratings dataset? in Reinforcing an Image Caption Generator Using Off-Line Human Feedback", "answer": ["1K images sampled from the Open Images Dataset", "validation and test splits containing approximately 130K, 7K and 7K"], "top_k_doc_id": [6477, 6478, 7085, 7086, 7087, 7088, 2258, 2413, 5681, 2417, 7089, 111, 7090, 4441, 32], "orig_top_k_doc_id": [7085, 7086, 7088, 7087, 7089, 111, 7090, 6478, 6477, 2417, 2413, 4441, 2258, 5681, 32]}, {"qid": 4540, "question": "How long does it take to train the model on the mentioned dataset?  in Reinforcing an Image Caption Generator Using Off-Line Human Feedback", "answer": ["No", "3M iterations with the batch size of 4,096"], "top_k_doc_id": [6477, 6478, 7085, 7086, 7087, 7088, 2258, 2413, 5681, 3428, 4747, 1137, 1943, 4746, 5969], "orig_top_k_doc_id": [7085, 7086, 7088, 7087, 5681, 3428, 6478, 6477, 4747, 1137, 1943, 2413, 2258, 4746, 5969]}, {"qid": 4539, "question": "What image caption datasets were used in this work? in Reinforcing an Image Caption Generator Using Off-Line Human Feedback", "answer": ["Conceptual Captions", "Conceptual Captions BIBREF0"], "top_k_doc_id": [6477, 6478, 7085, 7086, 7087, 7088, 2417, 2416, 4744, 2418, 1946, 7089, 4746, 7145, 1943], "orig_top_k_doc_id": [7085, 7086, 7088, 7087, 6478, 6477, 2417, 2416, 4744, 2418, 1946, 7089, 4746, 7145, 1943]}]}
{"group_id": 591, "group_size": 4, "items": [{"qid": 4579, "question": "By how much do they improve on domain classification? in Pseudo Labeling and Negative Feedback Learning for Large-scale Multi-label Domain Classification", "answer": ["F-1 score was improved by 1.19 percent points.", "F1 is improved from 80.15 to 80.50 and from 80.71 to 81.69 of Shortlister and Hipothesis Reranker models respectively."], "top_k_doc_id": [165, 7153, 7154, 7155, 7281, 1879, 7056, 2215, 2371, 1718, 3637, 7746, 2306, 7553, 2369], "orig_top_k_doc_id": [7153, 7155, 7154, 7056, 1718, 165, 7281, 3637, 1879, 7746, 2306, 2215, 7553, 2371, 2369]}, {"qid": 4580, "question": "Which dataset do they evaluate on? in Pseudo Labeling and Negative Feedback Learning for Large-scale Multi-label Domain Classification", "answer": ["10K random utterances from the user log data", "The dataset was created by extracting utterances from the user log data from an intelligent conversational system."], "top_k_doc_id": [165, 7153, 7154, 7155, 7281, 1879, 7056, 2215, 2371, 3603, 7088, 4363, 762, 2477, 197], "orig_top_k_doc_id": [7153, 7155, 7154, 7056, 3603, 7281, 165, 7088, 2371, 2215, 4363, 762, 1879, 2477, 197]}, {"qid": 4582, "question": "How do they decide by how much to decrease confidences of incorrectly predicted domains? in Pseudo Labeling and Negative Feedback Learning for Large-scale Multi-label Domain Classification", "answer": ["The confidence of the incorrectly predicted domain is decreased only when it is highest among all predictions.", "demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming"], "top_k_doc_id": [165, 7153, 7154, 7155, 7281, 1879, 7056, 1055, 1151, 5247, 1702, 1048, 1718, 3291, 4583], "orig_top_k_doc_id": [7153, 7154, 7155, 1055, 1151, 5247, 1702, 1048, 7281, 165, 1718, 3291, 4583, 7056, 1879]}, {"qid": 4581, "question": "How does their approach work for domains with few overlapping utterances?  in Pseudo Labeling and Negative Feedback Learning for Large-scale Multi-label Domain Classification", "answer": ["No", "No"], "top_k_doc_id": [165, 7153, 7154, 7155, 7281, 2276, 3679, 1711, 401, 2371, 2818, 3603, 2550, 7553, 7793], "orig_top_k_doc_id": [7153, 7155, 7154, 7281, 2276, 165, 3679, 1711, 401, 2371, 2818, 3603, 2550, 7553, 7793]}]}
{"group_id": 592, "group_size": 4, "items": [{"qid": 4583, "question": "Is some baseline method trained on new dataset? in Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives", "answer": ["Yes", "No"], "top_k_doc_id": [7156, 7157, 6250, 7158, 7159, 7162, 3794, 3680, 2247, 5919, 6249, 6780, 6776, 5921, 5246], "orig_top_k_doc_id": [7158, 7157, 7162, 7156, 7159, 6250, 3680, 2247, 3794, 5919, 6249, 6780, 6776, 5921, 5246]}, {"qid": 4584, "question": "What potential applications are demonstrated? in Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives", "answer": ["for a general AI that talks with human beings without making the users feel isolated, making up the paraphrase corpus, supporting the semantic web search", "a general AI that talks with human beings, making up the paraphrase corpus, supporting the semantic web search"], "top_k_doc_id": [7156, 7157, 6250, 7158, 7159, 7162, 3794, 400, 3789, 2091, 490, 6881, 5426, 2657, 7664], "orig_top_k_doc_id": [7158, 7156, 7159, 7157, 7162, 6250, 400, 3794, 3789, 2091, 490, 6881, 5426, 2657, 7664]}, {"qid": 4585, "question": "What method is proposed to mitigate class imbalance in final dataset? in Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives", "answer": ["we annotate an existing corpus and then augment the dataset ", "we increased the dataset size by obtaining various types of sentences from intent arguments, specifically via human-aided sentence rewriting"], "top_k_doc_id": [7156, 7157, 6250, 7158, 7159, 7162, 5498, 7160, 1297, 6079, 5434, 3054, 400, 5501, 5917], "orig_top_k_doc_id": [7159, 7156, 7158, 7157, 7162, 5498, 7160, 1297, 6250, 6079, 5434, 3054, 400, 5501, 5917]}, {"qid": 1578, "question": "What is the motivation behind the work? Why question generation is an important task? in Question Generation by Transformers", "answer": ["Such a system would benefit educators by saving time to generate quizzes and tests."], "top_k_doc_id": [7156, 7157, 491, 495, 2205, 490, 1091, 3033, 4760, 2414, 2915, 5229, 3358, 7664, 493], "orig_top_k_doc_id": [491, 495, 2205, 490, 1091, 7157, 3033, 4760, 2414, 2915, 5229, 3358, 7156, 7664, 493]}]}
{"group_id": 593, "group_size": 4, "items": [{"qid": 4592, "question": "How many instances are explored in the few-shot experiments? in Improving Few-shot Text Classification via Pretrained Language Representations.", "answer": ["No", "No"], "top_k_doc_id": [1979, 7166, 7167, 7168, 3568, 4161, 4216, 3273, 6034, 6395, 4160, 3571, 4074, 687, 3572], "orig_top_k_doc_id": [7166, 7167, 4160, 1979, 6034, 4161, 7168, 3568, 3571, 6395, 4074, 4216, 687, 3572, 3273]}, {"qid": 4593, "question": "What tasks are explored? in Improving Few-shot Text Classification via Pretrained Language Representations.", "answer": ["69 tasks", "No"], "top_k_doc_id": [1979, 7166, 7167, 7168, 3568, 4161, 4216, 3273, 6034, 6395, 6661, 4414, 203, 5621, 247], "orig_top_k_doc_id": [7167, 7166, 6034, 7168, 1979, 3568, 3273, 4216, 6661, 6395, 4414, 203, 5621, 4161, 247]}, {"qid": 2265, "question": "Do they compare with the MAML algorithm? in Diverse Few-Shot Text Classification with Multiple Metrics", "answer": ["No"], "top_k_doc_id": [1979, 7166, 7167, 7168, 3568, 4161, 4216, 3572, 3573, 6671, 3571, 6672, 4160, 3570, 3569], "orig_top_k_doc_id": [7167, 7168, 3568, 7166, 3572, 3573, 6671, 3571, 4161, 6672, 4160, 4216, 3570, 1979, 3569]}, {"qid": 4591, "question": "What pretrained language representations are used? in Improving Few-shot Text Classification via Pretrained Language Representations.", "answer": [" GloVe", "BERT BIBREF12"], "top_k_doc_id": [1979, 7166, 7167, 7168, 203, 4414, 6656, 5540, 3273, 5713, 5710, 4561, 1779, 4571, 6661], "orig_top_k_doc_id": [7167, 7166, 7168, 203, 4414, 6656, 5540, 3273, 1979, 5713, 5710, 4561, 1779, 4571, 6661]}]}
{"group_id": 594, "group_size": 4, "items": [{"qid": 4627, "question": "What tasks are the models trained on? in A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning", "answer": ["different average lengths and class numbers, Multi-Domain Product review datasets on different domains, Multi-Objective Classification datasets with different objectives", "Sentiment classification, topics classification, question classification."], "top_k_doc_id": [7237, 7238, 7240, 1325, 1987, 400, 2556, 7227, 2216, 1552, 575, 245, 5582, 3416, 7439], "orig_top_k_doc_id": [7237, 7240, 7238, 7227, 1987, 2556, 1325, 2216, 1552, 575, 400, 245, 5582, 3416, 7439]}, {"qid": 4628, "question": "What recurrent neural networks are explored? in A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning", "answer": ["LSTM", "LSTM with 4 types of recurrent neural layers."], "top_k_doc_id": [7237, 7238, 7240, 1325, 1987, 400, 2556, 7227, 2607, 686, 7434, 624, 281, 105, 2065], "orig_top_k_doc_id": [7240, 7237, 7238, 2556, 1987, 1325, 7227, 2607, 686, 7434, 400, 624, 281, 105, 2065]}, {"qid": 4625, "question": "Do they compare against state-of-the-art? in A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning", "answer": ["Yes", "Yes"], "top_k_doc_id": [7237, 7238, 7240, 1325, 1987, 2607, 1329, 1668, 512, 7239, 5582, 4813, 5540, 624, 2610], "orig_top_k_doc_id": [7240, 7237, 1987, 7238, 2607, 1329, 1668, 1325, 512, 7239, 5582, 4813, 5540, 624, 2610]}, {"qid": 4626, "question": "What are the benchmark datasets? in A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning", "answer": ["SST-1 BIBREF14, SST-2, IMDB BIBREF15, Multi-Domain Sentiment Dataset BIBREF16, RN BIBREF17, QC BIBREF18", "SST-1, SST-2, IMDB, Multi-Domain Sentiment Dataset, RN, QC"], "top_k_doc_id": [7237, 7238, 7240, 1329, 400, 1345, 2610, 575, 2874, 1560, 1327, 1155, 365, 2922, 1665], "orig_top_k_doc_id": [7237, 7240, 1329, 400, 7238, 1345, 2610, 575, 2874, 1560, 1327, 1155, 365, 2922, 1665]}]}
{"group_id": 595, "group_size": 4, "items": [{"qid": 4629, "question": "What extractive models were trained on this dataset? in GameWikiSum: a Novel Large Multi-Document Summarization Dataset", "answer": ["LEAD-$k$, TextRank, LexRank, SumBasic, C_SKIP", " LEAD-$k$ , TextRank, LexRank , SumBasic , C_SKIP "], "top_k_doc_id": [6566, 7241, 7242, 7243, 6492, 6955, 1132, 4478, 5540, 5542, 5544, 6496, 7281, 7137, 7280], "orig_top_k_doc_id": [7243, 7242, 7241, 6955, 5544, 6496, 5540, 5542, 7281, 6492, 4478, 1132, 6566, 7137, 7280]}, {"qid": 4630, "question": "What abstractive models were trained? in GameWikiSum: a Novel Large Multi-Document Summarization Dataset", "answer": ["Conv2Conv , Transformer ,  TransformerLM", "Conv2Conv, Transformer, TransformerLM"], "top_k_doc_id": [6566, 7241, 7242, 7243, 6492, 6955, 1132, 4478, 5540, 5542, 5544, 6496, 4619, 4482, 4760], "orig_top_k_doc_id": [7243, 7242, 7241, 5540, 5544, 4619, 6492, 4482, 4478, 5542, 6955, 1132, 6566, 6496, 4760]}, {"qid": 4632, "question": "What is the size of this dataset? in GameWikiSum: a Novel Large Multi-Document Summarization Dataset", "answer": ["14652", "$265\\,000$ professional reviews for around $72\\,000$ games and $26\\,000$ Wikipedia gameplay sections"], "top_k_doc_id": [6566, 7241, 7242, 7243, 6492, 6955, 5718, 460, 6715, 6931, 6716, 6925, 4619, 7280, 4762], "orig_top_k_doc_id": [7243, 7242, 7241, 6955, 5718, 460, 6715, 6931, 6492, 6716, 6925, 4619, 7280, 6566, 4762]}, {"qid": 4631, "question": "Do the reviews focus on a specific video game domain? in GameWikiSum: a Novel Large Multi-Document Summarization Dataset", "answer": ["No", "No"], "top_k_doc_id": [6566, 7241, 7242, 7243, 2689, 575, 6568, 2334, 3451, 2335, 7280, 4380, 460, 1568, 1924], "orig_top_k_doc_id": [7241, 7242, 7243, 2689, 6566, 575, 6568, 2334, 3451, 2335, 7280, 4380, 460, 1568, 1924]}]}
{"group_id": 596, "group_size": 4, "items": [{"qid": 4634, "question": "What metrics are used for evaluation? in Spoken Conversational Search for General Knowledge", "answer": ["macro precision, recall , F-1", "macro precision, recall and F-1, average precision, recall and F-1"], "top_k_doc_id": [6584, 1168, 1169, 898, 5800, 7244, 7245, 404, 1171, 1170, 6590, 3809, 6320, 902, 1817], "orig_top_k_doc_id": [6584, 404, 7245, 898, 1171, 1170, 6590, 5800, 1168, 3809, 6320, 1169, 902, 7244, 1817]}, {"qid": 4635, "question": "Is the proposed system compared to existing systems? in Spoken Conversational Search for General Knowledge", "answer": ["No", "No"], "top_k_doc_id": [6584, 1168, 1169, 898, 5800, 7244, 7245, 6723, 3775, 1807, 4669, 6776, 6479, 899, 5646], "orig_top_k_doc_id": [6584, 898, 7244, 6723, 7245, 3775, 5800, 1807, 4669, 6776, 6479, 899, 1169, 1168, 5646]}, {"qid": 910, "question": "Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents? in Combining Search with Structured Data to Create a More Engaging User Experience in Open Domain Dialogue", "answer": ["do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency"], "top_k_doc_id": [6584, 1168, 1169, 1171, 1170, 899, 5119, 3508, 4124, 7218, 2888, 5426, 6590, 6588, 101], "orig_top_k_doc_id": [1168, 1171, 1170, 1169, 899, 5119, 3508, 4124, 7218, 2888, 5426, 6584, 6590, 6588, 101]}, {"qid": 4633, "question": "What language(s) does the system answer questions in? in Spoken Conversational Search for General Knowledge", "answer": ["French", "French"], "top_k_doc_id": [6584, 7245, 7244, 5800, 6876, 4149, 6879, 144, 5426, 7147, 5119, 2264, 1170, 3094, 6338], "orig_top_k_doc_id": [7245, 7244, 6584, 5800, 6876, 4149, 6879, 144, 5426, 7147, 5119, 2264, 1170, 3094, 6338]}]}
{"group_id": 597, "group_size": 4, "items": [{"qid": 4636, "question": "How do they determine that a decoder handles an easier task than the encoder? in Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation", "answer": ["adding more layers to the encoder achieves larger improvements than adding more layers to the decoder, the decoder converges faster than the encoder", "adding more layers to the encoder achieves larger improvements than adding more layers to the decoder, decoder converges faster than the encoder"], "top_k_doc_id": [2494, 2491, 5835, 7246, 7247, 6864, 7248, 7249, 6041, 2635, 380, 1322, 6943, 3821, 2492], "orig_top_k_doc_id": [7246, 7247, 7248, 5835, 2494, 6041, 7249, 2491, 2635, 380, 1322, 6943, 3821, 2492, 6864]}, {"qid": 4638, "question": "How do they generate input noise for the encoder and decoder? in Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation", "answer": ["random dropping, random noising, random swapping", "random dropping, random noising, random swapping"], "top_k_doc_id": [2494, 2491, 5835, 7246, 7247, 6864, 7248, 7249, 6968, 2187, 2636, 28, 3655, 274, 1749], "orig_top_k_doc_id": [7246, 5835, 7247, 7248, 6968, 2187, 6864, 2494, 2636, 2491, 7249, 28, 3655, 274, 1749]}, {"qid": 4637, "question": "How do they measure conditional information strength? in Hard but Robust, Easy but Sensitive: How Encoder and Decoder Perform in Neural Machine Translation", "answer": ["by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart", "distance between the dropping token and the current predicted token"], "top_k_doc_id": [2494, 2491, 5835, 7246, 7247, 6968, 6041, 4456, 1414, 3686, 774, 112, 3821, 4767, 3820], "orig_top_k_doc_id": [6968, 2494, 2491, 7246, 6041, 7247, 4456, 1414, 3686, 5835, 774, 112, 3821, 4767, 3820]}, {"qid": 3088, "question": "How do their models decide how much improtance to give to the output words? in Towards Understanding Neural Machine Translation with Word Importance", "answer": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "top_k_doc_id": [2494, 5240, 3358, 2636, 3821, 6385, 37, 7847, 2578, 1081, 6943, 145, 2635, 4766, 7472], "orig_top_k_doc_id": [5240, 3358, 2636, 3821, 6385, 37, 7847, 2578, 1081, 6943, 145, 2635, 4766, 7472, 2494]}]}
{"group_id": 598, "group_size": 4, "items": [{"qid": 4692, "question": "How this system recommend features for the new application? in autoNLP: NLP Feature Recommendations for Text Analytics Applications", "answer": ["estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features, system would recommend features for the new application in a ranked order", "Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order"], "top_k_doc_id": [3332, 7313, 7314, 7315, 7316, 5067, 4317, 1956, 2533, 1955, 5362, 1394, 3757, 6590, 3102], "orig_top_k_doc_id": [7315, 7313, 7314, 7316, 5067, 4317, 1956, 2533, 3332, 1955, 5362, 1394, 3757, 6590, 3102]}, {"qid": 4693, "question": "What is the similarity of manually selected features across related applications in different domains? in autoNLP: NLP Feature Recommendations for Text Analytics Applications", "answer": ["Applications share similar sets of features (of the 7 set of features, 6 selected are the same)", "Examples of common features are: N-gram, POS, Context based Features, Morphological Features, Orthographic, Dependency and Lexical"], "top_k_doc_id": [3332, 7313, 7314, 7315, 7316, 1547, 1325, 6053, 3076, 7317, 6175, 4995, 5701, 7138, 2318], "orig_top_k_doc_id": [7315, 7314, 7313, 1547, 1325, 3332, 7316, 6053, 3076, 7317, 6175, 4995, 5701, 7138, 2318]}, {"qid": 4694, "question": "What type of features are extracted with this language? in autoNLP: NLP Feature Recommendations for Text Analytics Applications", "answer": ["Linguistic, Semantic, and Statistical.", "Linguistic Features, Semantic Similarity and Relatedness based Features, Statistical Features"], "top_k_doc_id": [3332, 7313, 7314, 7315, 1308, 7114, 4867, 7111, 4868, 7536, 5405, 4351, 7815, 1731, 3076], "orig_top_k_doc_id": [7315, 7313, 7314, 3332, 1308, 4867, 7111, 4868, 7536, 5405, 7114, 4351, 7815, 1731, 3076]}, {"qid": 4695, "question": "What are meta elements of language for specifying NLP features? in autoNLP: NLP Feature Recommendations for Text Analytics Applications", "answer": ["Analysis Unit (AU), Syntactic Unit (SU), LOGICAL, Normalize Morphosyntactic Variants", "Analysis Unit (AU) (Corpus level, Document level, Para (paragraph) level,  Sentence level);\nSyntactic Unit (SU) (Word, Phrase, N-gram, Regex, POS Regex,);\nLOGICAL (AND, OR, AND NOT,  OR NOT);\nNormalize Morphosyntactic Variants (yes or no)."], "top_k_doc_id": [3332, 7313, 7314, 7315, 1308, 7114, 7227, 7166, 3547, 6464, 6053, 1899, 4300, 7817, 5880], "orig_top_k_doc_id": [7313, 7315, 7314, 3332, 1308, 7227, 7166, 3547, 6464, 6053, 1899, 4300, 7817, 7114, 5880]}]}
{"group_id": 599, "group_size": 4, "items": [{"qid": 4702, "question": "How do they think this treebank will support research on second language acquisition? in Universal Dependencies for Learner English", "answer": ["No", "It will improve tagging and parsing performance, syntax based grammatical error correction."], "top_k_doc_id": [6100, 7327, 7332, 1773, 7331, 221, 222, 3979, 5209, 6467, 6765, 7329, 6766, 659, 6466], "orig_top_k_doc_id": [7327, 7332, 5209, 6467, 221, 6765, 3979, 1773, 7329, 6100, 7331, 6766, 659, 6466, 222]}, {"qid": 4704, "question": "How long is the dataset? in Universal Dependencies for Learner English", "answer": ["5124", " 5,124 sentences (97,681 tokens)"], "top_k_doc_id": [6100, 7327, 7332, 1773, 7331, 221, 222, 3979, 3358, 2806, 5485, 3638, 862, 863, 4633], "orig_top_k_doc_id": [7327, 7332, 221, 3358, 2806, 222, 5485, 6100, 3638, 1773, 862, 3979, 7331, 863, 4633]}, {"qid": 4703, "question": "What are their baseline models? in Universal Dependencies for Learner English", "answer": ["version 2.2 of the Turbo tagger and Turbo parser BIBREF18", "Turbo tagger, Turbo parser"], "top_k_doc_id": [6100, 7327, 7332, 1773, 7331, 863, 1221, 2806, 2917, 5209, 3832, 6852, 5622, 626, 6166], "orig_top_k_doc_id": [7327, 7332, 863, 1221, 6100, 2806, 2917, 5209, 3832, 6852, 5622, 7331, 626, 1773, 6166]}, {"qid": 4705, "question": "Did they use crowdsourcing to annotate the dataset? in Universal Dependencies for Learner English", "answer": ["No", "No"], "top_k_doc_id": [6100, 7327, 7332, 862, 6166, 7328, 222, 225, 4074, 2806, 5, 861, 2874, 3623, 221], "orig_top_k_doc_id": [7327, 7332, 6100, 862, 6166, 7328, 222, 225, 4074, 2806, 5, 861, 2874, 3623, 221]}]}
{"group_id": 600, "group_size": 4, "items": [{"qid": 4734, "question": "How do attention, recurrent and convolutional networks differ on the language classes they accept? in Sequential Neural Networks as Automata", "answer": ["No", "Attention neural networks can represent more languages than other networks. Simple recurring networks can describe regular languages. CNNs can describe only strictly local languages. "], "top_k_doc_id": [7391, 3980, 7363, 7365, 7367, 7368, 7364, 1578, 3559, 1325, 3088, 6556, 1609, 3357, 3273], "orig_top_k_doc_id": [7363, 7367, 7365, 7368, 7364, 7391, 1578, 3559, 3980, 1325, 3088, 6556, 1609, 3357, 3273]}, {"qid": 4735, "question": "What type of languages do they test LSTMs on? in Sequential Neural Networks as Automata", "answer": ["Counting, Counting with Noise, Reversing", "counter languages"], "top_k_doc_id": [7391, 3980, 7363, 7365, 7367, 7368, 7398, 6828, 3982, 2301, 3058, 6104, 4859, 6280, 4400], "orig_top_k_doc_id": [7363, 7365, 7367, 7398, 7391, 3980, 6828, 3982, 2301, 3058, 6104, 4859, 6280, 4400, 7368]}, {"qid": 4745, "question": "How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata? in On the Computational Power of RNNs", "answer": ["Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .", "They prove that for any context-free language L\nthere exists an RNN whose {0}-language is L."], "top_k_doc_id": [7391, 2301, 2302, 6968, 7398, 7472, 7395, 7363, 7365, 7394, 7392, 2607, 7493, 7367, 7390], "orig_top_k_doc_id": [7391, 7398, 2301, 7395, 2302, 7363, 7365, 7394, 7392, 7472, 2607, 7493, 6968, 7367, 7390]}, {"qid": 4746, "question": "What are edge weights? in On the Computational Power of RNNs", "answer": ["No", "No"], "top_k_doc_id": [7391, 2301, 2302, 6968, 7398, 7472, 7169, 7397, 4198, 3370, 3369, 6367, 831, 841, 1381], "orig_top_k_doc_id": [7391, 2301, 6968, 7398, 7169, 2302, 7472, 7397, 4198, 3370, 3369, 6367, 831, 841, 1381]}]}
{"group_id": 601, "group_size": 4, "items": [{"qid": 4741, "question": "Which models did they compare with? in \"Wait, I'm Still Talking!\"Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model", "answer": ["Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14", "Bi-GRUs, TextCNNs, BERT"], "top_k_doc_id": [4524, 5436, 5440, 5441, 7377, 7378, 7380, 7381, 7379, 7774, 1070, 7222, 6653, 3445, 6589], "orig_top_k_doc_id": [7381, 7380, 7377, 7378, 5436, 6653, 4524, 7222, 5441, 7379, 1070, 7774, 5440, 3445, 6589]}, {"qid": 4742, "question": "What is the source of their datasets? in \"Wait, I'm Still Talking!\"Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model", "answer": ["human-to-human conversations", "MultiWoz 2.0, DailyDialogue"], "top_k_doc_id": [4524, 5436, 5440, 5441, 7377, 7378, 7380, 7381, 7379, 7774, 1070, 7222, 7224, 592, 7223], "orig_top_k_doc_id": [7381, 7377, 7380, 7378, 5441, 4524, 7774, 5436, 7222, 7379, 1070, 5440, 7224, 592, 7223]}, {"qid": 4740, "question": "By how much does their model outperform the baseline? in \"Wait, I'm Still Talking!\"Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model", "answer": ["Best model outperforms baseline by 1.98% on MultiWoz dataset and .67% on DailyDialogue dataset", "Best accuracy result of proposed model is 82.73, 79.35 compared to best baseline result of 80.75, 78.68 on MultiWoz and DailyDialogue datasets respectively."], "top_k_doc_id": [4524, 5436, 5440, 5441, 7377, 7378, 7380, 7381, 7379, 7774, 6653, 2843, 317, 4941, 4942], "orig_top_k_doc_id": [7377, 7380, 7381, 7378, 5436, 6653, 7379, 4524, 5440, 7774, 2843, 317, 4941, 5441, 4942]}, {"qid": 4739, "question": "What evaluation metrics did they use? in \"Wait, I'm Still Talking!\"Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model", "answer": ["Bilingual Evaluation Understudy (BLEU) BIBREF22, accuracy", "BLEU, accuracy score"], "top_k_doc_id": [4524, 5436, 5440, 5441, 7377, 7378, 7380, 7381, 6653, 6861, 965, 7222, 4441, 1168, 7775], "orig_top_k_doc_id": [7381, 7380, 7377, 7378, 5436, 5440, 6653, 5441, 6861, 965, 4524, 7222, 4441, 1168, 7775]}]}
{"group_id": 602, "group_size": 4, "items": [{"qid": 4757, "question": "What languages are used as input? in \"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter", "answer": ["English ", "English", "English"], "top_k_doc_id": [332, 334, 335, 4941, 7411, 7412, 7414, 449, 4577, 4895, 6175, 6458, 3731, 5879, 4798], "orig_top_k_doc_id": [7412, 7414, 7411, 332, 334, 4941, 335, 3731, 6458, 5879, 4577, 4895, 6175, 449, 4798]}, {"qid": 4758, "question": "What are the components of the classifier? in \"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter", "answer": ["log-linear model,  five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword", "Veridicality  class,  log-linear model for  measuring  distribution over a tweet's veridicality, Twitter NER system  to  to identify named entities,  five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword."], "top_k_doc_id": [332, 334, 335, 4941, 7411, 7412, 7414, 449, 4577, 4895, 6175, 6458, 7413, 250, 6375], "orig_top_k_doc_id": [7414, 7412, 7411, 332, 334, 4941, 6175, 449, 7413, 4895, 335, 250, 6458, 4577, 6375]}, {"qid": 4759, "question": "Which uncertain outcomes are forecast using the wisdom of crowds? in \"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter", "answer": ["neutral (\u201cUncertain about the outcome\")"], "top_k_doc_id": [332, 334, 335, 4941, 7411, 7412, 7414, 336, 7413, 330, 5647, 3731, 4799, 5646, 452], "orig_top_k_doc_id": [7411, 7412, 7414, 336, 332, 7413, 330, 5647, 3731, 335, 4941, 4799, 334, 5646, 452]}, {"qid": 2792, "question": "What kind of celebrities do they obtain tweets from? in The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets", "answer": ["Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,\nEllen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey", "Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. "], "top_k_doc_id": [332, 4895, 4899, 4898, 5786, 6458, 5879, 443, 6375, 6456, 4112, 449, 6874, 3945, 1191], "orig_top_k_doc_id": [4895, 4899, 4898, 5786, 6458, 5879, 443, 6375, 6456, 4112, 449, 6874, 3945, 1191, 332]}]}
{"group_id": 603, "group_size": 4, "items": [{"qid": 4800, "question": "Is BAT smaller (in number of parameters) than post-trained BERT? in Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "answer": ["No", "No"], "top_k_doc_id": [2217, 2306, 3129, 7472, 7473, 7475, 5560, 5561, 3579, 6093, 7005, 3154, 5559, 2221, 393], "orig_top_k_doc_id": [7475, 7472, 7473, 7005, 3579, 6093, 5560, 3154, 2306, 5559, 2221, 5561, 393, 2217, 3129]}, {"qid": 4801, "question": "What are the modifications made to post-trained BERT? in Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "answer": ["adversarial examples from BERT embeddings using the gradient of the loss, we feed the perturbed examples to the BERT encoder ", "They added adversarial examples in training to improve the post-trained BERT model"], "top_k_doc_id": [2217, 2306, 3129, 7472, 7473, 7475, 5560, 5561, 3579, 6093, 7005, 369, 5351, 3578, 1896], "orig_top_k_doc_id": [7472, 7475, 5561, 7473, 5560, 369, 7005, 5351, 3129, 3578, 2306, 2217, 1896, 6093, 3579]}, {"qid": 4798, "question": "How long is the dataset? in Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "answer": ["SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences", "Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively."], "top_k_doc_id": [2217, 2306, 3129, 7472, 7473, 7475, 5560, 5561, 5559, 2215, 369, 3152, 3156, 725, 2307], "orig_top_k_doc_id": [7472, 7475, 2306, 7473, 5559, 5561, 2215, 369, 5560, 2217, 3152, 3156, 3129, 725, 2307]}, {"qid": 4802, "question": "What aspects are considered? in Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "answer": ["No", "No"], "top_k_doc_id": [2217, 2306, 3129, 7472, 7473, 7475, 2216, 2215, 3578, 3155, 725, 3579, 3152, 729, 3580], "orig_top_k_doc_id": [7472, 7473, 7475, 2217, 2216, 2215, 3129, 3578, 3155, 725, 3579, 3152, 729, 2306, 3580]}]}
{"group_id": 604, "group_size": 4, "items": [{"qid": 4863, "question": "What regularization methods are used? in Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits", "answer": ["dropout, embedding dropout, DropBlock", "dropout, DropBlock"], "top_k_doc_id": [1810, 3812, 6444, 7548, 7549, 7550, 7551, 7552, 2323, 6375, 5112, 34, 3795, 6443, 1597], "orig_top_k_doc_id": [7548, 7551, 7549, 7550, 7552, 5112, 3812, 1810, 6375, 34, 3795, 6443, 1597, 2323, 6444]}, {"qid": 4864, "question": "What metrics are used? in Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits", "answer": ["Accuracy, Precision, Recall, F1-score", "Accuracy, precision, recall and F1 score."], "top_k_doc_id": [1810, 3812, 6444, 7548, 7549, 7550, 7551, 7552, 2323, 6375, 1958, 744, 6555, 3813, 4447], "orig_top_k_doc_id": [7548, 7551, 7549, 7550, 7552, 1810, 1958, 3812, 744, 6555, 3813, 2323, 6444, 4447, 6375]}, {"qid": 4865, "question": "How long is the dataset? in Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits", "answer": ["almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits", "2022"], "top_k_doc_id": [1810, 3812, 6444, 7548, 7549, 7550, 7551, 7552, 2271, 4447, 6443, 6879, 5112, 745, 123], "orig_top_k_doc_id": [7548, 7551, 7549, 7550, 7552, 4447, 5112, 6444, 1810, 6443, 6879, 745, 123, 3812, 2271]}, {"qid": 4866, "question": "What dataset do they use? in Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits", "answer": ["manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them", "Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github"], "top_k_doc_id": [1810, 3812, 6444, 7548, 7549, 7550, 7551, 7552, 2271, 4447, 6443, 6879, 1958, 7387, 744], "orig_top_k_doc_id": [7548, 7551, 7549, 7550, 7552, 4447, 6443, 1810, 3812, 1958, 6444, 7387, 2271, 6879, 744]}]}
{"group_id": 605, "group_size": 4, "items": [{"qid": 4963, "question": "what dataset was used in their experiment? in YEDDA: A Lightweight Collaborative Text Span Annotation Tool", "answer": ["CoNLL 2003 English NER", "CoNLL 2003 English NER BIBREF8"], "top_k_doc_id": [7097, 7697, 7699, 1308, 5916, 5921, 6182, 6643, 7698, 1309, 2746, 5380, 7857, 2233, 1081], "orig_top_k_doc_id": [7697, 7699, 7698, 1308, 5380, 2746, 6643, 7857, 1309, 7097, 5921, 2233, 5916, 6182, 1081]}, {"qid": 4964, "question": "what are the existing annotation tools? in YEDDA: A Lightweight Collaborative Text Span Annotation Tool", "answer": ["BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7", "existing annotation tools BIBREF6 , BIBREF7"], "top_k_doc_id": [7097, 7697, 7699, 1308, 5916, 5921, 6182, 6643, 7698, 1309, 2746, 3145, 4106, 7074, 7091], "orig_top_k_doc_id": [7697, 7699, 7698, 1308, 1309, 7097, 2746, 3145, 5916, 6643, 4106, 7074, 6182, 7091, 5921]}, {"qid": 4962, "question": "how many sentences did they annotate? in YEDDA: A Lightweight Collaborative Text Span Annotation Tool", "answer": ["100 sentences", "100 sentences"], "top_k_doc_id": [7097, 7697, 7699, 1308, 5916, 5921, 6182, 6643, 7698, 7857, 5380, 5917, 6181, 6640, 6641], "orig_top_k_doc_id": [7697, 7699, 7698, 6182, 7857, 5916, 5921, 5380, 5917, 6643, 7097, 1308, 6181, 6640, 6641]}, {"qid": 1835, "question": "What programming language is the tool written in? in A Lightweight Front-end Tool for Interactive Entity Population", "answer": ["JavaScript"], "top_k_doc_id": [7097, 7697, 7699, 2675, 2673, 2270, 274, 2730, 3906, 7096, 3907, 273, 3752, 1309, 2272], "orig_top_k_doc_id": [2675, 2673, 7097, 7697, 2270, 274, 7699, 2730, 3906, 7096, 3907, 273, 3752, 1309, 2272]}]}
{"group_id": 606, "group_size": 4, "items": [{"qid": 4968, "question": "Do they evaluate binary paragraph vectors on a downstream task? in Binary Paragraph Vectors", "answer": ["Yes", "Yes"], "top_k_doc_id": [420, 7736, 7737, 7738, 7739, 7740, 4636, 4709, 6054, 6106, 6306, 2456, 2458, 4635, 2459], "orig_top_k_doc_id": [7736, 7738, 7739, 7737, 7740, 420, 6106, 2456, 4636, 2458, 4635, 4709, 6306, 6054, 2459]}, {"qid": 4970, "question": "Which training dataset do they use? in Binary Paragraph Vectors", "answer": ["20 Newsgroups, Reuters Corpus Volume, English Wikipedia", " 20 Newsgroups, RCV1, English Wikipedia"], "top_k_doc_id": [420, 7736, 7737, 7738, 7739, 7740, 4636, 4709, 6054, 6106, 6306, 1396, 1450, 2444, 1449], "orig_top_k_doc_id": [7738, 7736, 7737, 420, 7739, 6106, 7740, 4709, 4636, 1396, 6306, 6054, 1450, 2444, 1449]}, {"qid": 4971, "question": "Do they analyze the produced binary codes? in Binary Paragraph Vectors", "answer": ["Yes", "No"], "top_k_doc_id": [420, 7736, 7737, 7738, 7739, 7740, 4636, 4709, 3411, 2857, 5829, 1048, 5755, 2444, 1045], "orig_top_k_doc_id": [7736, 7738, 7739, 7740, 7737, 420, 3411, 2857, 5829, 4636, 1048, 5755, 4709, 2444, 1045]}, {"qid": 4969, "question": "How do they show that binary paragraph vectors capture semantics? in Binary Paragraph Vectors", "answer": ["They perform information-retrieval tasks on popular benchmarks", " trained Binary PV-DBOW with bigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets"], "top_k_doc_id": [420, 7736, 7737, 7738, 7739, 7740, 5092, 6306, 4448, 6106, 6460, 5091, 6718, 5973, 4635], "orig_top_k_doc_id": [420, 7739, 7738, 7736, 7737, 7740, 5092, 6306, 4448, 6106, 6460, 5091, 6718, 5973, 4635]}]}
{"group_id": 607, "group_size": 4, "items": [{"qid": 4972, "question": "How long is the dataset? in IAM at CLEF eHealth 2018: Concept Annotation and Coding in French Death Certificates", "answer": ["125383", "125383 death certificates"], "top_k_doc_id": [6419, 2387, 3145, 3743, 3744, 4967, 5910, 7741, 1794, 1796, 6418, 6741, 3486, 6420, 4115], "orig_top_k_doc_id": [7741, 3744, 5910, 2387, 1796, 6418, 6741, 1794, 3145, 3486, 3743, 4967, 6419, 6420, 4115]}, {"qid": 4973, "question": "Do they use machine learning? in IAM at CLEF eHealth 2018: Concept Annotation and Coding in French Death Certificates", "answer": ["No", "No"], "top_k_doc_id": [6419, 2387, 3145, 3743, 3744, 4967, 5910, 7741, 1794, 1796, 6418, 6741, 2074, 2399, 128], "orig_top_k_doc_id": [7741, 3744, 5910, 2387, 3145, 6741, 4967, 6418, 6419, 1796, 1794, 3743, 2074, 2399, 128]}, {"qid": 4974, "question": "What are the ICD-10 codes? in IAM at CLEF eHealth 2018: Concept Annotation and Coding in French Death Certificates", "answer": ["International Classification of Diseases, 10th revision (ICD-10) BIBREF1", "International Classification of Diseases"], "top_k_doc_id": [6419, 2387, 3145, 3743, 3744, 4967, 5910, 7741, 3079, 7742, 3078, 7384, 6195, 7387, 7388], "orig_top_k_doc_id": [7741, 3079, 7742, 3078, 3744, 7384, 3743, 6195, 2387, 5910, 3145, 7387, 7388, 4967, 6419]}, {"qid": 3966, "question": "by how much did their model outperform the other models? in Automatic Section Recognition in Obituaries", "answer": ["In terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.", "No", "Their model outperforms other models by 0.01 micro F1 and 0.07 macro F1"], "top_k_doc_id": [6419, 6417, 6423, 6422, 6418, 3006, 6421, 6420, 110, 4382, 2823, 5546, 1577, 57, 5476], "orig_top_k_doc_id": [6417, 6419, 6423, 6422, 6418, 3006, 6421, 6420, 110, 4382, 2823, 5546, 1577, 57, 5476]}]}
{"group_id": 608, "group_size": 4, "items": [{"qid": 5017, "question": "What are high level declarative abstractions Overton provides? in Overton: A Data System for Monitoring and Improving Machine-Learned Products", "answer": ["Code-free Deep Learning , Multitask Learning,  Weak Supervision", "data payloads, model tasks"], "top_k_doc_id": [7813, 7814, 7815, 7816, 7817, 7818, 7819, 2827, 898, 5429, 5428, 7096, 540, 7292, 3814], "orig_top_k_doc_id": [7813, 7814, 7818, 7815, 7816, 7817, 7819, 5429, 5428, 898, 7096, 540, 2827, 7292, 3814]}, {"qid": 5018, "question": "How are applications presented in Overton? in Overton: A Data System for Monitoring and Improving Machine-Learned Products", "answer": ["Applications are customized by providing supervision in a data file that conforms to the schema", "by manipulating data files\u2013not custom code"], "top_k_doc_id": [7813, 7814, 7815, 7816, 7817, 7818, 7819, 2827, 898, 4300, 130, 5577, 7382, 2873, 6463], "orig_top_k_doc_id": [7814, 7813, 7815, 7818, 7816, 7817, 7819, 4300, 130, 2827, 5577, 7382, 898, 2873, 6463]}, {"qid": 5019, "question": "Does Overton support customizing deep learning models without writing any code? in Overton: A Data System for Monitoring and Improving Machine-Learned Products", "answer": ["No", "Yes"], "top_k_doc_id": [7813, 7814, 7815, 7816, 7817, 7818, 7819, 2827, 250, 7548, 1488, 5105, 2835, 2203, 5680], "orig_top_k_doc_id": [7814, 7813, 7818, 7817, 7816, 7815, 7819, 250, 7548, 1488, 5105, 2827, 2835, 2203, 5680]}, {"qid": 5016, "question": "How does Overton handles contradictory or incomplete supervision data? in Overton: A Data System for Monitoring and Improving Machine-Learned Products", "answer": ["Overton learns the accuracy of these sources using ideas from the Snorkel project", "Overton uses techniques from Snorkel BIBREF1 and Google's Snorkel DryBell BIBREF0"], "top_k_doc_id": [7813, 7814, 7815, 7816, 7817, 7818, 7819, 2990, 2750, 7820, 3814, 96, 5257, 3004, 2622], "orig_top_k_doc_id": [7814, 7813, 7815, 7816, 7818, 7817, 7819, 2990, 2750, 7820, 3814, 96, 5257, 3004, 2622]}]}
{"group_id": 609, "group_size": 3, "items": [{"qid": 30, "question": "What aspects have been compared between various language models? in Progress and Tradeoffs in Neural Language Models", "answer": ["Quality measures using perplexity and recall, and performance measured using latency and energy usage. "], "top_k_doc_id": [24, 26, 2395, 3094, 6881, 1244, 490, 6842, 6206, 3789, 898, 6035, 3416, 1181, 2917], "orig_top_k_doc_id": [24, 1244, 490, 26, 6881, 6842, 3094, 2395, 6206, 3789, 898, 6035, 3416, 1181, 2917]}, {"qid": 31, "question": "what classic language models are mentioned in the paper? in Progress and Tradeoffs in Neural Language Models", "answer": ["Kneser\u2013Ney smoothing"], "top_k_doc_id": [24, 26, 2395, 3094, 6881, 1029, 4399, 4267, 5351, 7818, 5088, 6317, 2885, 5186, 5368], "orig_top_k_doc_id": [24, 2395, 26, 1029, 4399, 4267, 5351, 7818, 5088, 6317, 3094, 6881, 2885, 5186, 5368]}, {"qid": 32, "question": "What is a commonly used evaluation metric for language models? in Progress and Tradeoffs in Neural Language Models", "answer": ["perplexity", "perplexity"], "top_k_doc_id": [24, 26, 3719, 1969, 3359, 3807, 3949, 7655, 7103, 491, 7138, 2435, 4795, 865, 898], "orig_top_k_doc_id": [24, 3719, 1969, 3359, 26, 3807, 3949, 7655, 7103, 491, 7138, 2435, 4795, 865, 898]}]}
{"group_id": 610, "group_size": 3, "items": [{"qid": 49, "question": "What were the word embeddings trained on? in Is there Gender bias and stereotype in Portuguese Word Embeddings?", "answer": ["large Portuguese corpus"], "top_k_doc_id": [52, 53, 3547, 3548, 7064, 4630, 5006, 1444, 1799, 4631, 6559, 3007, 6737, 1443, 5010], "orig_top_k_doc_id": [52, 53, 3547, 3548, 7064, 5006, 1444, 1799, 4630, 4631, 6559, 3007, 6737, 1443, 5010]}, {"qid": 50, "question": "Which word embeddings are analysed? in Is there Gender bias and stereotype in Portuguese Word Embeddings?", "answer": ["Continuous Bag-of-Words (CBOW)"], "top_k_doc_id": [52, 53, 3547, 3548, 7064, 4630, 6962, 1903, 41, 5155, 5949, 3941, 7454, 6964, 6738], "orig_top_k_doc_id": [52, 53, 3547, 3548, 6962, 4630, 7064, 1903, 41, 5155, 5949, 3941, 7454, 6964, 6738]}, {"qid": 48, "question": "Does this paper target European or Brazilian Portuguese? in Is there Gender bias and stereotype in Portuguese Word Embeddings?", "answer": ["No", "No"], "top_k_doc_id": [52, 53, 3547, 3548, 7064, 5984, 5983, 1410, 1432, 4008, 5210, 4593, 7324, 3007, 633], "orig_top_k_doc_id": [52, 53, 5984, 5983, 1410, 1432, 4008, 3548, 5210, 4593, 3547, 7324, 7064, 3007, 633]}]}
{"group_id": 611, "group_size": 3, "items": [{"qid": 69, "question": "Does the model use both spectrogram images and raw waveforms as features? in Spoken Language Identification using ConvNets", "answer": ["No"], "top_k_doc_id": [75, 76, 79, 78, 2768, 4671, 4673, 4973, 4972, 6119, 1063, 4974, 382, 77, 844], "orig_top_k_doc_id": [75, 76, 78, 79, 4973, 4972, 4673, 6119, 2768, 4671, 1063, 4974, 382, 77, 844]}, {"qid": 71, "question": "What is the accuracy reported by state-of-the-art methods? in Spoken Language Identification using ConvNets", "answer": ["Answer with content missing: (Table 1)\nPrevious state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)"], "top_k_doc_id": [75, 76, 79, 78, 2768, 4671, 4673, 404, 2769, 5257, 5896, 2634, 2785, 4652, 2630], "orig_top_k_doc_id": [76, 404, 2769, 75, 5257, 4673, 2768, 78, 5896, 4671, 2634, 79, 2785, 4652, 2630]}, {"qid": 70, "question": "Is the performance compared against a baseline model? in Spoken Language Identification using ConvNets", "answer": ["Yes", "No"], "top_k_doc_id": [75, 76, 79, 1266, 5146, 3437, 6119, 2300, 4960, 5181, 404, 4007, 6833, 1320, 6749], "orig_top_k_doc_id": [76, 1266, 75, 5146, 3437, 6119, 2300, 4960, 5181, 79, 404, 4007, 6833, 1320, 6749]}]}
{"group_id": 612, "group_size": 3, "items": [{"qid": 75, "question": "Did they experiment on all the tasks? in AraNet: A Deep Learning Toolkit for Arabic Social Media", "answer": ["Yes"], "top_k_doc_id": [85, 86, 87, 88, 987, 1318, 1725, 1726, 2329, 5168, 5275, 2810, 1734, 1735, 2811], "orig_top_k_doc_id": [88, 85, 87, 86, 2810, 1725, 1726, 5168, 987, 5275, 1318, 1734, 1735, 2811, 2329]}, {"qid": 77, "question": "What datasets are used in training? in AraNet: A Deep Learning Toolkit for Arabic Social Media", "answer": ["Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34", " Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects"], "top_k_doc_id": [85, 86, 87, 88, 987, 1318, 1725, 1726, 2329, 5168, 5275, 2798, 2402, 5812, 4223], "orig_top_k_doc_id": [88, 85, 87, 86, 2798, 2402, 1726, 987, 1725, 2329, 1318, 5812, 4223, 5168, 5275]}, {"qid": 76, "question": "What models did they compare to? in AraNet: A Deep Learning Toolkit for Arabic Social Media", "answer": [" we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)"], "top_k_doc_id": [85, 86, 87, 88, 987, 1318, 1725, 1726, 2329, 5168, 5979, 4739, 2798, 1494, 5600], "orig_top_k_doc_id": [88, 85, 87, 1725, 987, 1726, 1318, 86, 5979, 2329, 4739, 2798, 1494, 5168, 5600]}]}
{"group_id": 613, "group_size": 3, "items": [{"qid": 136, "question": "What intrinsic evaluation metrics are used? in Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "answer": ["Class Membership Tests, Class Distinction Test, Word Equivalence Test", "coverage metric, being distinct (cosine INLINEFORM0 0.7 or 0.8), belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), being equivalent (cosine INLINEFORM2 0.85 or 0.95)"], "top_k_doc_id": [155, 156, 159, 446, 157, 158, 3547, 6491, 6490, 3612, 7029, 53, 3615, 2534, 982], "orig_top_k_doc_id": [159, 155, 446, 156, 158, 6491, 6490, 3612, 7029, 157, 53, 3615, 3547, 2534, 982]}, {"qid": 137, "question": "What experimental results suggest that using less than 50% of the available training examples might result in overfitting? in Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "answer": ["consistent increase in the validation loss after about 15 epochs"], "top_k_doc_id": [155, 156, 159, 446, 157, 158, 3547, 3986, 2834, 5813, 7861, 5814, 4287, 2680, 4323], "orig_top_k_doc_id": [159, 155, 158, 156, 3986, 446, 2834, 3547, 5813, 7861, 5814, 4287, 2680, 4323, 157]}, {"qid": 135, "question": "What new metrics are suggested to track progress? in Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "answer": [" For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine"], "top_k_doc_id": [155, 156, 159, 446, 879, 1726, 53, 2542, 2041, 5368, 1743, 5716, 2080, 2534, 4154], "orig_top_k_doc_id": [159, 155, 156, 879, 1726, 53, 2542, 2041, 5368, 446, 1743, 5716, 2080, 2534, 4154]}]}
{"group_id": 614, "group_size": 3, "items": [{"qid": 193, "question": "Are language-specific and language-neutral components disjunctive? in How Language-Neutral is Multilingual BERT?", "answer": ["No"], "top_k_doc_id": [247, 248, 2750, 6270, 2219, 6268, 784, 5716, 2310, 7756, 5881, 2307, 7062, 87, 6701], "orig_top_k_doc_id": [247, 248, 2219, 784, 2750, 5716, 6270, 2310, 7756, 5881, 2307, 7062, 87, 6268, 6701]}, {"qid": 195, "question": "What challenges this work presents that must be solved to build better language-neutral representations? in How Language-Neutral is Multilingual BERT?", "answer": ["contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks"], "top_k_doc_id": [247, 248, 2750, 6270, 2219, 6268, 6752, 6471, 7064, 5735, 4510, 875, 5133, 873, 2284], "orig_top_k_doc_id": [247, 2750, 6752, 6268, 6471, 248, 7064, 5735, 4510, 6270, 875, 5133, 873, 2219, 2284]}, {"qid": 192, "question": "How they demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment? in How Language-Neutral is Multilingual BERT?", "answer": ["Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.", "explicit projection had a negligible effect on the performance"], "top_k_doc_id": [247, 248, 2750, 6270, 257, 3617, 2306, 2494, 6332, 5699, 873, 3374, 365, 2751, 3941], "orig_top_k_doc_id": [247, 248, 2750, 257, 3617, 2306, 2494, 6332, 6270, 5699, 873, 3374, 365, 2751, 3941]}]}
{"group_id": 615, "group_size": 3, "items": [{"qid": 304, "question": "How big is their model? in Attentional Encoder Network for Targeted Sentiment Classification", "answer": ["Proposed model has 1.16 million parameters and 11.04 MB."], "top_k_doc_id": [365, 366, 367, 368, 1679, 2217, 2219, 1040, 2306, 1052, 4812, 3357, 6863, 5897, 6864], "orig_top_k_doc_id": [365, 368, 367, 2219, 1052, 366, 1679, 1040, 2306, 4812, 3357, 6863, 5897, 2217, 6864]}, {"qid": 305, "question": "How is their model different from BERT? in Attentional Encoder Network for Targeted Sentiment Classification", "answer": ["overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer."], "top_k_doc_id": [365, 366, 367, 368, 1679, 2217, 2219, 1040, 2306, 7005, 2220, 5559, 1318, 7472, 3578], "orig_top_k_doc_id": [365, 368, 367, 2219, 366, 2217, 7005, 2220, 5559, 2306, 1318, 7472, 1679, 3578, 1040]}, {"qid": 303, "question": "Do they use multi-attention heads? in Attentional Encoder Network for Targeted Sentiment Classification", "answer": ["Yes"], "top_k_doc_id": [365, 366, 367, 368, 1679, 2217, 2219, 2944, 3360, 2943, 1567, 3361, 2602, 1566, 1683], "orig_top_k_doc_id": [365, 367, 368, 366, 2217, 2219, 2944, 3360, 2943, 1567, 1679, 3361, 2602, 1566, 1683]}]}
{"group_id": 616, "group_size": 3, "items": [{"qid": 364, "question": "Which dataset(s) do they use? in A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient", "answer": ["14 TDs, BIBREF15"], "top_k_doc_id": [306, 434, 990, 1729, 3614, 3935, 4838, 5705, 5098, 5423, 6321, 4768, 5874, 5708, 6573], "orig_top_k_doc_id": [4838, 3614, 3935, 5705, 306, 434, 1729, 990, 5423, 4768, 6321, 5098, 5874, 5708, 6573]}, {"qid": 365, "question": "How do they evaluate knowledge extraction performance? in A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient", "answer": ["SRCC"], "top_k_doc_id": [306, 434, 990, 1729, 3614, 3935, 4838, 5705, 5098, 5423, 6321, 2420, 433, 2406, 3342], "orig_top_k_doc_id": [4838, 434, 1729, 3935, 3614, 5423, 2420, 433, 6321, 306, 5705, 990, 2406, 5098, 3342]}, {"qid": 363, "question": "What representations for textual documents do they use? in A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient", "answer": ["finite sequence of terms"], "top_k_doc_id": [306, 434, 990, 1729, 3614, 3935, 4838, 5705, 433, 307, 2406, 2903, 6573, 2900, 131], "orig_top_k_doc_id": [4838, 433, 434, 306, 3935, 307, 2406, 990, 2903, 3614, 6573, 1729, 5705, 2900, 131]}]}
{"group_id": 617, "group_size": 3, "items": [{"qid": 390, "question": "How big is dataset domain-specific embedding are trained on? in Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization", "answer": ["No"], "top_k_doc_id": [456, 459, 460, 4216, 5718, 457, 4275, 5138, 6053, 7281, 7280, 6571, 7242, 2190, 7241], "orig_top_k_doc_id": [456, 460, 459, 7281, 4275, 5718, 5138, 4216, 457, 6053, 7280, 6571, 7242, 2190, 7241]}, {"qid": 391, "question": "How big is unrelated corpus universal embedding is traned on? in Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization", "answer": ["No"], "top_k_doc_id": [456, 459, 460, 4216, 5718, 457, 4275, 5138, 6053, 2806, 3872, 2623, 458, 2917, 2339], "orig_top_k_doc_id": [456, 460, 4275, 5718, 2806, 3872, 457, 5138, 6053, 4216, 2623, 458, 459, 2917, 2339]}, {"qid": 392, "question": "How better are state-of-the-art results than this model?  in Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization", "answer": ["we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features,  RegSum achieves a similar ROUGE-2 score"], "top_k_doc_id": [456, 459, 460, 4216, 5718, 5139, 5554, 5540, 6571, 458, 1591, 7762, 6927, 5140, 1255], "orig_top_k_doc_id": [459, 460, 456, 5139, 5554, 5540, 6571, 458, 1591, 5718, 7762, 6927, 5140, 1255, 4216]}]}
{"group_id": 618, "group_size": 3, "items": [{"qid": 407, "question": "Has STES been previously used in the literature to evaluate similar tasks? in Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "answer": ["No"], "top_k_doc_id": [476, 479, 480, 481, 5562, 5920, 7664, 1159, 1689, 4499, 7665, 6597, 5929, 5560, 6271], "orig_top_k_doc_id": [476, 480, 479, 481, 4499, 7664, 5562, 1159, 5920, 6597, 7665, 5929, 5560, 6271, 1689]}, {"qid": 408, "question": "What are the baseline models mentioned in the paper? in Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "answer": ["Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), Word2Vec Semantic Text Exchange Model (W2V-STEM)"], "top_k_doc_id": [476, 479, 480, 481, 5562, 5920, 7664, 1159, 1689, 4499, 7665, 7685, 1887, 5844, 1273], "orig_top_k_doc_id": [476, 481, 480, 479, 5562, 7664, 1159, 5920, 7665, 4499, 1689, 7685, 1887, 5844, 1273]}, {"qid": 406, "question": "Does the model proposed beat the baseline models for all the values of the masking parameter tested? in Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "answer": ["No"], "top_k_doc_id": [476, 479, 480, 481, 5562, 5920, 7664, 478, 5560, 1041, 4135, 7380, 7003, 5974, 477], "orig_top_k_doc_id": [476, 481, 480, 479, 478, 5560, 5920, 7664, 1041, 5562, 4135, 7380, 7003, 5974, 477]}]}
{"group_id": 619, "group_size": 3, "items": [{"qid": 516, "question": "How big is data provided by this research? in Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities", "answer": ["16k images and 740k corresponding region descriptions"], "top_k_doc_id": [635, 636, 2900, 4166, 4427, 4414, 7233, 788, 2650, 7138, 4413, 4415, 3394, 4267, 3789], "orig_top_k_doc_id": [635, 636, 4414, 7233, 788, 2900, 2650, 7138, 4413, 4415, 4427, 3394, 4166, 4267, 3789]}, {"qid": 517, "question": "How they complete a user query prefix conditioned upon an image? in Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities", "answer": ["we replace user embeddings with a low-dimensional image representation"], "top_k_doc_id": [635, 636, 2900, 4166, 4427, 3804, 720, 7142, 6090, 160, 3074, 4721, 276, 4428, 144], "orig_top_k_doc_id": [635, 636, 4427, 3804, 720, 7142, 6090, 160, 3074, 4721, 276, 2900, 4428, 144, 4166]}, {"qid": 515, "question": "How better does auto-completion perform when using both language and vision than only language? in Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities", "answer": ["No"], "top_k_doc_id": [635, 636, 4414, 7138, 6095, 4560, 3594, 4209, 3595, 4826, 778, 7140, 5218, 4967, 1827], "orig_top_k_doc_id": [635, 636, 4414, 7138, 6095, 4560, 3594, 4209, 3595, 4826, 778, 7140, 5218, 4967, 1827]}]}
{"group_id": 620, "group_size": 3, "items": [{"qid": 557, "question": "How do they model a city description using embeddings? in Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism", "answer": ["We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm."], "top_k_doc_id": [12, 678, 679, 3878, 5909, 1443, 1446, 4444, 6297, 7531, 2534, 7533, 6298, 7530, 6900], "orig_top_k_doc_id": [678, 679, 5909, 3878, 12, 6297, 7531, 1443, 2534, 7533, 6298, 7530, 1446, 4444, 6900]}, {"qid": 558, "question": "How do they obtain human judgements? in Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism", "answer": ["Using crowdsourcing "], "top_k_doc_id": [12, 678, 679, 3878, 5909, 1443, 1446, 4444, 111, 824, 982, 3212, 5914, 6896, 3207], "orig_top_k_doc_id": [678, 679, 111, 5909, 1446, 824, 4444, 3878, 12, 982, 3212, 1443, 5914, 6896, 3207]}, {"qid": 559, "question": "Which clustering method do they use to cluster city description embeddings? in Casting Light on Invisible Cities: Computationally Engaging with Literary Criticism", "answer": [" We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define \u201ccluster strength\u201d to be the relative difference between \u201cintra-group\u201d Euclidean distance and \u201cinter-group\u201d Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. "], "top_k_doc_id": [12, 678, 679, 3878, 5909, 3796, 3077, 4401, 7531, 927, 2325, 3078, 4860, 4687, 3079], "orig_top_k_doc_id": [678, 679, 3796, 5909, 3878, 3077, 4401, 7531, 927, 2325, 12, 3078, 4860, 4687, 3079]}]}
{"group_id": 621, "group_size": 3, "items": [{"qid": 651, "question": "What simulations are performed by the authors to validate their approach? in Imitation Learning of Robot Policies by Combining Language, Vision and Demonstration", "answer": ["a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command"], "top_k_doc_id": [796, 797, 798, 6588, 1284, 3279, 3906, 3332, 3286, 3284, 3868, 3907, 6939, 3282, 1940], "orig_top_k_doc_id": [796, 797, 798, 6588, 3906, 3332, 3286, 3284, 3279, 3868, 3907, 6939, 1284, 3282, 1940]}, {"qid": 652, "question": "Does proposed end-to-end approach learn in reinforcement or supervised learning manner? in Imitation Learning of Robot Policies by Combining Language, Vision and Demonstration", "answer": ["supervised learning"], "top_k_doc_id": [796, 797, 798, 6588, 1284, 3279, 7839, 5744, 638, 215, 7420, 1637, 7037, 3358, 7818], "orig_top_k_doc_id": [796, 6588, 797, 798, 7839, 5744, 638, 215, 1284, 3279, 7420, 1637, 7037, 3358, 7818]}, {"qid": 650, "question": "What is task success rate achieved?  in Imitation Learning of Robot Policies by Combining Language, Vision and Demonstration", "answer": ["96-97.6% using the objects color or shape and 79% using shape alone"], "top_k_doc_id": [796, 797, 798, 6588, 5749, 1075, 3286, 6589, 5748, 7818, 3775, 3188, 7843, 3774, 3185], "orig_top_k_doc_id": [796, 798, 797, 6588, 5749, 1075, 3286, 6589, 5748, 7818, 3775, 3188, 7843, 3774, 3185]}]}
{"group_id": 622, "group_size": 3, "items": [{"qid": 666, "question": "Which datasets are used to train this model? in Automating Reading Comprehension by Generating Question and Answer Pairs", "answer": ["SQUAD"], "top_k_doc_id": [2442, 3805, 2205, 2445, 2519, 7728, 1853, 2012, 1405, 7589, 7359, 2464, 4257, 4189, 352], "orig_top_k_doc_id": [2442, 1853, 2519, 7728, 2012, 2445, 1405, 3805, 7589, 7359, 2464, 2205, 4257, 4189, 352]}, {"qid": 2636, "question": "Where is a question generation model used? in Knowledge Based Machine Reading Comprehension", "answer": ["The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. ", "framework consisting of both a question answering model and a question generation model"], "top_k_doc_id": [2442, 3805, 2205, 2445, 2519, 7728, 4637, 1961, 4641, 2011, 2836, 1822, 4640, 1512, 2446], "orig_top_k_doc_id": [4637, 3805, 2205, 1961, 2442, 4641, 2011, 2445, 2519, 2836, 7728, 1822, 4640, 1512, 2446]}, {"qid": 1608, "question": "What is the baseline model used? in Review Conversational Reading Comprehension", "answer": ["The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data"], "top_k_doc_id": [2442, 3805, 2264, 2661, 2268, 3827, 1332, 3825, 6879, 2048, 4518, 2220, 3829, 2011, 2836], "orig_top_k_doc_id": [2264, 2661, 2268, 3827, 3805, 1332, 3825, 6879, 2048, 4518, 2220, 3829, 2442, 2011, 2836]}]}
{"group_id": 623, "group_size": 3, "items": [{"qid": 692, "question": "By how much do they outperform existing state-of-the-art VQA models? in Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining", "answer": ["the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X"], "top_k_doc_id": [868, 871, 3175, 867, 869, 870, 2413, 2900, 787, 3033, 3176, 3037, 2578, 510, 7138], "orig_top_k_doc_id": [871, 867, 869, 870, 3175, 868, 2900, 787, 2413, 3033, 3176, 3037, 2578, 510, 7138]}, {"qid": 693, "question": "How do they measure the correlation between manual groundings and model generated ones? in Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining", "answer": ["rank-correlation BIBREF25"], "top_k_doc_id": [868, 871, 3175, 867, 869, 870, 2413, 2900, 7163, 2899, 2733, 1217, 4278, 4523, 1527], "orig_top_k_doc_id": [871, 867, 870, 869, 2900, 868, 7163, 2899, 2413, 2733, 1217, 4278, 4523, 3175, 1527]}, {"qid": 2508, "question": "To which previous papers does this work compare its results? in Task-driven Visual Saliency and Attention-based Visual Question Answering", "answer": ["holistic, TraAtt, RegAtt, ConAtt, ConAtt, iBOWIMG , VQA, VQA, WTL , NMN , SAN , AMA , FDA , D-NMN, DMN+"], "top_k_doc_id": [868, 871, 3175, 4267, 4268, 5968, 3033, 510, 7148, 4523, 2899, 1120, 511, 1940, 3798], "orig_top_k_doc_id": [4267, 4268, 5968, 3033, 510, 7148, 4523, 2899, 868, 871, 1120, 511, 1940, 3798, 3175]}]}
{"group_id": 624, "group_size": 3, "items": [{"qid": 752, "question": "Does the performance increase using their method? in Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "answer": ["The multi-task model outperforms the single-task model at all data sizes, but none have an overall benefit from the open vocabulary system"], "top_k_doc_id": [1325, 2725, 5850, 7541, 400, 4928, 6508, 5991, 3357, 6395, 7237, 7285, 7138, 5455, 4727], "orig_top_k_doc_id": [2725, 1325, 5991, 7541, 3357, 6395, 7237, 7285, 7138, 5850, 5455, 4928, 6508, 400, 4727]}, {"qid": 753, "question": "What tasks are they experimenting with in this paper? in Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "answer": ["Slot filling, we consider the actions that a user might perform via apps on their phone, The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant"], "top_k_doc_id": [1325, 2725, 5850, 7541, 400, 4928, 6508, 2556, 655, 4545, 1389, 686, 7363, 1560, 3175], "orig_top_k_doc_id": [2725, 1325, 2556, 6508, 7541, 5850, 4928, 400, 655, 4545, 1389, 686, 7363, 1560, 3175]}, {"qid": 754, "question": "What is the size of the open vocabulary? in Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "answer": ["No"], "top_k_doc_id": [1325, 2725, 5850, 7541, 4951, 6498, 7163, 1165, 7476, 6016, 7351, 3839, 3069, 940, 4414], "orig_top_k_doc_id": [4951, 7541, 1325, 6498, 5850, 2725, 7163, 1165, 7476, 6016, 7351, 3839, 3069, 940, 4414]}]}
{"group_id": 625, "group_size": 3, "items": [{"qid": 806, "question": "Which baseline methods are used? in Efficient Attention using a Fixed-Size Memory Representation", "answer": ["standard parametrized attention and a non-attention baseline"], "top_k_doc_id": [1012, 1013, 163, 3559, 4522, 4804, 6510, 7336, 3652, 3825, 7490, 1545, 6543, 2188, 5649], "orig_top_k_doc_id": [1013, 7336, 3652, 4804, 4522, 3825, 6510, 1012, 7490, 1545, 163, 6543, 3559, 2188, 5649]}, {"qid": 808, "question": "Which datasets are used in experiments? in Efficient Attention using a Fixed-Size Memory Representation", "answer": ["Sequence Copy Task and WMT'17"], "top_k_doc_id": [1012, 1013, 163, 3559, 4522, 4804, 6510, 6693, 6509, 3840, 7455, 6545, 682, 7371, 7612], "orig_top_k_doc_id": [1013, 6510, 1012, 4522, 3559, 163, 6693, 6509, 3840, 7455, 4804, 6545, 682, 7371, 7612]}, {"qid": 807, "question": "How much is the BLEU score? in Efficient Attention using a Fixed-Size Memory Representation", "answer": ["Ranges from 44.22 to 100.00 depending on K and the sequence length."], "top_k_doc_id": [1012, 1013, 4850, 1011, 666, 6598, 3824, 1946, 2815, 1235, 3687, 5495, 4213, 7582, 4849], "orig_top_k_doc_id": [4850, 1011, 666, 6598, 1012, 3824, 1946, 2815, 1235, 3687, 5495, 1013, 4213, 7582, 4849]}]}
{"group_id": 626, "group_size": 3, "items": [{"qid": 858, "question": "How is the data in RAFAEL labelled? in Boosting Question Answering by Deep Entity Recognition", "answer": ["Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner"], "top_k_doc_id": [1090, 1094, 1096, 1098, 1100, 1101, 1092, 7677, 1091, 1095, 7244, 2733, 3530, 1152, 2823], "orig_top_k_doc_id": [1094, 1100, 1090, 1098, 1101, 1096, 1091, 1095, 7244, 1092, 7677, 2733, 3530, 1152, 2823]}, {"qid": 859, "question": "How do they handle polysemous words in their entity library? in Boosting Question Answering by Deep Entity Recognition", "answer": ["only the first word sense (usually the most common) is taken into account"], "top_k_doc_id": [1090, 1094, 1096, 1098, 1100, 1101, 1092, 7677, 1422, 1099, 4755, 4841, 5048, 5761, 22], "orig_top_k_doc_id": [1096, 1094, 1098, 1422, 1090, 1092, 1099, 4755, 1100, 1101, 7677, 4841, 5048, 5761, 22]}, {"qid": 857, "question": "Do they compare DeepER against other approaches? in Boosting Question Answering by Deep Entity Recognition", "answer": ["Yes"], "top_k_doc_id": [1090, 1094, 1096, 1098, 1100, 1101, 1099, 7389, 352, 2733, 4819, 3344, 6257, 3540, 6070], "orig_top_k_doc_id": [1098, 1094, 1100, 1090, 1099, 1101, 1096, 7389, 352, 2733, 4819, 3344, 6257, 3540, 6070]}]}
{"group_id": 627, "group_size": 3, "items": [{"qid": 931, "question": "By how much do they outperform previous state-of-the-art models? in Sex Trafficking Detection with Ordinal Regression Neural Networks", "answer": ["Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)"], "top_k_doc_id": [1199, 1200, 1201, 1202, 1203, 1204, 3201, 5008, 449, 7764, 7232, 5485, 6106, 3507, 1077], "orig_top_k_doc_id": [1199, 1203, 1201, 1202, 1204, 1200, 7764, 7232, 449, 5485, 6106, 3507, 3201, 1077, 5008]}, {"qid": 932, "question": "Do they use pretrained word embeddings? in Sex Trafficking Detection with Ordinal Regression Neural Networks", "answer": ["Yes"], "top_k_doc_id": [1199, 1200, 1201, 1202, 1203, 1204, 3201, 5008, 449, 7764, 1704, 105, 450, 7234, 5737], "orig_top_k_doc_id": [1199, 1203, 1201, 1204, 1202, 1200, 1704, 105, 449, 5008, 450, 3201, 7764, 7234, 5737]}, {"qid": 933, "question": "How is the lexicon of trafficking flags expanded? in Sex Trafficking Detection with Ordinal Regression Neural Networks", "answer": ["re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones"], "top_k_doc_id": [1199, 1200, 1201, 1202, 1203, 1204, 3201, 5008, 853, 854, 5525, 2343, 5233, 859, 1480], "orig_top_k_doc_id": [1199, 1203, 1204, 1201, 1202, 853, 854, 5525, 2343, 1200, 5233, 859, 3201, 5008, 1480]}]}
{"group_id": 628, "group_size": 3, "items": [{"qid": 965, "question": "By how much they improve over the previous state-of-the-art? in StructSum: Incorporating Latent and Explicit Sentence Dependencies for Single Document Summarization", "answer": ["1.08 points in ROUGE-L over our base pointer-generator model , 0.6 points in ROUGE-1"], "top_k_doc_id": [1251, 1254, 6927, 7761, 599, 1252, 1253, 1255, 7763, 2335, 1132, 7762, 7137, 7764, 4829], "orig_top_k_doc_id": [1255, 1251, 1253, 1254, 6927, 1252, 599, 7763, 2335, 7761, 1132, 7762, 7137, 7764, 4829]}, {"qid": 966, "question": "Is there any evidence that encoders with latent structures work well on other tasks? in StructSum: Incorporating Latent and Explicit Sentence Dependencies for Single Document Summarization", "answer": ["Yes"], "top_k_doc_id": [1251, 1254, 6927, 7761, 599, 1252, 1253, 1255, 5540, 598, 5541, 1768, 2597, 6433, 1699], "orig_top_k_doc_id": [1251, 1255, 1253, 1254, 1252, 5540, 598, 5541, 599, 1768, 6927, 2597, 6433, 7761, 1699]}, {"qid": 2881, "question": "Why are current ELS's not sufficiently effective? in Entity Commonsense Representation for Neural Abstractive Summarization", "answer": ["Linked entities may be ambiguous or too common", "linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity."], "top_k_doc_id": [1251, 1254, 6927, 7761, 5076, 5077, 5474, 5079, 3718, 7514, 6928, 3157, 5540, 7762, 5804], "orig_top_k_doc_id": [5076, 5077, 5474, 5079, 3718, 7761, 7514, 6928, 6927, 3157, 5540, 7762, 5804, 1254, 1251]}]}
{"group_id": 629, "group_size": 3, "items": [{"qid": 967, "question": "Do they report results only on English? in Effective Use of Transformer Networks for Entity Tracking", "answer": ["No"], "top_k_doc_id": [1256, 1257, 1258, 1259, 1260, 1261, 3358, 2421, 4546, 426, 3361, 2554, 3261, 5277, 2494], "orig_top_k_doc_id": [1256, 1261, 1257, 1259, 1260, 426, 4546, 3358, 3361, 1258, 2554, 2421, 3261, 5277, 2494]}, {"qid": 969, "question": "In what way is the input restructured? in Effective Use of Transformer Networks for Entity Tracking", "answer": ["In four entity-centric ways - entity-first, entity-last, document-level and sentence-level"], "top_k_doc_id": [1256, 1257, 1258, 1259, 1260, 1261, 3358, 2421, 4546, 3357, 7373, 5576, 4545, 5497, 2083], "orig_top_k_doc_id": [1256, 1257, 1260, 1261, 3358, 1258, 3357, 1259, 7373, 4546, 5576, 4545, 2421, 5497, 2083]}, {"qid": 968, "question": "What evidence do they present that the model attends to shallow context clues? in Effective Use of Transformer Networks for Entity Tracking", "answer": ["Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues"], "top_k_doc_id": [1256, 1257, 1258, 1259, 1260, 1261, 3358, 684, 2048, 6041, 3549, 2050, 2005, 5184, 1033], "orig_top_k_doc_id": [1256, 1260, 1257, 1258, 684, 1259, 2048, 6041, 3549, 2050, 1261, 2005, 5184, 1033, 3358]}]}
{"group_id": 630, "group_size": 3, "items": [{"qid": 995, "question": "How many documents are in the Indiscapes dataset? in Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts", "answer": ["508"], "top_k_doc_id": [1308, 1309, 1310, 1311, 1312, 6014, 7044, 285, 1790, 6010, 6011, 2734, 2584, 2736, 2578], "orig_top_k_doc_id": [1312, 1308, 1309, 1311, 1310, 6014, 6010, 7044, 6011, 2734, 285, 2584, 2736, 1790, 2578]}, {"qid": 996, "question": "What language(s) are the manuscripts written in? in Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts", "answer": ["No"], "top_k_doc_id": [1308, 1309, 1310, 1311, 1312, 6014, 7044, 285, 1790, 2544, 2714, 2587, 2545, 2715, 3465], "orig_top_k_doc_id": [1312, 1308, 1309, 1311, 1310, 6014, 2544, 2714, 2587, 2545, 2715, 1790, 7044, 3465, 285]}, {"qid": 994, "question": "What accuracy does CNN model achieve? in Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts", "answer": ["Combined per-pixel accuracy for character line segments is 74.79"], "top_k_doc_id": [1308, 1309, 1310, 1311, 1312, 6014, 7044, 4527, 3903, 3653, 4362, 6881, 4844, 5655, 6280], "orig_top_k_doc_id": [1308, 1312, 1309, 1311, 1310, 6014, 7044, 4527, 3903, 3653, 4362, 6881, 4844, 5655, 6280]}]}
{"group_id": 631, "group_size": 3, "items": [{"qid": 1008, "question": "Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought? in Gated Convolutional Neural Networks for Domain Adaptation", "answer": ["No"], "top_k_doc_id": [1325, 1326, 1327, 1328, 831, 4274, 7546, 2701, 418, 1267, 3815, 2056, 1013, 3825, 7363], "orig_top_k_doc_id": [1325, 2701, 1328, 1327, 418, 1326, 831, 1267, 3815, 2056, 7546, 4274, 1013, 3825, 7363]}, {"qid": 1009, "question": "Are there conceptual benefits to using GCNs over more complex architectures like attention? in Gated Convolutional Neural Networks for Domain Adaptation", "answer": ["Yes"], "top_k_doc_id": [1325, 1326, 1327, 1328, 831, 4274, 7546, 4811, 6016, 7077, 2306, 2607, 2979, 5053, 886], "orig_top_k_doc_id": [1325, 1328, 1327, 1326, 4811, 4274, 6016, 7077, 2306, 2607, 2979, 7546, 5053, 831, 886]}, {"qid": 1007, "question": "For the purposes of this paper, how is something determined to be domain specific knowledge? in Gated Convolutional Neural Networks for Domain Adaptation", "answer": ["reviews under distinct product categories are considered specific domain knowledge"], "top_k_doc_id": [1325, 1326, 1327, 1328, 2306, 2725, 4487, 6016, 2056, 1625, 1318, 165, 7362, 6479, 3507], "orig_top_k_doc_id": [1325, 1328, 2306, 1326, 1327, 2725, 4487, 6016, 2056, 1625, 1318, 165, 7362, 6479, 3507]}]}
{"group_id": 632, "group_size": 3, "items": [{"qid": 1062, "question": "Which dataset do they experiment with? in Neural Language Modeling by Jointly Learning Syntax and Lexicon", "answer": ["Penn Treebank, Text8, WSJ10"], "top_k_doc_id": [1389, 2948, 4674, 4678, 624, 1784, 2132, 4221, 6070, 2136, 2135, 2021, 249, 2133, 5422], "orig_top_k_doc_id": [2136, 4221, 4678, 1389, 2135, 4674, 6070, 2132, 1784, 2021, 2948, 624, 249, 2133, 5422]}, {"qid": 1063, "question": "How do they measure performance of language model tasks? in Neural Language Modeling by Jointly Learning Syntax and Lexicon", "answer": ["BPC, Perplexity"], "top_k_doc_id": [1389, 2948, 4674, 4678, 624, 1784, 2132, 4221, 6070, 3150, 3490, 4257, 4538, 7066, 3938], "orig_top_k_doc_id": [1389, 3150, 4674, 3490, 4221, 6070, 2948, 4678, 2132, 1784, 4257, 4538, 624, 7066, 3938]}, {"qid": 1061, "question": "How do they show their model discovers underlying syntactic structure? in Neural Language Modeling by Jointly Learning Syntax and Lexicon", "answer": ["By visualizing syntactic distance estimated by the parsing network"], "top_k_doc_id": [1389, 2948, 4674, 4678, 5183, 3490, 2701, 7053, 3938, 3985, 2506, 2136, 5885, 4340, 4676], "orig_top_k_doc_id": [4674, 1389, 5183, 3490, 2701, 2948, 4678, 7053, 3938, 3985, 2506, 2136, 5885, 4340, 4676]}]}
{"group_id": 633, "group_size": 3, "items": [{"qid": 1074, "question": "What QA models were used? in QA4IE: A Question Answering based Framework for Information Extraction", "answer": ["A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer."], "top_k_doc_id": [1423, 1424, 1422, 1425, 1426, 1427, 1428, 559, 1091, 490, 7351, 5735, 7352, 2519, 19], "orig_top_k_doc_id": [1422, 1423, 1424, 1428, 1426, 1425, 1427, 559, 1091, 490, 7351, 5735, 7352, 2519, 19]}, {"qid": 1076, "question": "Was this benchmark automatically created from an existing dataset? in QA4IE: A Question Answering based Framework for Information Extraction", "answer": ["No"], "top_k_doc_id": [1423, 1424, 1422, 1425, 1426, 1427, 1428, 1961, 4637, 361, 606, 883, 5997, 2893, 7605], "orig_top_k_doc_id": [1422, 1424, 1423, 1428, 1426, 1425, 1961, 1427, 4637, 361, 606, 883, 5997, 2893, 7605]}, {"qid": 4720, "question": "Which movie subtitles dataset did they use? in Translating Questions into Answers using DBPedia n-triples", "answer": ["the OpenSubtitles dataset BIBREF18", "OpenSubtitles dataset BIBREF18"], "top_k_doc_id": [1423, 1424, 7352, 7351, 7353, 6951, 5985, 6950, 4201, 3412, 2169, 2907, 5844, 2914, 6949], "orig_top_k_doc_id": [7352, 7351, 7353, 6951, 5985, 1424, 6950, 4201, 3412, 2169, 2907, 5844, 1423, 2914, 6949]}]}
{"group_id": 634, "group_size": 3, "items": [{"qid": 1096, "question": "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies? in Efficient Vector Representation for Documents through Corruption", "answer": ["Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained."], "top_k_doc_id": [1447, 1448, 1449, 5750, 5751, 5754, 4997, 1396, 4890, 5753, 1469, 5755, 827, 799, 4684], "orig_top_k_doc_id": [5750, 1449, 1447, 5754, 4997, 1448, 1396, 4890, 5753, 1469, 5755, 827, 5751, 799, 4684]}, {"qid": 1097, "question": "How do they determine which words are informative? in Efficient Vector Representation for Documents through Corruption", "answer": ["Informative are those that will not be suppressed by regularization performed."], "top_k_doc_id": [1447, 1448, 1449, 5750, 5751, 5754, 1450, 1451, 3190, 4317, 5297, 747, 3670, 6256, 5518], "orig_top_k_doc_id": [1447, 1448, 1449, 1450, 1451, 3190, 5751, 5750, 4317, 5297, 747, 3670, 6256, 5754, 5518]}, {"qid": 1095, "question": "Which language models do they compare against? in Efficient Vector Representation for Documents through Corruption", "answer": ["RNNLM BIBREF11"], "top_k_doc_id": [1447, 1448, 1449, 1450, 2098, 5087, 5752, 2649, 5157, 2657, 1934, 804, 1683, 6810, 1886], "orig_top_k_doc_id": [1447, 1449, 1448, 1450, 2098, 5087, 5752, 2649, 5157, 2657, 1934, 804, 1683, 6810, 1886]}]}
{"group_id": 635, "group_size": 3, "items": [{"qid": 1099, "question": "How does soft contextual data augmentation work? in Microsoft Research Asia's Systems for WMT19", "answer": ["softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary"], "top_k_doc_id": [1453, 1455, 1456, 1458, 7854, 4484, 3299, 7848, 488, 4239, 485, 1457, 6274, 4242, 2209], "orig_top_k_doc_id": [1458, 4484, 3299, 7848, 1453, 1456, 488, 4239, 485, 1457, 1455, 6274, 4242, 7854, 2209]}, {"qid": 1100, "question": "How does muli-agent dual learning work? in Microsoft Research Asia's Systems for WMT19", "answer": ["MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models."], "top_k_doc_id": [1453, 1455, 1456, 1458, 7854, 4627, 5429, 2700, 3233, 3232, 3772, 5427, 5428, 2841, 4626], "orig_top_k_doc_id": [1458, 1453, 4627, 5429, 2700, 3233, 3232, 3772, 5427, 5428, 2841, 1455, 7854, 1456, 4626]}, {"qid": 1098, "question": "What is their best performance on the largest language direction dataset? in Microsoft Research Asia's Systems for WMT19", "answer": ["No"], "top_k_doc_id": [1453, 1455, 1456, 1458, 1560, 7269, 5429, 2276, 6274, 6560, 2967, 7853, 801, 4862, 4415], "orig_top_k_doc_id": [1458, 1456, 1560, 7269, 5429, 2276, 6274, 6560, 2967, 1455, 1453, 7853, 801, 4862, 4415]}]}
{"group_id": 636, "group_size": 3, "items": [{"qid": 1115, "question": "Which language-pair had the better performance? in Using Whole Document Context in Neural Machine Translation", "answer": ["French-English"], "top_k_doc_id": [1247, 1322, 1476, 2761, 1250, 5669, 6595, 1374, 2760, 7669, 5835, 4863, 2519, 4540, 1479], "orig_top_k_doc_id": [1476, 2761, 1374, 2760, 7669, 1250, 6595, 5835, 4863, 2519, 1322, 5669, 4540, 1479, 1247]}, {"qid": 1117, "question": "What evaluation metrics did they use? in Using Whole Document Context in Neural Machine Translation", "answer": ["BLEU and TER scores"], "top_k_doc_id": [1247, 1322, 1476, 2761, 1250, 5669, 6595, 2225, 6472, 354, 6041, 6927, 4712, 6259, 1246], "orig_top_k_doc_id": [1322, 5669, 1247, 1250, 1476, 2225, 6472, 354, 6595, 6041, 6927, 4712, 6259, 2761, 1246]}, {"qid": 1116, "question": "Which datasets were used in the experiment? in Using Whole Document Context in Neural Machine Translation", "answer": ["WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset"], "top_k_doc_id": [1247, 1322, 1476, 2761, 4712, 1249, 7784, 4827, 2648, 1374, 5835, 1246, 7352, 5869, 5672], "orig_top_k_doc_id": [4712, 1476, 1247, 1249, 7784, 4827, 2648, 1374, 5835, 1246, 2761, 7352, 1322, 5869, 5672]}]}
{"group_id": 637, "group_size": 3, "items": [{"qid": 1142, "question": "Is the problem of determining whether a given model would generate an infinite sequence is a decidable problem?   in Consistency of a Recurrent Language Model With Respect to Incomplete Decoding", "answer": ["No"], "top_k_doc_id": [381, 1521, 1522, 1523, 1524, 1525, 1526, 4951, 5634, 7364, 7642, 6669, 6670, 7656, 5506], "orig_top_k_doc_id": [1521, 1523, 1524, 1526, 1522, 1525, 7642, 6669, 6670, 4951, 7656, 381, 7364, 5634, 5506]}, {"qid": 1143, "question": "Is infinite-length sequence generation a result of training with maximum likelihood? in Consistency of a Recurrent Language Model With Respect to Incomplete Decoding", "answer": ["There are is a strong conjecture that it might be the reason but it is not proven."], "top_k_doc_id": [381, 1521, 1522, 1523, 1524, 1525, 1526, 4951, 5634, 7364, 5691, 572, 3808, 2024, 5823], "orig_top_k_doc_id": [1521, 1526, 1524, 1525, 1523, 1522, 4951, 7364, 5691, 572, 3808, 381, 5634, 2024, 5823]}, {"qid": 1141, "question": "How much improvement is gained from the proposed approaches? in Consistency of a Recurrent Language Model With Respect to Incomplete Decoding", "answer": ["It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio."], "top_k_doc_id": [381, 1521, 1522, 1523, 1524, 1525, 1526, 572, 573, 7693, 4766, 96, 382, 4844, 7643], "orig_top_k_doc_id": [1524, 1523, 1526, 1525, 1521, 1522, 572, 573, 7693, 4766, 96, 381, 382, 4844, 7643]}]}
{"group_id": 638, "group_size": 3, "items": [{"qid": 1149, "question": "What open relation extraction tasks did they experiment on? in Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction", "answer": ["verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation."], "top_k_doc_id": [1533, 1536, 1537, 1538, 1539, 1540, 2917, 4159, 5186, 5735, 4075, 4074, 5736, 7351, 4158], "orig_top_k_doc_id": [1540, 1533, 1539, 1538, 1537, 4159, 1536, 5186, 5735, 2917, 4075, 4074, 5736, 7351, 4158]}, {"qid": 1150, "question": "How is Logician different from traditional seq2seq models? in Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction", "answer": ["restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information"], "top_k_doc_id": [1533, 1536, 1537, 1538, 1539, 1540, 2917, 1535, 490, 6543, 7479, 3857, 5632, 3851, 404], "orig_top_k_doc_id": [1540, 1533, 1539, 1538, 1537, 1536, 1535, 490, 6543, 7479, 3857, 5632, 3851, 404, 2917]}, {"qid": 1151, "question": "What's the size of the previous largest OpenIE dataset? in Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction", "answer": ["3,200 sentences"], "top_k_doc_id": [1533, 1536, 1537, 1538, 1539, 1540, 1853, 1535, 854, 4799, 2168, 5226, 6083, 7351, 4440], "orig_top_k_doc_id": [1540, 1533, 1539, 1853, 1535, 854, 1538, 4799, 1537, 2168, 5226, 6083, 1536, 7351, 4440]}]}
{"group_id": 639, "group_size": 3, "items": [{"qid": 1152, "question": "How is data for RTFM collected? in RTFM: Generalising to Novel Environment Dynamics via Reading", "answer": ["No"], "top_k_doc_id": [312, 1541, 1542, 1544, 1545, 1546, 2840, 7625, 242, 7530, 642, 1070, 1074, 3805, 313], "orig_top_k_doc_id": [1541, 1545, 1544, 1542, 7530, 2840, 312, 1546, 642, 242, 1070, 1074, 3805, 313, 7625]}, {"qid": 1154, "question": "How does propose model model that capture three-way interactions? in RTFM: Generalising to Novel Environment Dynamics via Reading", "answer": [" We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for . The final output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation."], "top_k_doc_id": [312, 1541, 1542, 1544, 1545, 1546, 2840, 7625, 242, 7530, 1543, 2446, 5519, 4140, 6276], "orig_top_k_doc_id": [1541, 1545, 1542, 1544, 1543, 312, 242, 7530, 2446, 5519, 2840, 4140, 1546, 7625, 6276]}, {"qid": 1153, "question": "How better is performance of proposed model compared to baselines? in RTFM: Generalising to Novel Environment Dynamics via Reading", "answer": ["Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 ."], "top_k_doc_id": [312, 1541, 1542, 1544, 1545, 1546, 2840, 7625, 642, 1543, 2121, 406, 4942, 4915, 2264], "orig_top_k_doc_id": [1541, 1544, 1545, 2840, 312, 1542, 642, 7625, 1543, 2121, 406, 4942, 1546, 4915, 2264]}]}
{"group_id": 640, "group_size": 3, "items": [{"qid": 1158, "question": "Will these findings be robust through different datasets and different question answering algorithms? in Improving Span-based Question Answering Systems with Coarsely Labeled Data", "answer": ["Yes"], "top_k_doc_id": [407, 1551, 1552, 1553, 1554, 1555, 2519, 787, 788, 1362, 2461, 2578, 1560, 2442, 2464], "orig_top_k_doc_id": [1555, 1554, 1551, 1553, 2519, 787, 1552, 2461, 407, 2578, 1560, 2442, 1362, 2464, 788]}, {"qid": 1159, "question": "What is the underlying question answering algorithm? in Improving Span-based Question Answering Systems with Coarsely Labeled Data", "answer": ["The system extends BiDAF BIBREF4 with self-attention"], "top_k_doc_id": [407, 1551, 1552, 1553, 1554, 1555, 2519, 787, 788, 1362, 5236, 5238, 1120, 4640, 491], "orig_top_k_doc_id": [1554, 1551, 1555, 1553, 1552, 2519, 5236, 5238, 1362, 407, 1120, 787, 4640, 788, 491]}, {"qid": 1160, "question": "What datasets have this method been evaluated on? in Improving Span-based Question Answering Systems with Coarsely Labeled Data", "answer": ["document-level variants of the SQuAD dataset "], "top_k_doc_id": [407, 1551, 1552, 1553, 1554, 1555, 2519, 1109, 5157, 5044, 2442, 491, 1853, 5609, 384], "orig_top_k_doc_id": [1555, 1554, 1551, 1553, 2519, 1552, 1109, 5157, 5044, 2442, 491, 1853, 407, 5609, 384]}]}
{"group_id": 641, "group_size": 3, "items": [{"qid": 1167, "question": "How much is performance improved by disabling attention in certain heads? in Revealing the Dark Secrets of BERT", "answer": ["disabling the first layer in the RTE task gives a significant boost, resulting in an absolute performance gain of 3.2%,  this operation vary across tasks"], "top_k_doc_id": [1560, 1562, 1561, 1563, 1564, 2051, 2494, 2682, 2944, 3276, 7002, 3274, 22, 534, 2268], "orig_top_k_doc_id": [1563, 1564, 1562, 2051, 2682, 1561, 7002, 3274, 2494, 2944, 22, 1560, 534, 2268, 3276]}, {"qid": 1168, "question": "In which certain heads was attention disabled in experiments? in Revealing the Dark Secrets of BERT", "answer": ["single head, disabling a whole layer, that is, all 12 heads in a given layer"], "top_k_doc_id": [1560, 1562, 1561, 1563, 1564, 2051, 2494, 2682, 2944, 3276, 4271, 3736, 2050, 2121, 7119], "orig_top_k_doc_id": [1563, 1561, 1562, 4271, 3736, 3276, 2050, 2121, 2682, 2051, 1560, 1564, 7119, 2944, 2494]}, {"qid": 1169, "question": "What handcrafter features-of-interest are used? in Revealing the Dark Secrets of BERT", "answer": ["nouns, verbs, pronouns, subjects, objects, negation words, special BERT tokens"], "top_k_doc_id": [1560, 1562, 676, 2121, 1896, 7121, 4739, 2088, 3130, 6356, 853, 1955, 3736, 4718, 3949], "orig_top_k_doc_id": [676, 2121, 1896, 7121, 4739, 2088, 1560, 3130, 6356, 853, 1955, 3736, 1562, 4718, 3949]}]}
{"group_id": 642, "group_size": 3, "items": [{"qid": 1261, "question": "How do they align the synthetic data? in Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "answer": ["By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section."], "top_k_doc_id": [1720, 1721, 1724, 4952, 5839, 1722, 1723, 3260, 3868, 5166, 5167, 5672, 5838, 6266, 4031], "orig_top_k_doc_id": [1720, 1721, 5167, 6266, 5672, 5838, 4952, 3260, 1724, 5166, 3868, 1723, 5839, 1722, 4031]}, {"qid": 1262, "question": "Where do they collect the synthetic data? in Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "answer": ["Yes"], "top_k_doc_id": [1720, 1721, 1724, 4952, 5839, 1722, 1723, 3260, 3868, 5166, 5167, 5672, 5838, 6266, 4027], "orig_top_k_doc_id": [1720, 1721, 5167, 3868, 6266, 1724, 3260, 5166, 1723, 5838, 4027, 5672, 5839, 4952, 1722]}, {"qid": 1260, "question": "How many improvements on the French-German translation benchmark? in Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "answer": ["one"], "top_k_doc_id": [1720, 1721, 1724, 4952, 5839, 4030, 4031, 5455, 4027, 5456, 4696, 7436, 7437, 7023, 4953], "orig_top_k_doc_id": [1720, 4030, 4031, 1721, 1724, 5455, 4027, 5839, 5456, 4696, 7436, 7437, 7023, 4953, 4952]}]}
{"group_id": 643, "group_size": 3, "items": [{"qid": 1299, "question": "Which translation system do they use to translate to English? in Back Attention Knowledge Transfer for Low-resource Named Entity Recognition", "answer": ["Attention-based translation model with convolution sequence to sequence model"], "top_k_doc_id": [45, 1777, 1781, 1782, 4590, 4591, 6031, 6943, 4573, 5621, 2836, 3618, 5868, 5026, 6036], "orig_top_k_doc_id": [1781, 1782, 1777, 4590, 4591, 6943, 2836, 5621, 3618, 6031, 5868, 5026, 6036, 45, 4573]}, {"qid": 1300, "question": "Which languages do they work with? in Back Attention Knowledge Transfer for Low-resource Named Entity Recognition", "answer": ["German, Spanish, Chinese"], "top_k_doc_id": [45, 1777, 1781, 1782, 4590, 4591, 6031, 6943, 4573, 5621, 501, 3617, 659, 497, 2823], "orig_top_k_doc_id": [1782, 1781, 4590, 4573, 1777, 501, 6943, 4591, 3617, 45, 5621, 6031, 659, 497, 2823]}, {"qid": 1301, "question": "Which pre-trained English NER model do they use? in Back Attention Knowledge Transfer for Low-resource Named Entity Recognition", "answer": ["Bidirectional LSTM based NER model of Flair"], "top_k_doc_id": [45, 1777, 1781, 1782, 4590, 4591, 6031, 6943, 5081, 2679, 500, 4756, 2823, 501, 3140], "orig_top_k_doc_id": [1781, 1782, 4590, 4591, 6031, 45, 5081, 2679, 6943, 500, 4756, 1777, 2823, 501, 3140]}]}
{"group_id": 644, "group_size": 3, "items": [{"qid": 1336, "question": "Did they experiment with the tool? in Seshat: A tool for managing and verifying annotation campaigns of audio data", "answer": ["Yes"], "top_k_doc_id": [1834, 1835, 1836, 1837, 1838, 2931, 228, 3756, 225, 3907, 7097, 6111, 7697, 7857, 3757], "orig_top_k_doc_id": [1834, 1838, 1837, 1835, 1836, 2931, 225, 3907, 7097, 6111, 7697, 228, 7857, 3756, 3757]}, {"qid": 1338, "question": "Is this software available to the public? in Seshat: A tool for managing and verifying annotation campaigns of audio data", "answer": ["Yes"], "top_k_doc_id": [1834, 1835, 1836, 1837, 1838, 2931, 228, 3756, 6110, 1147, 7548, 229, 6464, 6155, 2803], "orig_top_k_doc_id": [1834, 1835, 1836, 1837, 1838, 2931, 6110, 1147, 228, 3756, 7548, 229, 6464, 6155, 2803]}, {"qid": 1337, "question": "Can it be used for any language? in Seshat: A tool for managing and verifying annotation campaigns of audio data", "answer": ["No"], "top_k_doc_id": [1834, 1835, 1836, 1837, 1838, 2931, 2634, 4646, 6370, 2633, 2631, 7149, 6110, 4692, 762], "orig_top_k_doc_id": [1834, 1835, 1836, 1838, 1837, 2931, 2634, 4646, 6370, 2633, 2631, 7149, 6110, 4692, 762]}]}
{"group_id": 645, "group_size": 3, "items": [{"qid": 1340, "question": "How are labels propagated using this approach? in Discriminating between similar languages in Twitter using label propagation", "answer": ["We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. "], "top_k_doc_id": [1432, 1839, 3575, 2794, 6819, 6821, 6820, 2788, 340, 6122, 3015, 2776, 1062, 2796, 2793], "orig_top_k_doc_id": [1839, 6820, 2788, 1432, 3575, 2794, 340, 6122, 6819, 6821, 3015, 2776, 1062, 2796, 2793]}, {"qid": 1341, "question": "What information is contained in the social graph of tweet authors? in Discriminating between similar languages in Twitter using label propagation", "answer": [" the graph, composed of three types of nodes: tweets (T), users (U) and the \u201cworld\u201d (W). Edges are created between nodes and weighted as follows: T-T the unigram cosine similarity between tweets, T-U weighted 100 between a tweet and its author, U-U weighted 1 between two users in a \u201cfollows\u201d relationship and U-W weighted 0.001 to ensure a connected graph for the mad algorithm."], "top_k_doc_id": [1432, 1839, 3575, 2794, 6819, 6821, 5135, 7625, 5324, 331, 443, 1174, 2404, 1499, 442], "orig_top_k_doc_id": [1839, 3575, 2794, 6821, 1432, 5135, 7625, 5324, 331, 443, 1174, 2404, 1499, 442, 6819]}, {"qid": 1339, "question": "What shared task does this system achieve SOTA in? in Discriminating between similar languages in Twitter using label propagation", "answer": ["tweetLID workshop shared task"], "top_k_doc_id": [1432, 1839, 3575, 2788, 2789, 1435, 2792, 3344, 3348, 4214, 6033, 2798, 2797, 3347, 2618], "orig_top_k_doc_id": [1839, 2788, 1432, 2789, 1435, 2792, 3344, 3348, 4214, 6033, 2798, 2797, 3347, 2618, 3575]}]}
{"group_id": 646, "group_size": 3, "items": [{"qid": 1374, "question": "Which SBD systems did they compare? in WiSeBE: Window-based Sentence Boundary Evaluation", "answer": ["Convolutional Neural Network , bidirectional Recurrent Neural Network model with attention mechanism"], "top_k_doc_id": [1889, 1890, 1891, 1892, 819, 864, 5660, 1691, 2621, 7770, 2623, 6490, 3181, 5656, 4323], "orig_top_k_doc_id": [1889, 1890, 1891, 1892, 1691, 864, 819, 2621, 5660, 7770, 2623, 6490, 3181, 5656, 4323]}, {"qid": 1375, "question": "What makes it a more reliable metric? in WiSeBE: Window-based Sentence Boundary Evaluation", "answer": ["It takes into account the agreement between different systems"], "top_k_doc_id": [1889, 1890, 1891, 1892, 819, 864, 5660, 1126, 1127, 820, 4872, 7656, 7160, 3957, 6253], "orig_top_k_doc_id": [1890, 1889, 1891, 1892, 819, 1126, 5660, 864, 1127, 820, 4872, 7656, 7160, 3957, 6253]}, {"qid": 1373, "question": "What kind of Youtube video transcripts did they use? in WiSeBE: Window-based Sentence Boundary Evaluation", "answer": ["youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics"], "top_k_doc_id": [1889, 1890, 1891, 1892, 7557, 2213, 7558, 7556, 2345, 2210, 1484, 1813, 1814, 5066, 2211], "orig_top_k_doc_id": [1891, 1890, 1889, 7557, 1892, 2213, 7558, 7556, 2345, 2210, 1484, 1813, 1814, 5066, 2211]}]}
{"group_id": 647, "group_size": 3, "items": [{"qid": 1443, "question": "Do they use attention? in Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models", "answer": ["Yes"], "top_k_doc_id": [490, 1921, 95, 1471, 1998, 2519, 4760, 5186, 7138, 7140, 6716, 4530, 7664, 2998, 6723], "orig_top_k_doc_id": [2519, 95, 1998, 7140, 7138, 1921, 6716, 4530, 4760, 490, 5186, 7664, 1471, 2998, 6723]}, {"qid": 1444, "question": "What non-annotated datasets are considered? in Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models", "answer": ["E2E NLG challenge Dataset, The Wikipedia Company Dataset"], "top_k_doc_id": [490, 1921, 95, 1471, 1998, 2519, 4760, 5186, 7138, 3805, 5044, 6448, 94, 2900, 2435], "orig_top_k_doc_id": [1998, 95, 490, 2519, 5186, 3805, 5044, 6448, 94, 2900, 1471, 4760, 7138, 1921, 2435]}, {"qid": 3827, "question": "How does sentence construction component works? in Natural Language Generation for Non-Expert Users", "answer": ["Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph."], "top_k_doc_id": [490, 1921, 2888, 6184, 5800, 7162, 1005, 7389, 7176, 3107, 3805, 2910, 2151, 2595, 5850], "orig_top_k_doc_id": [1921, 2888, 490, 6184, 5800, 7162, 1005, 7389, 7176, 3107, 3805, 2910, 2151, 2595, 5850]}]}
{"group_id": 648, "group_size": 3, "items": [{"qid": 1465, "question": "What ensemble methods are used for best model? in BERTQA -- Attention on Steroids", "answer": ["choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer"], "top_k_doc_id": [2048, 2049, 2051, 2052, 6921, 1080, 7479, 6826, 2785, 2763, 2784, 5370, 2013, 5698, 3742], "orig_top_k_doc_id": [2051, 2052, 6921, 2049, 2048, 7479, 6826, 2785, 2763, 1080, 2784, 5370, 2013, 5698, 3742]}, {"qid": 1467, "question": "How much F1 was improved after adding skip connections? in BERTQA -- Attention on Steroids", "answer": ["Simple Skip improves F1 from 74.34 to 74.81\nTransformer Skip improes F1 from 74.34 to 74.95 "], "top_k_doc_id": [2048, 2049, 2051, 2052, 6921, 1080, 2050, 5569, 3653, 3088, 1079, 77, 78, 3291, 79], "orig_top_k_doc_id": [2051, 2049, 2052, 2048, 2050, 5569, 3653, 3088, 6921, 1080, 1079, 77, 78, 3291, 79]}, {"qid": 1466, "question": "What hyperparameters have been tuned? in BERTQA -- Attention on Steroids", "answer": ["number of coattention blocks, the batch size, and the number of epochs trained and ensembled our three best networks"], "top_k_doc_id": [2048, 2049, 2051, 2052, 6921, 1220, 2129, 7442, 6613, 3020, 1653, 2905, 2050, 4791, 3609], "orig_top_k_doc_id": [6921, 2051, 2048, 1220, 2052, 2049, 2129, 7442, 6613, 3020, 1653, 2905, 2050, 4791, 3609]}]}
{"group_id": 649, "group_size": 3, "items": [{"qid": 1468, "question": "Where do they retrieve neighbor n-grams from in their approach? in Non-Parametric Adaptation for Neural Machine Translation", "answer": ["represent every sentence by their reduced n-gram set"], "top_k_doc_id": [2053, 2054, 2055, 2056, 4541, 7687, 7780, 7429, 1922, 6520, 7430, 4526, 1000, 2238, 2284], "orig_top_k_doc_id": [2055, 2053, 2056, 2054, 7429, 1922, 7687, 6520, 7430, 4526, 7780, 1000, 4541, 2238, 2284]}, {"qid": 1471, "question": "Which similarity measure do they use in their n-gram retrieval approach? in Non-Parametric Adaptation for Neural Machine Translation", "answer": ["we define the similarity between INLINEFORM7 and INLINEFORM8 by, DISPLAYFORM0"], "top_k_doc_id": [2053, 2054, 2055, 2056, 4541, 7687, 7780, 7688, 6843, 6660, 6750, 6723, 6661, 4304, 6070], "orig_top_k_doc_id": [2056, 2053, 2055, 2054, 7687, 7688, 6843, 6660, 7780, 6750, 6723, 6661, 4304, 6070, 4541]}, {"qid": 1470, "question": "Does their combination of a non-parametric retrieval and neural network get trained end-to-end? in Non-Parametric Adaptation for Neural Machine Translation", "answer": ["Yes"], "top_k_doc_id": [2053, 2054, 2055, 2056, 4541, 7687, 7780, 4194, 7166, 6901, 1977, 5708, 1637, 2501, 4237], "orig_top_k_doc_id": [2056, 2053, 2055, 7780, 2054, 7687, 4194, 4541, 7166, 6901, 1977, 5708, 1637, 2501, 4237]}]}
{"group_id": 650, "group_size": 3, "items": [{"qid": 1484, "question": "What elements of natural language processing are proposed to analyze qualitative data? in Data Innovation for International Development: An overview of natural language processing for qualitative data analysis", "answer": ["translated the responses in multiple languages into English using machine translation, words without functional meaning (e.g. `I'), rare words that occurred in only one narrative, numbers, and punctuation were all removed, remaining words were stemmed to remove plural forms of nouns or conjugations of verbs"], "top_k_doc_id": [6010, 236, 2074, 2077, 5529, 3143, 2168, 7579, 55, 879, 523, 2078, 2181, 4470, 5966], "orig_top_k_doc_id": [2074, 2077, 236, 5529, 3143, 6010, 2168, 7579, 55, 879, 523, 2078, 2181, 4470, 5966]}, {"qid": 1866, "question": "how well this method is compared to other method? in Crowd Sourced Data Analysis: Mapping of Programming Concepts to Syntactical Patterns", "answer": ["No"], "top_k_doc_id": [6010, 236, 2730, 2270, 237, 6014, 2732, 2022, 1137, 1659, 2731, 5701, 273, 903, 3784], "orig_top_k_doc_id": [2730, 2270, 237, 236, 6014, 2732, 6010, 2022, 1137, 1659, 2731, 5701, 273, 903, 3784]}, {"qid": 3828, "question": "What are two use cases that demonstrate capability of created system? in Natural Language Generation for Non-Expert Users", "answer": ["The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project, The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference", "natural language description for workflow created by the system built in the Phylotastic project, about people and includes descriptions for certain class", "The first application is to build a natural language description of the ontologies built in an evolutionary biology project called Phylotastic, so that biologists can understand the output, without knowledge of ontologies. The second aims to create an abstract or intermediate representation of the Wikipedia pages from the BlueSky session in 2018."], "top_k_doc_id": [6010, 6184, 3587, 3798, 1921, 898, 6805, 5426, 4766, 6188, 5439, 7181, 1887, 7514, 2750], "orig_top_k_doc_id": [6184, 3587, 6010, 3798, 1921, 898, 6805, 5426, 4766, 6188, 5439, 7181, 1887, 7514, 2750]}]}
{"group_id": 651, "group_size": 3, "items": [{"qid": 1565, "question": "Are datasets publicly available? in Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery", "answer": ["Yes"], "top_k_doc_id": [2181, 2182, 2183, 2184, 2186, 2188, 2189, 2190, 854, 3744, 2187, 1664, 2104, 3300, 6194], "orig_top_k_doc_id": [2181, 2189, 2190, 2182, 2188, 2186, 2184, 2183, 2187, 3744, 1664, 2104, 854, 3300, 6194]}, {"qid": 1567, "question": "Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery? in Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery", "answer": ["Yes"], "top_k_doc_id": [2181, 2182, 2183, 2184, 2186, 2188, 2189, 2190, 854, 3744, 2730, 2127, 6195, 3742, 557], "orig_top_k_doc_id": [2181, 2189, 2190, 2182, 2184, 2188, 2186, 854, 2730, 2183, 2127, 6195, 3744, 3742, 557]}, {"qid": 1566, "question": "Are this models usually semi/supervised or unsupervised? in Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery", "answer": ["Both supervised and unsupervised, depending on the task that needs to be solved."], "top_k_doc_id": [2181, 2182, 2183, 2184, 2186, 2188, 2189, 2190, 2104, 2948, 4049, 4300, 2187, 4399, 2371], "orig_top_k_doc_id": [2189, 2181, 2190, 2184, 2182, 2188, 2186, 2104, 2948, 4049, 2183, 4300, 2187, 4399, 2371]}]}
{"group_id": 652, "group_size": 3, "items": [{"qid": 1572, "question": "What type of recurrent layers does the model use? in Encoding Word Confusion Networks with Recurrent Neural Networks for Dialog State Tracking", "answer": ["GRU"], "top_k_doc_id": [1987, 2197, 2311, 686, 1033, 4545, 4546, 7584, 7841, 7373, 4878, 1034, 5430, 4812, 3980], "orig_top_k_doc_id": [2197, 4545, 4546, 7584, 686, 1987, 7373, 2311, 4878, 1034, 5430, 1033, 4812, 3980, 7841]}, {"qid": 1573, "question": "What is a word confusion network? in Encoding Word Confusion Networks with Recurrent Neural Networks for Dialog State Tracking", "answer": ["It is a network used to encode speech lattices to maintain a rich hypothesis space."], "top_k_doc_id": [1987, 2197, 2311, 686, 1033, 4545, 4546, 7584, 7841, 2313, 6777, 2314, 1667, 7840, 7839], "orig_top_k_doc_id": [2197, 1987, 2311, 4545, 4546, 2313, 6777, 686, 2314, 7584, 1667, 7840, 7841, 1033, 7839]}, {"qid": 1631, "question": "What is a confusion network or lattice? in Bi-Directional Lattice Recurrent Neural Networks for Confidence Estimation", "answer": ["graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences"], "top_k_doc_id": [1987, 2197, 2311, 2314, 2313, 3705, 2312, 3707, 3709, 3708, 1185, 6777, 1186, 7458, 3706], "orig_top_k_doc_id": [2311, 2314, 2313, 3705, 2312, 3707, 3709, 3708, 1987, 1185, 6777, 1186, 7458, 2197, 3706]}]}
{"group_id": 653, "group_size": 3, "items": [{"qid": 1590, "question": "what is the size of this dataset? in Construction of a Japanese Word Similarity Dataset", "answer": ["No"], "top_k_doc_id": [71, 82, 2223, 2224, 5702, 83, 4191, 4192, 5703, 5699, 3351, 2956, 5709, 5839, 3350], "orig_top_k_doc_id": [2223, 2224, 82, 5702, 4192, 83, 5699, 4191, 5703, 3351, 2956, 71, 5709, 5839, 3350]}, {"qid": 1592, "question": "where does the data come from? in Construction of a Japanese Word Similarity Dataset", "answer": ["Evaluation Dataset of Japanese Lexical Simplification kodaira"], "top_k_doc_id": [71, 82, 2223, 2224, 5702, 83, 4191, 4192, 5703, 7182, 4390, 7044, 1778, 4823, 1604], "orig_top_k_doc_id": [2223, 2224, 5702, 71, 5703, 82, 4192, 83, 7182, 4390, 7044, 1778, 4823, 4191, 1604]}, {"qid": 1591, "question": "did they use a crowdsourcing platform for annotations? in Construction of a Japanese Word Similarity Dataset", "answer": ["Yes"], "top_k_doc_id": [71, 82, 2223, 2224, 5702, 6805, 6995, 5145, 5720, 5038, 2800, 7182, 3587, 3961, 414], "orig_top_k_doc_id": [2224, 2223, 5702, 6805, 6995, 5145, 5720, 5038, 2800, 7182, 82, 3587, 3961, 71, 414]}]}
{"group_id": 654, "group_size": 3, "items": [{"qid": 1594, "question": "What are the three limitations? in Towards Neural Language Evaluators", "answer": ["High scores to semantically opposite translations/summaries, Low scores to semantically related translations/summaries and High scores to unintelligible translations/summaries."], "top_k_doc_id": [2225, 2226, 3192, 1135, 1446, 4307, 4622, 5844, 6068, 4623, 567, 6171, 819, 7085, 566], "orig_top_k_doc_id": [5844, 2225, 2226, 4307, 6068, 3192, 4623, 1446, 567, 4622, 6171, 1135, 819, 7085, 566]}, {"qid": 4348, "question": "How is human evaluators' judgement measured, what was the criteria? in SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization", "answer": ["We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 \u2013 it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.", "$-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary, 1 \u2013 it is understandable and gives a brief overview of the text"], "top_k_doc_id": [2225, 2226, 3192, 1135, 1446, 4307, 4622, 6858, 6862, 6860, 6861, 2969, 2970, 5142, 2438], "orig_top_k_doc_id": [6858, 2226, 6862, 6860, 2225, 6861, 1135, 2969, 3192, 4307, 4622, 2970, 1446, 5142, 2438]}, {"qid": 1593, "question": "What is the criteria for a good metric? in Towards Neural Language Evaluators", "answer": ["The first one is that it should be highly correlated with human judgement of similarity. The second one is that it should be able to distinguish sentences which are in logical contradiction, logically unrelated or in logical agreement. The third one is that a robust evaluator should also be able to identify unintelligible sentences. The last criteria is that a good evaluation metric should not give high scores to semantically distant sentences and low scores to semantically related sentences."], "top_k_doc_id": [2225, 2226, 3192, 7085, 566, 6936, 5844, 5558, 252, 4441, 2438, 5562, 819, 6861, 5703], "orig_top_k_doc_id": [2226, 2225, 7085, 3192, 566, 6936, 5844, 5558, 252, 4441, 2438, 5562, 819, 6861, 5703]}]}
{"group_id": 655, "group_size": 3, "items": [{"qid": 1599, "question": "What is the performance proposed model achieved on AlgoList benchmark? in Natural- to formal-language generation using Tensor Product Representations", "answer": ["Full Testing Set Accuracy: 84.02\nCleaned Testing Set Accuracy: 93.48"], "top_k_doc_id": [2243, 2244, 2245, 5513, 6951, 7143, 7144, 7363, 7493, 7498, 1329, 2882, 7496, 3100, 7072], "orig_top_k_doc_id": [2243, 2244, 2245, 7493, 7144, 1329, 7498, 2882, 6951, 7143, 7496, 3100, 7363, 5513, 7072]}, {"qid": 1600, "question": "What is the performance proposed model achieved on MathQA? in Natural- to formal-language generation using Tensor Product Representations", "answer": ["Operation accuracy: 71.89\nExecution accuracy: 55.95"], "top_k_doc_id": [2243, 2244, 2245, 5513, 6951, 7143, 7144, 7363, 7493, 7498, 2248, 2246, 2247, 2249, 2523], "orig_top_k_doc_id": [2243, 2248, 2246, 2244, 2245, 7493, 2247, 2249, 7498, 6951, 7144, 7143, 7363, 5513, 2523]}, {"qid": 1598, "question": "How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed? in Natural- to formal-language generation using Tensor Product Representations", "answer": ["Full Testing Set accuracy: 84.02\nCleaned Testing Set accuracy: 93.48"], "top_k_doc_id": [2243, 2244, 2245, 2246, 2250, 2248, 2247, 2251, 2249, 6355, 6067, 4984, 4774, 7367, 1322], "orig_top_k_doc_id": [2245, 2244, 2243, 2246, 2250, 2248, 2247, 2251, 2249, 6355, 6067, 4984, 4774, 7367, 1322]}]}
{"group_id": 656, "group_size": 3, "items": [{"qid": 1606, "question": "How does the model perform in comparison to end-to-end headline generation models? in Select and Attend: Towards Controllable Content Selection in Text Generation", "answer": ["No"], "top_k_doc_id": [2258, 2259, 2260, 2262, 2263, 7447, 4934, 7194, 7200, 6566, 6716, 4425, 7617, 1473, 2887], "orig_top_k_doc_id": [2258, 2262, 6566, 2260, 2263, 6716, 4425, 2259, 7194, 4934, 7617, 1473, 7200, 2887, 7447]}, {"qid": 1607, "question": "How is the model trained to do only content selection? in Select and Attend: Towards Controllable Content Selection in Text Generation", "answer": ["target some heuristically extracted contents, treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood, reinforcement learning to approximate the marginal likelihood,  Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction"], "top_k_doc_id": [2258, 2259, 2260, 2262, 2263, 7447, 4934, 7194, 7200, 3152, 2261, 7215, 4456, 7443, 517], "orig_top_k_doc_id": [2258, 2262, 2263, 3152, 2261, 2260, 7215, 4456, 2259, 7443, 7447, 517, 4934, 7200, 7194]}, {"qid": 1605, "question": "Does the performance necessarily drop when more control is desired? in Select and Attend: Towards Controllable Content Selection in Text Generation", "answer": ["Yes"], "top_k_doc_id": [2258, 2259, 2260, 2262, 2263, 7447, 517, 516, 7617, 2261, 1445, 7443, 1235, 1863, 6687], "orig_top_k_doc_id": [2258, 2262, 2263, 517, 516, 7617, 2260, 2261, 7447, 1445, 7443, 1235, 2259, 1863, 6687]}]}
{"group_id": 657, "group_size": 3, "items": [{"qid": 1621, "question": "Is the proposed method compared to previous methods? in Markov Chain Monte-Carlo Phylogenetic Inference Construction in Computational Historical Linguistics", "answer": ["Yes"], "top_k_doc_id": [550, 551, 1284, 1381, 1870, 2287, 2288, 2289, 2290, 3383, 3384, 4687, 7232, 7233, 3491], "orig_top_k_doc_id": [2288, 2287, 2289, 2290, 551, 550, 7233, 4687, 7232, 1870, 3384, 1284, 1381, 3491, 3383]}, {"qid": 1622, "question": "What metrics are used to evaluate results? in Markov Chain Monte-Carlo Phylogenetic Inference Construction in Computational Historical Linguistics", "answer": ["No"], "top_k_doc_id": [550, 551, 1284, 1381, 1870, 2287, 2288, 2289, 2290, 3383, 3384, 4687, 7232, 7233, 6759], "orig_top_k_doc_id": [2287, 2288, 2290, 2289, 551, 1870, 550, 4687, 3383, 7233, 1284, 7232, 1381, 3384, 6759]}, {"qid": 465, "question": "How does the model compare with the MMR baseline? in Query-oriented text summarization based on hypergraph transversals", "answer": [" Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ )"], "top_k_doc_id": [550, 551, 552, 549, 544, 548, 545, 553, 543, 547, 6496, 6495, 6494, 6492, 546], "orig_top_k_doc_id": [551, 552, 549, 544, 550, 548, 545, 553, 543, 547, 6496, 6495, 6494, 6492, 546]}]}
{"group_id": 658, "group_size": 3, "items": [{"qid": 1626, "question": "What training settings did they try? in On Evaluating the Generalization of LSTM Models in Formal Languages", "answer": ["Training and testing are done in alternating steps: In each epoch, for training, we first present to an LSTM network 1000 samples in a given language, which are generated according to a certain discrete probability distribution supported on a closed finite interval. We then freeze all the weights in our model, exhaustively enumerate all the sequences in the language by their lengths, and determine the first $k$ shortest sequences whose outputs the model produces inaccurately. , experimented with 1, 2, 3, and 36 hidden units for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ . , Following the traditional approach adopted by BIBREF7 , BIBREF12 , BIBREF9 and many other studies, we train our neural network as follows. At each time step, we present one input character to our model and then ask it to predict the set of next possible characters, based on the current character and the prior hidden states. Given a vocabulary $\\mathcal {V}^{(i)}$ of size $d$ , we use a one-hot representation to encode the input values; therefore, all the input vectors are $d$ -dimensional binary vectors. The output values are $(d+1)$ -dimensional though, since they may further contain the termination symbol $\\dashv $ , in addition to the symbols in $\\mathcal {V}^{(i)}$ . The output values are not always one-hot encoded, because there can be multiple possibilities for the next character in the sequence, therefore we instead use a $k$ -hot representation to encode the output values. Our objective is to minimize the mean-squared error (MSE) of the sequence predictions."], "top_k_doc_id": [2301, 2304, 2305, 7367, 254, 689, 3956, 7284, 2906, 7509, 7439, 1389, 1670, 4526, 1874], "orig_top_k_doc_id": [2301, 2304, 2305, 689, 7367, 254, 3956, 7284, 2906, 7509, 7439, 1389, 1670, 4526, 1874]}, {"qid": 1627, "question": "How do they get the formal languages? in On Evaluating the Generalization of LSTM Models in Formal Languages", "answer": ["These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs."], "top_k_doc_id": [2301, 2304, 2305, 7367, 254, 689, 3956, 5845, 2302, 7365, 7363, 6834, 5844, 5639, 1577], "orig_top_k_doc_id": [2301, 2305, 7367, 5845, 2302, 2304, 7365, 7363, 6834, 3956, 5844, 689, 5639, 254, 1577]}, {"qid": 1628, "question": "Are the unobserved samples from the same distribution as the training data? in On Evaluating the Generalization of LSTM Models in Formal Languages", "answer": ["No"], "top_k_doc_id": [2301, 2304, 2305, 7367, 5639, 2437, 2303, 5638, 1472, 2438, 1874, 2436, 2302, 6834, 4771], "orig_top_k_doc_id": [2304, 2301, 2305, 5639, 2437, 7367, 2303, 5638, 1472, 2438, 1874, 2436, 2302, 6834, 4771]}]}
{"group_id": 659, "group_size": 3, "items": [{"qid": 1634, "question": "what are the evaluation metrics? in Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features", "answer": ["Precision, Recall, F1"], "top_k_doc_id": [2318, 2319, 2321, 2322, 2328, 2878, 5184, 5698, 4399, 4590, 6153, 1773, 6050, 2320, 5655], "orig_top_k_doc_id": [2318, 2322, 2319, 2321, 2328, 5184, 5698, 4399, 6153, 1773, 6050, 2320, 4590, 2878, 5655]}, {"qid": 1636, "question": "what are the baselines? in Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features", "answer": ["Perceptron model using the local features."], "top_k_doc_id": [2318, 2319, 2321, 2322, 2328, 2878, 5184, 5698, 4399, 4590, 4529, 1256, 6036, 4531, 7100], "orig_top_k_doc_id": [2318, 2322, 5184, 2321, 4529, 1256, 5698, 6036, 2319, 4399, 4531, 2328, 4590, 2878, 7100]}, {"qid": 1635, "question": "which datasets were used in evaluation? in Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features", "answer": ["CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0"], "top_k_doc_id": [2318, 2319, 2321, 2322, 2328, 2878, 5184, 5698, 2320, 5912, 6153, 7100, 2326, 6036, 2876], "orig_top_k_doc_id": [2318, 2319, 2322, 2320, 2328, 2878, 2321, 5912, 6153, 5698, 5184, 7100, 2326, 6036, 2876]}]}
{"group_id": 660, "group_size": 3, "items": [{"qid": 1656, "question": "Does this approach perform better than context-based word embeddings? in Synonym Discovery with Etymology-based Word Embeddings", "answer": ["Yes"], "top_k_doc_id": [2356, 5074, 5075, 434, 800, 2355, 2357, 2358, 4618, 687, 5071, 207, 5501, 7126, 2948], "orig_top_k_doc_id": [2357, 2355, 2356, 2358, 5074, 800, 5075, 4618, 687, 5071, 434, 207, 5501, 7126, 2948]}, {"qid": 1657, "question": "Have the authors tried this approach on other languages? in Synonym Discovery with Etymology-based Word Embeddings", "answer": ["No"], "top_k_doc_id": [2356, 5074, 5075, 434, 800, 2355, 2357, 2358, 4618, 3748, 714, 3750, 3751, 1926, 5073], "orig_top_k_doc_id": [2355, 2357, 2356, 2358, 4618, 800, 3748, 434, 714, 3750, 3751, 5075, 5074, 1926, 5073]}, {"qid": 2880, "question": "What dataset do they use to evaluate their method? in Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network", "answer": ["antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik", "English Wikipedia dump from June 2016"], "top_k_doc_id": [2356, 5074, 5075, 5071, 5073, 5072, 4735, 291, 4364, 156, 5249, 155, 5405, 1656, 1047], "orig_top_k_doc_id": [5071, 5073, 5072, 5075, 5074, 4735, 291, 4364, 156, 5249, 155, 5405, 2356, 1656, 1047]}]}
{"group_id": 661, "group_size": 3, "items": [{"qid": 1659, "question": "How big is the test set? in Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering", "answer": ["No"], "top_k_doc_id": [2363, 4455, 2052, 2362, 2364, 2365, 3458, 4647, 5046, 5047, 3506, 5611, 1551, 1555, 453], "orig_top_k_doc_id": [2362, 2363, 2364, 2365, 5047, 5046, 3506, 5611, 3458, 1551, 4455, 1555, 453, 2052, 4647]}, {"qid": 1660, "question": "What is coattention? in Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering", "answer": ["No"], "top_k_doc_id": [2363, 4455, 2052, 2362, 2364, 2365, 3458, 4647, 5046, 5047, 4452, 2049, 4453, 2048, 2051], "orig_top_k_doc_id": [2362, 2363, 2364, 2365, 4452, 4455, 2049, 4453, 4647, 2052, 2048, 2051, 5047, 5046, 3458]}, {"qid": 2553, "question": "How much is the gap between using the proposed objective and using only cross-entropy objective? in DCN+: Mixed Objective and Deep Residual Coattention for Question Answering", "answer": ["The mixed objective improves EM by 2.5% and F1 by 2.2%"], "top_k_doc_id": [2363, 4455, 4452, 4453, 4454, 1651, 4369, 4791, 5554, 5556, 4792, 2307, 4680, 2857, 2413], "orig_top_k_doc_id": [4452, 4455, 4453, 4454, 1651, 4369, 4791, 5554, 5556, 2363, 4792, 2307, 4680, 2857, 2413]}]}
{"group_id": 662, "group_size": 3, "items": [{"qid": 1717, "question": "How big are improvements with multilingual ASR training vs single language training? in Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for Ainu Language", "answer": ["relative WER improvement of 10%."], "top_k_doc_id": [2450, 2451, 2452, 2453, 2454, 381, 6310, 6312, 5564, 4377, 2995, 1623, 1784, 5987, 2996], "orig_top_k_doc_id": [2450, 2451, 2452, 2453, 2454, 6312, 381, 5564, 4377, 2995, 6310, 1623, 1784, 5987, 2996]}, {"qid": 1718, "question": "How much transcribed data is available for for Ainu language? in Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for Ainu Language", "answer": ["Transcribed data is available for duration of 38h 54m 38s for 8 speakers."], "top_k_doc_id": [2450, 2451, 2452, 2453, 2454, 381, 6310, 622, 4862, 3402, 2634, 373, 2632, 3401, 6110], "orig_top_k_doc_id": [2450, 2451, 2453, 2452, 2454, 622, 4862, 3402, 2634, 373, 381, 2632, 3401, 6110, 6310]}, {"qid": 1719, "question": "What is the difference between speaker-open and speaker-closed setting? in Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for Ainu Language", "answer": ["In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets., In the speaker-open condition, all the data except for the test speaker's were used for training"], "top_k_doc_id": [2450, 2451, 2452, 2453, 2454, 4863, 6110, 5765, 7244, 2351, 4862, 6350, 7793, 2352, 4368], "orig_top_k_doc_id": [2450, 2453, 2451, 2452, 2454, 4863, 6110, 5765, 7244, 2351, 4862, 6350, 7793, 2352, 4368]}]}
{"group_id": 663, "group_size": 3, "items": [{"qid": 1784, "question": "How is the robustness of the model evaluated? in Learning Explicit and Implicit Structures for Targeted Sentiment Analysis", "answer": ["10-fold cross validation"], "top_k_doc_id": [598, 2216, 2597, 2598, 2599, 2600, 2601, 2602, 3161, 4632, 2215, 3693, 6640, 6616, 5144], "orig_top_k_doc_id": [2601, 2597, 2602, 2600, 2598, 2599, 598, 4632, 2215, 2216, 3693, 6640, 6616, 3161, 5144]}, {"qid": 1785, "question": "How is the effectiveness of the model evaluated? in Learning Explicit and Implicit Structures for Targeted Sentiment Analysis", "answer": ["precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment"], "top_k_doc_id": [598, 2216, 2597, 2598, 2599, 2600, 2601, 2602, 3161, 4632, 2215, 3693, 6640, 7788, 4781], "orig_top_k_doc_id": [2597, 2602, 2601, 2600, 2598, 2599, 7788, 598, 4632, 4781, 2215, 2216, 3693, 6640, 3161]}, {"qid": 1783, "question": "Which model is used to capture the implicit structure? in Learning Explicit and Implicit Structures for Targeted Sentiment Analysis", "answer": ["Bi-directional LSTM, self-attention "], "top_k_doc_id": [598, 2216, 2597, 2598, 2599, 2600, 2601, 2602, 3161, 4632, 6427, 4633, 4635, 4636, 737], "orig_top_k_doc_id": [2601, 2602, 2597, 2600, 2598, 2599, 4632, 3161, 598, 6427, 4633, 4635, 2216, 4636, 737]}]}
{"group_id": 664, "group_size": 3, "items": [{"qid": 1811, "question": "By how much does their model outperform both the state-of-the-art systems? in Efficient Summarization with Read-Again and Copy Mechanism", "answer": ["w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%"], "top_k_doc_id": [1132, 2636, 2635, 2638, 2639, 3234, 4760, 6570, 6929, 5544, 1138, 6569, 5997, 4482, 3233], "orig_top_k_doc_id": [2638, 2635, 2639, 2636, 3234, 5544, 6570, 1132, 6929, 1138, 6569, 5997, 4482, 4760, 3233]}, {"qid": 1812, "question": "What is the state-of-the art? in Efficient Summarization with Read-Again and Copy Mechanism", "answer": ["neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder"], "top_k_doc_id": [1132, 2636, 2635, 2638, 2639, 3234, 4760, 6570, 6929, 6567, 6931, 2637, 3137, 5541, 494], "orig_top_k_doc_id": [2635, 2639, 2636, 2638, 6570, 1132, 6929, 3234, 4760, 6567, 6931, 2637, 3137, 5541, 494]}, {"qid": 2382, "question": "So we do not use pre-trained embedding in this case? in Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models", "answer": ["Yes"], "top_k_doc_id": [1132, 2636, 2232, 3777, 5805, 5276, 3835, 5806, 3833, 6515, 3236, 3830, 7117, 4921, 3238], "orig_top_k_doc_id": [2232, 3777, 5805, 5276, 3835, 5806, 3833, 6515, 2636, 3236, 3830, 7117, 4921, 3238, 1132]}]}
{"group_id": 665, "group_size": 3, "items": [{"qid": 1832, "question": "how was the dataset built? in BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "answer": ["Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as \u201cnot answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is \u201cyes\" or \u201cno\""], "top_k_doc_id": [2661, 2662, 2663, 2664, 2665, 2666, 5609, 788, 824, 5562, 2581, 5570, 5236, 2898, 7157], "orig_top_k_doc_id": [2663, 2664, 2661, 2666, 2662, 2665, 5609, 824, 5562, 2581, 5570, 5236, 788, 2898, 7157]}, {"qid": 1833, "question": "what is the size of BoolQ dataset? in BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "answer": [" 16k questions"], "top_k_doc_id": [2661, 2662, 2663, 2664, 2665, 2666, 5609, 788, 824, 5562, 4415, 5680, 787, 5473, 7158], "orig_top_k_doc_id": [2663, 2664, 2661, 2666, 2665, 2662, 5609, 4415, 5562, 788, 5680, 824, 787, 5473, 7158]}, {"qid": 1831, "question": "did they use other pretrained language models besides bert? in BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "answer": ["Yes"], "top_k_doc_id": [2661, 2662, 2663, 2664, 2665, 2666, 5609, 5473, 5474, 4415, 1334, 4900, 5570, 5231, 5572], "orig_top_k_doc_id": [2661, 2663, 2664, 2665, 2666, 2662, 5473, 5474, 4415, 1334, 4900, 5609, 5570, 5231, 5572]}]}
{"group_id": 666, "group_size": 3, "items": [{"qid": 1842, "question": "How do they gather human reviews? in Paying Attention to Attention: Highlighting Influential Samples in Sequential Analysis", "answer": ["human representative to review the IVA chat history and resume the failed task"], "top_k_doc_id": [2684, 2687, 2688, 5281, 5089, 6471, 2417, 6692, 6274, 2970, 926, 7006, 5158, 5468, 1347], "orig_top_k_doc_id": [2684, 5281, 2687, 5089, 6471, 2417, 2688, 6692, 6274, 2970, 926, 7006, 5158, 5468, 1347]}, {"qid": 1843, "question": "Do they explain model predictions solely on attention weights? in Paying Attention to Attention: Highlighting Influential Samples in Sequential Analysis", "answer": ["Yes"], "top_k_doc_id": [2684, 2687, 2688, 5281, 642, 2418, 3980, 3501, 4152, 3034, 2804, 3981, 3983, 256, 2838], "orig_top_k_doc_id": [2684, 2687, 2688, 642, 2418, 5281, 3980, 3501, 4152, 3034, 2804, 3981, 3983, 256, 2838]}, {"qid": 1844, "question": "Can their method of creating more informative visuals be applied to tasks other than turn taking in conversations? in Paying Attention to Attention: Highlighting Influential Samples in Sequential Analysis", "answer": ["computationally inexpensive means to understand what happened at the stopping point"], "top_k_doc_id": [2684, 2687, 2688, 5281, 2685, 1718, 7625, 848, 6232, 1711, 6667, 1715, 3537, 1716, 1804], "orig_top_k_doc_id": [2684, 2687, 2685, 2688, 1718, 7625, 848, 6232, 1711, 6667, 1715, 3537, 1716, 5281, 1804]}]}
{"group_id": 667, "group_size": 3, "items": [{"qid": 1931, "question": "Do they compare results against state-of-the-art language models? in Multiplicative Models for Recurrent Language Modeling", "answer": ["Yes"], "top_k_doc_id": [2881, 7498, 7688, 1953, 6503, 7351, 4813, 1389, 3621, 3863, 2754, 2948, 6723, 3357, 804], "orig_top_k_doc_id": [2881, 6503, 4813, 1953, 1389, 3621, 3863, 7351, 7688, 2754, 2948, 6723, 3357, 804, 7498]}, {"qid": 1933, "question": "Which dataset do they train their models on? in Multiplicative Models for Recurrent Language Modeling", "answer": ["Penn Treebank, Text8"], "top_k_doc_id": [2881, 7498, 7688, 1953, 6503, 7351, 7687, 1525, 1526, 3085, 2882, 3084, 5053, 6065, 3864], "orig_top_k_doc_id": [2881, 7688, 7687, 7351, 1525, 7498, 6503, 1526, 3085, 2882, 3084, 5053, 1953, 6065, 3864]}, {"qid": 1932, "question": "Do they integrate the second-order term in the mLSTM? in Multiplicative Models for Recurrent Language Modeling", "answer": ["No"], "top_k_doc_id": [2881, 7498, 7688, 2882, 2883, 2066, 2067, 7493, 2116, 2753, 2754, 3867, 1526, 3375, 4841], "orig_top_k_doc_id": [2881, 2882, 2883, 7498, 2066, 7688, 2067, 7493, 2116, 2753, 2754, 3867, 1526, 3375, 4841]}]}
{"group_id": 668, "group_size": 3, "items": [{"qid": 1960, "question": "Do they argue that all words can be derived from other (elementary) words? in From quantum foundations via natural language meaning to a theory of everything", "answer": ["No"], "top_k_doc_id": [554, 1181, 2933, 2935, 2936, 2937, 1183, 1184, 5528, 4229, 3183, 3118, 1967, 6595, 557], "orig_top_k_doc_id": [2937, 2936, 2933, 2935, 4229, 3183, 1184, 5528, 3118, 554, 1181, 1183, 1967, 6595, 557]}, {"qid": 1961, "question": "Do they break down word meanings into elementary particles as in the standard model of quantum theory? in From quantum foundations via natural language meaning to a theory of everything", "answer": ["No"], "top_k_doc_id": [554, 1181, 2933, 2935, 2936, 2937, 1183, 1184, 5528, 2934, 6852, 4577, 555, 4082, 5032], "orig_top_k_doc_id": [2936, 2937, 2935, 2933, 2934, 6852, 4577, 554, 5528, 1184, 1183, 1181, 555, 4082, 5032]}, {"qid": 1959, "question": "Do they address abstract meanings and concepts separately? in From quantum foundations via natural language meaning to a theory of everything", "answer": ["No"], "top_k_doc_id": [554, 1181, 2933, 2935, 2936, 2937, 557, 555, 6594, 4082, 3489, 5537, 3118, 6184, 3207], "orig_top_k_doc_id": [2937, 2936, 2935, 557, 2933, 554, 1181, 555, 6594, 4082, 3489, 5537, 3118, 6184, 3207]}]}
{"group_id": 669, "group_size": 3, "items": [{"qid": 1971, "question": "Are their corpus and software public? in Vietnamese Semantic Role Labelling", "answer": ["Yes"], "top_k_doc_id": [2956, 2958, 2959, 2957, 2965, 2966, 2962, 2961, 2960, 1084, 6464, 2873, 7548, 1147, 2963], "orig_top_k_doc_id": [2956, 2959, 2965, 2957, 2958, 2966, 2962, 2961, 2960, 1084, 6464, 2873, 7548, 1147, 2963]}, {"qid": 2056, "question": "How many roles are proposed? in Categorization of Semantic Roles for Dictionary Definitions", "answer": ["12"], "top_k_doc_id": [2956, 2958, 2959, 2957, 3118, 3122, 3119, 7302, 3120, 3121, 2984, 1958, 2142, 3622, 781], "orig_top_k_doc_id": [3118, 3122, 2957, 3119, 7302, 3120, 3121, 2956, 2984, 2958, 1958, 2959, 2142, 3622, 781]}, {"qid": 1801, "question": "Are the grammar clauses manually created? in Vietnamese Open Information Extraction", "answer": ["Yes"], "top_k_doc_id": [2956, 2958, 2959, 5721, 7357, 153, 5316, 4399, 7355, 2017, 2018, 7610, 2022, 2014, 5720], "orig_top_k_doc_id": [2958, 2956, 5721, 7357, 2959, 153, 5316, 4399, 7355, 2017, 2018, 7610, 2022, 2014, 5720]}]}
{"group_id": 670, "group_size": 3, "items": [{"qid": 1972, "question": "How are EAC evaluated? in Emotionally-Aware Chatbots: A Survey", "answer": ["Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement."], "top_k_doc_id": [852, 2967, 2968, 2969, 2970, 5427, 5428, 5429, 6590, 6932, 851, 5646, 5793, 6725, 6724], "orig_top_k_doc_id": [2970, 2967, 2968, 2969, 852, 5429, 6590, 851, 5428, 5427, 5646, 6725, 5793, 6932, 6724]}, {"qid": 1973, "question": "What are the currently available datasets for EAC? in Emotionally-Aware Chatbots: A Survey", "answer": ["EMPATHETICDIALOGUES dataset, a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries, SEMAINE corpus BIBREF30"], "top_k_doc_id": [852, 2967, 2968, 2969, 2970, 5427, 5428, 5429, 6590, 6932, 851, 5646, 5793, 1899, 490], "orig_top_k_doc_id": [2970, 2968, 2967, 2969, 5429, 5428, 851, 5646, 852, 5427, 6932, 6590, 5793, 1899, 490]}, {"qid": 1974, "question": "What are the research questions posed in the paper regarding EAC studies? in Emotionally-Aware Chatbots: A Survey", "answer": ["how to incorporate affective information into chatbots, what are resources that available and can be used to build EAC, and how to evaluate EAC performance"], "top_k_doc_id": [852, 2967, 2968, 2969, 2970, 5427, 5428, 5429, 6590, 6932, 490, 6723, 7566, 6343, 3437], "orig_top_k_doc_id": [2967, 2970, 2968, 2969, 5429, 490, 5428, 6590, 5427, 6932, 852, 6723, 7566, 6343, 3437]}]}
{"group_id": 671, "group_size": 3, "items": [{"qid": 1976, "question": "What is triangulation? in Should All Cross-Lingual Embeddings Speak English?", "answer": ["Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt\u2013En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho."], "top_k_doc_id": [247, 5872, 6060, 629, 1039, 2154, 5716, 6853, 2971, 3749, 3141, 2811, 5709, 3750, 3748], "orig_top_k_doc_id": [2971, 247, 3749, 6060, 3141, 5716, 5872, 2811, 1039, 2154, 6853, 5709, 3750, 629, 3748]}, {"qid": 1977, "question": "What languages are explored in this paper? in Should All Cross-Lingual Embeddings Speak English?", "answer": ["No"], "top_k_doc_id": [247, 5872, 6060, 629, 1039, 2154, 5716, 6853, 6854, 6035, 1040, 6034, 5868, 2329, 6036], "orig_top_k_doc_id": [6853, 6854, 6035, 1040, 247, 1039, 6034, 6060, 5716, 5868, 5872, 629, 2154, 2329, 6036]}, {"qid": 1975, "question": "What evaluation metrics did they use? in Should All Cross-Lingual Embeddings Speak English?", "answer": ["we report P@1, which is equivalent to accuracy, we also provide results with P@5 and P@10 in the Appendix"], "top_k_doc_id": [247, 5872, 6060, 5090, 3750, 3749, 6062, 3748, 6063, 5572, 2811, 1040, 7408, 1410, 2971], "orig_top_k_doc_id": [5090, 3750, 3749, 6062, 3748, 247, 6063, 5872, 5572, 2811, 1040, 7408, 6060, 1410, 2971]}]}
{"group_id": 672, "group_size": 3, "items": [{"qid": 2022, "question": "How do they perform data augmentation? in An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models", "answer": ["They randomly sample sentences from Wikipedia that contains an object RC and add them to training data"], "top_k_doc_id": [1703, 3058, 3059, 3062, 3063, 3612, 1704, 3061, 3956, 1361, 7664, 2578, 1255, 4707, 3957], "orig_top_k_doc_id": [3058, 3062, 3956, 3063, 1361, 3061, 3059, 7664, 2578, 1704, 1255, 4707, 3612, 3957, 1703]}, {"qid": 2023, "question": "What proportion of negative-examples do they use? in An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models", "answer": ["No"], "top_k_doc_id": [1703, 3058, 3059, 3062, 3063, 3612, 1704, 3061, 450, 3060, 1254, 4896, 703, 4077, 1705], "orig_top_k_doc_id": [3058, 3062, 3063, 3059, 3061, 450, 3060, 1703, 1254, 4896, 703, 3612, 1704, 4077, 1705]}, {"qid": 2021, "question": "What neural language models are explored? in An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models", "answer": ["LSTM-LM "], "top_k_doc_id": [1703, 3058, 3059, 3062, 3063, 3612, 7053, 1255, 7115, 7217, 703, 6660, 450, 861, 5933], "orig_top_k_doc_id": [3058, 3062, 3612, 3059, 3063, 7053, 1703, 1255, 7115, 7217, 703, 6660, 450, 861, 5933]}]}
{"group_id": 673, "group_size": 3, "items": [{"qid": 2053, "question": "How much better performing is the proposed method over the baselines? in Improving Visually Grounded Sentence Representations with Self-Attention", "answer": ["original models were better in some tasks (CR, MPQA, MRPC), utilizing self-attentive sentence representation further improves performances in 5 out of 8 tasks"], "top_k_doc_id": [494, 495, 3115, 3116, 6406, 312, 1258, 1544, 4276, 82, 4277, 1345, 6259, 4179, 4183], "orig_top_k_doc_id": [3115, 3116, 494, 4276, 1544, 495, 1258, 82, 312, 4277, 1345, 6259, 4179, 6406, 4183]}, {"qid": 2054, "question": "What baselines are the proposed method compared against? in Improving Visually Grounded Sentence Representations with Self-Attention", "answer": ["(Layer Normalized Skip-Thoughts, ST-LN) BIBREF31, Cap2All, Cap2Cap, Cap2Img"], "top_k_doc_id": [494, 495, 3115, 3116, 6406, 312, 1258, 1544, 4276, 1257, 1259, 2414, 3117, 4744, 133], "orig_top_k_doc_id": [3115, 494, 3116, 495, 1258, 4276, 1257, 1259, 2414, 3117, 1544, 312, 4744, 6406, 133]}, {"qid": 2055, "question": "What dataset/corpus is this work evaluated over? in Improving Visually Grounded Sentence Representations with Self-Attention", "answer": ["Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10"], "top_k_doc_id": [494, 495, 3115, 3116, 6406, 4977, 2414, 3117, 2733, 709, 7596, 116, 1560, 82, 6407], "orig_top_k_doc_id": [3115, 494, 3116, 495, 4977, 2414, 6406, 3117, 2733, 709, 7596, 116, 1560, 82, 6407]}]}
{"group_id": 674, "group_size": 3, "items": [{"qid": 2061, "question": "On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers? in Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods", "answer": ["No"], "top_k_doc_id": [3131, 3132, 3133, 3134, 3135, 3136, 520, 521, 3007, 7016, 7104, 5461, 3487, 4405, 7029], "orig_top_k_doc_id": [3135, 3136, 3131, 3133, 3134, 3132, 521, 7016, 7104, 520, 5461, 3487, 4405, 3007, 7029]}, {"qid": 2062, "question": "How many demographic attributes they try to predict? in Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods", "answer": ["62"], "top_k_doc_id": [3131, 3132, 3133, 3134, 3135, 3136, 520, 521, 3007, 7016, 3008, 2533, 5627, 7530, 3879], "orig_top_k_doc_id": [3135, 3136, 3131, 3134, 3133, 3132, 521, 7016, 3008, 2533, 3007, 5627, 7530, 520, 3879]}, {"qid": 2060, "question": "What do the correlation demonstrate?  in Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods", "answer": ["the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods"], "top_k_doc_id": [3131, 3132, 3133, 3134, 3135, 3136, 998, 4406, 5470, 3878, 447, 3879, 7530, 4895, 3583], "orig_top_k_doc_id": [3135, 3136, 3131, 3134, 3133, 3132, 998, 4406, 5470, 3878, 447, 3879, 7530, 4895, 3583]}]}
{"group_id": 675, "group_size": 3, "items": [{"qid": 2099, "question": "Do they differentiate insights where they are dealing with learned or engineered representations? in INFODENS: An Open-source Framework for Learning Text Representations", "answer": ["Yes"], "top_k_doc_id": [3195, 3196, 3197, 1255, 805, 3300, 1744, 3545, 6566, 7549, 2146, 5537, 6257, 6344, 540], "orig_top_k_doc_id": [3195, 3197, 805, 3300, 3196, 1744, 3545, 6566, 1255, 7549, 2146, 5537, 6257, 6344, 540]}, {"qid": 2100, "question": "Do they show an example of usage for INFODENS? in INFODENS: An Open-source Framework for Learning Text Representations", "answer": ["Yes"], "top_k_doc_id": [3195, 3196, 3197, 1255, 7351, 4475, 5462, 7548, 5804, 5461, 4424, 887, 436, 5885, 1048], "orig_top_k_doc_id": [3197, 3195, 3196, 7351, 4475, 5462, 1255, 7548, 5804, 5461, 4424, 887, 436, 5885, 1048]}, {"qid": 2101, "question": "What kind of representation exploration does INFODENS provide? in INFODENS: An Open-source Framework for Learning Text Representations", "answer": ["No"], "top_k_doc_id": [3195, 3196, 3197, 1899, 4716, 6025, 7743, 2868, 1745, 7685, 6833, 6257, 5684, 6348, 490], "orig_top_k_doc_id": [3197, 3196, 3195, 1899, 4716, 6025, 7743, 2868, 1745, 7685, 6833, 6257, 5684, 6348, 490]}]}
{"group_id": 676, "group_size": 3, "items": [{"qid": 2125, "question": "What news dataset was used? in Short-Text Classification Using Unsupervised Keyword Expansion", "answer": ["collection of headlines published by HuffPost BIBREF12 between 2012 and 2018"], "top_k_doc_id": [420, 1113, 3241, 3242, 3575, 4359, 6716, 4300, 5115, 5116, 2396, 322, 5186, 2083, 2626], "orig_top_k_doc_id": [3241, 3242, 5116, 4359, 6716, 1113, 420, 5115, 2396, 322, 4300, 5186, 2083, 2626, 3575]}, {"qid": 2127, "question": "What is the language model pre-trained on? in Short-Text Classification Using Unsupervised Keyword Expansion", "answer": ["Wikipedea Corpus and BooksCorpus"], "top_k_doc_id": [420, 1113, 3241, 3242, 3575, 4359, 6716, 4300, 5115, 5116, 4535, 5328, 1112, 124, 6351], "orig_top_k_doc_id": [3241, 3242, 1113, 5116, 4535, 4300, 3575, 5328, 1112, 420, 6716, 124, 4359, 5115, 6351]}, {"qid": 2126, "question": "How do they determine similarity between predicted word and topics? in Short-Text Classification Using Unsupervised Keyword Expansion", "answer": ["number of relevant output words as a function of the headline\u2019s category label"], "top_k_doc_id": [420, 1113, 3241, 3242, 3575, 4359, 6716, 1112, 2403, 5328, 2396, 6715, 5822, 2794, 5333], "orig_top_k_doc_id": [3242, 3241, 1112, 1113, 420, 2403, 4359, 5328, 2396, 6715, 6716, 3575, 5822, 2794, 5333]}]}
{"group_id": 677, "group_size": 3, "items": [{"qid": 2128, "question": "What languages are represented in the dataset? in Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN", "answer": ["EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO"], "top_k_doc_id": [3244, 3245, 3248, 3249, 3247, 4216, 6512, 7377, 2798, 2968, 7541, 2788, 6879, 243, 7746], "orig_top_k_doc_id": [3244, 3249, 3248, 3245, 3247, 2798, 7377, 6512, 2968, 7541, 4216, 2788, 6879, 243, 7746]}, {"qid": 2129, "question": "Which existing language ID systems are tested? in Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN", "answer": ["langid.py library, encoder-decoder EquiLID system, GRU neural network LanideNN system, CLD2, CLD3"], "top_k_doc_id": [3244, 3245, 3248, 3249, 3247, 4216, 6512, 7377, 5736, 7114, 2330, 6606, 1787, 4424, 3639], "orig_top_k_doc_id": [3244, 3245, 3249, 3248, 5736, 7377, 4216, 7114, 2330, 6606, 6512, 1787, 3247, 4424, 3639]}, {"qid": 2130, "question": "How was the one year worth of data collected? in Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN", "answer": ["No"], "top_k_doc_id": [3244, 3245, 3248, 3249, 4798, 6804, 2968, 1865, 7114, 5085, 6545, 5889, 2256, 4136, 965], "orig_top_k_doc_id": [3244, 3249, 3248, 3245, 4798, 6804, 2968, 1865, 7114, 5085, 6545, 5889, 2256, 4136, 965]}]}
{"group_id": 678, "group_size": 3, "items": [{"qid": 2140, "question": "What other embedding models are tested? in Learning Rare Word Representations using Semantic Bridging", "answer": ["GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300), DeepWalk , node2vec"], "top_k_doc_id": [290, 3269, 3270, 4873, 5303, 5304, 5582, 203, 1374, 3271, 991, 5737, 289, 34, 7686], "orig_top_k_doc_id": [991, 3270, 3269, 5582, 290, 1374, 5737, 5303, 5304, 203, 289, 4873, 34, 3271, 7686]}, {"qid": 2142, "question": "How are rare words defined? in Learning Rare Word Representations using Semantic Bridging", "answer": ["judged by 10 raters on a [0,10] scale"], "top_k_doc_id": [290, 3269, 3270, 4873, 5303, 5304, 5582, 203, 1374, 3271, 2964, 904, 856, 5584, 207], "orig_top_k_doc_id": [1374, 203, 2964, 904, 5303, 5304, 4873, 3269, 5582, 3270, 856, 3271, 5584, 290, 207]}, {"qid": 2141, "question": "How is performance measured? in Learning Rare Word Representations using Semantic Bridging", "answer": ["To verify the reliability of the transformed semantic space, we propose an evaluation benchmark on the basis of word similarity datasets. Given an enriched space INLINEFORM0 and a similarity dataset INLINEFORM1 , we compute the similarity of each word pair INLINEFORM2 as the cosine similarity of their corresponding transformed vectors INLINEFORM3 and INLINEFORM4 from the two spaces, where INLINEFORM5 and INLINEFORM6 for LS and INLINEFORM7 and INLINEFORM8 for CCA. "], "top_k_doc_id": [290, 3269, 3270, 4873, 5303, 5304, 5582, 5701, 5564, 3945, 3372, 904, 5305, 1742, 786], "orig_top_k_doc_id": [5303, 290, 5701, 5564, 3945, 3372, 4873, 904, 5304, 5582, 3270, 5305, 1742, 3269, 786]}]}
{"group_id": 679, "group_size": 3, "items": [{"qid": 2156, "question": "Could you learn such embedding simply from the image annotations and without using visual information? in Learning Multilingual Word Embeddings Using Image-Text Data", "answer": ["Yes"], "top_k_doc_id": [3293, 3294, 3295, 3297, 2900, 2998, 7140, 7141, 414, 80, 1058, 4976, 538, 82, 7000], "orig_top_k_doc_id": [7140, 7141, 3295, 2900, 414, 80, 3293, 1058, 4976, 3294, 3297, 538, 2998, 82, 7000]}, {"qid": 2157, "question": "How much important is the visual grounding in the learning of the multilingual representations? in Learning Multilingual Word Embeddings Using Image-Text Data", "answer": ["performance is significantly degraded without pixel data"], "top_k_doc_id": [3293, 3294, 3295, 3297, 2900, 2998, 3115, 3296, 2899, 7138, 3175, 2413, 4184, 3116, 3617], "orig_top_k_doc_id": [3297, 3293, 3115, 3295, 2900, 3296, 2899, 7138, 3175, 2998, 2413, 4184, 3294, 3116, 3617]}, {"qid": 2155, "question": "Do the images have multilingual annotations or monolingual ones? in Learning Multilingual Word Embeddings Using Image-Text Data", "answer": ["monolingual"], "top_k_doc_id": [3293, 3294, 3295, 3297, 3296, 1992, 5976, 2330, 1991, 3001, 2796, 2331, 6332, 5699, 6060], "orig_top_k_doc_id": [3293, 3296, 3294, 3297, 3295, 1992, 5976, 2330, 1991, 3001, 2796, 2331, 6332, 5699, 6060]}]}
{"group_id": 680, "group_size": 3, "items": [{"qid": 2171, "question": "Which countries and languages do the political speeches and manifestos come from? in Automating Political Bias Prediction", "answer": ["german "], "top_k_doc_id": [3332, 3333, 3334, 3335, 2076, 2671, 4889, 2667, 7416, 2409, 3594, 2077, 4885, 140, 4887], "orig_top_k_doc_id": [3333, 3334, 3332, 2667, 7416, 2076, 3335, 2409, 4889, 3594, 2077, 4885, 2671, 140, 4887]}, {"qid": 2172, "question": "Do changes in policies of the political actors account for all of the mistakes the model made? in Automating Political Bias Prediction", "answer": ["Yes"], "top_k_doc_id": [3332, 3333, 3334, 3335, 2076, 2671, 4889, 3336, 5465, 3337, 7388, 7387, 7382, 5468, 5463], "orig_top_k_doc_id": [3335, 3332, 3336, 3333, 2076, 2671, 5465, 4889, 3337, 7388, 7387, 7382, 5468, 5463, 3334]}, {"qid": 2173, "question": "What model are the text features used in to provide predictions? in Automating Political Bias Prediction", "answer": [" multinomial logistic regression"], "top_k_doc_id": [3332, 3333, 3334, 3335, 3337, 3336, 7059, 7772, 5006, 5906, 7454, 5322, 4780, 6735, 7773], "orig_top_k_doc_id": [3332, 3335, 3337, 3336, 3334, 7059, 7772, 5006, 5906, 3333, 7454, 5322, 4780, 6735, 7773]}]}
{"group_id": 681, "group_size": 3, "items": [{"qid": 2174, "question": "By how much does their method outperform the multi-head attention model? in Multi-Head Decoder for End-to-End Speech Recognition", "answer": ["Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points."], "top_k_doc_id": [1618, 1770, 3338, 3339, 3340, 4918, 3556, 3823, 22, 2238, 3938, 2917, 5790, 4310, 4975], "orig_top_k_doc_id": [3338, 3340, 3339, 22, 1770, 2238, 3556, 3938, 1618, 3823, 4918, 2917, 5790, 4310, 4975]}, {"qid": 2176, "question": "Does each attention head in the decoder calculate the same output? in Multi-Head Decoder for End-to-End Speech Recognition", "answer": ["No"], "top_k_doc_id": [1618, 1770, 3338, 3339, 3340, 4918, 3556, 3823, 973, 1769, 2924, 1771, 529, 6217, 3023], "orig_top_k_doc_id": [3338, 3340, 3339, 1770, 973, 3823, 1618, 1769, 2924, 1771, 3556, 529, 6217, 3023, 4918]}, {"qid": 2175, "question": "How large is the corpus they use? in Multi-Head Decoder for End-to-End Speech Recognition", "answer": ["449050"], "top_k_doc_id": [1618, 1770, 3338, 3339, 3340, 4918, 2451, 4972, 4863, 3834, 973, 6110, 3023, 6350, 4975], "orig_top_k_doc_id": [3340, 3338, 3339, 2451, 4972, 4918, 4863, 3834, 973, 6110, 1770, 3023, 6350, 1618, 4975]}]}
{"group_id": 682, "group_size": 3, "items": [{"qid": 2178, "question": "Which benchmark datasets are used? in Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora", "answer": ["noun-noun subset of bless, leds BIBREF13, bless, wbless, bibless, hyperlex BIBREF20"], "top_k_doc_id": [56, 3341, 3342, 3343, 3750, 4475, 1000, 4341, 7016, 1164, 5406, 6789, 3535, 3574, 3161], "orig_top_k_doc_id": [3341, 3343, 3342, 3750, 56, 4475, 1164, 7016, 5406, 4341, 6789, 3535, 1000, 3574, 3161]}, {"qid": 2179, "question": "What hypernymy tasks do they study? in Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora", "answer": ["Detection, Direction, Graded Entailment"], "top_k_doc_id": [56, 3341, 3342, 3343, 3750, 4475, 1000, 4341, 7016, 3208, 4699, 5071, 4476, 7046, 5073], "orig_top_k_doc_id": [3341, 3343, 3342, 3750, 3208, 4475, 4699, 56, 5071, 4341, 4476, 7016, 7046, 1000, 5073]}, {"qid": 2177, "question": "Which distributional methods did they consider? in Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora", "answer": ["WeedsPrec BIBREF8, invCL BIBREF11, SLQS model, cosine similarity"], "top_k_doc_id": [56, 3341, 3342, 3343, 3750, 4475, 2103, 2114, 5071, 2104, 5073, 803, 6334, 6738, 3208], "orig_top_k_doc_id": [3341, 3343, 3342, 3750, 4475, 2103, 2114, 5071, 2104, 5073, 56, 803, 6334, 6738, 3208]}]}
{"group_id": 683, "group_size": 3, "items": [{"qid": 2188, "question": "Is human evaluation performed? in Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems", "answer": ["No"], "top_k_doc_id": [3193, 3357, 3358, 3359, 3360, 3679, 5850, 7584, 196, 965, 3451, 6651, 3507, 3361, 1771], "orig_top_k_doc_id": [3357, 3360, 3358, 3359, 965, 7584, 3451, 6651, 3507, 3361, 3679, 5850, 196, 3193, 1771]}, {"qid": 2189, "question": "What are the three datasets used? in Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems", "answer": ["DSTC2, M2M-sim-M, M2M-sim-R"], "top_k_doc_id": [3193, 3357, 3358, 3359, 3360, 3679, 5850, 7584, 196, 965, 3451, 400, 5800, 3190, 1768], "orig_top_k_doc_id": [3357, 3359, 3358, 3360, 7584, 5850, 400, 965, 3451, 3679, 5800, 3190, 3193, 1768, 196]}, {"qid": 2187, "question": "How many layers of self-attention does the model have? in Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems", "answer": ["1, 4, 8, 16, 32, 64"], "top_k_doc_id": [3193, 3357, 3358, 3359, 3360, 3679, 5850, 7584, 3361, 400, 1768, 6068, 1683, 3537, 405], "orig_top_k_doc_id": [3357, 3358, 3360, 3359, 7584, 3361, 400, 1768, 3193, 5850, 6068, 1683, 3537, 3679, 405]}]}
{"group_id": 684, "group_size": 3, "items": [{"qid": 2213, "question": "What evaluation metric is used? in Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack", "answer": ["F1 and Weighted-F1"], "top_k_doc_id": [3445, 3446, 3447, 3448, 3449, 3450, 4204, 4205, 4207, 5246, 5251, 5353, 5355, 4199, 5351], "orig_top_k_doc_id": [3448, 3445, 3449, 3447, 4205, 3446, 3450, 5251, 5353, 5355, 5246, 4207, 4199, 5351, 4204]}, {"qid": 2214, "question": "What datasets are used? in Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack", "answer": ["The Wikipedia Toxic Comments dataset"], "top_k_doc_id": [3445, 3446, 3447, 3448, 3449, 3450, 4204, 4205, 4207, 5246, 5251, 5353, 5355, 5250, 182], "orig_top_k_doc_id": [3445, 3449, 3448, 3447, 4205, 5251, 3446, 3450, 5353, 5246, 5355, 5250, 4207, 4204, 182]}, {"qid": 2565, "question": "what datasets did the authors use? in Impact of Sentiment Detection to Recognize Toxic and Subversive Online Comments", "answer": ["Kaggle\nSubversive Kaggle\nWikipedia\nSubversive Wikipedia\nReddit\nSubversive Reddit "], "top_k_doc_id": [3445, 3446, 3447, 4499, 4503, 4504, 4500, 1876, 1877, 5144, 1879, 3007, 7260, 6285, 3893], "orig_top_k_doc_id": [4499, 4503, 4504, 4500, 1876, 1877, 5144, 3445, 1879, 3447, 3446, 3007, 7260, 6285, 3893]}]}
{"group_id": 685, "group_size": 3, "items": [{"qid": 2270, "question": "How much do they outperform previous state-of-the-art? in Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence", "answer": ["On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.\nOn subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity."], "top_k_doc_id": [3578, 4833, 7005, 728, 893, 3579, 3580, 7472, 606, 2306, 7006, 874, 7473, 4698, 2310], "orig_top_k_doc_id": [3578, 7005, 3579, 3580, 893, 7472, 606, 2306, 7006, 874, 728, 7473, 4833, 4698, 2310]}, {"qid": 2271, "question": "How do they generate the auxiliary sentence? in Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence", "answer": ["The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same., For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler., For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution, auxiliary sentence changes from a question to a pseudo-sentence"], "top_k_doc_id": [3578, 4833, 7005, 728, 893, 3579, 3580, 756, 3169, 7282, 1882, 1325, 3102, 1883, 3980], "orig_top_k_doc_id": [3578, 3579, 7005, 3580, 893, 728, 756, 3169, 7282, 1882, 1325, 3102, 1883, 3980, 4833]}, {"qid": 2563, "question": "Do they analyze ELMo? in Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "answer": ["No"], "top_k_doc_id": [3578, 4833, 7005, 5184, 3611, 6534, 5185, 5292, 7235, 4900, 756, 3606, 6768, 2238, 6769], "orig_top_k_doc_id": [5184, 4833, 3611, 3578, 6534, 7005, 5185, 5292, 7235, 4900, 756, 3606, 6768, 2238, 6769]}]}
{"group_id": 686, "group_size": 3, "items": [{"qid": 2311, "question": "How are the two different models trained? in Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector", "answer": ["They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set."], "top_k_doc_id": [1957, 3688, 3689, 3690, 3961, 369, 1959, 3927, 4834, 6107, 6176, 6268, 3810, 1499, 6640], "orig_top_k_doc_id": [3688, 3690, 3689, 1957, 3961, 6176, 369, 1959, 6268, 3810, 6107, 3927, 1499, 6640, 4834]}, {"qid": 2312, "question": "How long is the dataset? in Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector", "answer": ["645, 600000"], "top_k_doc_id": [1957, 3688, 3689, 3690, 3961, 369, 1959, 3927, 4834, 6107, 6176, 6268, 4672, 4515, 2041], "orig_top_k_doc_id": [3688, 3690, 3689, 1957, 3961, 6176, 1959, 6107, 4672, 6268, 4834, 4515, 3927, 369, 2041]}, {"qid": 2310, "question": "Do they use the cased or uncased BERT model? in Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector", "answer": ["No"], "top_k_doc_id": [1957, 3688, 3689, 3690, 3961, 7118, 7286, 7119, 714, 7288, 7287, 7289, 2309, 2308, 3721], "orig_top_k_doc_id": [3688, 3690, 3689, 1957, 7118, 7286, 7119, 714, 7288, 7287, 7289, 2309, 2308, 3961, 3721]}]}
{"group_id": 687, "group_size": 3, "items": [{"qid": 2339, "question": "Which dataset do they use for text modelling? in A Stable Variational Autoencoder for Text Modelling", "answer": ["Penn Treebank (PTB), end-to-end (E2E) text generation corpus"], "top_k_doc_id": [3739, 3740, 5282, 6427, 7372, 1156, 7371, 5217, 2730, 310, 6585, 5935, 4177, 3358, 6251], "orig_top_k_doc_id": [3739, 3740, 5217, 7372, 2730, 7371, 310, 6427, 6585, 5282, 5935, 4177, 1156, 3358, 6251]}, {"qid": 2340, "question": "Do they compare against state of the art text generation? in A Stable Variational Autoencoder for Text Modelling", "answer": ["Yes"], "top_k_doc_id": [3739, 3740, 5282, 6427, 7372, 1156, 7371, 1768, 2065, 1940, 1005, 2187, 2262, 3357, 3916], "orig_top_k_doc_id": [3739, 3740, 1768, 6427, 7371, 7372, 2065, 1940, 1005, 5282, 2187, 2262, 1156, 3357, 3916]}, {"qid": 2341, "question": "How do they evaluate generated text quality? in A Stable Variational Autoencoder for Text Modelling", "answer": ["Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting."], "top_k_doc_id": [3739, 3740, 5282, 6427, 7372, 3233, 5281, 118, 5561, 5562, 7664, 3235, 3234, 2187, 1005], "orig_top_k_doc_id": [3739, 3740, 3233, 5281, 5282, 118, 5561, 5562, 7664, 3235, 6427, 3234, 2187, 7372, 1005]}]}
{"group_id": 688, "group_size": 3, "items": [{"qid": 2371, "question": "Who annotated the data? in Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "answer": ["annotators who were not security experts, researchers in either NLP or computer security"], "top_k_doc_id": [2265, 3812, 3814, 3815, 5377, 6547, 7256, 2861, 5387, 2264, 6640, 2268, 7813, 1325, 6616], "orig_top_k_doc_id": [3812, 3815, 5377, 7256, 5387, 6547, 3814, 2861, 2264, 6640, 2268, 7813, 1325, 2265, 6616]}, {"qid": 2372, "question": "What are the four forums the data comes from? in Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "answer": ["Darkode,  Hack Forums, Blackhat and Nulled."], "top_k_doc_id": [2265, 3812, 3814, 3815, 5377, 6547, 7256, 2861, 3813, 2308, 2798, 369, 1711, 7266, 7268], "orig_top_k_doc_id": [3812, 3815, 3814, 3813, 6547, 7256, 2308, 2798, 369, 1711, 7266, 5377, 2265, 7268, 2861]}, {"qid": 2370, "question": "What supervised models are experimented with? in Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "answer": ["No"], "top_k_doc_id": [2265, 3812, 3814, 3815, 5377, 6547, 7256, 6731, 2030, 4046, 2308, 7816, 2725, 6005, 1625], "orig_top_k_doc_id": [3812, 3815, 5377, 6731, 2030, 6547, 4046, 3814, 2265, 2308, 7816, 2725, 7256, 6005, 1625]}]}
{"group_id": 689, "group_size": 3, "items": [{"qid": 2373, "question": "How do they obtain parsed source sentences? in Multi-Source Syntactic Neural Machine Translation", "answer": ["Stanford CoreNLP BIBREF11 "], "top_k_doc_id": [1124, 3817, 3818, 3819, 4766, 6295, 1389, 4047, 7437, 4695, 3260, 6216, 2907, 2491, 7342], "orig_top_k_doc_id": [3817, 3818, 3819, 4766, 7437, 4047, 4695, 1124, 3260, 1389, 6295, 6216, 2907, 2491, 7342]}, {"qid": 2375, "question": "Whas is the performance drop of their model when there is no parsed input? in Multi-Source Syntactic Neural Machine Translation", "answer": [" improvements of up to 1.5 BLEU over the seq2seq baseline"], "top_k_doc_id": [1124, 3817, 3818, 3819, 4766, 6295, 1389, 4047, 7437, 2906, 2494, 2705, 565, 697, 4309], "orig_top_k_doc_id": [3817, 3818, 3819, 2906, 6295, 4766, 2494, 7437, 1124, 2705, 4047, 565, 1389, 697, 4309]}, {"qid": 2374, "question": "What kind of encoders are used for the parsed source sentence? in Multi-Source Syntactic Neural Machine Translation", "answer": ["RNN encoders"], "top_k_doc_id": [1124, 3817, 3818, 3819, 4766, 6295, 2705, 6043, 4178, 4693, 4692, 1237, 4561, 4179, 4694], "orig_top_k_doc_id": [3817, 3818, 3819, 2705, 6043, 4178, 4766, 4693, 4692, 6295, 1237, 1124, 4561, 4179, 4694]}]}
{"group_id": 690, "group_size": 3, "items": [{"qid": 2421, "question": "Do they experiment with language modeling on large datasets? in Good-Enough Compositional Data Augmentation", "answer": ["No"], "top_k_doc_id": [3956, 3957, 1216, 1217, 5582, 6558, 2746, 1077, 3407, 4561, 1211, 5835, 6579, 7610, 5360], "orig_top_k_doc_id": [3956, 3957, 2746, 1217, 1216, 5582, 1077, 3407, 4561, 1211, 6558, 5835, 6579, 7610, 5360]}, {"qid": 2422, "question": "Which languages do they test on? in Good-Enough Compositional Data Augmentation", "answer": ["Answer with content missing: (Applications section) We use Wikipedia articles\nin five languages\n(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams\net al. (2017).\nSelect:\nKinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English"], "top_k_doc_id": [3956, 3957, 1216, 1217, 5582, 6558, 6310, 7818, 4707, 2708, 3564, 3565, 5837, 2701, 5314], "orig_top_k_doc_id": [3956, 1217, 3957, 6310, 5582, 7818, 1216, 4707, 2708, 3564, 3565, 5837, 6558, 2701, 5314]}, {"qid": 2420, "question": "How do they determine similar environments for fragments in their data augmentation scheme? in Good-Enough Compositional Data Augmentation", "answer": ["fragments are interchangeable if they occur in at least one lexical environment that is exactly the same"], "top_k_doc_id": [3956, 3957, 3958, 2746, 2052, 5499, 485, 2442, 2862, 2372, 5692, 5500, 3564, 7537, 7818], "orig_top_k_doc_id": [3956, 3957, 3958, 2746, 2052, 5499, 485, 2442, 2862, 2372, 5692, 5500, 3564, 7537, 7818]}]}
{"group_id": 691, "group_size": 3, "items": [{"qid": 2423, "question": "What limitations are mentioned? in DpgMedia2019: A Dutch News Dataset for Partisanship Detection", "answer": ["deciding publisher partisanship, risk annotator bias because of short description text provided to annotators"], "top_k_doc_id": [3688, 3959, 3960, 3961, 3962, 1124, 6622, 6623, 6624, 7408, 7120, 3928, 1499, 4499, 7257], "orig_top_k_doc_id": [3959, 3961, 3962, 6622, 3960, 3688, 7120, 3928, 1124, 1499, 6623, 4499, 6624, 7408, 7257]}, {"qid": 2424, "question": "What examples of applications are mentioned? in DpgMedia2019: A Dutch News Dataset for Partisanship Detection", "answer": ["partisan news detector"], "top_k_doc_id": [3688, 3959, 3960, 3961, 3962, 1124, 6622, 6623, 6624, 7408, 7499, 4137, 2827, 6628, 7500], "orig_top_k_doc_id": [3959, 3961, 3962, 1124, 3960, 3688, 6623, 6622, 7499, 4137, 2827, 6628, 6624, 7500, 7408]}, {"qid": 2425, "question": "Did they crowdsource the annotations? in DpgMedia2019: A Dutch News Dataset for Partisanship Detection", "answer": ["Yes"], "top_k_doc_id": [3688, 3959, 3960, 3961, 3962, 3608, 7257, 6625, 3895, 3893, 2875, 6640, 3894, 3896, 6743], "orig_top_k_doc_id": [3959, 3961, 3962, 3960, 3608, 7257, 6625, 3895, 3893, 2875, 6640, 3894, 3896, 6743, 3688]}]}
{"group_id": 692, "group_size": 3, "items": [{"qid": 2429, "question": "What semantic features help in detecting whether a piece of text is genuine or generated? of  in Limits of Detecting Text Generated by Large-Scale Language Models", "answer": ["No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework."], "top_k_doc_id": [3968, 3969, 3970, 7071, 27, 32, 4204, 3926, 1263, 1262, 7077, 7072, 1265, 1725, 33], "orig_top_k_doc_id": [3968, 3970, 3926, 1263, 1262, 32, 7071, 7077, 7072, 1265, 27, 4204, 1725, 33, 3969]}, {"qid": 2430, "question": "Which language models generate text that can be easier to classify as genuine or generated? in Limits of Detecting Text Generated by Large-Scale Language Models", "answer": ["Outputs from models that are better in the sense of cross-entropy or perplexity are harder to distinguish from authentic text."], "top_k_doc_id": [3968, 3969, 3970, 7071, 27, 32, 4204, 6058, 6066, 28, 2442, 5288, 3596, 6440, 6001], "orig_top_k_doc_id": [3968, 3970, 3969, 6058, 27, 6066, 28, 2442, 7071, 5288, 3596, 6440, 32, 6001, 4204]}, {"qid": 2431, "question": "Is the assumption that natural language is stationary and ergodic valid? in Limits of Detecting Text Generated by Large-Scale Language Models", "answer": ["It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement."], "top_k_doc_id": [3968, 3969, 3970, 7071, 2289, 1322, 5442, 5998, 4949, 5443, 318, 3956, 7780, 7072, 4793], "orig_top_k_doc_id": [3970, 3969, 3968, 2289, 1322, 5442, 5998, 4949, 5443, 7071, 318, 3956, 7780, 7072, 4793]}]}
{"group_id": 693, "group_size": 3, "items": [{"qid": 2440, "question": "What SMT models did they look at? in A Parallel Corpus of Theses and Dissertations Abstracts", "answer": ["automatic translator with Moses"], "top_k_doc_id": [4008, 4009, 1720, 4010, 5791, 4856, 274, 4851, 7344, 4312, 6788, 7724, 7342, 4591, 6791], "orig_top_k_doc_id": [4008, 4009, 4010, 4856, 274, 4851, 7344, 4312, 6788, 5791, 7724, 1720, 7342, 4591, 6791]}, {"qid": 2441, "question": "Which NMT models did they experiment with? in A Parallel Corpus of Theses and Dissertations Abstracts", "answer": ["2-layer LSTM model with 500 hidden units in both encoder and decoder"], "top_k_doc_id": [4008, 4009, 1720, 4010, 5791, 4713, 1149, 2762, 231, 3687, 4105, 1722, 5029, 6190, 2761], "orig_top_k_doc_id": [4008, 4009, 4010, 4713, 1149, 2762, 231, 3687, 5791, 4105, 1722, 5029, 6190, 1720, 2761]}, {"qid": 2573, "question": "What are three challenging tasks authors evaluated their sequentially aligned representations? in Back to the Future -- Sequential Alignment of Text Representations", "answer": ["paper acceptance prediction, Named Entity Recognition (NER), author stance prediction"], "top_k_doc_id": [4008, 4009, 4529, 4533, 7169, 6561, 7023, 6562, 247, 4534, 1325, 4571, 6555, 6635, 786], "orig_top_k_doc_id": [4529, 4533, 7169, 4008, 4009, 6561, 7023, 6562, 247, 4534, 1325, 4571, 6555, 6635, 786]}]}
{"group_id": 694, "group_size": 3, "items": [{"qid": 2468, "question": "Did they experiment with the corpus? in Introducing RONEC -- the Romanian Named Entity Corpus", "answer": ["Yes"], "top_k_doc_id": [4141, 4142, 4145, 7056, 20, 21, 3743, 4431, 6153, 4573, 2479, 4952, 608, 4951, 7553], "orig_top_k_doc_id": [4141, 4145, 4142, 6153, 7056, 4573, 2479, 4952, 608, 4951, 21, 3743, 20, 7553, 4431]}, {"qid": 2469, "question": "What writing styles are present in the corpus? in Introducing RONEC -- the Romanian Named Entity Corpus", "answer": ["current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials."], "top_k_doc_id": [4141, 4142, 4145, 7056, 20, 21, 3743, 4431, 4667, 5157, 1865, 5911, 2694, 7597, 953], "orig_top_k_doc_id": [4141, 4145, 4142, 4431, 20, 4667, 5157, 1865, 21, 5911, 7056, 2694, 7597, 953, 3743]}, {"qid": 2470, "question": "How did they determine the distinct classes? in Introducing RONEC -- the Romanian Named Entity Corpus", "answer": ["inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8"], "top_k_doc_id": [4141, 4142, 4145, 7056, 2731, 2732, 4144, 608, 4143, 7688, 5123, 6153, 4353, 4859, 5130], "orig_top_k_doc_id": [4141, 4142, 4145, 2731, 2732, 4144, 608, 4143, 7688, 5123, 6153, 4353, 7056, 4859, 5130]}]}
{"group_id": 695, "group_size": 3, "items": [{"qid": 2471, "question": "Do they jointly tackle multiple tagging problems? in A General-Purpose Tagger with Convolutional Neural Networks", "answer": ["No"], "top_k_doc_id": [2607, 4146, 626, 1579, 2177, 4148, 4288, 627, 397, 1459, 7237, 624, 3602, 3904, 3162], "orig_top_k_doc_id": [4288, 4146, 627, 2177, 2607, 397, 626, 1459, 7237, 624, 4148, 3602, 3904, 3162, 1579]}, {"qid": 2472, "question": "How many parameters does their CNN have? in A General-Purpose Tagger with Convolutional Neural Networks", "answer": ["No"], "top_k_doc_id": [2607, 4146, 626, 1579, 2177, 4148, 418, 6151, 5569, 2900, 417, 7472, 1345, 1989, 419], "orig_top_k_doc_id": [4146, 418, 6151, 5569, 2900, 417, 4148, 7472, 2177, 626, 1345, 1579, 1989, 2607, 419]}, {"qid": 2473, "question": "How do they confirm their model working well on out-of-vocabulary problems? in A General-Purpose Tagger with Convolutional Neural Networks", "answer": ["conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set"], "top_k_doc_id": [2607, 4146, 7472, 1179, 6804, 4680, 4288, 7363, 6678, 1468, 1345, 3822, 2654, 919, 165], "orig_top_k_doc_id": [4146, 7472, 1179, 6804, 4680, 4288, 7363, 6678, 1468, 1345, 3822, 2654, 2607, 919, 165]}]}
{"group_id": 696, "group_size": 3, "items": [{"qid": 2483, "question": "What experiment result led to conclussion that reducing the number of layers of the decoder does not matter much? in The Transference Architecture for Automatic Post-Editing", "answer": ["Exp. 5.1"], "top_k_doc_id": [4178, 4179, 4181, 4182, 4183, 7566, 1411, 2743, 4180, 6310, 4693, 3655, 4963, 1584, 4812], "orig_top_k_doc_id": [4178, 4182, 4183, 4179, 6310, 4181, 1411, 4693, 4180, 3655, 7566, 4963, 1584, 4812, 2743]}, {"qid": 2484, "question": "How much is performance hurt when using too small amount of layers in encoder? in The Transference Architecture for Automatic Post-Editing", "answer": ["comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. "], "top_k_doc_id": [4178, 4179, 4181, 4182, 4183, 7566, 1411, 2743, 4180, 6649, 5672, 2760, 2815, 3688, 4310], "orig_top_k_doc_id": [4182, 4178, 4183, 4181, 4179, 7566, 2743, 1411, 6649, 5672, 2760, 2815, 4180, 3688, 4310]}, {"qid": 2485, "question": "What was previous state of the art model for automatic post editing? in The Transference Architecture for Automatic Post-Editing", "answer": ["pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders, tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics., shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. , The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$."], "top_k_doc_id": [4178, 4179, 4181, 4182, 4183, 7566, 5672, 4963, 2760, 7570, 5673, 5813, 6107, 6646, 4962], "orig_top_k_doc_id": [4178, 4182, 5672, 4963, 2760, 4183, 7566, 7570, 5673, 5813, 6107, 4181, 6646, 4962, 4179]}]}
{"group_id": 697, "group_size": 3, "items": [{"qid": 2495, "question": "Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit? in Deep Neural Machine Translation with Linear Associative Unit", "answer": ["Yes"], "top_k_doc_id": [3781, 686, 2607, 4212, 4215, 3825, 7493, 3980, 5835, 7080, 3981, 2608, 3152, 7391, 3820], "orig_top_k_doc_id": [4215, 4212, 2607, 3781, 3825, 7493, 3980, 5835, 7080, 3981, 686, 2608, 3152, 7391, 3820]}, {"qid": 2581, "question": "What does recurrent deep stacking network do? in Recurrent Deep Stacking Networks for Speech Recognition", "answer": ["Stacks and joins outputs of previous frames with inputs of the current frame"], "top_k_doc_id": [3781, 686, 2607, 4212, 4543, 4544, 5475, 7079, 1618, 5109, 96, 2814, 5110, 7081, 7150], "orig_top_k_doc_id": [4543, 4544, 5475, 7079, 1618, 2607, 3781, 5109, 96, 4212, 686, 2814, 5110, 7081, 7150]}, {"qid": 2590, "question": "Did the authors try stacking multiple convolutional layers? in A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network", "answer": ["No"], "top_k_doc_id": [3781, 2048, 6086, 3559, 4974, 4564, 2979, 4543, 6609, 3016, 6608, 5475, 7545, 96, 7630], "orig_top_k_doc_id": [2048, 6086, 3559, 4974, 4564, 2979, 4543, 3781, 6609, 3016, 6608, 5475, 7545, 96, 7630]}]}
{"group_id": 698, "group_size": 3, "items": [{"qid": 2501, "question": "Did participants behave unexpectedly? in Speakers account for asymmetries in visual perspective so listeners don't have to", "answer": ["No"], "top_k_doc_id": [3375, 3376, 3394, 3397, 4235, 3381, 4229, 4230, 4231, 4232, 4233, 4234, 228, 3388, 3382], "orig_top_k_doc_id": [4230, 4235, 4229, 4233, 4231, 3381, 4234, 4232, 3394, 3376, 228, 3388, 3382, 3375, 3397]}, {"qid": 2502, "question": "Was this experiment done in a lab? in Speakers account for asymmetries in visual perspective so listeners don't have to", "answer": ["No"], "top_k_doc_id": [3375, 3376, 3394, 3397, 4235, 3381, 4229, 4230, 4231, 4232, 4233, 4234, 7059, 3377, 3390], "orig_top_k_doc_id": [4229, 4230, 4233, 4235, 4234, 3397, 3381, 3376, 4231, 3394, 4232, 3375, 7059, 3377, 3390]}, {"qid": 2193, "question": "Does the paper describe experiments with real humans? in When redundancy is rational: A Bayesian approach to 'overinformative' referring expressions", "answer": ["Yes"], "top_k_doc_id": [3375, 3376, 3394, 3397, 4235, 3393, 3374, 3392, 3378, 3377, 3390, 3380, 3388, 3395, 3385], "orig_top_k_doc_id": [3393, 3374, 3397, 3392, 3394, 3375, 3378, 3377, 3390, 4235, 3380, 3388, 3395, 3385, 3376]}]}
{"group_id": 699, "group_size": 3, "items": [{"qid": 2583, "question": "Do the authors test on datasets other than bAbl? in Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "answer": ["No"], "top_k_doc_id": [3357, 4545, 4546, 4547, 4550, 6588, 7839, 7843, 3808, 4548, 6589, 5916, 3507, 6679, 3358], "orig_top_k_doc_id": [4545, 6588, 4546, 4550, 4547, 7843, 4548, 5916, 3507, 3357, 7839, 3808, 6589, 6679, 3358]}, {"qid": 2584, "question": "What is the reward model for the reinforcement learning appraoch? in Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "answer": ["reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail"], "top_k_doc_id": [3357, 4545, 4546, 4547, 4550, 6588, 7839, 7843, 3808, 4548, 6589, 4549, 5690, 2867, 4444], "orig_top_k_doc_id": [4545, 6588, 4550, 4546, 7843, 3808, 4549, 6589, 5690, 2867, 3357, 4548, 4444, 7839, 4547]}, {"qid": 2582, "question": "Does the latent dialogue state heklp their model? in Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning", "answer": ["Yes"], "top_k_doc_id": [3357, 4545, 4546, 4547, 4550, 6588, 7839, 7843, 3507, 3358, 2550, 4484, 2554, 4669, 4298], "orig_top_k_doc_id": [4545, 6588, 4546, 4550, 3357, 3507, 7843, 3358, 4547, 2550, 4484, 2554, 7839, 4669, 4298]}]}
{"group_id": 700, "group_size": 3, "items": [{"qid": 2602, "question": "What accuracy does the proposed system achieve? in Adversarial Learning for Chinese NER from Crowd Annotations", "answer": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "top_k_doc_id": [4583, 5873, 4584, 4585, 4587, 4588, 4589, 4791, 3565, 3566, 6029, 3642, 5941, 1782, 3141], "orig_top_k_doc_id": [4583, 4589, 4584, 4588, 4585, 4587, 4791, 5873, 3565, 3566, 6029, 3642, 5941, 1782, 3141]}, {"qid": 2603, "question": "What crowdsourcing platform is used? in Adversarial Learning for Chinese NER from Crowd Annotations", "answer": ["No", "They did not use any platform, instead they hired undergraduate students to do the annotation."], "top_k_doc_id": [4583, 5873, 4584, 4585, 4587, 4588, 4589, 5038, 8, 5113, 6995, 5037, 6140, 5042, 780], "orig_top_k_doc_id": [4583, 4584, 4589, 4588, 4585, 4587, 5038, 5873, 8, 5113, 6995, 5037, 6140, 5042, 780]}, {"qid": 243, "question": "what crowdsourcing platform is used? in Learning to Rank Scientific Documents from the Crowd", "answer": ["asked the authors to rank by closeness five citations we selected from their paper"], "top_k_doc_id": [4583, 5873, 303, 302, 5038, 5719, 5113, 5043, 5037, 5115, 5112, 3225, 8, 6140, 3587], "orig_top_k_doc_id": [303, 302, 5038, 5719, 5113, 5043, 5037, 5115, 5112, 3225, 8, 6140, 3587, 5873, 4583]}]}
{"group_id": 701, "group_size": 3, "items": [{"qid": 2638, "question": "Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people? in Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "answer": ["No", "No"], "top_k_doc_id": [520, 523, 4645, 6804, 3007, 3008, 3132, 521, 4964, 309, 5525, 558, 6882, 1801, 7443], "orig_top_k_doc_id": [4645, 520, 521, 6804, 3008, 523, 4964, 309, 3007, 5525, 558, 6882, 3132, 1801, 7443]}, {"qid": 2639, "question": "Which demographic dimensions of people do they obtain? in Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "answer": ["occupation, industry, profile information, language use, gender ", "density of users, gender distribution"], "top_k_doc_id": [520, 523, 4645, 6804, 3007, 3008, 3132, 3009, 2036, 3011, 4409, 2045, 3381, 4408, 5537], "orig_top_k_doc_id": [4645, 520, 523, 3132, 3008, 3009, 2036, 3007, 6804, 3011, 4409, 2045, 3381, 4408, 5537]}, {"qid": 2640, "question": "How do they obtain psychological dimensions of people? in Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "answer": ["using the Meaning Extraction Method", "No"], "top_k_doc_id": [520, 523, 4645, 6804, 6631, 5537, 6525, 5525, 5056, 5524, 5784, 6630, 4, 3318, 3381], "orig_top_k_doc_id": [4645, 6631, 5537, 523, 6804, 6525, 5525, 5056, 5524, 5784, 520, 6630, 4, 3318, 3381]}]}
{"group_id": 702, "group_size": 3, "items": [{"qid": 2656, "question": "What is the GhostVLAD approach? in Identification of Indian Languages using Ghost-VLAD pooling", "answer": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "top_k_doc_id": [4671, 4672, 4673, 6403, 6834, 7172, 716, 2773, 3685, 4590, 4592, 6399, 2797, 3598, 76], "orig_top_k_doc_id": [4671, 4673, 4672, 7172, 6399, 6403, 6834, 4592, 3685, 2797, 3598, 4590, 716, 76, 2773]}, {"qid": 2657, "question": "Which 7 Indian languages do they experiment with? in Identification of Indian Languages using Ghost-VLAD pooling", "answer": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "top_k_doc_id": [4671, 4672, 4673, 6403, 6834, 7172, 716, 2773, 3685, 4590, 4592, 7046, 3687, 7044, 7173], "orig_top_k_doc_id": [4671, 4673, 4672, 7172, 3685, 4592, 6834, 4590, 716, 7046, 3687, 6403, 2773, 7044, 7173]}, {"qid": 2655, "question": "How was the audio data gathered? in Identification of Indian Languages using Ghost-VLAD pooling", "answer": ["Through the All India Radio new channel where actors read news.", " $\\textbf {All India Radio}$ news channel"], "top_k_doc_id": [4671, 4672, 4673, 6403, 6834, 7172, 76, 1622, 75, 6481, 6399, 1623, 78, 3664, 1266], "orig_top_k_doc_id": [4671, 4673, 4672, 76, 7172, 1622, 75, 6481, 6834, 6399, 1623, 78, 6403, 3664, 1266]}]}
{"group_id": 703, "group_size": 3, "items": [{"qid": 2662, "question": "Which neural architecture do they use as a base for their attention conflict mechanisms? in Conflict as an Inverse of Attention in Sequence Relationship", "answer": ["GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.", "two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers"], "top_k_doc_id": [143, 2253, 4680, 4681, 4682, 4683, 4884, 4886, 4887, 4894, 3633, 3795, 4888, 569, 6772], "orig_top_k_doc_id": [4682, 4683, 4680, 4681, 143, 2253, 569, 4888, 4886, 4894, 3633, 6772, 3795, 4887, 4884]}, {"qid": 2663, "question": "On which tasks do they test their conflict method? in Conflict as an Inverse of Attention in Sequence Relationship", "answer": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "top_k_doc_id": [143, 2253, 4680, 4681, 4682, 4683, 4884, 4886, 4887, 4894, 3633, 3795, 4888, 3634, 7589], "orig_top_k_doc_id": [4682, 4683, 4680, 143, 4681, 4886, 4884, 4894, 4888, 4887, 3634, 2253, 3795, 3633, 7589]}, {"qid": 2661, "question": "Do they show on which examples how conflict works better than attention? in Conflict as an Inverse of Attention in Sequence Relationship", "answer": ["Yes", "Yes"], "top_k_doc_id": [143, 2253, 4680, 4681, 4682, 4683, 4884, 4886, 4887, 4894, 569, 7388, 4893, 3634, 287], "orig_top_k_doc_id": [4682, 4683, 4680, 4681, 143, 4894, 569, 7388, 4886, 4884, 4893, 2253, 3634, 287, 4887]}]}
{"group_id": 704, "group_size": 3, "items": [{"qid": 2665, "question": "What are the sources of the datasets? in Multilingual Clustering of Streaming News", "answer": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "top_k_doc_id": [4684, 4687, 5189, 5322, 5323, 2878, 3007, 4359, 2689, 7285, 2794, 2877, 5784, 3735, 2079], "orig_top_k_doc_id": [4687, 5322, 3007, 2689, 4684, 2878, 5323, 5189, 4359, 7285, 2794, 2877, 5784, 3735, 2079]}, {"qid": 2666, "question": "What metric is used for evaluation? in Multilingual Clustering of Streaming News", "answer": ["F1, precision, recall, accuracy", "Precision, recall, F1, accuracy"], "top_k_doc_id": [4684, 4687, 5189, 5322, 5323, 2878, 3007, 4359, 2405, 5930, 5931, 247, 5127, 6574, 2080], "orig_top_k_doc_id": [4687, 4684, 2405, 5322, 5323, 5930, 5189, 5931, 247, 5127, 6574, 3007, 2878, 2080, 4359]}, {"qid": 2664, "question": "Do they use graphical models? in Multilingual Clustering of Streaming News", "answer": ["No", "No"], "top_k_doc_id": [4684, 4687, 5189, 5322, 5323, 6158, 5784, 2405, 4131, 2404, 3904, 2794, 6485, 3280, 5186], "orig_top_k_doc_id": [4687, 6158, 5784, 2405, 5323, 4131, 4684, 2404, 3904, 2794, 6485, 5189, 3280, 5322, 5186]}]}
{"group_id": 705, "group_size": 3, "items": [{"qid": 2667, "question": "Which eight NER tasks did they evaluate on? in Inexpensive Domain Adaptation of Pretrained Language Models: A Case Study on Biomedical Named Entity Recognition", "answer": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "top_k_doc_id": [5133, 5674, 6810, 607, 3743, 4688, 4689, 4690, 4946, 7597, 929, 2477, 4589, 4945, 2348], "orig_top_k_doc_id": [4688, 4689, 4946, 607, 5674, 3743, 4690, 6810, 5133, 7597, 929, 2477, 4589, 4945, 2348]}, {"qid": 2668, "question": "What in-domain text did they use? in Inexpensive Domain Adaptation of Pretrained Language Models: A Case Study on Biomedical Named Entity Recognition", "answer": ["PubMed+PMC", "PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset)"], "top_k_doc_id": [5133, 5674, 6810, 607, 3743, 4688, 4689, 4690, 20, 1106, 21, 3812, 3839, 4414, 6656], "orig_top_k_doc_id": [4688, 3743, 4689, 607, 20, 5674, 4690, 1106, 21, 6810, 3812, 3839, 5133, 4414, 6656]}, {"qid": 3427, "question": "What is NER? in A Biomedical Information Extraction Primer for NLP Researchers", "answer": ["Named Entity Recognition", "Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain"], "top_k_doc_id": [5133, 5674, 6810, 5675, 2348, 5677, 6711, 3812, 7002, 2694, 3052, 2127, 20, 929, 2973], "orig_top_k_doc_id": [5674, 5133, 5675, 2348, 6810, 5677, 6711, 3812, 7002, 2694, 3052, 2127, 20, 929, 2973]}]}
{"group_id": 706, "group_size": 3, "items": [{"qid": 2742, "question": "What are method's improvements of F1 for NER task for English and Chinese datasets? in Dice Loss for Data-imbalanced NLP Tasks", "answer": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "top_k_doc_id": [4788, 4789, 4790, 4791, 4792, 1782, 3505, 4583, 4752, 5941, 4750, 5133, 5940, 4749, 2321], "orig_top_k_doc_id": [4791, 4788, 4792, 4790, 1782, 3505, 4789, 4583, 4752, 5941, 4750, 5133, 5940, 4749, 2321]}, {"qid": 2743, "question": "What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets? in Dice Loss for Data-imbalanced NLP Tasks", "answer": ["+1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4", " +1.86"], "top_k_doc_id": [4788, 4789, 4790, 4791, 4792, 1782, 3505, 4583, 4752, 3780, 4654, 4054, 3903, 2839, 1763], "orig_top_k_doc_id": [4791, 4790, 4788, 4792, 3780, 4789, 4654, 4583, 3505, 4054, 4752, 3903, 2839, 1782, 1763]}, {"qid": 2741, "question": "What are method improvements of F1 for paraphrase identification? in Dice Loss for Data-imbalanced NLP Tasks", "answer": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "top_k_doc_id": [4788, 4789, 4790, 4791, 4792, 1894, 5499, 7763, 1893, 5980, 5498, 4116, 4948, 2807, 5146], "orig_top_k_doc_id": [4791, 4788, 4792, 4790, 4789, 1894, 5499, 7763, 1893, 5980, 5498, 4116, 4948, 2807, 5146]}]}
{"group_id": 707, "group_size": 3, "items": [{"qid": 2787, "question": "which datasets did they experiment with? in External Lexical Information for Multilingual Part-of-Speech Tagging", "answer": ["Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,\nIndonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish", "Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2"], "top_k_doc_id": [4431, 224, 3547, 4880, 4881, 5716, 4882, 2906, 7688, 7439, 225, 4526, 6153, 5710, 3269], "orig_top_k_doc_id": [4880, 4881, 4882, 5716, 224, 2906, 7688, 7439, 225, 4526, 6153, 5710, 3269, 4431, 3547]}, {"qid": 2788, "question": "which languages are explored? in External Lexical Information for Multilingual Part-of-Speech Tagging", "answer": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "top_k_doc_id": [4431, 224, 3547, 4880, 4881, 5716, 2329, 6036, 4289, 1000, 6333, 398, 3938, 1773, 7285], "orig_top_k_doc_id": [4880, 4881, 2329, 6036, 5716, 4431, 3547, 224, 4289, 1000, 6333, 398, 3938, 1773, 7285]}, {"qid": 2547, "question": "How big is multilingual dataset? in MULTEXT-East", "answer": ["No"], "top_k_doc_id": [4431, 4437, 4436, 4435, 4438, 4432, 4433, 4434, 2329, 3353, 4510, 1318, 6312, 5980, 5977], "orig_top_k_doc_id": [4431, 4437, 4436, 4435, 4438, 4432, 4433, 4434, 2329, 3353, 4510, 1318, 6312, 5980, 5977]}]}
{"group_id": 708, "group_size": 3, "items": [{"qid": 2839, "question": "Which architecture do they use for the encoder and decoder? in Attention-based Wav2Text with Feature Transfer Learning", "answer": ["we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)", "In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM "], "top_k_doc_id": [2528, 4972, 4973, 4974, 4975, 6943, 4590, 5681, 5835, 398, 2529, 6031, 2995, 1683, 5682], "orig_top_k_doc_id": [4974, 4975, 4973, 4972, 2528, 5835, 4590, 398, 5681, 6943, 2529, 6031, 2995, 1683, 5682]}, {"qid": 2840, "question": "How does their decoder generate text? in Attention-based Wav2Text with Feature Transfer Learning", "answer": ["decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information", "Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy"], "top_k_doc_id": [2528, 4972, 4973, 4974, 4975, 6943, 4590, 5681, 4034, 6371, 1005, 4424, 492, 4571, 6864], "orig_top_k_doc_id": [4974, 4975, 4973, 4972, 2528, 5681, 4034, 6943, 6371, 1005, 4424, 492, 4571, 6864, 4590]}, {"qid": 2841, "question": "Which dataset do they use? in Attention-based Wav2Text with Feature Transfer Learning", "answer": ["WSJ", "WSJ-SI84, WSJ-SI284"], "top_k_doc_id": [2528, 4972, 4973, 4974, 4975, 6943, 5815, 2951, 1548, 1547, 2953, 5269, 5175, 3815, 5682], "orig_top_k_doc_id": [4974, 4975, 4973, 4972, 5815, 2951, 1548, 1547, 6943, 2953, 5269, 5175, 2528, 3815, 5682]}]}
{"group_id": 709, "group_size": 3, "items": [{"qid": 3080, "question": "What metrics do they use? in Improving Question Generation With to the Point Context", "answer": ["BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19", "BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)"], "top_k_doc_id": [1946, 2519, 3809, 491, 1082, 1322, 3807, 492, 7668, 3190, 1642, 1323, 788, 3193, 52], "orig_top_k_doc_id": [1082, 3809, 492, 3807, 7668, 491, 3190, 1642, 1322, 1323, 788, 1946, 3193, 2519, 52]}, {"qid": 3081, "question": "On what datasets are experiments performed? in Improving Question Generation With to the Point Context", "answer": ["SQuAD", "SQuAD"], "top_k_doc_id": [1946, 2519, 3809, 491, 1082, 1322, 3807, 1945, 3810, 3530, 7218, 2520, 4640, 3805, 2267], "orig_top_k_doc_id": [2519, 1946, 1945, 3810, 3530, 7218, 3809, 2520, 4640, 1082, 491, 3807, 1322, 3805, 2267]}, {"qid": 3079, "question": "How big are significant improvements? in Improving Question Generation With to the Point Context", "answer": ["Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1"], "top_k_doc_id": [1946, 2519, 3809, 3805, 6721, 2818, 5229, 4641, 3394, 1409, 2341, 5226, 6833, 6294, 1362], "orig_top_k_doc_id": [3805, 2519, 6721, 3809, 2818, 5229, 4641, 3394, 1409, 2341, 5226, 1946, 6833, 6294, 1362]}]}
{"group_id": 710, "group_size": 3, "items": [{"qid": 3087, "question": "How do they measure which words are under-translated by NMT models? in Towards Understanding Neural Machine Translation with Word Importance", "answer": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "top_k_doc_id": [5240, 5243, 5244, 3260, 5241, 6041, 2763, 7827, 774, 2501, 6266, 2764, 4697, 3920, 3686], "orig_top_k_doc_id": [5240, 3260, 5243, 5244, 2763, 6041, 7827, 5241, 774, 2501, 6266, 2764, 4697, 3920, 3686]}, {"qid": 3089, "question": "Which model architectures do they test their word importance approach on? in Towards Understanding Neural Machine Translation with Word Importance", "answer": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "top_k_doc_id": [5240, 5243, 5244, 3260, 5241, 6041, 37, 7787, 2636, 203, 4412, 1987, 6723, 4033, 5242], "orig_top_k_doc_id": [5240, 37, 7787, 2636, 5244, 203, 4412, 3260, 5243, 1987, 6723, 6041, 4033, 5241, 5242]}, {"qid": 3086, "question": "Does their model suffer exhibit performance drops when incorporating word importance? in Towards Understanding Neural Machine Translation with Word Importance", "answer": ["No"], "top_k_doc_id": [5240, 5243, 5244, 6723, 1247, 337, 4033, 3941, 7470, 7334, 7581, 117, 3489, 7787, 3819], "orig_top_k_doc_id": [5240, 6723, 5244, 1247, 337, 4033, 5243, 3941, 7470, 7334, 7581, 117, 3489, 7787, 3819]}]}
{"group_id": 711, "group_size": 3, "items": [{"qid": 3144, "question": "What types of subwords do they incorporate in their model? in Incorporating Subword Information into Matrix Factorization Word Embeddings", "answer": ["n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords ", "simple n-grams (like fastText) and unsupervised morphemes"], "top_k_doc_id": [4510, 5302, 5303, 1370, 1371, 5623, 6767, 3845, 5306, 3817, 2348, 5837, 3844, 6785, 5622], "orig_top_k_doc_id": [5302, 3845, 1371, 5303, 1370, 5306, 6767, 3817, 2348, 4510, 5837, 3844, 6785, 5623, 5622]}, {"qid": 4272, "question": "How long is the vocabulary of subwords? in Subword ELMo", "answer": ["500", "500"], "top_k_doc_id": [4510, 5302, 5303, 1370, 1371, 5623, 6767, 6768, 7043, 6769, 7042, 6784, 6499, 3818, 1115], "orig_top_k_doc_id": [6767, 6768, 7043, 6769, 7042, 5302, 1370, 5303, 4510, 6784, 5623, 1371, 6499, 3818, 1115]}, {"qid": 3870, "question": "Do they compare to other models that include subword information such as fastText? in A Joint Model for Word Embedding and Word Morphology", "answer": ["No", "No"], "top_k_doc_id": [4510, 5302, 5303, 2448, 922, 131, 1691, 3845, 1584, 5709, 3844, 5963, 3846, 7686, 6265], "orig_top_k_doc_id": [5302, 2448, 922, 4510, 131, 1691, 3845, 1584, 5709, 3844, 5963, 5303, 3846, 7686, 6265]}]}
{"group_id": 712, "group_size": 3, "items": [{"qid": 3199, "question": "How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset? in Embracing data abundance: BookTest Dataset for Reading Comprehension", "answer": ["INLINEFORM2 ", "Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively."], "top_k_doc_id": [2013, 5367, 5368, 5369, 5370, 5371, 1370, 2011, 5372, 2012, 1373, 1372, 2752, 1512, 3416], "orig_top_k_doc_id": [5370, 5371, 5369, 5367, 5372, 2012, 5368, 1373, 2013, 1372, 2752, 1370, 1512, 2011, 3416]}, {"qid": 3200, "question": "How do they show there is space for further improvement? in Embracing data abundance: BookTest Dataset for Reading Comprehension", "answer": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "top_k_doc_id": [2013, 5367, 5368, 5369, 5370, 5371, 1370, 2011, 2442, 2446, 2234, 2836, 4637, 5518, 7590], "orig_top_k_doc_id": [5371, 5370, 5369, 5367, 2442, 2446, 2234, 2013, 2836, 4637, 1370, 5518, 5368, 7590, 2011]}, {"qid": 3198, "question": "How does their ensemble method work? in Embracing data abundance: BookTest Dataset for Reading Comprehension", "answer": ["simply averaging the predictions from the constituent single models"], "top_k_doc_id": [2013, 5367, 5368, 5369, 5370, 5371, 1512, 5372, 4074, 2910, 4915, 5848, 2442, 1964, 4075], "orig_top_k_doc_id": [5370, 5371, 5369, 1512, 5368, 5372, 2013, 5367, 4074, 2910, 4915, 5848, 2442, 1964, 4075]}]}
{"group_id": 713, "group_size": 3, "items": [{"qid": 3223, "question": "Which benchmark datasets are used? in A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks", "answer": ["Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector", "This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector"], "top_k_doc_id": [1329, 1967, 2105, 2112, 5405, 5406, 5408, 6172, 6666, 7115, 7116, 4284, 7118, 5407, 1330], "orig_top_k_doc_id": [1329, 2105, 2112, 7115, 5408, 5405, 7116, 7118, 5406, 5407, 6666, 6172, 1330, 1967, 4284]}, {"qid": 3224, "question": "What are the network's baseline features? in A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks", "answer": [" The features extracted from CNN."], "top_k_doc_id": [1329, 1967, 2105, 2112, 5405, 5406, 5408, 6172, 6666, 7115, 7116, 4284, 7233, 6667, 5410], "orig_top_k_doc_id": [2112, 2105, 6172, 5408, 1329, 7116, 5405, 6666, 1967, 5406, 7233, 6667, 4284, 7115, 5410]}, {"qid": 3222, "question": "What are the state of the art models? in A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks", "answer": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "top_k_doc_id": [1329, 1967, 2105, 2112, 5405, 5406, 5408, 6172, 6666, 7115, 7116, 7118, 5410, 6884, 709], "orig_top_k_doc_id": [1329, 7116, 6666, 2112, 5405, 1967, 5408, 2105, 5406, 7118, 7115, 6172, 5410, 6884, 709]}]}
{"group_id": 714, "group_size": 3, "items": [{"qid": 3255, "question": "Does the experiments focus on a specific domain? in Language Use Matters: Analysis of the Linguistic Structure of Question Texts Can Characterize Answerability in Quora", "answer": ["No", "No"], "top_k_doc_id": [325, 5461, 5462, 5579, 5580, 5736, 7667, 1560, 2103, 3857, 3511, 4987, 6451, 5597, 3810], "orig_top_k_doc_id": [5461, 5462, 5579, 1560, 325, 5736, 2103, 3857, 3511, 7667, 5580, 4987, 6451, 5597, 3810]}, {"qid": 3257, "question": "Do the answered questions measure for the usefulness of the answer? in Language Use Matters: Analysis of the Linguistic Structure of Question Texts Can Characterize Answerability in Quora", "answer": ["No"], "top_k_doc_id": [325, 5461, 5462, 5579, 5580, 5736, 7667, 2693, 2521, 7615, 3095, 3807, 6848, 5743, 4257], "orig_top_k_doc_id": [5461, 5462, 5579, 5580, 5736, 2693, 2521, 7615, 3095, 3807, 6848, 325, 7667, 5743, 4257]}, {"qid": 3256, "question": "how many training samples do you have for training? in Language Use Matters: Analysis of the Linguistic Structure of Question Texts Can Characterize Answerability in Quora", "answer": ["No", "No"], "top_k_doc_id": [325, 5461, 5462, 5579, 5580, 3810, 4579, 1272, 3807, 7158, 2103, 4986, 7185, 5259, 3096], "orig_top_k_doc_id": [5461, 5462, 5580, 3810, 5579, 4579, 1272, 3807, 325, 7158, 2103, 4986, 7185, 5259, 3096]}]}
{"group_id": 715, "group_size": 3, "items": [{"qid": 3258, "question": "What profile metadata is used for this analysis? in Is change the only constant? Profile change perspective on #LokSabhaElections2019", "answer": ["username, display name, profile image, location, description", "username, display name, profile image, location and description"], "top_k_doc_id": [4126, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 4124, 4127, 235, 3947, 6693, 522, 5191], "orig_top_k_doc_id": [5463, 5464, 5468, 5465, 5467, 5466, 4126, 235, 3947, 4127, 6693, 522, 5191, 4124, 5469]}, {"qid": 3260, "question": "How do profile changes vary for influential leads and their followers over the social movement? in Is change the only constant? Profile change perspective on #LokSabhaElections2019", "answer": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "top_k_doc_id": [4126, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 4124, 4127, 4140, 554, 926, 1482, 4139], "orig_top_k_doc_id": [5463, 5468, 5464, 5465, 5467, 5466, 5469, 4140, 4124, 554, 926, 1482, 4126, 4139, 4127]}, {"qid": 3259, "question": "What are the organic and inorganic ways to show political affiliation through profile changes? in Is change the only constant? Profile change perspective on #LokSabhaElections2019", "answer": ["Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.\nInorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.", "Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way."], "top_k_doc_id": [4126, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 6357, 1481, 7016, 3947, 1380, 1486, 1482], "orig_top_k_doc_id": [5463, 5468, 5464, 5466, 5467, 5469, 5465, 6357, 1481, 7016, 3947, 1380, 4126, 1486, 1482]}]}
{"group_id": 716, "group_size": 3, "items": [{"qid": 3261, "question": "What evaluation metrics do they use? in TWEETQA: A Social Media Focused Question Answering Dataset", "answer": ["BLEU-1, Meteor , Rouge-L ", "BLEU-1, Meteor ,  Rouge-L "], "top_k_doc_id": [447, 2157, 5470, 5471, 5472, 5473, 7029, 2398, 5906, 491, 286, 788, 787, 5147, 955], "orig_top_k_doc_id": [5470, 5473, 5472, 5471, 447, 491, 2157, 286, 788, 787, 2398, 7029, 5906, 5147, 955]}, {"qid": 3262, "question": "What is the size of this dataset? in TWEETQA: A Social Media Focused Question Answering Dataset", "answer": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "top_k_doc_id": [447, 2157, 5470, 5471, 5472, 5473, 7029, 2398, 5906, 1172, 5812, 2210, 6285, 3627, 5085], "orig_top_k_doc_id": [5470, 5472, 5473, 5471, 1172, 2157, 447, 5812, 5906, 2210, 6285, 2398, 3627, 7029, 5085]}, {"qid": 3263, "question": "How do they determine if tweets have been used by journalists? in TWEETQA: A Social Media Focused Question Answering Dataset", "answer": [" we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles"], "top_k_doc_id": [447, 2157, 5470, 5471, 5472, 5473, 7029, 6341, 4113, 4119, 4136, 1172, 4112, 3486, 2079], "orig_top_k_doc_id": [5470, 5472, 5471, 2157, 5473, 6341, 447, 4113, 4119, 4136, 1172, 4112, 3486, 2079, 7029]}]}
{"group_id": 717, "group_size": 3, "items": [{"qid": 3264, "question": "how small of a dataset did they train on? in Deep LSTM for Large Vocabulary Continuous Speech Recognition", "answer": ["1000 hours data", "23085 hours of data"], "top_k_doc_id": [4972, 5475, 5987, 3438, 3439, 3705, 4526, 5476, 1228, 3087, 7688, 3594, 2709, 380, 7793], "orig_top_k_doc_id": [5475, 4972, 5476, 3439, 1228, 3087, 3705, 3438, 4526, 7688, 5987, 3594, 2709, 380, 7793]}, {"qid": 3266, "question": "which lstm models did they compare with? in Deep LSTM for Large Vocabulary Continuous Speech Recognition", "answer": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "top_k_doc_id": [4972, 5475, 5987, 3438, 3439, 3705, 4526, 5476, 768, 2120, 2238, 1975, 2027, 5478, 4209], "orig_top_k_doc_id": [5475, 4972, 768, 2120, 4526, 3705, 2238, 1975, 2027, 5478, 3439, 4209, 5476, 5987, 3438]}, {"qid": 3265, "question": "what was their character error rate? in Deep LSTM for Large Vocabulary Continuous Speech Recognition", "answer": ["2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.", "Their best model achieved a 2.49% Character Error Rate."], "top_k_doc_id": [4972, 5475, 5987, 5989, 5478, 768, 1161, 5351, 5990, 2709, 4863, 1791, 4974, 3407, 942], "orig_top_k_doc_id": [5475, 5987, 5989, 4972, 5478, 768, 1161, 5351, 5990, 2709, 4863, 1791, 4974, 3407, 942]}]}
{"group_id": 718, "group_size": 3, "items": [{"qid": 3274, "question": "How many annotators participated? in Automatic Classification of Pathology Reports using TF-IDF Features", "answer": ["No", "No"], "top_k_doc_id": [1402, 1403, 5489, 5490, 5491, 5757, 749, 3087, 1432, 7857, 4322, 6971, 3753, 1838, 6179], "orig_top_k_doc_id": [5490, 5489, 5491, 1403, 5757, 3087, 1432, 7857, 1402, 4322, 6971, 749, 3753, 1838, 6179]}, {"qid": 3275, "question": "What features are used? in Automatic Classification of Pathology Reports using TF-IDF Features", "answer": ["No"], "top_k_doc_id": [1402, 1403, 5489, 5490, 5491, 5757, 749, 3087, 6164, 6623, 752, 125, 5755, 21, 6603], "orig_top_k_doc_id": [5491, 5490, 5489, 1403, 5757, 1402, 6164, 3087, 6623, 752, 749, 125, 5755, 21, 6603]}, {"qid": 3273, "question": "What is the reported agreement for the annotation? in Automatic Classification of Pathology Reports using TF-IDF Features", "answer": ["No", "No"], "top_k_doc_id": [1402, 1403, 5489, 5490, 5491, 5757, 645, 1838, 6623, 752, 371, 7258, 5549, 6177, 6006], "orig_top_k_doc_id": [5491, 5490, 5489, 1403, 1402, 645, 1838, 6623, 752, 371, 7258, 5549, 6177, 5757, 6006]}]}
{"group_id": 719, "group_size": 3, "items": [{"qid": 3309, "question": "What is novel about their document-level encoder? in Text Summarization with Pretrained Encoders", "answer": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "top_k_doc_id": [734, 3157, 5540, 5541, 735, 4760, 5542, 5545, 4825, 4826, 1251, 1683, 5544, 4761, 730], "orig_top_k_doc_id": [5540, 735, 5541, 5542, 4825, 734, 4826, 1251, 1683, 5544, 5545, 4761, 730, 4760, 3157]}, {"qid": 3311, "question": "What are the datasets used for evaluation? in Text Summarization with Pretrained Encoders", "answer": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "top_k_doc_id": [734, 3157, 5540, 5541, 735, 4760, 5542, 5545, 4561, 5709, 4649, 5716, 5711, 4619, 2226], "orig_top_k_doc_id": [5540, 735, 4561, 734, 3157, 5709, 5545, 5541, 5542, 4649, 5716, 5711, 4619, 4760, 2226]}, {"qid": 3310, "question": "What rouge score do they achieve? in Text Summarization with Pretrained Encoders", "answer": ["Best results on unigram:\nCNN/Daily Mail: Rogue F1 43.85\nNYT: Rogue Recall 49.02\nXSum: Rogue F1 38.81", "Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55"], "top_k_doc_id": [734, 3157, 5540, 5541, 6860, 2226, 3160, 4649, 1134, 5804, 730, 3201, 2225, 5151, 5152], "orig_top_k_doc_id": [6860, 5540, 734, 2226, 3157, 3160, 4649, 1134, 5804, 730, 3201, 5541, 2225, 5151, 5152]}]}
{"group_id": 720, "group_size": 3, "items": [{"qid": 3316, "question": "What is the benchmark dataset and is its quality high? in Detecting\"Smart\"Spammers On Social Network: A Topic Model Approach", "answer": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "top_k_doc_id": [1725, 5551, 5552, 5553, 6056, 442, 6285, 447, 2464, 4780, 5101, 7104, 5067, 286, 3736], "orig_top_k_doc_id": [5551, 5552, 6056, 5553, 447, 1725, 2464, 4780, 5101, 442, 7104, 6285, 5067, 286, 3736]}, {"qid": 3317, "question": "How do they detect spammers? in Detecting\"Smart\"Spammers On Social Network: A Topic Model Approach", "answer": ["Extract features from the LDA model and use them in a binary classification task"], "top_k_doc_id": [1725, 5551, 5552, 5553, 6056, 442, 6285, 4392, 1876, 4393, 6455, 521, 5953, 6286, 6817], "orig_top_k_doc_id": [5551, 5552, 6056, 5553, 1725, 4392, 1876, 4393, 6455, 521, 6285, 5953, 442, 6286, 6817]}, {"qid": 3315, "question": "LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection? in Detecting\"Smart\"Spammers On Social Network: A Topic Model Approach", "answer": ["No", "No"], "top_k_doc_id": [1725, 5551, 5552, 5553, 6056, 4417, 6817, 4131, 1727, 5417, 5069, 2794, 2104, 1726, 5067], "orig_top_k_doc_id": [5551, 6056, 4417, 5552, 6817, 5553, 4131, 1727, 1725, 5417, 5069, 2794, 2104, 1726, 5067]}]}
{"group_id": 721, "group_size": 3, "items": [{"qid": 3429, "question": "Does jiant involve datasets for the 50 NLU tasks? in jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models", "answer": ["Yes"], "top_k_doc_id": [4412, 5678, 352, 2966, 3071, 5679, 5680, 2150, 4278, 1853, 898, 6448, 7572, 2149, 4663], "orig_top_k_doc_id": [5678, 5680, 5679, 3071, 352, 2150, 4278, 1853, 898, 6448, 4412, 2966, 7572, 2149, 4663]}, {"qid": 3430, "question": "Is jiant compatible with models in any programming language? in jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models", "answer": ["Yes", "No"], "top_k_doc_id": [4412, 5678, 352, 2966, 3071, 5679, 5680, 275, 353, 2965, 2270, 2730, 2956, 7552, 349], "orig_top_k_doc_id": [5678, 5680, 5679, 3071, 352, 2966, 275, 353, 2965, 2270, 2730, 2956, 4412, 7552, 349]}, {"qid": 1537, "question": "How are their changes evaluated? in Incrementalizing RASA's Open-Source Natural Language Understanding Pipeline", "answer": ["The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset"], "top_k_doc_id": [4412, 5678, 2152, 6575, 2153, 2151, 7382, 4413, 7351, 2455, 4908, 3805, 7548, 7383, 6350], "orig_top_k_doc_id": [2152, 6575, 2153, 2151, 7382, 4413, 4412, 7351, 5678, 2455, 4908, 3805, 7548, 7383, 6350]}]}
{"group_id": 722, "group_size": 3, "items": [{"qid": 3459, "question": "What is the model architecture used? in Visual Question: Predicting If a Crowd Will Agree on the Answer", "answer": ["LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.", "random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer."], "top_k_doc_id": [870, 871, 3037, 5727, 5728, 5729, 5730, 5731, 3033, 3530, 789, 4523, 7805, 3799, 788], "orig_top_k_doc_id": [5729, 5728, 5727, 5731, 5730, 870, 3033, 871, 3530, 789, 3037, 4523, 7805, 3799, 788]}, {"qid": 3460, "question": "How is the data used for training annotated? in Visual Question: Predicting If a Crowd Will Agree on the Answer", "answer": ["The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions."], "top_k_doc_id": [870, 871, 3037, 5727, 5728, 5729, 5730, 5731, 6010, 521, 6014, 8, 5877, 2662, 5873], "orig_top_k_doc_id": [5729, 5728, 5727, 5731, 5730, 870, 6010, 3037, 521, 6014, 871, 8, 5877, 2662, 5873]}, {"qid": 2571, "question": "Do the authors perform experiments using their proposed method? in Grounded Agreement Games: Emphasizing Conversational Grounding in Visual Dialogue Settings", "answer": ["Yes"], "top_k_doc_id": [870, 871, 4523, 318, 1541, 7299, 5793, 4524, 6036, 1808, 312, 6793, 1715, 6344, 3115], "orig_top_k_doc_id": [4523, 318, 871, 1541, 7299, 5793, 870, 4524, 6036, 1808, 312, 6793, 1715, 6344, 3115]}]}
{"group_id": 723, "group_size": 3, "items": [{"qid": 3625, "question": "How much data did they gather from crowdsourcing? in A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents", "answer": ["600K", "9960", "9960 HITs from 472 crowd workers", "9960 HITs"], "top_k_doc_id": [2888, 5953, 5954, 1718, 1817, 3225, 6590, 2969, 2970, 899, 4785, 4445, 4523, 6589, 6586], "orig_top_k_doc_id": [5953, 3225, 2969, 2970, 5954, 1718, 6590, 899, 4785, 1817, 4445, 4523, 6589, 6586, 2888]}, {"qid": 3626, "question": "How many different strategies were evaluated? in A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents", "answer": ["14", "12", "14", "No"], "top_k_doc_id": [2888, 5953, 5954, 1718, 1817, 3225, 6590, 28, 6344, 1719, 7810, 4786, 7808, 6038, 3479], "orig_top_k_doc_id": [5953, 5954, 6590, 1817, 28, 2888, 3225, 1718, 6344, 1719, 7810, 4786, 7808, 6038, 3479]}, {"qid": 3624, "question": "How do data-driven models usually respond to abuse? in A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents", "answer": ["either by refusing politely, or, with flirtatious responses, or, by retaliating", "Data-driven systems rank low in general", "politely refuse, politely refuses, flirtatious responses", "flirt; retaliation"], "top_k_doc_id": [2888, 5953, 5954, 250, 3585, 7807, 7808, 7809, 3581, 1171, 3805, 3584, 7810, 6584, 3588], "orig_top_k_doc_id": [5953, 5954, 250, 3585, 7807, 7808, 7809, 3581, 1171, 2888, 3805, 3584, 7810, 6584, 3588]}]}
{"group_id": 724, "group_size": 3, "items": [{"qid": 3703, "question": "What metrics are used for evaluation? in Common-Knowledge Concept Recognition for SEVA", "answer": ["F1-score", "precision, recall, f1-score, and support", "Precision, recall, f1-score, and support."], "top_k_doc_id": [6050, 6051, 6052, 1591, 3122, 788, 404, 1136, 6153, 1590, 787, 793, 1138, 7759, 3807], "orig_top_k_doc_id": [6050, 6052, 6051, 788, 404, 1136, 6153, 1590, 787, 793, 1591, 1138, 3122, 7759, 3807]}, {"qid": 3706, "question": "How does labeling scheme look like? in Common-Knowledge Concept Recognition for SEVA", "answer": ["Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.", "BIO Labelling Scheme\nabb: represents abbreviations such as TRL representing Technology Readiness Level.\n\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n\norg: represents an organization such as `NASA', `aerospace industry', etc.\n\nart: represents names of artifacts or instruments such as `AS1300'\n\ncardinal: represents numerical values such as `1', `100', 'one' etc.\n\nloc: represents location-like entities such as component facilities or centralized facility.\n\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.", "1. abb\n2. grp\n3. syscon\n4. opcon\n5. seterm\n6. event\n7. org\n8. art\n9. cardinal\n10. loc\n11. mea"], "top_k_doc_id": [6050, 6051, 6052, 1591, 3122, 5910, 3121, 1577, 2387, 3143, 5908, 4817, 4900, 789, 4277], "orig_top_k_doc_id": [6052, 6050, 6051, 5910, 3121, 1577, 2387, 3143, 5908, 4817, 1591, 3122, 4900, 789, 4277]}, {"qid": 3702, "question": "How many domain experts were involved into creation of dataset? in Common-Knowledge Concept Recognition for SEVA", "answer": ["No", "1", "One domain expert."], "top_k_doc_id": [6050, 6051, 6052, 2386, 5674, 4105, 3052, 4106, 2694, 2391, 5908, 5911, 1533, 3743, 5879], "orig_top_k_doc_id": [6050, 6052, 6051, 2386, 5674, 4105, 3052, 4106, 2694, 2391, 5908, 5911, 1533, 3743, 5879]}]}
{"group_id": 725, "group_size": 3, "items": [{"qid": 3745, "question": "what models did they compare with? in To Tune or Not To Tune? How About the Best of Both Worlds?", "answer": ["BERT, BERT adding a Bi-LSTM on top, DenseNet BIBREF33 and HighwayLSTM BIBREF34, BERT+ BIMPM, remove the first bi-LSTM of BIMPM, Sim-Transformer", "BERT, BERT+ Bi-LSTM ,  BERT+ DenseNet, BERT+HighwayLSTM,   Ensembled model, BERT+ BIMPM, BERT+ BIMPM(first bi-LSTM removed),  BERT + Sim-Transformer .", "BERT, BERT + Bi-LSTM, BERT + HighwayLSTM, BERT + DenseNet, Ensembled Model, BERT + BIMPM, BERT + Sim-Transformer"], "top_k_doc_id": [1145, 558, 3790, 6613, 1879, 7830, 5207, 5970, 823, 2948, 6490, 559, 890, 6270, 3130], "orig_top_k_doc_id": [1879, 7830, 3790, 5207, 5970, 823, 558, 2948, 1145, 6613, 6490, 559, 890, 6270, 3130]}, {"qid": 3746, "question": "what datasets were used for testing? in To Tune or Not To Tune? How About the Best of Both Worlds?", "answer": ["CoNLL03 , Yahoo Answer Classification Dataset, \u201cQuora-Question-Pair\u201d dataset 1", "CoNLL03,  Yahoo Answer Classification Dataset, \u201cQuora-Question-Pair\u201d dataset 1", "CoNLL03 dataset BIBREF5, Yahoo Answer Classification Dataset,  \u201cQuora-Question-Pair\u201d dataset"], "top_k_doc_id": [1145, 558, 3790, 6613, 3842, 5542, 3841, 5560, 5349, 87, 2666, 771, 3702, 4752, 6012], "orig_top_k_doc_id": [3842, 5542, 3841, 5560, 558, 6613, 5349, 87, 2666, 1145, 771, 3702, 4752, 6012, 3790]}, {"qid": 3744, "question": "did they test with other pretrained models besides bert? in To Tune or Not To Tune? How About the Best of Both Worlds?", "answer": ["No", "No", "No"], "top_k_doc_id": [1145, 5560, 5542, 4993, 5572, 6135, 7830, 7817, 5710, 5473, 3167, 6270, 7633, 1981, 6661], "orig_top_k_doc_id": [5560, 5542, 4993, 5572, 6135, 7830, 7817, 5710, 5473, 3167, 6270, 7633, 1981, 1145, 6661]}]}
{"group_id": 726, "group_size": 3, "items": [{"qid": 3748, "question": "How did they annotate the corpus? in A Corpus of Adpositional Supersenses for Mandarin Chinese", "answer": ["The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.", "Tokenization, Adposition Targets, Data Format, Reliability of Annotation", "The corpus is jointly annotated by three native Mandarin Chinese speakers, Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication, Annotation was conducted in two phases"], "top_k_doc_id": [861, 6096, 6097, 6098, 6100, 973, 6099, 6311, 736, 739, 3623, 741, 2906, 7346, 2948], "orig_top_k_doc_id": [6096, 6100, 6097, 6098, 861, 736, 973, 739, 6311, 3623, 741, 2906, 7346, 2948, 6099]}, {"qid": 3749, "question": "What is the size of the corpus? in A Corpus of Adpositional Supersenses for Mandarin Chinese", "answer": ["933 manually identified adpositions", "20287", "933 manually identified adpositions"], "top_k_doc_id": [861, 6096, 6097, 6098, 6100, 973, 6099, 6311, 3406, 2315, 3407, 3024, 3939, 974, 1595], "orig_top_k_doc_id": [6096, 6100, 6098, 6097, 973, 3406, 2315, 6311, 3407, 3024, 3939, 861, 974, 6099, 1595]}, {"qid": 3747, "question": "What inter-annotator agreement did they obtain? in A Corpus of Adpositional Supersenses for Mandarin Chinese", "answer": [" two inter-annotator agreement , aw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons", "Raw agreement is around .90 for this dataset.", "The average agreement on scene, function and construal is 0.915"], "top_k_doc_id": [861, 6096, 6097, 6098, 6100, 2392, 862, 740, 2393, 5911, 3895, 5704, 6742, 6808, 5705], "orig_top_k_doc_id": [6096, 6098, 6097, 6100, 2392, 861, 862, 740, 2393, 5911, 3895, 5704, 6742, 6808, 5705]}]}
{"group_id": 727, "group_size": 3, "items": [{"qid": 3762, "question": "By how much do they outperform existing methods? in Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "answer": ["In Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt", "Their best implementation for semantic relatedness task comparison outperforms standard MaxEnt by 0,052 Pearson  Correlation.\nTheir best implementation for Textual Entailment task comparison (84,2 accuracy) DOES NOT outperform standard SVM (84,6 accuracy).\n", "Best proposed result had 0.851 and  0.842 compared to best previous result of 0.828 and 0.846 on person correlation and accuracy respectively."], "top_k_doc_id": [6119, 7056, 338, 1392, 3412, 4826, 5212, 337, 709, 6767, 3642, 1077, 4755, 1393, 826], "orig_top_k_doc_id": [6119, 5212, 7056, 337, 709, 6767, 3642, 3412, 338, 1077, 4755, 4826, 1393, 826, 1392]}, {"qid": 3763, "question": "Which datasets do they evaluate on? in Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "answer": ["SICK (Sentences Involving Compositional Knowledge) dataset ", "SICK (Sentences Involving Compositional Knowledge) dataset", "SICK (Sentences Involving Compositional Knowledge) dataset"], "top_k_doc_id": [6119, 7056, 338, 1392, 3412, 4826, 6070, 2433, 1391, 2429, 2434, 7057, 5709, 469, 3468], "orig_top_k_doc_id": [6119, 7056, 6070, 2433, 1391, 3412, 2429, 2434, 1392, 7057, 4826, 5709, 469, 3468, 338]}, {"qid": 3764, "question": "Do they separately evaluate performance of their learned representations (before forwarding them to the CNN layer)? in Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding", "answer": ["Yes", "No", "No"], "top_k_doc_id": [6119, 7056, 3846, 6070, 6503, 709, 4652, 7367, 1517, 5314, 1344, 5938, 1343, 2615, 6937], "orig_top_k_doc_id": [6119, 3846, 6070, 6503, 7056, 709, 4652, 7367, 1517, 5314, 1344, 5938, 1343, 2615, 6937]}]}
{"group_id": 728, "group_size": 3, "items": [{"qid": 3787, "question": "How much data do they use to train the embeddings? in Chinese Embedding via Stroke and Glyph Information: A Dual-channel View", "answer": ["11,529,432 segmented words and 20,402 characters", "11,529,432 segmented words", "11,529,432 segmented words"], "top_k_doc_id": [6148, 6149, 6150, 1453, 3230, 4790, 4791, 6385, 6390, 2058, 3233, 4624, 6546, 922, 3017], "orig_top_k_doc_id": [6148, 6149, 6150, 6385, 6390, 2058, 3233, 4624, 4790, 6546, 922, 4791, 3230, 1453, 3017]}, {"qid": 3789, "question": "What dialects of Chinese are explored? in Chinese Embedding via Stroke and Glyph Information: A Dual-channel View", "answer": ["No", "No", "No"], "top_k_doc_id": [6148, 6149, 6150, 1453, 3230, 4790, 4791, 6385, 2329, 3456, 1266, 1248, 6097, 1874, 2315], "orig_top_k_doc_id": [6148, 6149, 6150, 2329, 4790, 3456, 6385, 4791, 1266, 1248, 1453, 6097, 3230, 1874, 2315]}, {"qid": 3788, "question": "Do they evaluate their embeddings in any downstream task appart from word similarity and word analogy? in Chinese Embedding via Stroke and Glyph Information: A Dual-channel View", "answer": ["No", "No", "No"], "top_k_doc_id": [6148, 6149, 6150, 2526, 2525, 6964, 398, 3992, 3207, 4933, 6965, 5303, 397, 4932, 4627], "orig_top_k_doc_id": [6149, 6148, 6150, 2526, 2525, 6964, 398, 3992, 3207, 4933, 6965, 5303, 397, 4932, 4627]}]}
{"group_id": 729, "group_size": 3, "items": [{"qid": 3808, "question": "Do single-language BERT outperforms multilingual BERT? in Does BERT agree? Evaluating knowledge of structure dependence through agreement relations", "answer": ["No", "For some language yes, but not for another.", "No"], "top_k_doc_id": [6165, 6166, 6167, 3069, 5713, 6870, 125, 4290, 3777, 3621, 5710, 3778, 2907, 4331, 1971], "orig_top_k_doc_id": [6165, 6166, 6167, 5713, 6870, 125, 3069, 4290, 3777, 3621, 5710, 3778, 2907, 4331, 1971]}, {"qid": 3809, "question": "What types of agreement relations do they explore? in Does BERT agree? Evaluating knowledge of structure dependence through agreement relations", "answer": ["subject-verb, noun-determiner, noun-attributive adjective, subject-predicate adjective", "The subject and verb in (UNKREF3) agree for number, while the noun and determiner in (UNKREF4), the noun and attributive adjective in (UNKREF5) and the subject and predicated adjective in (UNKREF6) agree for both number and gender.", "Subject-verb agreement,  noun-determiner agreement,  noun -attributive adjective agreement and noun-predicate adjective agreement."], "top_k_doc_id": [6165, 6166, 6167, 3069, 5727, 3626, 2171, 4355, 6168, 875, 740, 4108, 3070, 5729, 242], "orig_top_k_doc_id": [6165, 6166, 6167, 3069, 5727, 3626, 2171, 4355, 6168, 875, 740, 4108, 3070, 5729, 242]}, {"qid": 2637, "question": "Were any of these tasks evaluated in any previous work? in Assessing BERT's Syntactic Abilities", "answer": ["Yes", "Yes"], "top_k_doc_id": [6165, 6166, 6167, 6449, 6870, 2682, 2969, 4688, 5063, 5543, 3487, 4689, 6763, 1216, 4642], "orig_top_k_doc_id": [6449, 6167, 6166, 6165, 6870, 2682, 2969, 4688, 5063, 5543, 3487, 4689, 6763, 1216, 4642]}]}
{"group_id": 730, "group_size": 3, "items": [{"qid": 3811, "question": "which models is their approach compared to? in BAE: BERT-based Adversarial Examples for Text Classification", "answer": ["TextFooler", "word-LSTM BIBREF20, word-CNN BIBREF21,  fine-tuned BERT BIBREF12 base-uncased ", "word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier"], "top_k_doc_id": [5351, 5560, 5561, 6169, 6170, 6171, 7472, 5558, 5559, 7473, 7475, 6863, 6868, 3446, 1896], "orig_top_k_doc_id": [6170, 6171, 6169, 5351, 5561, 5560, 7472, 6863, 7475, 5559, 5558, 6868, 3446, 7473, 1896]}, {"qid": 4799, "question": "How are adversarial examples generated? in Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "answer": ["we are searching for the worst perturbations while trying to minimize the loss of the model", "By using a white-box method using perturbation calculated based on the gradient of the loss function."], "top_k_doc_id": [5351, 5560, 5561, 6169, 6170, 6171, 7472, 5558, 5559, 7473, 7475, 5562, 4199, 4205, 1897], "orig_top_k_doc_id": [7472, 7475, 5561, 7473, 5560, 5559, 5562, 6170, 6169, 4199, 6171, 4205, 5351, 5558, 1897]}, {"qid": 3810, "question": "what text classification datasets do they evaluate on? in BAE: BERT-based Adversarial Examples for Text Classification", "answer": ["Amazon, Yelp, IMDB, MR, MPQA, Subj, TREC", "Amazon, Yelp, IMDB , MR , MPQA , Subj, TREC", "Amazon, Yelp, IMDB, MR BIBREF16, MPQA BIBREF17, Subj BIBREF18, TREC BIBREF19"], "top_k_doc_id": [5351, 5560, 5561, 6169, 6170, 6171, 7472, 6863, 4202, 3140, 2676, 4207, 3141, 4199, 6868], "orig_top_k_doc_id": [6170, 6169, 6171, 5561, 5351, 5560, 7472, 6863, 4202, 3140, 2676, 4207, 3141, 4199, 6868]}]}
{"group_id": 731, "group_size": 3, "items": [{"qid": 3829, "question": "Do they explore how their word representations vary across languages? in Continuous multilinguality with language vectors", "answer": ["Yes", "Yes", "No"], "top_k_doc_id": [3621, 4692, 6190, 3617, 5700, 5885, 3212, 5712, 5711, 6870, 6491, 4186, 6240, 1040, 247], "orig_top_k_doc_id": [5700, 6190, 4692, 5885, 3212, 5712, 3621, 5711, 6870, 6491, 4186, 3617, 6240, 1040, 247]}, {"qid": 3831, "question": "How do they show genetic relationships between languages? in Continuous multilinguality with language vectors", "answer": ["hierarchical clustering", "By doing hierarchical clustering of word vectors", "By applying hierarchical clustering on language vectors found during training"], "top_k_doc_id": [3621, 4692, 6190, 3617, 5700, 4618, 4615, 4184, 4185, 6191, 4203, 7042, 2908, 2355, 2329], "orig_top_k_doc_id": [4618, 4615, 4692, 4184, 4185, 6191, 3621, 5700, 4203, 7042, 6190, 3617, 2908, 2355, 2329]}, {"qid": 3830, "question": "Which neural language model architecture do they use? in Continuous multilinguality with language vectors", "answer": ["character-level RNN", "standard stacked character-based LSTM BIBREF4", "LSTM"], "top_k_doc_id": [3621, 4692, 6190, 7848, 4185, 3619, 768, 315, 398, 4301, 4695, 1183, 3338, 3368, 774], "orig_top_k_doc_id": [4692, 7848, 4185, 3621, 3619, 768, 315, 398, 6190, 4301, 4695, 1183, 3338, 3368, 774]}]}
{"group_id": 732, "group_size": 3, "items": [{"qid": 3886, "question": "What is the sign language recognition task investigated? in Sign Language Recognition Analysis using Multimodal Data", "answer": ["We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. ", " American Sign Language recognition ", "No"], "top_k_doc_id": [3455, 3460, 6274, 7786, 7787, 931, 1066, 4199, 6275, 6276, 6277, 6278, 5015, 589, 5481], "orig_top_k_doc_id": [6274, 6275, 6278, 6277, 7787, 3460, 931, 6276, 5015, 7786, 3455, 589, 5481, 4199, 1066]}, {"qid": 3887, "question": "What is the performance of the best model in the sign language recognition task? in Sign Language Recognition Analysis using Multimodal Data", "answer": ["Spatial AI-LSTM", "Accuracy 81%", "Best performing model is Spatial AI-LSTM with accuracy 81% and Std. Deviation 6%"], "top_k_doc_id": [3455, 3460, 6274, 7786, 7787, 931, 1066, 4199, 6275, 6276, 6277, 6278, 230, 7264, 226], "orig_top_k_doc_id": [6274, 6278, 6275, 6277, 7787, 3460, 931, 6276, 7786, 1066, 3455, 230, 7264, 226, 4199]}, {"qid": 2218, "question": "Do they build a generative probabilistic language model for sign language? in A human-editable Sign Language representation for software editing---and a writing system?", "answer": ["No"], "top_k_doc_id": [3455, 3460, 6274, 7786, 7787, 3465, 3464, 3463, 3458, 3459, 3457, 3466, 7567, 6444, 7566], "orig_top_k_doc_id": [3455, 3465, 3464, 3463, 3460, 3458, 6274, 3459, 3457, 3466, 7786, 7787, 7567, 6444, 7566]}]}
{"group_id": 733, "group_size": 3, "items": [{"qid": 3893, "question": "Do humans assess the quality of the generated responses? in A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [5291, 5906, 6285, 6286, 6375, 3445, 6287, 6288, 6289, 6290, 6795, 1725, 5914, 6770, 3309], "orig_top_k_doc_id": [6286, 6289, 3445, 6290, 6288, 6285, 5906, 6795, 1725, 6287, 5914, 6770, 5291, 3309, 6375]}, {"qid": 3894, "question": "What models are used to generate responses? in A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "answer": ["Seq2Seq, Variational Auto-Encoder (VAE), Reinforcement Learning (RL)", "Seq2Seq BIBREF25, BIBREF24, Variational Auto-Encoder (VAE) BIBREF26, Reinforcement Learning (RL)", "Seq2Seq BIBREF25, BIBREF24, Variational Auto-Encoder (VAE) BIBREF26, Reinforcement Learning (RL)"], "top_k_doc_id": [5291, 5906, 6285, 6286, 6375, 3445, 6287, 6288, 6289, 6290, 5907, 3581, 5641, 3582, 413], "orig_top_k_doc_id": [6286, 6285, 6289, 5906, 6287, 6290, 5291, 6375, 6288, 5907, 3445, 3581, 5641, 3582, 413]}, {"qid": 3895, "question": "What types of hate speech are considered? in A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "answer": ["No", " Potentially hateful comments are identified using hate keywords.", "race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability."], "top_k_doc_id": [5291, 5906, 6285, 6286, 6375, 412, 3309, 6176, 413, 5976, 5085, 3582, 3581, 1077, 5915], "orig_top_k_doc_id": [6286, 5291, 6285, 412, 3309, 5906, 6375, 6176, 413, 5976, 5085, 3582, 3581, 1077, 5915]}]}
{"group_id": 734, "group_size": 3, "items": [{"qid": 4029, "question": "How are the results evaluated? in Neural Joking Machine : Humorous image captioning", "answer": ["The captions are ranked by humans in order of \"funniness\".", "a questionnaire", "With a questionnaire asking subjects to rank methods according to its \"funniness\". Also, by posting the captions to Bokete to evaluate them by received stars"], "top_k_doc_id": [3026, 3034, 4033, 6477, 7355, 7580, 7085, 7138, 4744, 7357, 2900, 1743, 4748, 2899, 3800], "orig_top_k_doc_id": [6477, 7355, 4744, 4033, 7357, 3026, 7580, 2900, 7085, 3034, 1743, 4748, 7138, 2899, 3800]}, {"qid": 4030, "question": "How big is the self-collected corpus? in Neural Joking Machine : Humorous image captioning", "answer": ["999,571 funny captions for 70,981 images", " 999,571 funny captions for 70,981 images", "999571 captions for 70981 images."], "top_k_doc_id": [3026, 3034, 4033, 6477, 7355, 7580, 7085, 7138, 3027, 2922, 4034, 3031, 490, 1865, 2414], "orig_top_k_doc_id": [6477, 3026, 7085, 7355, 3027, 2922, 4033, 7580, 4034, 3031, 7138, 3034, 490, 1865, 2414]}, {"qid": 4028, "question": "What is the performance of NJM? in Neural Joking Machine : Humorous image captioning", "answer": ["NJM vas selected as the funniest caption among the three options 22.59% of the times, and NJM captions posted to Bokete averaged 3.23 stars", "It obtained a score of 22.59%", "Captions generated by NJM were ranked \"funniest\" 22.59% of the time."], "top_k_doc_id": [3026, 3034, 4033, 6477, 7355, 7580, 6478, 2922, 3426, 2085, 490, 1743, 4744, 7083, 3175], "orig_top_k_doc_id": [6477, 6478, 4033, 3026, 2922, 3426, 7355, 2085, 490, 1743, 4744, 3034, 7083, 3175, 7580]}]}
{"group_id": 735, "group_size": 3, "items": [{"qid": 4058, "question": "What is the performance of the model on English, Spanish and Arabic? in Gender Prediction from Tweets: Improving Neural Representations with Hand-Crafted Features", "answer": ["on PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic", "Accuracy: English 82.31, Spanish 80.22, Arabic 80.50", "In terms of accuracy, 81.789% for English, 78.227% for Spanish and 78.5% for Arabic"], "top_k_doc_id": [6512, 6513, 6514, 3547, 7260, 5979, 2329, 5976, 6752, 1433, 1875, 2331, 5977, 1432, 1318], "orig_top_k_doc_id": [6514, 6512, 6513, 5979, 2329, 5976, 6752, 1433, 7260, 1875, 2331, 5977, 1432, 3547, 1318]}, {"qid": 4059, "question": "How is this model different from a LSTM? in Gender Prediction from Tweets: Improving Neural Representations with Hand-Crafted Features", "answer": ["It's a recurrent neural network with n-gram model", "bidirectional RNN with GRU"], "top_k_doc_id": [6512, 6513, 6514, 3547, 7260, 1345, 7125, 7628, 5175, 6520, 7458, 4553, 699, 5292, 5590], "orig_top_k_doc_id": [6512, 6514, 6513, 1345, 7125, 7628, 5175, 6520, 7260, 7458, 4553, 699, 3547, 5292, 5590]}, {"qid": 4057, "question": "Are LSA-reduced n-gram features considered hand-crafted features? in Gender Prediction from Tweets: Improving Neural Representations with Hand-Crafted Features", "answer": ["Yes", "No", "Yes"], "top_k_doc_id": [6512, 6513, 6514, 5417, 3196, 3197, 5407, 1435, 7258, 6070, 7125, 5406, 3195, 4781, 1434], "orig_top_k_doc_id": [6512, 6514, 6513, 5417, 3196, 3197, 5407, 1435, 7258, 6070, 7125, 5406, 3195, 4781, 1434]}]}
{"group_id": 736, "group_size": 3, "items": [{"qid": 4075, "question": "How is a per-word reward tuned with the perceptron algorithm? in Correcting Length Bias in Neural Machine Translation", "answer": ["Optimal per-word reward is found using SGD, which in this case is the same as the perceptron algorithm", "hen the stochastic gradient descent update is just the familiar perceptron rule: INLINEFORM1"], "top_k_doc_id": [6535, 6536, 6538, 6539, 1469, 3173, 4312, 6537, 6559, 3250, 1743, 1953, 5442, 1669, 1323], "orig_top_k_doc_id": [6538, 6535, 6539, 6536, 6537, 3173, 6559, 3250, 4312, 1743, 1953, 5442, 1669, 1323, 1469]}, {"qid": 4076, "question": "What methods are used to correct the brevity problem? in Correcting Length Bias in Neural Machine Translation", "answer": [" tuned word reward ", "Length normalization; Google\u2019s NMT correction; constant word reward", "Length normalization, Google's NMT, constant word reward"], "top_k_doc_id": [6535, 6536, 6538, 6539, 1469, 3173, 4312, 6537, 2188, 3359, 7693, 5162, 6558, 6560, 1124], "orig_top_k_doc_id": [6538, 6539, 6535, 6537, 6536, 3173, 4312, 2188, 3359, 7693, 5162, 6558, 1469, 6560, 1124]}, {"qid": 4077, "question": "Why does wider beam search hurt NMT? in Correcting Length Bias in Neural Machine Translation", "answer": ["Using a wider beam increases the probability of a shorter translation to remain in the top k variants and eventually score higher than any longer and more accurate translation variant", "brevity problem", "if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem."], "top_k_doc_id": [6535, 6536, 6538, 6539, 6668, 7345, 6669, 2894, 7693, 7432, 28, 5240, 3260, 6600, 7248], "orig_top_k_doc_id": [6535, 6668, 7345, 6669, 6538, 2894, 6539, 7693, 7432, 28, 5240, 6536, 3260, 6600, 7248]}]}
{"group_id": 737, "group_size": 3, "items": [{"qid": 4087, "question": "How did they obtain the dataset? in HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset", "answer": ["The authors crawled all areas listed an TripAdvisor's SiteIndex and gathered all links related to hotels. Using Selenium, they put a time gap between opening each page, to mimic human behaviour and avoid having their scraper being detected. They discarded pages without a review and for pages with a review, they collected the review's profile, the overall rating, the summary, the written text and subratings, where given. ", "hotel reviews from TripAdvisor", "TripAdvisor hotel reviews"], "top_k_doc_id": [4317, 5066, 5067, 5365, 6547, 6548, 6549, 6550, 6635, 6637, 196, 197, 3451, 6636, 820], "orig_top_k_doc_id": [6550, 6547, 6548, 6549, 197, 196, 6635, 3451, 4317, 5067, 5066, 5365, 820, 6636, 6637]}, {"qid": 4089, "question": "Did they experiment on this dataset? in HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [4317, 5066, 5067, 5365, 6547, 6548, 6549, 6550, 6635, 6637, 196, 197, 3451, 6636, 4510], "orig_top_k_doc_id": [6547, 6550, 6549, 6548, 197, 196, 6635, 5066, 5365, 6636, 6637, 5067, 4510, 3451, 4317]}, {"qid": 4088, "question": "Are the recommendations specific to a region? in HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset", "answer": ["No", "No"], "top_k_doc_id": [4317, 5066, 5067, 5365, 6547, 6548, 6549, 6550, 6635, 6637, 5068, 5362, 7758, 4394, 4395], "orig_top_k_doc_id": [6547, 6550, 6548, 6549, 5067, 5068, 5066, 4317, 5365, 6637, 5362, 6635, 7758, 4394, 4395]}]}
{"group_id": 738, "group_size": 3, "items": [{"qid": 4105, "question": "Do they ensure the that the architecture is differentiable everywhere after adding the Hungarian layer? in Hungarian Layer: Logics Empowered Neural Architecture", "answer": ["Yes", "Yes"], "top_k_doc_id": [626, 656, 6561, 6562, 6563, 6564, 7106, 655, 1267, 6565, 3902, 4057, 2929, 4693, 7493], "orig_top_k_doc_id": [6561, 6562, 6563, 3902, 6565, 7106, 656, 1267, 626, 655, 4057, 2929, 4693, 6564, 7493]}, {"qid": 4106, "question": "Which dataset(s) do they train on? in Hungarian Layer: Logics Empowered Neural Architecture", "answer": ["Quora Question Pairs", "Quora Question Pairs", "the public benchmark dataset of \u201cQuora Question Pairs\u201d"], "top_k_doc_id": [626, 656, 6561, 6562, 6563, 6564, 7106, 655, 1267, 6565, 934, 468, 5198, 5493, 1740], "orig_top_k_doc_id": [6563, 6561, 7106, 6562, 1267, 655, 626, 6564, 934, 468, 656, 5198, 6565, 5493, 1740]}, {"qid": 4107, "question": "By how much does their model outperform state-of-the-art baselines? in Hungarian Layer: Logics Empowered Neural Architecture", "answer": ["0.78% over the best state-of-the-art baseline", "The average improvement in accuracy of their model over baselines is 3.026 points.", "by more than  0.18"], "top_k_doc_id": [626, 656, 6561, 6562, 6563, 6564, 7106, 2013, 4601, 459, 2610, 6259, 3828, 1568, 7320], "orig_top_k_doc_id": [626, 6562, 6561, 6564, 6563, 2013, 4601, 7106, 459, 2610, 656, 6259, 3828, 1568, 7320]}]}
{"group_id": 739, "group_size": 3, "items": [{"qid": 4117, "question": "What is the new metric? in Expeditious Generation of Knowledge Graph Embeddings", "answer": ["They propose two new metrics. One, which they call the Neighbour Similarity Test, calculates how many shared characteristics there are between entities whose representations are neighbors in the embedding space. The second, which they call the Type and Category Test, is the same as the  Neighbour Similarity Test, except it uses entity types and categories in the place of individual entity characteristics.", "Neighbour Similarity Test; Type and Category Test", "Neighbour Similarity Test (NST) and Type and Category Test (TCT)"], "top_k_doc_id": [2007, 6587, 559, 3412, 4490, 4982, 134, 4161, 4160, 1858, 3193, 558, 2006, 2004, 458], "orig_top_k_doc_id": [134, 4161, 4160, 4982, 2007, 1858, 3412, 3193, 6587, 558, 559, 4490, 2006, 2004, 458]}, {"qid": 4119, "question": "What context is used when computing the embedding for an entity? in Expeditious Generation of Knowledge Graph Embeddings", "answer": ["a subject, a predicate, and an object in a knowledge base", "context window of 2"], "top_k_doc_id": [2007, 6587, 559, 3412, 4490, 4982, 560, 5336, 4318, 341, 7678, 3631, 5494, 3634, 4981], "orig_top_k_doc_id": [559, 560, 5336, 4318, 341, 7678, 4982, 3631, 6587, 5494, 3412, 3634, 4490, 2007, 4981]}, {"qid": 4118, "question": "How long do other state-of-the-art models take to process the same amount of data? in Expeditious Generation of Knowledge Graph Embeddings", "answer": ["RDF2Vec takes 123 minutes to generate random walks and an estimated 96 hours to train word2vec. KGloVe takes an estimated 12 hours to train GloVe. fastText takes an estimated 72 hours to train", "RDF2Vec: 123 minutes runtime with >96 hours training, FastText: 5 minutes with >72 hours training", "between 12 hours and 96 hours"], "top_k_doc_id": [2007, 6587, 1137, 131, 2173, 558, 7351, 3634, 6543, 4981, 4160, 123, 6586, 3633, 6066], "orig_top_k_doc_id": [1137, 131, 6587, 2173, 558, 7351, 3634, 6543, 4981, 2007, 4160, 123, 6586, 3633, 6066]}]}
{"group_id": 740, "group_size": 3, "items": [{"qid": 4185, "question": "Which dialogue data do they use to evaluate on? in ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons", "answer": ["Datasets from PersonaChat and Wizard of Wikipedia tasks.", "PersonaChat, Wizard of Wikipedia", "PersonaChat BIBREF5, Wizard of Wikipedia BIBREF7"], "top_k_doc_id": [6651, 6652, 6653, 6655, 576, 1445, 1446, 2227, 6038, 6654, 575, 1817, 3507, 7371, 1806], "orig_top_k_doc_id": [6651, 6655, 6652, 6654, 6038, 1446, 1445, 575, 6653, 2227, 576, 1817, 3507, 7371, 1806]}, {"qid": 4186, "question": "How much faster are pairwise annotations than other annotations? in ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons", "answer": ["by 5 times", "No"], "top_k_doc_id": [6651, 6652, 6653, 6655, 576, 1445, 1446, 2227, 6038, 6654, 7406, 199, 1714, 1718, 2438], "orig_top_k_doc_id": [6655, 6654, 6651, 6652, 6653, 1446, 6038, 576, 7406, 1445, 2227, 199, 1714, 1718, 2438]}, {"qid": 3434, "question": "How does final model rate on Likert scale? in Prose for a Painting", "answer": ["average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 ", "average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9"], "top_k_doc_id": [6651, 6652, 6653, 6655, 5682, 5681, 5720, 3499, 5719, 3452, 901, 1866, 4577, 902, 1366], "orig_top_k_doc_id": [5682, 5681, 5720, 3499, 6652, 5719, 3452, 901, 1866, 4577, 902, 1366, 6655, 6653, 6651]}]}
{"group_id": 741, "group_size": 3, "items": [{"qid": 4245, "question": "Do the authors report only on English in Towards Understanding Gender Bias in Relation Extraction", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [5006, 6735, 6736, 6737, 6738, 5976, 6964, 7267, 7064, 5764, 5765, 52, 7063, 3007, 3011], "orig_top_k_doc_id": [6735, 7267, 7064, 5764, 5976, 5765, 6737, 52, 5006, 6736, 7063, 6964, 3007, 3011, 6738]}, {"qid": 4248, "question": "How does name anonymization affect gender bias in predictions and performance? in Towards Understanding Gender Bias in Relation Extraction", "answer": ["Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations.", "substantially increases F1 score gap for the hypernym relation, slightly decreases F1 score gap for all other relations"], "top_k_doc_id": [5006, 6735, 6736, 6737, 6738, 5976, 6964, 7267, 6739, 1446, 3942, 5011, 6962, 6558, 6700], "orig_top_k_doc_id": [6735, 6738, 6739, 6737, 5006, 6736, 7267, 1446, 3942, 5011, 6964, 6962, 5976, 6558, 6700]}, {"qid": 4249, "question": "How are the sentences in WikiGenderBias curated? in Towards Understanding Gender Bias in Relation Extraction", "answer": ["Distant Supervision assumption, any sentence from an article written about one of those entities that mentions the other entity expresses the relation", "for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation"], "top_k_doc_id": [5006, 6735, 6736, 6737, 6738, 6739, 7063, 7061, 7060, 3941, 3942, 4139, 5191, 5949, 7064], "orig_top_k_doc_id": [6735, 6736, 6739, 6737, 6738, 7063, 7061, 5006, 7060, 3941, 3942, 4139, 5191, 5949, 7064]}]}
{"group_id": 742, "group_size": 3, "items": [{"qid": 4268, "question": "were these categories human evaluated? in Categorization in the Wild: Generalizing Cognitive Models to Naturalistic Data across Languages", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6755, 6756, 6757, 6758, 6765, 6766, 2132, 6760, 6761, 6763, 7859, 7311, 7182, 5871, 1296], "orig_top_k_doc_id": [6756, 6758, 6757, 6755, 6765, 6766, 6763, 6761, 7859, 7311, 7182, 6760, 2132, 5871, 1296]}, {"qid": 4269, "question": "do language share categories?  in Categorization in the Wild: Generalizing Cognitive Models to Naturalistic Data across Languages", "answer": ["Yes"], "top_k_doc_id": [6755, 6756, 6757, 6758, 6765, 6766, 2132, 6760, 6761, 6871, 5699, 7517, 6762, 1784, 82], "orig_top_k_doc_id": [6756, 6758, 6765, 6755, 6757, 6766, 6871, 6761, 2132, 5699, 7517, 6760, 6762, 1784, 82]}, {"qid": 4267, "question": "what languages did they evaluate on? in Categorization in the Wild: Generalizing Cognitive Models to Naturalistic Data across Languages", "answer": ["Arabic, Chinese, French, English, and German", "Arabic, Chinese, French, English, and German", "Arabic, Chinese, English, French, and German"], "top_k_doc_id": [6755, 6756, 6757, 6758, 6765, 6766, 6005, 6871, 5872, 5871, 1583, 5701, 5709, 7517, 2767], "orig_top_k_doc_id": [6756, 6758, 6765, 6755, 6766, 6757, 6005, 6871, 5872, 5871, 1583, 5701, 5709, 7517, 2767]}]}
{"group_id": 743, "group_size": 3, "items": [{"qid": 4273, "question": "what rnn classifiers were used? in Detecting Offensive Language in Tweets Using Deep Learning", "answer": ["LSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.", "experiment with combining various LSTM models enhanced with a number of novel features (O No additional features,  NS Neutral & Sexism,  NR Neutral & Racism, RS Racism & Sexism,  NRS Neutral, Racism & Sexism) in an ensemble.", "Long Short-Term Memory Network (LSTM)"], "top_k_doc_id": [245, 3445, 5168, 5171, 5288, 6176, 6179, 6770, 5145, 5172, 7259, 6774, 6082, 6141, 4948], "orig_top_k_doc_id": [5288, 5168, 245, 6176, 6179, 6774, 5172, 6770, 7259, 3445, 5171, 5145, 6082, 6141, 4948]}, {"qid": 4274, "question": "what results did their system obtain? in Detecting Offensive Language in Tweets Using Deep Learning", "answer": ["Best authors' system achieved 0.9320 F1 score.", "The best model achieved a 0.9320 F-score", "The best performing single classifier produces F1 0.9265. The best ensemble classifier  (O+NS+RS+NR+NRS) produce  F1 0.9320."], "top_k_doc_id": [245, 3445, 5168, 5171, 5288, 6176, 6179, 6770, 5145, 5172, 7259, 6771, 5295, 6131, 3446], "orig_top_k_doc_id": [5168, 245, 6176, 3445, 6770, 6179, 6771, 5145, 5288, 5295, 5171, 5172, 6131, 3446, 7259]}, {"qid": 4275, "question": "what are the existing approaches? in Detecting Offensive Language in Tweets Using Deep Learning", "answer": ["BIBREF12 , in which character n-grams and gender information were used as features, BIBREF5 investigated the impact of the experience of the annotator in the performance, two step classification BIBREF16, BIBREF13 , which focuses on various classes of Sexism, CNN in, BIBREF16"], "top_k_doc_id": [245, 3445, 5168, 5171, 5288, 6176, 6179, 6770, 6771, 6774, 412, 1727, 6131, 5291, 5979], "orig_top_k_doc_id": [6771, 6770, 6176, 5168, 6179, 3445, 5288, 245, 6774, 412, 5171, 1727, 6131, 5291, 5979]}]}
{"group_id": 744, "group_size": 3, "items": [{"qid": 4282, "question": "Which metrics do they use to evaluate results? in Polylingual Wordnet", "answer": ["BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics, precision, recall and F-measure", "BLEU , METEOR , chrF", "BLEU BIBREF35, METEOR BIBREF36, chrF BIBREF37, precision, recall , F-measure"], "top_k_doc_id": [6787, 6788, 6789, 6791, 145, 480, 6321, 6320, 6088, 5306, 4725, 4724, 7097, 6087, 1082], "orig_top_k_doc_id": [6788, 6789, 6791, 6787, 480, 6321, 6320, 6088, 5306, 4725, 4724, 7097, 6087, 145, 1082]}, {"qid": 4284, "question": "By how much do they outperform translating without contextual information? in Polylingual Wordnet", "answer": ["No", "No"], "top_k_doc_id": [6787, 6788, 6789, 6791, 145, 480, 7046, 7509, 4698, 488, 3703, 5417, 3702, 2823, 2746], "orig_top_k_doc_id": [6791, 6787, 6788, 6789, 7046, 7509, 4698, 480, 488, 145, 3703, 5417, 3702, 2823, 2746]}, {"qid": 4283, "question": "Does the performance increase with the number of used languages? in Polylingual Wordnet", "answer": ["Yes", "No"], "top_k_doc_id": [6787, 6788, 6789, 6791, 800, 4342, 73, 4341, 799, 6790, 68, 5713, 6128, 7509, 4852], "orig_top_k_doc_id": [6788, 6789, 6787, 6791, 800, 4342, 73, 4341, 799, 6790, 68, 5713, 6128, 7509, 4852]}]}
{"group_id": 745, "group_size": 3, "items": [{"qid": 4412, "question": "How long is the training dataset? in Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "answer": ["3,492 documents", "3492"], "top_k_doc_id": [5557, 6939, 6940, 6941, 6942, 7085, 2717, 3184, 6042, 7583, 5891, 5893, 1057, 6930, 4747], "orig_top_k_doc_id": [6942, 6939, 6941, 6940, 7583, 7085, 5891, 6042, 3184, 5893, 1057, 6930, 5557, 4747, 2717]}, {"qid": 4413, "question": "What dataset do they use? in Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "answer": ["CoNLL 2012", "English portion of CoNLL 2012 data BIBREF15"], "top_k_doc_id": [5557, 6939, 6940, 6941, 6942, 7085, 2717, 3184, 6042, 3943, 3183, 2100, 2024, 788, 2029], "orig_top_k_doc_id": [6942, 6939, 6941, 6940, 3943, 5557, 3183, 7085, 6042, 3184, 2717, 2100, 2024, 788, 2029]}, {"qid": 4411, "question": "Do they compare against Reinforment-Learning approaches? in Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "answer": ["Yes", "Yes"], "top_k_doc_id": [5557, 6939, 6940, 6941, 6942, 7085, 5556, 2024, 1336, 4035, 2435, 2847, 6652, 1337, 3183], "orig_top_k_doc_id": [6942, 6939, 6941, 6940, 5556, 2024, 5557, 1336, 4035, 7085, 2435, 2847, 6652, 1337, 3183]}]}
{"group_id": 746, "group_size": 3, "items": [{"qid": 4474, "question": "How do they bootstrap with contextual information? in GWU NLP Lab at SemEval-2019 Task 3: EmoContext: Effective Contextual Information in Models for Emotion Detection in Sentence-level in a Multigenre Corpus", "answer": ["pre-trained word embeddings need to be tuned with local context during our experiments", "No"], "top_k_doc_id": [758, 1967, 7008, 7009, 1329, 1330, 5410, 372, 6616, 851, 5739, 3445, 5406, 757, 1476], "orig_top_k_doc_id": [7008, 7009, 1967, 758, 5410, 372, 6616, 851, 1329, 5739, 3445, 1330, 5406, 757, 1476]}, {"qid": 4475, "question": "Which word embeddings do they utilize for the EmoContext task? in GWU NLP Lab at SemEval-2019 Task 3: EmoContext: Effective Contextual Information in Models for Emotion Detection in Sentence-level in a Multigenre Corpus", "answer": ["ELMo, fasttext", "word2vec, GloVe BIBREF7, fasttext BIBREF8, ELMo"], "top_k_doc_id": [758, 1967, 7008, 7009, 1329, 1330, 6093, 6176, 3543, 4278, 6558, 7661, 5175, 2968, 5611], "orig_top_k_doc_id": [7008, 7009, 758, 1967, 1330, 6093, 6176, 3543, 1329, 4278, 6558, 7661, 5175, 2968, 5611]}, {"qid": 4473, "question": "Do they treat differerent turns of conversation differently when modeling features? in GWU NLP Lab at SemEval-2019 Task 3: EmoContext: Effective Contextual Information in Models for Emotion Detection in Sentence-level in a Multigenre Corpus", "answer": ["No", "No"], "top_k_doc_id": [758, 1967, 7008, 7009, 5881, 2968, 5882, 7261, 1715, 6610, 851, 2969, 5355, 4924, 5410], "orig_top_k_doc_id": [7008, 7009, 5881, 2968, 5882, 7261, 1967, 1715, 6610, 851, 2969, 758, 5355, 4924, 5410]}]}
{"group_id": 747, "group_size": 3, "items": [{"qid": 4616, "question": "Which ontologies did they use? in Generating Texts with Integer Linear Programming", "answer": ["We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc. ", "Wine Ontology, Consumer Electronics Ontology, Disease Ontology"], "top_k_doc_id": [7176, 7206, 7723, 7194, 7722, 7205, 7712, 7195, 2956, 7713, 2335, 2342, 7198, 1357, 1035], "orig_top_k_doc_id": [7194, 7205, 7712, 7195, 2956, 7176, 7713, 2335, 7206, 2342, 7723, 7198, 7722, 1357, 1035]}, {"qid": 4965, "question": "what ontologies did they use? in Extracting Linguistic Resources from the Web for Concept-to-Text Generation", "answer": ["Wine Ontology, m-piro ontology, Disease Ontology", "Wine Ontology, m-piro ontology, Disease Ontology"], "top_k_doc_id": [7176, 7206, 7723, 7194, 7722, 7700, 7725, 7718, 7711, 4867, 6184, 4156, 5719, 7216, 853], "orig_top_k_doc_id": [7700, 7725, 7206, 7718, 7723, 7194, 7711, 4867, 6184, 4156, 7722, 5719, 7216, 7176, 853]}, {"qid": 2555, "question": "How many domains of ontologies do they gather data from? in Competency Questions and SPARQL-OWL Queries Dataset and Analysis", "answer": ["5 domains: software, stuff, african wildlife, healthcare, datatypes"], "top_k_doc_id": [7176, 7206, 7723, 4461, 4462, 7180, 7179, 1217, 5985, 7195, 7700, 6947, 7711, 1225, 4785], "orig_top_k_doc_id": [4461, 4462, 7723, 7180, 7176, 7179, 1217, 5985, 7195, 7700, 6947, 7711, 1225, 7206, 4785]}]}
{"group_id": 748, "group_size": 3, "items": [{"qid": 4681, "question": "what do they mean by description length? in Verb Pattern: A Probabilistic Semantic Representation on Verbs", "answer": ["the code length of phrases.", "Minimum description length (MDL) as the basic framework to reconcile the two contradicting objectives: generality and specificity."], "top_k_doc_id": [3062, 3121, 4338, 4340, 7302, 7304, 7305, 7306, 3122, 4339, 7303, 5709, 4337, 1429, 6457], "orig_top_k_doc_id": [7302, 7305, 7303, 4338, 7306, 7304, 3122, 5709, 4337, 3062, 3121, 1429, 4339, 4340, 6457]}, {"qid": 4683, "question": "what evaluation metrics are used? in Verb Pattern: A Probabilistic Semantic Representation on Verbs", "answer": ["coverage and precision", "INLINEFORM0 , INLINEFORM1 "], "top_k_doc_id": [3062, 3121, 4338, 4340, 7302, 7304, 7305, 7306, 3122, 4339, 7303, 1958, 780, 4341, 5943], "orig_top_k_doc_id": [7302, 7304, 7305, 4338, 3121, 3122, 7306, 7303, 3062, 4340, 1958, 780, 4341, 5943, 4339]}, {"qid": 4682, "question": "do they focus on english verbs? in Verb Pattern: A Probabilistic Semantic Representation on Verbs", "answer": ["Yes", "Yes"], "top_k_doc_id": [3062, 3121, 4338, 4340, 7302, 7304, 7305, 7306, 4404, 4344, 1429, 5703, 2154, 4343, 1557], "orig_top_k_doc_id": [7302, 7305, 7304, 4404, 4344, 4340, 1429, 5703, 7306, 4338, 2154, 4343, 3121, 3062, 1557]}]}
{"group_id": 749, "group_size": 3, "items": [{"qid": 4706, "question": "How significant are the improvements over previous approaches? in Rethinking Attribute Representation and Injection for Sentiment Classification", "answer": ["with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively", "Increase of 2.4%, 1.3%, and 1.6% accuracy on IMDB, Yelp 2013, and Yelp 2014"], "top_k_doc_id": [7333, 7334, 7335, 7336, 7337, 7338, 7518, 7519, 7520, 7789, 1005, 7751, 5303, 7418, 6017], "orig_top_k_doc_id": [7336, 7337, 7333, 7338, 7334, 7335, 7519, 7520, 7518, 7789, 1005, 7751, 5303, 7418, 6017]}, {"qid": 4707, "question": "Which other tasks are evaluated? in Rethinking Attribute Representation and Injection for Sentiment Classification", "answer": ["product category classification and review headline generation", "Product Category Classification, Review Headline Generation"], "top_k_doc_id": [7333, 7334, 7335, 7336, 7337, 7338, 7518, 7519, 7520, 5354, 5417, 5174, 5978, 6966, 2307], "orig_top_k_doc_id": [7338, 7337, 7336, 7333, 7334, 7335, 7519, 7520, 5354, 7518, 5417, 5174, 5978, 6966, 2307]}, {"qid": 4708, "question": "What are the performances associated to different attribute placing? in Rethinking Attribute Representation and Injection for Sentiment Classification", "answer": ["Best accuracy is for proposed CHIM methods (~56% IMDB, ~68.5 YELP datasets), most common bias attention (~53%IMDB, ~65%YELP), and oll others are worse than proposed method.", "Sentiment classification (datasets IMDB, Yelp 2013, Yelp 2014): \nembedding 56.4% accuracy, 1.161 RMSE, 67.8% accuracy, 0.646 RMSE, 69.2% accuracy, 0.629 RMSE;\nencoder 55.9% accuracy, 1.234 RMSE, 67.0% accuracy, 0.659 RMSE, 68.4% accuracy, 0.631 RMSE;\nattention 54.4% accuracy, 1.219 RMSE, 66.5% accuracy, 0.664 RMSE, 68.5% accuracy, 0.634 RMSE;\nclassifier 55.5% accuracy, 1.219 RMSE, 67.5% accuracy, 0.641 RMSE, 68.9% accuracy, 0.622 RMSE.\n\nProduct category classification and review headline generation:\nembedding 62.26 \u00b1 0.22% accuracy, 42.71 perplexity;\nencoder 64.62 \u00b1 0.34% accuracy, 42.65 perplexity;\nattention 60.95 \u00b1 0.15% accuracy, 42.78 perplexity;\nclassifier 61.83 \u00b1 0.43% accuracy, 42.69 perplexity."], "top_k_doc_id": [7333, 7334, 7335, 7336, 7337, 7338, 3134, 5174, 3135, 1005, 3133, 339, 6632, 131, 6966], "orig_top_k_doc_id": [7338, 7337, 7336, 7334, 7333, 3134, 5174, 7335, 3135, 1005, 3133, 339, 6632, 131, 6966]}]}
{"group_id": 750, "group_size": 3, "items": [{"qid": 4730, "question": "What deep learning models do they plan to use? in Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!", "answer": ["CNNs in combination with LSTMs, create word embeddings from domain specific materials, Tree\u2013Structured LSTMs", "CNNs in combination with LSTMs , Tree\u2013Structured LSTMs"], "top_k_doc_id": [7059, 7223, 7361, 7481, 5906, 6477, 7222, 5810, 495, 6082, 490, 145, 772, 7854, 494], "orig_top_k_doc_id": [7361, 7059, 7481, 7222, 7223, 5810, 495, 6082, 5906, 490, 145, 6477, 772, 7854, 494]}, {"qid": 4731, "question": "What baseline, if any, is used? in Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!", "answer": ["No", "No"], "top_k_doc_id": [7059, 7223, 7361, 7481, 5906, 6477, 7222, 5272, 4440, 2085, 7225, 2935, 5908, 6732, 2389], "orig_top_k_doc_id": [7223, 5272, 7481, 4440, 2085, 7059, 7222, 6477, 7225, 5906, 7361, 2935, 5908, 6732, 2389]}, {"qid": 4733, "question": "What type of language models are used? e.g. trigrams, bigrams? in Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!", "answer": ["bigrams and trigrams as features, KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique", "bigrams , trigrams "], "top_k_doc_id": [7059, 7223, 7361, 7481, 6732, 6733, 6595, 2618, 2772, 2619, 5272, 2773, 2774, 2783, 6882], "orig_top_k_doc_id": [7361, 6732, 6733, 6595, 2618, 2772, 2619, 7481, 7223, 5272, 2773, 2774, 7059, 2783, 6882]}]}
{"group_id": 751, "group_size": 3, "items": [{"qid": 4755, "question": "Which baselines were they used for evaluation? in A Corpus for Multilingual Document Classification in Eight Languages", "answer": ["aggregation of multilingual word embeddings, multilingual sentence representations", "we use MultiCCA word embeddings published by BIBREF3, joint multilingual sentence representations"], "top_k_doc_id": [3010, 3141, 2786, 3296, 3618, 4029, 7408, 7409, 7410, 3007, 2796, 6032, 6486, 2795, 3008], "orig_top_k_doc_id": [7410, 7408, 7409, 3010, 3007, 2786, 3296, 2796, 6032, 6486, 4029, 2795, 3618, 3008, 3141]}, {"qid": 4756, "question": "What is the difference in size compare to the previous model? in A Corpus for Multilingual Document Classification in Eight Languages", "answer": ["larger", "No"], "top_k_doc_id": [3010, 3141, 2786, 3296, 3618, 4029, 7408, 7409, 7410, 6208, 534, 6031, 7339, 3621, 2284], "orig_top_k_doc_id": [7410, 7408, 7409, 3010, 3296, 3618, 6208, 534, 6031, 7339, 3141, 3621, 2786, 2284, 4029]}, {"qid": 2278, "question": "is the dataset balanced across the four languages? in ALL-IN-1: Short Text Classification with One Model for All Languages", "answer": ["No"], "top_k_doc_id": [3010, 3141, 5572, 6032, 3007, 3438, 6033, 3437, 5709, 2796, 3008, 2954, 3603, 6031, 5699], "orig_top_k_doc_id": [5572, 6032, 3007, 3010, 3438, 6033, 3437, 5709, 2796, 3008, 2954, 3603, 6031, 3141, 5699]}]}
{"group_id": 752, "group_size": 3, "items": [{"qid": 4760, "question": "What set topics are looked at? in Topic-Specific Sentiment Analysis Can Help Identify Political Ideology", "answer": ["We obtained 50 topics using LDA", "debate topics such as healthcare, military programs, administration processes, worker safety, energy projects, gun control."], "top_k_doc_id": [2404, 6357, 6358, 7416, 7418, 447, 449, 4739, 6005, 4743, 7528, 2533, 2077, 7743, 5091], "orig_top_k_doc_id": [7416, 7418, 4743, 447, 7528, 6357, 6358, 2533, 2077, 449, 4739, 7743, 2404, 5091, 6005]}, {"qid": 4761, "question": "What were the baselines? in Topic-Specific Sentiment Analysis Can Help Identify Political Ideology", "answer": ["We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.", "GloVe-d2v"], "top_k_doc_id": [2404, 6357, 6358, 7416, 7418, 447, 449, 4739, 6005, 3332, 3337, 5979, 1758, 6006, 7746], "orig_top_k_doc_id": [7416, 7418, 6005, 447, 6358, 2404, 449, 3332, 6357, 4739, 3337, 5979, 1758, 6006, 7746]}, {"qid": 4762, "question": "Which widely used dataset did the authors use? in Topic-Specific Sentiment Analysis Can Help Identify Political Ideology", "answer": ["Convote dataset BIBREF3", "Convote dataset BIBREF3"], "top_k_doc_id": [2404, 6357, 6358, 7416, 7418, 597, 5091, 599, 309, 3332, 3730, 7414, 5879, 6640, 762], "orig_top_k_doc_id": [7416, 7418, 6358, 597, 5091, 599, 309, 3332, 2404, 3730, 7414, 6357, 5879, 6640, 762]}]}
{"group_id": 753, "group_size": 3, "items": [{"qid": 4769, "question": "Which data-selection algorithms do they use? in Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation", "answer": ["Infrequent N-gram Recovery (INR), Feature Decay Algorithms (FDA)", "Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA)"], "top_k_doc_id": [1165, 5456, 7266, 7429, 7430, 7431, 7432, 5026, 6619, 6656, 1555, 661, 7270, 7269, 7660], "orig_top_k_doc_id": [7429, 7432, 7266, 6656, 7431, 7430, 1165, 1555, 6619, 661, 5026, 7270, 5456, 7269, 7660]}, {"qid": 4771, "question": "What domain is their test set? in Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation", "answer": ["biomedical, News", "WMT 2017 biomedical translation, WMT 2015 News Translation"], "top_k_doc_id": [1165, 5456, 7266, 7429, 7430, 7431, 7432, 5026, 6619, 6656, 7268, 6266, 6618, 6267, 5458], "orig_top_k_doc_id": [7429, 7266, 7430, 7431, 6619, 7432, 6656, 1165, 5456, 5026, 7268, 6266, 6618, 6267, 5458]}, {"qid": 4770, "question": "How are the artificial sentences generated? in Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation", "answer": ["they can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model", "generating sentences in the source language by translating monolingual sentences in the target language"], "top_k_doc_id": [1165, 5456, 7266, 7429, 7430, 7431, 7432, 5163, 3174, 5459, 3173, 4753, 5455, 5458, 5030], "orig_top_k_doc_id": [7429, 7432, 7430, 7431, 7266, 5456, 5163, 3174, 5459, 1165, 3173, 4753, 5455, 5458, 5030]}]}
{"group_id": 754, "group_size": 3, "items": [{"qid": 4793, "question": "What is the training objective in the method introduced in this paper? in Forget Me Not: Reducing Catastrophic Forgetting for Domain Adaptation in Reading Comprehension", "answer": ["we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data. ", "elastic weight consolidation, L2, cosine distance"], "top_k_doc_id": [1107, 1109, 1110, 7459, 2268, 7266, 7268, 7463, 7464, 6508, 6509, 2013, 2149, 6511, 4572], "orig_top_k_doc_id": [7459, 7464, 7463, 7268, 1110, 2268, 1107, 7266, 6508, 1109, 6509, 2013, 2149, 6511, 4572]}, {"qid": 4794, "question": "Does regularization of the fine-tuning process hurt performance in the target domain? in Forget Me Not: Reducing Catastrophic Forgetting for Domain Adaptation in Reading Comprehension", "answer": ["No", "No"], "top_k_doc_id": [1107, 1109, 1110, 7459, 2268, 7266, 7268, 7463, 7464, 7462, 7269, 7460, 4281, 7270, 1166], "orig_top_k_doc_id": [7459, 7464, 7463, 7268, 1110, 7266, 2268, 7462, 1107, 7269, 1109, 7460, 4281, 7270, 1166]}, {"qid": 861, "question": "Among various transfer learning techniques, which technique yields to the best performance? in Neural Domain Adaptation for Biomedical Question Answering", "answer": ["No"], "top_k_doc_id": [1107, 1109, 1110, 7459, 3839, 1106, 5026, 5133, 3840, 4928, 2012, 2468, 6863, 3743, 5327], "orig_top_k_doc_id": [3839, 1106, 1107, 1109, 1110, 7459, 5026, 5133, 3840, 4928, 2012, 2468, 6863, 3743, 5327]}]}
{"group_id": 755, "group_size": 3, "items": [{"qid": 4832, "question": "what elements of each profile did they use? in Digital Stylometry: Linking Profiles Across Social Networks", "answer": ["No profile elements", "time and the linguistic content of posts by the users"], "top_k_doc_id": [1949, 7522, 7523, 7526, 7527, 522, 1481, 2343, 1482, 1483, 5463, 1486, 1480, 7016, 5128], "orig_top_k_doc_id": [7522, 7527, 7523, 7526, 1482, 1481, 2343, 1483, 5463, 1486, 1480, 1949, 522, 7016, 5128]}, {"qid": 4834, "question": "How is the gold standard defined? in Digital Stylometry: Linking Profiles Across Social Networks", "answer": ["We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth, We discarded all users who did not link to an account for both Twitter and Facebook", "We used a third party social media site (i.e., Google Plus)"], "top_k_doc_id": [1949, 7522, 7523, 7526, 7527, 522, 1481, 2343, 514, 4436, 4859, 5909, 4348, 608, 6226], "orig_top_k_doc_id": [7522, 7527, 7526, 7523, 1481, 1949, 2343, 514, 4436, 4859, 5909, 4348, 608, 6226, 522]}, {"qid": 4833, "question": "Does this paper discuss the potential these techniques have for invading user privacy? in Digital Stylometry: Linking Profiles Across Social Networks", "answer": ["Yes", "Yes"], "top_k_doc_id": [1949, 7522, 7523, 7526, 7527, 6155, 1948, 6621, 7016, 6711, 7260, 514, 4348, 7018, 5906], "orig_top_k_doc_id": [7522, 7526, 7527, 7523, 1949, 6155, 1948, 6621, 7016, 6711, 7260, 514, 4348, 7018, 5906]}]}
{"group_id": 756, "group_size": 3, "items": [{"qid": 4898, "question": "Which knowledge destilation methods do they introduce? in Sequence-Level Knowledge Distillation", "answer": ["standard knowledge distillation for NMT ", "Word-Level Knowledge Distillation, Sequence-Level Knowledge Distillation, Sequence-Level Interpolation"], "top_k_doc_id": [1234, 7600, 7601, 7602, 7603, 2149, 4624, 4817, 5477, 7604, 7630, 2148, 3328, 1235, 4625], "orig_top_k_doc_id": [7604, 7603, 4624, 7602, 7601, 2149, 7600, 7630, 2148, 5477, 3328, 1234, 4817, 1235, 4625]}, {"qid": 4900, "question": "Which dataset do they train on? in Sequence-Level Knowledge Distillation", "answer": ["WMT 2014, IWSLT 2015", "IWSLT 2015,  WMT 2014"], "top_k_doc_id": [1234, 7600, 7601, 7602, 7603, 2149, 4624, 4817, 5477, 7604, 1456, 1457, 3234, 4626, 5478], "orig_top_k_doc_id": [7601, 7604, 7603, 7602, 1456, 1457, 4624, 3234, 1234, 4817, 7600, 4626, 5477, 2149, 5478]}, {"qid": 4901, "question": "Do they reason why greedy decoding works better then beam search? in Sequence-Level Knowledge Distillation", "answer": ["Yes", "No"], "top_k_doc_id": [1234, 7600, 7601, 7602, 7603, 2024, 6668, 1235, 1456, 1457, 2028, 5283, 1522, 3092, 1621], "orig_top_k_doc_id": [7603, 7602, 2024, 7600, 7601, 6668, 1235, 1456, 1457, 2028, 5283, 1522, 3092, 1234, 1621]}]}
{"group_id": 757, "group_size": 3, "items": [{"qid": 4956, "question": "How do they determine demographics on an image? in Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets", "answer": ["using model driven face detection, apparent age annotation and gender annotation", " a model-driven demographic annotation pipeline for apparent age and gender, analysis of said annotation models and the presentation of annotations for each image in the training set of the ILSVRC 2012 subset of ImageNet"], "top_k_doc_id": [1743, 3007, 3008, 3011, 7082, 7083, 7690, 7691, 523, 3084, 2899, 520, 2900, 4745, 5246], "orig_top_k_doc_id": [7690, 7691, 7083, 7082, 3011, 2899, 520, 3007, 3084, 3008, 2900, 523, 4745, 1743, 5246]}, {"qid": 4958, "question": "What is the most underrepresented person group in ILSVRC? in Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets", "answer": ["people over the age of 60", "Females and males with age 75+"], "top_k_doc_id": [1743, 3007, 3008, 3011, 7082, 7083, 7690, 7691, 523, 3084, 1518, 4268, 5907, 3634, 3588], "orig_top_k_doc_id": [7690, 7691, 3007, 3011, 1518, 3008, 4268, 523, 7083, 3084, 5907, 1743, 3634, 7082, 3588]}, {"qid": 4957, "question": "Do they assume binary gender? in Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets", "answer": ["No", "No"], "top_k_doc_id": [1743, 3007, 3008, 3011, 7082, 7083, 7690, 7691, 521, 3988, 2899, 5246, 3411, 525, 222], "orig_top_k_doc_id": [7690, 7691, 3011, 3007, 3008, 521, 7082, 3988, 2899, 5246, 1743, 7083, 3411, 525, 222]}]}
{"group_id": 758, "group_size": 3, "items": [{"qid": 4959, "question": "How long did the training take? in Enhanced Neural Machine Translation by Learning from Draft", "answer": ["No", "No"], "top_k_doc_id": [7693, 7695, 7696, 628, 2136, 4298, 7342, 2190, 2705, 3820, 7059, 6751, 2761, 6595, 3455], "orig_top_k_doc_id": [2136, 7693, 2190, 628, 7695, 2705, 3820, 7696, 4298, 7059, 6751, 2761, 7342, 6595, 3455]}, {"qid": 4961, "question": "Do they compare to state-of-the-art models? in Enhanced Neural Machine Translation by Learning from Draft", "answer": ["Yes", "Yes"], "top_k_doc_id": [7693, 7695, 7696, 628, 2136, 4298, 7342, 7694, 4984, 438, 5656, 4316, 6943, 3655, 3825], "orig_top_k_doc_id": [7693, 7695, 2136, 628, 7342, 7694, 4984, 4298, 438, 5656, 4316, 6943, 3655, 3825, 7696]}, {"qid": 4960, "question": "Is the proposed model smaller or bigger than the conventional NMT system? in Enhanced Neural Machine Translation by Learning from Draft", "answer": ["No", "No"], "top_k_doc_id": [7693, 7695, 7696, 7694, 3655, 3920, 1165, 3687, 5839, 4822, 4212, 3917, 4387, 3918, 7600], "orig_top_k_doc_id": [7695, 7696, 7693, 7694, 3655, 3920, 1165, 3687, 5839, 4822, 4212, 3917, 4387, 3918, 7600]}]}
{"group_id": 759, "group_size": 3, "items": [{"qid": 5038, "question": "What baselines do they compare to? in Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation", "answer": ["a encoder-decoder architecture with attention incorporating LSTMs and transformers", "A neural encoder-decoder architecture with attention using LSTMs or Transformers"], "top_k_doc_id": [1040, 4813, 6943, 6944, 7847, 7848, 7849, 7850, 7853, 4409, 4561, 367, 1044, 1449, 1953], "orig_top_k_doc_id": [7847, 7849, 7850, 6943, 7853, 1040, 6944, 4813, 4561, 4409, 7848, 367, 1044, 1449, 1953]}, {"qid": 5039, "question": "What training set sizes do they use? in Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation", "answer": ["219,777, 114,243, 89,413, over 5M ", "89k, 114k, 291k, 5M"], "top_k_doc_id": [1040, 4813, 6943, 6944, 7847, 7848, 7849, 7850, 7853, 4409, 1860, 6016, 1450, 799, 3165], "orig_top_k_doc_id": [7847, 7849, 7850, 7853, 4813, 6944, 6943, 7848, 1860, 6016, 1450, 4409, 1040, 799, 3165]}, {"qid": 5040, "question": "What languages do they experiment with? in Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation", "answer": ["German, English, French, Czech, Basque", "German-English, English-French, Czech-English, Basque-English pairs"], "top_k_doc_id": [1040, 4813, 6943, 6944, 7847, 7848, 7849, 7850, 7853, 6190, 1053, 7339, 4615, 4712, 5835], "orig_top_k_doc_id": [7847, 7849, 6943, 7850, 6944, 7853, 1040, 7848, 6190, 1053, 4813, 7339, 4615, 4712, 5835]}]}
{"group_id": 760, "group_size": 2, "items": [{"qid": 313, "question": "Can SCRF be used to pretrain the model? in Multitask Learning with CTC and Segmental CRF for Speech Recognition", "answer": ["No"], "top_k_doc_id": [380, 438, 3834, 4972, 5987, 373, 376, 375, 374, 1577, 381, 2528, 1593, 2995, 4922], "orig_top_k_doc_id": [373, 376, 375, 374, 1577, 381, 5987, 2528, 3834, 1593, 438, 380, 2995, 4972, 4922]}, {"qid": 1738, "question": "Which acoustic units are more suited to model the French language? in End-to-End Speech Recognition: A review for the French Language", "answer": ["No"], "top_k_doc_id": [380, 438, 3834, 4972, 5987, 2484, 2486, 7244, 5053, 2488, 5564, 3406, 2451, 2487, 78], "orig_top_k_doc_id": [2484, 2486, 7244, 5053, 2488, 4972, 5564, 3406, 2451, 438, 380, 2487, 3834, 5987, 78]}]}
{"group_id": 761, "group_size": 2, "items": [{"qid": 333, "question": "Which publicly available NLU dataset is used? in Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU", "answer": ["ROMULUS dataset, NLU-Benchmark dataset"], "top_k_doc_id": [101, 325, 400, 401, 402, 403, 404, 405, 898, 3094, 3868, 4663, 5015, 5246, 98], "orig_top_k_doc_id": [400, 405, 404, 402, 403, 101, 401, 898, 3868, 5015, 5246, 4663, 325, 98, 3094]}, {"qid": 334, "question": "What metrics other than entity tagging are compared? in Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU", "answer": ["We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities"], "top_k_doc_id": [101, 325, 400, 401, 402, 403, 404, 405, 898, 3094, 3868, 4663, 6036, 3562, 4414], "orig_top_k_doc_id": [400, 404, 405, 403, 402, 101, 401, 3868, 4663, 3094, 898, 325, 6036, 3562, 4414]}]}
{"group_id": 762, "group_size": 2, "items": [{"qid": 427, "question": "What is their model? in Open Named Entity Modeling from Embedding Distribution", "answer": ["cross-lingual NE recognition"], "top_k_doc_id": [497, 499, 500, 501, 930, 3845, 5189, 5186, 3938, 6498, 4841, 2755, 4353, 7673, 498], "orig_top_k_doc_id": [501, 497, 5189, 930, 5186, 3938, 500, 3845, 499, 6498, 4841, 2755, 4353, 7673, 498]}, {"qid": 428, "question": "Do they evaluate on NER data sets? in Open Named Entity Modeling from Embedding Distribution", "answer": ["Yes"], "top_k_doc_id": [497, 499, 500, 501, 930, 3845, 3430, 1422, 3642, 1098, 65, 4858, 4946, 7457, 1100], "orig_top_k_doc_id": [501, 497, 3430, 930, 1422, 3642, 499, 1098, 65, 3845, 4858, 500, 4946, 7457, 1100]}]}
{"group_id": 763, "group_size": 2, "items": [{"qid": 463, "question": "What experiments authors perform? in Natural Language State Representation for Reinforcement Learning", "answer": ["a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios"], "top_k_doc_id": [215, 537, 2868, 4545, 1322, 2922, 6588, 1683, 3426, 4522, 3531, 5683, 273, 425, 214], "orig_top_k_doc_id": [215, 1322, 4545, 2922, 537, 6588, 1683, 3426, 4522, 3531, 5683, 2868, 273, 425, 214]}, {"qid": 464, "question": "How is state to learn and complete tasks represented via natural language? in Natural Language State Representation for Reinforcement Learning", "answer": [" represent the state using natural language"], "top_k_doc_id": [215, 537, 2868, 4545, 2867, 5540, 7839, 1637, 6635, 1674, 7166, 538, 3805, 4716, 5554], "orig_top_k_doc_id": [537, 2868, 2867, 5540, 7839, 1637, 6635, 4545, 1674, 7166, 538, 3805, 215, 4716, 5554]}]}
{"group_id": 764, "group_size": 2, "items": [{"qid": 472, "question": "How well did the system do? in Bringing Stories Alive: Generating Interactive Fiction Worlds", "answer": ["the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all"], "top_k_doc_id": [558, 559, 1082, 1293, 1630, 2868, 4976, 6068, 3669, 1081, 4656, 3790, 7402, 5667, 539], "orig_top_k_doc_id": [558, 559, 6068, 1630, 2868, 4976, 1082, 3669, 1081, 4656, 1293, 3790, 7402, 5667, 539]}, {"qid": 473, "question": "How is the information extracted? in Bringing Stories Alive: Generating Interactive Fiction Worlds", "answer": ["neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique"], "top_k_doc_id": [558, 559, 1082, 1293, 1630, 2868, 4976, 6068, 1058, 3752, 7037, 4659, 7446, 3798, 2867], "orig_top_k_doc_id": [558, 559, 1630, 6068, 1058, 3752, 2868, 7037, 4976, 4659, 1293, 7446, 3798, 1082, 2867]}]}
{"group_id": 765, "group_size": 2, "items": [{"qid": 575, "question": "What evaluation metrics are used? in Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation", "answer": ["the evaluation metrics include BLEU and ROUGE (1, 2, L) scores"], "top_k_doc_id": [2065, 5492, 5495, 5497, 705, 4937, 707, 898, 3807, 491, 107, 6472, 4793, 820, 6067], "orig_top_k_doc_id": [705, 5495, 4937, 5497, 707, 898, 3807, 491, 107, 5492, 6472, 4793, 820, 2065, 6067]}, {"qid": 3277, "question": "Which qualitative metric are used for evaluation? in A Hierarchical Model for Data-to-Text Generation", "answer": [" Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)", "Relation Generation (RG), Content Selection (CS), Content Ordering (CO)"], "top_k_doc_id": [2065, 5492, 5495, 5497, 5494, 2007, 7446, 1138, 7336, 5506, 5496, 6689, 3805, 683, 1806], "orig_top_k_doc_id": [5497, 5495, 5494, 2007, 7446, 1138, 7336, 5506, 5496, 6689, 5492, 3805, 683, 2065, 1806]}]}
{"group_id": 766, "group_size": 2, "items": [{"qid": 694, "question": "How do they obtain region descriptions and object annotations? in Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining", "answer": ["they are available in the Visual Genome dataset"], "top_k_doc_id": [869, 2413, 2414, 3798, 4033, 4267, 4268, 867, 871, 870, 2900, 789, 790, 4428, 82], "orig_top_k_doc_id": [867, 869, 871, 870, 3798, 2413, 4268, 2900, 789, 2414, 4033, 790, 4428, 82, 4267]}, {"qid": 2507, "question": "Does the new system utilize pre-extracted bounding boxes and/or features? in Task-driven Visual Saliency and Attention-based Visual Question Answering", "answer": ["Yes"], "top_k_doc_id": [869, 2413, 2414, 3798, 4033, 4267, 4268, 2378, 7163, 7164, 7147, 3799, 2377, 7146, 2380], "orig_top_k_doc_id": [4267, 3798, 2378, 4033, 7163, 7164, 7147, 4268, 3799, 869, 2413, 2377, 2414, 7146, 2380]}]}
{"group_id": 767, "group_size": 2, "items": [{"qid": 819, "question": "How is correctness of automatic derivation proved? in Automatic Differentiation in ROOT", "answer": ["empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)"], "top_k_doc_id": [1020, 1021, 1022, 1023, 5412, 325, 7725, 6361, 6203, 1417, 6063, 2960, 3669, 2002, 7721], "orig_top_k_doc_id": [1023, 5412, 1021, 1022, 1020, 325, 7725, 6361, 6203, 1417, 6063, 2960, 3669, 2002, 7721]}, {"qid": 820, "question": "Is this AD implementation used in any deep learning framework? in Automatic Differentiation in ROOT", "answer": ["No"], "top_k_doc_id": [1020, 1021, 1022, 1023, 5412, 815, 5380, 2993, 7399, 1036, 695, 3692, 2987, 2864, 6196], "orig_top_k_doc_id": [1021, 1020, 1023, 1022, 815, 5412, 5380, 2993, 7399, 1036, 695, 3692, 2987, 2864, 6196]}]}
{"group_id": 768, "group_size": 2, "items": [{"qid": 872, "question": "What are the sizes of both datasets? in Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction", "answer": ["The Dutch section consists of 2,333,816 sentences and 53,487,257 words., The SONAR500 corpus consists of more than 500 million words obtained from different domains."], "top_k_doc_id": [1124, 1125, 1126, 1127, 1128, 2048, 535, 536, 3895, 1218, 3894, 5976, 6852, 5977, 1219], "orig_top_k_doc_id": [1124, 1127, 1128, 1126, 1125, 535, 536, 3895, 1218, 3894, 5976, 6852, 5977, 2048, 1219]}, {"qid": 4717, "question": "Do they use multitask learning? in One Model to Learn Both: Zero Pronoun Prediction and Translation", "answer": ["Yes", "Yes"], "top_k_doc_id": [1124, 1125, 1126, 1127, 1128, 2048, 7346, 6042, 6041, 6057, 4561, 5485, 7347, 5773, 6044], "orig_top_k_doc_id": [1124, 7346, 1126, 1127, 1125, 1128, 6042, 6041, 6057, 4561, 5485, 7347, 2048, 5773, 6044]}]}
{"group_id": 769, "group_size": 2, "items": [{"qid": 918, "question": "How does Frege's holistic and functional approach to meaning relates to general distributional hypothesis? in Semantic Holism and Word Representations in Artificial Neural Networks", "answer": ["interpretation of Frege's work are examples of holistic approaches to meaning"], "top_k_doc_id": [537, 1181, 1182, 1183, 1184, 4184, 4508, 6251, 6442, 2103, 5071, 5701, 538, 5513, 2447], "orig_top_k_doc_id": [1182, 1184, 1183, 1181, 4184, 4508, 537, 2103, 6251, 5071, 5701, 538, 6442, 5513, 2447]}, {"qid": 919, "question": "What does Frege's holistic and functional approach to meaning states? in Semantic Holism and Word Representations in Artificial Neural Networks", "answer": ["Only in the context of a sentence does a word have a meaning."], "top_k_doc_id": [537, 1181, 1182, 1183, 1184, 4184, 4508, 6251, 6442, 2733, 4931, 2132, 3912, 6016, 6345], "orig_top_k_doc_id": [1181, 1182, 1184, 1183, 4508, 2733, 6442, 4931, 2132, 3912, 6016, 4184, 6251, 537, 6345]}]}
{"group_id": 770, "group_size": 2, "items": [{"qid": 979, "question": "What factors contribute to interpretive biases according to this research? in Bias in Semantic and Discourse Interpretation", "answer": ["Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march"], "top_k_doc_id": [1270, 1271, 1272, 1273, 1274, 1275, 1277, 1283, 5525, 1798, 1279, 5524, 5537, 3893, 3979], "orig_top_k_doc_id": [1273, 1274, 1277, 1272, 1275, 1270, 1283, 1271, 5525, 1798, 1279, 5524, 5537, 3893, 3979]}, {"qid": 980, "question": "Which interpretative biases are analyzed in this paper? in Bias in Semantic and Discourse Interpretation", "answer": ["in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury"], "top_k_doc_id": [1270, 1271, 1272, 1273, 1274, 1275, 1277, 1283, 5525, 1355, 3118, 1280, 1799, 3953, 718], "orig_top_k_doc_id": [1273, 1271, 1270, 1274, 1277, 1275, 1272, 1355, 3118, 5525, 1280, 1283, 1799, 3953, 718]}]}
{"group_id": 771, "group_size": 2, "items": [{"qid": 1069, "question": "Do they impose any grammatical constraints over the generated output? in Exploration on Generating Traditional Chinese Medicine Prescriptions from Symptoms with an End-to-End Approach", "answer": ["No"], "top_k_doc_id": [21, 1413, 1414, 1415, 1416, 1418, 7832, 7833, 3023, 1186, 3686, 1417, 1832, 3938, 6042], "orig_top_k_doc_id": [1413, 1416, 1414, 1418, 7833, 3023, 21, 1415, 1186, 3686, 1417, 7832, 1832, 3938, 6042]}, {"qid": 1070, "question": "Why did they think this was a good idea? in Exploration on Generating Traditional Chinese Medicine Prescriptions from Symptoms with an End-to-End Approach", "answer": ["They think it will help human TCM practitioners make prescriptions."], "top_k_doc_id": [21, 1413, 1414, 1415, 1416, 1418, 7832, 7833, 7775, 7836, 490, 4205, 4441, 6212, 2915], "orig_top_k_doc_id": [1413, 1416, 1414, 7833, 1418, 7832, 7775, 1415, 7836, 21, 490, 4205, 4441, 6212, 2915]}]}
{"group_id": 772, "group_size": 2, "items": [{"qid": 1174, "question": "What evaluation metrics did they look at? in Generaci\\'on autom\\'atica de frases literarias en espa\\~nol", "answer": ["accuracy with standard deviation"], "top_k_doc_id": [890, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 3692, 7583, 2742, 3724, 2743, 1463, 4906], "orig_top_k_doc_id": [1572, 1576, 1570, 1573, 1571, 1575, 1574, 7583, 2742, 3724, 3692, 2743, 1463, 4906, 890]}, {"qid": 1175, "question": "What datasets are used? in Generaci\\'on autom\\'atica de frases literarias en espa\\~nol", "answer": ["Corpus 5KL, Corpus 8KF"], "top_k_doc_id": [890, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 3692, 7583, 889, 7582, 7269, 6239, 7580], "orig_top_k_doc_id": [1572, 1576, 1570, 1573, 1571, 1575, 1574, 890, 7583, 889, 7582, 7269, 6239, 7580, 3692]}]}
{"group_id": 773, "group_size": 2, "items": [{"qid": 1227, "question": "Is the filter based feature selection (FSE) a form of regularization? in A Robust Hybrid Approach for Textual Document Classification", "answer": ["No"], "top_k_doc_id": [1664, 1665, 1667, 1668, 2978, 2979, 2980, 4811, 6397, 1861, 1860, 1308, 6258, 2403, 5824], "orig_top_k_doc_id": [1664, 1665, 4811, 1668, 2978, 2979, 6397, 2980, 1861, 1667, 1860, 1308, 6258, 2403, 5824]}, {"qid": 1979, "question": "How better does the hybrid tiled CNN model perform than its counterparts? in Hybrid Tiled Convolutional Neural Networks for Text Sentiment Classification", "answer": ["No"], "top_k_doc_id": [1664, 1665, 1667, 1668, 2978, 2979, 2980, 2981, 76, 75, 6867, 4209, 462, 3091, 7472], "orig_top_k_doc_id": [2979, 2978, 2980, 2981, 1665, 76, 75, 6867, 1668, 1667, 4209, 462, 3091, 1664, 7472]}]}
{"group_id": 774, "group_size": 2, "items": [{"qid": 1295, "question": "How big is dataset used for fine-tuning model for detection of red flag medical symptoms in individual statements? in Language Transfer for Early Warning of Epidemics from Social Media", "answer": ["a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh)"], "top_k_doc_id": [245, 1777, 2827, 3574, 5650, 7522, 4282, 394, 3944, 5739, 5646, 60, 5291, 522, 5651], "orig_top_k_doc_id": [2827, 1777, 4282, 245, 394, 3944, 3574, 5739, 5650, 5646, 60, 5291, 522, 5651, 7522]}, {"qid": 1296, "question": "Is there any explanation why some choice of language pair is better than the other? in Language Transfer for Early Warning of Epidemics from Social Media", "answer": ["translations that were reasonable but not consistent with the labels"], "top_k_doc_id": [245, 1777, 2827, 3574, 5650, 7522, 1480, 4780, 4111, 4280, 4279, 6498, 3300, 5474, 3393], "orig_top_k_doc_id": [7522, 1480, 4780, 4111, 4280, 1777, 4279, 6498, 3300, 3574, 5474, 245, 5650, 2827, 3393]}]}
{"group_id": 775, "group_size": 2, "items": [{"qid": 1362, "question": "What are the languages they consider in this paper? in Towards Language Agnostic Universal Representations", "answer": ["The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French"], "top_k_doc_id": [1872, 1873, 1875, 2917, 6448, 6166, 247, 861, 783, 6853, 6280, 6852, 6881, 3439, 4186], "orig_top_k_doc_id": [1875, 1872, 1873, 6166, 247, 861, 2917, 783, 6853, 6280, 6852, 6881, 3439, 6448, 4186]}, {"qid": 1363, "question": "Did they experiment with tasks other than word problems in math? in Towards Language Agnostic Universal Representations", "answer": ["They experimented with sentiment analysis and natural language inference task"], "top_k_doc_id": [1872, 1873, 1875, 2917, 6448, 425, 3100, 424, 427, 2248, 7168, 2247, 4603, 1874, 3099], "orig_top_k_doc_id": [1872, 425, 3100, 424, 427, 1875, 2248, 6448, 7168, 2247, 1873, 4603, 2917, 1874, 3099]}]}
{"group_id": 776, "group_size": 2, "items": [{"qid": 1431, "question": "Do they use pretrained models? in Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach", "answer": ["Yes"], "top_k_doc_id": [687, 1978, 1979, 1980, 1982, 2886, 4571, 4993, 5868, 5871, 2885, 4415, 3140, 6135, 1981], "orig_top_k_doc_id": [1979, 1982, 1978, 4993, 2886, 1980, 687, 5868, 2885, 4571, 4415, 3140, 6135, 5871, 1981]}, {"qid": 1432, "question": "What are their baseline models? in Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach", "answer": ["Majority, ESA, Word2Vec , Binary-BERT"], "top_k_doc_id": [687, 1978, 1979, 1980, 1982, 2886, 4571, 4993, 5868, 5871, 686, 6032, 784, 5573, 5624], "orig_top_k_doc_id": [1982, 1979, 1978, 1980, 5868, 687, 686, 4993, 6032, 5871, 784, 2886, 5573, 4571, 5624]}]}
{"group_id": 777, "group_size": 2, "items": [{"qid": 1508, "question": "What modalities are being used in different datasets? in Multi-attention Recurrent Network for Human Communication Comprehension", "answer": ["Language, Vision, Acoustic"], "top_k_doc_id": [2116, 2117, 2121, 2120, 2118, 491, 5257, 4877, 575, 312, 7138, 2927, 5966, 2752, 3416], "orig_top_k_doc_id": [2116, 2121, 2117, 2120, 2118, 491, 5257, 4877, 575, 312, 7138, 2927, 5966, 2752, 3416]}, {"qid": 1509, "question": "What is the difference between Long-short Term Hybrid Memory and LSTMs? in Multi-attention Recurrent Network for Human Communication Comprehension", "answer": ["Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) "], "top_k_doc_id": [2116, 2117, 2121, 3358, 3825, 4387, 2607, 756, 4149, 6585, 1512, 7365, 6082, 6256, 1665], "orig_top_k_doc_id": [2116, 2117, 2121, 3358, 3825, 4387, 2607, 756, 4149, 6585, 1512, 7365, 6082, 6256, 1665]}]}
{"group_id": 778, "group_size": 2, "items": [{"qid": 1579, "question": "Why did they choose WER as evaluation metric? in Question Generation by Transformers", "answer": ["WER can reflect our model's effectiveness in generating questions that are similar to those of SQuAD, WER can be used for initial analyses"], "top_k_doc_id": [491, 492, 2207, 1138, 6135, 4993, 2208, 3359, 5920, 2209, 6312, 424, 6313, 2915, 2261], "orig_top_k_doc_id": [2207, 1138, 6135, 4993, 2208, 3359, 5920, 2209, 6312, 424, 6313, 2915, 2261, 492, 491]}, {"qid": 2832, "question": "What evaluation criteria and metrics were used to evaluate the generated text? in Template-free Data-to-Text Generation of Finnish Sports News", "answer": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "top_k_doc_id": [491, 492, 4958, 4963, 1866, 4961, 7285, 1135, 7289, 7286, 4962, 4794, 4938, 4744, 5186], "orig_top_k_doc_id": [4958, 4963, 1866, 4961, 7285, 1135, 491, 7289, 7286, 4962, 4794, 492, 4938, 4744, 5186]}]}
{"group_id": 779, "group_size": 2, "items": [{"qid": 1609, "question": "Is this auto translation tool based on neural networks? in Human Languages in Source Code: Auto-Translation for Localized Instruction", "answer": ["Yes"], "top_k_doc_id": [2270, 2271, 2274, 2275, 7346, 1886, 2998, 4309, 7827, 4569, 6291, 7340, 1885, 2187, 3000], "orig_top_k_doc_id": [2270, 2275, 2274, 2271, 7346, 1886, 2998, 4309, 7827, 4569, 6291, 7340, 1885, 2187, 3000]}, {"qid": 1610, "question": "What are results of public code repository study? in Human Languages in Source Code: Auto-Translation for Localized Instruction", "answer": ["Non-English code is a large-scale phenomena., Transliteration is common in identifiers for all languages., Languages clusters into three distinct groups based on how speakers use identifiers/comments/transliteration., Non-latin script users write comments in their L1 script but write identifiers in English., Right-to-left (RTL) language scripts, such as Arabic, have no observed prevalence on GitHub identifiers"], "top_k_doc_id": [2270, 2271, 2274, 2275, 4753, 4754, 3641, 2272, 2273, 243, 6443, 7548, 274, 4415, 273], "orig_top_k_doc_id": [2270, 2274, 2275, 2271, 4753, 4754, 3641, 2272, 2273, 243, 6443, 7548, 274, 4415, 273]}]}
{"group_id": 780, "group_size": 2, "items": [{"qid": 1669, "question": "How big are FigureQA and DVQA datasets? in Data Interpretation over Plots", "answer": ["No"], "top_k_doc_id": [2376, 2377, 2379, 2380, 2381, 7857, 3528, 5905, 6107, 3889, 2677, 4772, 7233, 3379, 3394], "orig_top_k_doc_id": [2377, 2376, 2381, 2380, 2379, 7857, 3528, 5905, 6107, 3889, 2677, 4772, 7233, 3379, 3394]}, {"qid": 1670, "question": "What models other than SAN-VOES are trained on new PlotQA dataset? in Data Interpretation over Plots", "answer": ["IMG-only, QUES-only, SAN, SANDY,  VOES-Oracle, VOES"], "top_k_doc_id": [2376, 2377, 2379, 2380, 2381, 4521, 3810, 3416, 3420, 7147, 4520, 4704, 3031, 3419, 3029], "orig_top_k_doc_id": [2380, 2381, 2379, 2376, 2377, 4521, 3810, 3416, 3420, 7147, 4520, 4704, 3031, 3419, 3029]}]}
{"group_id": 781, "group_size": 2, "items": [{"qid": 1691, "question": "What is different in BERT-gen from standard BERT? in BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations", "answer": ["They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens."], "top_k_doc_id": [2149, 2413, 2414, 2415, 2417, 2418, 6870, 2416, 1560, 7283, 5561, 5559, 6871, 4699, 7473], "orig_top_k_doc_id": [2418, 2413, 2415, 2414, 2416, 2417, 6870, 1560, 2149, 7283, 5561, 5559, 6871, 4699, 7473]}, {"qid": 1692, "question": "How are multimodal representations combined? in BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations", "answer": ["The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards."], "top_k_doc_id": [2149, 2413, 2414, 2415, 2417, 2418, 6870, 7140, 7138, 7139, 7143, 7141, 2904, 2900, 6863], "orig_top_k_doc_id": [2418, 2413, 7140, 2415, 7138, 2414, 7139, 7143, 7141, 2904, 2417, 6870, 2149, 2900, 6863]}]}
{"group_id": 782, "group_size": 2, "items": [{"qid": 1724, "question": "How much better performance of proposed model compared to answer-selection models? in Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks", "answer": ["significant MAP performance improvement compared to the previous best model, CompClip-LM (0.696 to 0.734 absolute)"], "top_k_doc_id": [2461, 2462, 2463, 3487, 2370, 1361, 4273, 510, 4278, 511, 7677, 7800, 2910, 1641, 4914], "orig_top_k_doc_id": [2461, 2462, 2463, 3487, 2370, 1361, 4273, 510, 4278, 511, 7677, 7800, 2910, 1641, 4914]}, {"qid": 1725, "question": "How are some nodes initially connected based on text structure? in Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks", "answer": ["we fully connect nodes that represent sentences from the same passage, we fully connect nodes that represent the first sentence of each passage, we add an edge between the question and every node for each passage"], "top_k_doc_id": [2461, 2462, 2463, 7761, 3932, 1760, 4216, 6818, 1839, 6011, 2097, 3875, 443, 5898, 3162], "orig_top_k_doc_id": [2461, 2462, 2463, 7761, 3932, 1760, 4216, 6818, 1839, 6011, 2097, 3875, 443, 5898, 3162]}]}
{"group_id": 783, "group_size": 2, "items": [{"qid": 1732, "question": "Which dataset do they use to learn embeddings? in Towards an Unsupervised Entrainment Distance in Conversational Speech using Deep Neural Networks", "answer": ["Fisher Corpus English Part 1"], "top_k_doc_id": [1175, 2473, 2474, 2475, 2476, 3837, 3648, 3835, 3838, 6894, 7472, 6770, 3699, 6314, 3834], "orig_top_k_doc_id": [2473, 2476, 2474, 2475, 1175, 3837, 3648, 3835, 3838, 6894, 7472, 6770, 3699, 6314, 3834]}, {"qid": 1733, "question": "How do they correlate NED with emotional bond levels? in Towards an Unsupervised Entrainment Distance in Conversational Speech using Deep Neural Networks", "answer": ["They compute Pearson\u2019s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating"], "top_k_doc_id": [1175, 2473, 2474, 2475, 2476, 226, 6723, 6590, 1365, 7299, 1368, 1712, 6771, 5646, 589], "orig_top_k_doc_id": [2476, 2473, 2475, 2474, 226, 6723, 6590, 1365, 7299, 1368, 1175, 1712, 6771, 5646, 589]}]}
{"group_id": 784, "group_size": 2, "items": [{"qid": 1737, "question": "What will be in focus for future work? in End-to-End Speech Recognition: A review for the French Language", "answer": ["1) investigate errors produced by the end-to-end methods and explore several approaches to correct common errors done in French, 2) compare the end-to-end methods in a SLU context and evaluate the semantic value of the partially correct produced words"], "top_k_doc_id": [2450, 5391, 5564, 5566, 5765, 6110, 6310, 7244, 19, 1835, 7138, 3340, 1790, 3582, 2490], "orig_top_k_doc_id": [5564, 19, 1835, 6310, 7244, 6110, 7138, 3340, 2450, 1790, 5566, 5765, 3582, 5391, 2490]}, {"qid": 1739, "question": "What are the existing end-to-end ASR approaches for the French language? in End-to-End Speech Recognition: A review for the French Language", "answer": ["1) Connectionist Temporal Classification (CTC), 2) Attention-based methods, 3) RNN-tranducer"], "top_k_doc_id": [2450, 5391, 5564, 5566, 5765, 6110, 6310, 7244, 2484, 1987, 4918, 4972, 6314, 2451, 380], "orig_top_k_doc_id": [5564, 2484, 5566, 6310, 6110, 5765, 1987, 4918, 7244, 4972, 5391, 6314, 2450, 2451, 380]}]}
{"group_id": 785, "group_size": 2, "items": [{"qid": 1740, "question": "How much is decoding speed increased by increasing encoder and decreasing decoder depth? in Analyzing Word Translation of Transformer Layers", "answer": ["the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer"], "top_k_doc_id": [2493, 2495, 3781, 3782, 3783, 2494, 2491, 7246, 2492, 7247, 888, 4214, 4215, 890, 6500], "orig_top_k_doc_id": [2493, 2494, 2495, 3781, 2491, 3783, 7246, 2492, 3782, 7247, 888, 4214, 4215, 890, 6500]}, {"qid": 2361, "question": "Is the proposed layer smaller in parameters than a Transformer? in Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention", "answer": ["No"], "top_k_doc_id": [2493, 2495, 3781, 3782, 3783, 3556, 3027, 7649, 4542, 7646, 2739, 6367, 5287, 5284, 6664], "orig_top_k_doc_id": [3781, 3783, 3782, 3556, 3027, 7649, 2495, 4542, 2493, 7646, 2739, 6367, 5287, 5284, 6664]}]}
{"group_id": 786, "group_size": 2, "items": [{"qid": 1776, "question": "What is grounded language understanding? in Systematic Generalization: What Is Required and Can It Be Learned?", "answer": ["No"], "top_k_doc_id": [491, 2578, 2733, 2584, 686, 2585, 2705, 1187, 115, 7039, 2149, 558, 490, 4273, 6870], "orig_top_k_doc_id": [2584, 2578, 686, 2585, 2705, 1187, 115, 2733, 7039, 2149, 558, 490, 491, 4273, 6870]}, {"qid": 1867, "question": "What benchmark datasets they use? in Learning to Compose Neural Networks for Question Answering", "answer": ["VQA and GeoQA"], "top_k_doc_id": [491, 2578, 2733, 791, 1217, 1155, 2752, 1357, 6135, 1560, 1154, 1547, 2893, 2900, 1329], "orig_top_k_doc_id": [791, 1217, 1155, 2752, 1357, 2578, 6135, 2733, 491, 1560, 1154, 1547, 2893, 2900, 1329]}]}
{"group_id": 787, "group_size": 2, "items": [{"qid": 1859, "question": "How much does this system outperform prior work? in Cooperative Learning of Disjoint Syntax and Semantics", "answer": ["The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM"], "top_k_doc_id": [2701, 2704, 2705, 4234, 6348, 3489, 3490, 4338, 502, 4321, 3374, 4230, 2702, 575, 7441], "orig_top_k_doc_id": [3489, 3490, 4338, 2705, 4234, 502, 6348, 2704, 4321, 3374, 2701, 4230, 2702, 575, 7441]}, {"qid": 1860, "question": "What are the baseline systems that are compared against? in Cooperative Learning of Disjoint Syntax and Semantics", "answer": ["The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM"], "top_k_doc_id": [2701, 2704, 2705, 4234, 6348, 6184, 2917, 6332, 4204, 4450, 1389, 6584, 863, 7168, 1222], "orig_top_k_doc_id": [4234, 6184, 2701, 2917, 6332, 4204, 4450, 1389, 2704, 6584, 2705, 863, 6348, 7168, 1222]}]}
{"group_id": 788, "group_size": 2, "items": [{"qid": 1876, "question": "What other solutions do they compare to? in Smarnet: Teaching Machines to Read and Comprehend Like Human", "answer": [" strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard"], "top_k_doc_id": [1370, 2755, 2756, 2758, 2759, 5367, 5900, 6256, 3100, 7097, 5006, 7098, 7572, 1804, 6823], "orig_top_k_doc_id": [2755, 2759, 3100, 2758, 5900, 5367, 6256, 7097, 5006, 7098, 2756, 7572, 1370, 1804, 6823]}, {"qid": 1877, "question": "How does the gatint mechanism combine word and character information? in Smarnet: Teaching Machines to Read and Comprehend Like Human", "answer": ["when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place,  for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure"], "top_k_doc_id": [1370, 2755, 2756, 2758, 2759, 5367, 5900, 6256, 5257, 7541, 1822, 7373, 289, 6879, 1393], "orig_top_k_doc_id": [2755, 2759, 1370, 2756, 2758, 5900, 5257, 7541, 1822, 5367, 7373, 289, 6879, 6256, 1393]}]}
{"group_id": 789, "group_size": 2, "items": [{"qid": 1887, "question": "what evaluation methods are discussed? in Automatic Language Identification in Texts: A Survey", "answer": ["document-level accuracy, precision, recall, F-score"], "top_k_doc_id": [236, 2618, 2768, 2795, 2796, 5201, 7597, 6833, 5562, 1067, 2767, 7160, 4002, 5138, 3332], "orig_top_k_doc_id": [2618, 5201, 2768, 6833, 5562, 1067, 236, 2796, 7597, 2767, 7160, 4002, 2795, 5138, 3332]}, {"qid": 1888, "question": "what are the off-the-shelf systems discussed in the paper? in Automatic Language Identification in Texts: A Survey", "answer": ["Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier."], "top_k_doc_id": [236, 2618, 2768, 2795, 2796, 5201, 7597, 2791, 2790, 4780, 2769, 3244, 1262, 404, 5144], "orig_top_k_doc_id": [2791, 2768, 2790, 2796, 2618, 4780, 2769, 3244, 5201, 1262, 404, 2795, 7597, 5144, 236]}]}
{"group_id": 790, "group_size": 2, "items": [{"qid": 2018, "question": "Which embeddings do they detect biases in? in Fair is Better than Sensational:Man is to Doctor as Woman is to Doctor", "answer": ["Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset"], "top_k_doc_id": [3354, 3355, 6558, 3050, 3049, 3048, 41, 7375, 6700, 42, 6702, 7267, 3941, 6737, 2496], "orig_top_k_doc_id": [3050, 3049, 3048, 41, 3354, 3355, 7375, 6700, 42, 6702, 7267, 6558, 3941, 6737, 2496]}, {"qid": 2185, "question": "Do they propose any solution to debias the embeddings? in Measuring Social Bias in Knowledge Graph Embeddings", "answer": ["No"], "top_k_doc_id": [3354, 3355, 6558, 3356, 6735, 1800, 6962, 5155, 5977, 5976, 6268, 6816, 6963, 1801, 4278], "orig_top_k_doc_id": [3356, 6735, 3354, 1800, 6962, 5155, 6558, 5977, 3355, 5976, 6268, 6816, 6963, 1801, 4278]}]}
{"group_id": 791, "group_size": 2, "items": [{"qid": 2036, "question": "What is proof that proposed functional form approximates well generalization error in practice? in A Constructive Prediction of the Generalization Error Across Scales", "answer": ["estimated test accuracy is highly correlated with actual test accuracy for various datasets, appropriateness of the proposed function for modeling the complex error landscape"], "top_k_doc_id": [1216, 1217, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 4526, 6441, 254, 6137, 2581, 2582], "orig_top_k_doc_id": [3080, 3081, 3083, 3084, 3085, 3086, 3082, 254, 6137, 1216, 1217, 6441, 4526, 2581, 2582]}, {"qid": 2037, "question": "How is proposed functional form constructed for some model? in A Constructive Prediction of the Generalization Error Across Scales", "answer": ["No"], "top_k_doc_id": [1216, 1217, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 4526, 6441, 1389, 3230, 4191, 121], "orig_top_k_doc_id": [3080, 3081, 3084, 3085, 3083, 3086, 3082, 1216, 6441, 1217, 1389, 3230, 4526, 4191, 121]}]}
{"group_id": 792, "group_size": 2, "items": [{"qid": 2091, "question": "Which machine learning models do they use to correct run-on sentences? in How do you correct run-on sentences it's not as easy as it seems", "answer": ["conditional random field model, Seq2Seq attention model"], "top_k_doc_id": [715, 823, 3919, 4550, 358, 3174, 3173, 1478, 1777, 4152, 4901, 3972, 503, 7059, 377], "orig_top_k_doc_id": [3919, 358, 3174, 3173, 715, 1478, 1777, 4152, 4550, 4901, 3972, 503, 823, 7059, 377]}, {"qid": 2092, "question": "How large is the dataset they generate? in How do you correct run-on sentences it's not as easy as it seems", "answer": ["4.756 million sentences"], "top_k_doc_id": [715, 823, 3919, 4550, 4577, 824, 6083, 3532, 7775, 5562, 1136, 5385, 2086, 3370, 4510], "orig_top_k_doc_id": [823, 715, 4577, 824, 6083, 3532, 7775, 5562, 1136, 5385, 4550, 2086, 3919, 3370, 4510]}]}
{"group_id": 793, "group_size": 2, "items": [{"qid": 2168, "question": "How often are the newspaper websites crawled daily? in The Logoscope: a Semi-Automatic Tool for Detecting and Documenting French New Words", "answer": ["RSS feeds in French on a daily basis"], "top_k_doc_id": [1481, 2157, 3327, 3287, 6418, 1000, 748, 3288, 1876, 7097, 6110, 2827, 2104, 413, 6181], "orig_top_k_doc_id": [3327, 3287, 6418, 1000, 748, 3288, 1876, 7097, 2157, 6110, 2827, 2104, 413, 1481, 6181]}, {"qid": 2305, "question": "Why there is only user study to evaluate the model? in A Semi-automatic Method for Efficient Detection of Stories on Social Media", "answer": ["No"], "top_k_doc_id": [1481, 2157, 3671, 3669, 6155, 803, 6623, 5272, 452, 4279, 3574, 4112, 7047, 4948, 5927], "orig_top_k_doc_id": [3671, 3669, 1481, 6155, 803, 6623, 5272, 452, 4279, 2157, 3574, 4112, 7047, 4948, 5927]}]}
{"group_id": 794, "group_size": 2, "items": [{"qid": 2196, "question": "Do they compare speed performance of their model compared to the ones using the LID model? in Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition", "answer": ["No"], "top_k_doc_id": [381, 1739, 2484, 2487, 2488, 3023, 3024, 3405, 3406, 3407, 2292, 2296, 6262, 2486, 2293], "orig_top_k_doc_id": [3406, 3405, 3407, 3023, 381, 2292, 2296, 3024, 6262, 2486, 2488, 2293, 2484, 2487, 1739]}, {"qid": 2197, "question": "How do they obtain language identities? in Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition", "answer": ["model is trained to predict language IDs as well as the subwords, we add language IDs in the CS point of transcriptio"], "top_k_doc_id": [381, 1739, 2484, 2487, 2488, 3023, 3024, 3405, 3406, 3407, 3938, 973, 4863, 2490, 1742], "orig_top_k_doc_id": [3023, 3405, 3406, 3024, 3407, 3938, 2484, 2488, 381, 1739, 973, 2487, 4863, 2490, 1742]}]}
{"group_id": 795, "group_size": 2, "items": [{"qid": 2200, "question": "Do they compare computational time of AM-softmax versus Softmax? in Additive Margin SincNet for Speaker Recognition", "answer": ["No"], "top_k_doc_id": [3422, 3423, 3424, 3425, 3655, 3976, 6124, 6365, 7375, 7795, 6262, 2118, 1786, 3999, 7273], "orig_top_k_doc_id": [3423, 3422, 3424, 3425, 6124, 3976, 7375, 6365, 6262, 3655, 7795, 2118, 1786, 3999, 7273]}, {"qid": 2201, "question": "Do they visualize the difference between AM-Softmax and regular softmax? in Additive Margin SincNet for Speaker Recognition", "answer": ["Yes"], "top_k_doc_id": [3422, 3423, 3424, 3425, 3655, 3976, 6124, 6365, 7375, 7795, 3977, 482, 4001, 6503, 3656], "orig_top_k_doc_id": [3423, 3422, 3424, 3425, 6124, 3977, 6365, 3976, 482, 7375, 4001, 6503, 7795, 3655, 3656]}]}
{"group_id": 796, "group_size": 2, "items": [{"qid": 2202, "question": "what metrics were used for evaluation? in Bidirectional Long-Short Term Memory for Video Description", "answer": ["METEOR"], "top_k_doc_id": [2922, 3426, 3427, 3428, 3429, 5102, 6275, 4976, 7264, 3705, 2449, 283, 2346, 282, 4373], "orig_top_k_doc_id": [3426, 3429, 3428, 2922, 5102, 3427, 4976, 7264, 6275, 3705, 2449, 283, 2346, 282, 4373]}, {"qid": 2203, "question": "what are the state of the art methods? in Bidirectional Long-Short Term Memory for Video Description", "answer": ["S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al."], "top_k_doc_id": [2922, 3426, 3427, 3428, 3429, 5102, 6275, 4267, 6162, 6123, 930, 5213, 7251, 6122, 2121], "orig_top_k_doc_id": [3426, 3429, 2922, 5102, 3428, 3427, 4267, 6275, 6162, 6123, 930, 5213, 7251, 6122, 2121]}]}
{"group_id": 797, "group_size": 2, "items": [{"qid": 2211, "question": "Does DCA or GMM-based attention perform better in experiments? in Location-Relative Attention Mechanisms For Robust Long-Form Speech Synthesis", "answer": ["About the same performance"], "top_k_doc_id": [3441, 3442, 3443, 3444, 1653, 2484, 653, 3138, 2485, 2486, 2488, 5938, 891, 5941, 2487], "orig_top_k_doc_id": [3444, 3443, 3441, 3442, 1653, 2484, 653, 3138, 2485, 2486, 2488, 5938, 891, 5941, 2487]}, {"qid": 2212, "question": "How they compare varioius mechanisms in terms of naturalness? in Location-Relative Attention Mechanisms For Robust Long-Form Speech Synthesis", "answer": ["using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters"], "top_k_doc_id": [3441, 3442, 3443, 3444, 2531, 4450, 2040, 4404, 510, 4470, 5428, 6720, 4447, 5270, 3267], "orig_top_k_doc_id": [3441, 3444, 3443, 3442, 2531, 4450, 2040, 4404, 510, 4470, 5428, 6720, 4447, 5270, 3267]}]}
{"group_id": 798, "group_size": 2, "items": [{"qid": 2255, "question": "How do they incorporate human advice? in Learning Relational Dependency Networks for Relation Extraction", "answer": ["by converting human advice to first-order logic format and use as an input to calculate gradient"], "top_k_doc_id": [3162, 3538, 3539, 3540, 3541, 4216, 7055, 7250, 1533, 4278, 3161, 2917, 4218, 1603, 3163], "orig_top_k_doc_id": [3541, 3538, 3539, 3540, 4216, 3162, 7055, 1533, 4278, 3161, 2917, 4218, 1603, 3163, 7250]}, {"qid": 2256, "question": "What do they learn jointly? in Learning Relational Dependency Networks for Relation Extraction", "answer": ["relations"], "top_k_doc_id": [3162, 3538, 3539, 3540, 3541, 4216, 7055, 7250, 7606, 3633, 5485, 3634, 2022, 2023, 7605], "orig_top_k_doc_id": [4216, 3540, 3541, 3539, 3162, 7055, 3538, 7250, 7606, 3633, 5485, 3634, 2022, 2023, 7605]}]}
{"group_id": 799, "group_size": 2, "items": [{"qid": 2263, "question": "How big is performance improvement proposed methods are used? in Improving Robustness of Task Oriented Dialog Systems", "answer": ["Data augmentation (es)  improved Adv es by 20% comparing to baseline \nData augmentation (cs) improved Adv cs by 16.5% comparing to baseline\nData augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline \nAll models show improvements over adversarial sets  \n"], "top_k_doc_id": [3562, 3566, 3567, 6589, 7839, 7840, 4124, 3193, 1940, 4128, 898, 1473, 1471, 5917, 1475], "orig_top_k_doc_id": [7839, 7840, 4124, 3562, 3193, 3566, 1940, 4128, 898, 3567, 1473, 6589, 1471, 5917, 1475]}, {"qid": 2264, "question": "How authors create adversarial test set to measure model robustness? in Improving Robustness of Task Oriented Dialog Systems", "answer": ["we devise a test set consisting of \u2018adversarial\u2019 examples, i.e, perturbed examples that can potentially change the base model's prediction. , We use two approaches described in literature: back-translation and noisy sequence autoencoder."], "top_k_doc_id": [3562, 3566, 3567, 6589, 7839, 7840, 4206, 4205, 4204, 5351, 3563, 5664, 1893, 1898, 1064], "orig_top_k_doc_id": [3562, 3567, 3566, 4206, 4205, 7839, 7840, 4204, 5351, 3563, 5664, 1893, 6589, 1898, 1064]}]}
{"group_id": 800, "group_size": 2, "items": [{"qid": 2279, "question": "what evaluation metrics were used? in ALL-IN-1: Short Text Classification with One Model for All Languages", "answer": ["weighted F1-score"], "top_k_doc_id": [3603, 7746, 7752, 7264, 6400, 6788, 6398, 219, 6401, 3010, 2796, 1692, 3604, 6861, 3335], "orig_top_k_doc_id": [7746, 7264, 3603, 6400, 6788, 6398, 219, 6401, 7752, 3010, 2796, 1692, 3604, 6861, 3335]}, {"qid": 2280, "question": "what dataset was used? in ALL-IN-1: Short Text Classification with One Model for All Languages", "answer": ["The dataset from a joint ADAPT-Microsoft project"], "top_k_doc_id": [3603, 7746, 7752, 3637, 7024, 2954, 3712, 2083, 3438, 7100, 1432, 2795, 7622, 5171, 5813], "orig_top_k_doc_id": [7746, 3637, 7024, 2954, 3712, 2083, 3438, 7100, 7752, 3603, 1432, 2795, 7622, 5171, 5813]}]}
{"group_id": 801, "group_size": 2, "items": [{"qid": 2288, "question": "How is quality of annotation measured? in GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception", "answer": ["Annotators went through various phases to make sure their annotations did not deviate from the mean."], "top_k_doc_id": [3622, 3623, 3624, 3627, 7488, 3626, 3625, 5, 1001, 4, 8, 1494, 5547, 1499, 7487], "orig_top_k_doc_id": [3622, 3627, 3626, 3623, 3624, 3625, 5, 1001, 4, 7488, 8, 1494, 5547, 1499, 7487]}, {"qid": 2476, "question": "Which news organisations are the headlines sourced from? in Principles for Developing a Knowledge Graph of Interlinked Events from News Headlines on Twitter", "answer": ["BBC and CNN "], "top_k_doc_id": [3622, 3623, 3624, 3627, 7488, 4154, 4159, 4155, 4800, 5889, 4158, 7481, 1863, 1865, 6716], "orig_top_k_doc_id": [4154, 4159, 4155, 3623, 4800, 3624, 5889, 4158, 3622, 3627, 7481, 1863, 1865, 7488, 6716]}]}
{"group_id": 802, "group_size": 2, "items": [{"qid": 2289, "question": "On what data is the model evaluated? in LinkNBed: Multi-Graph Representation Learning with Entity Linkage", "answer": ["D-IMDB (derived from large scale IMDB data snapshot), D-FB (derived from large scale Freebase data snapshot)"], "top_k_doc_id": [4318, 4320, 4321, 3633, 3632, 3628, 3631, 3634, 3629, 3630, 6084, 1155, 6085, 6113, 236], "orig_top_k_doc_id": [3633, 3632, 3628, 3631, 3634, 3629, 3630, 6084, 1155, 6085, 4321, 6113, 4318, 236, 4320]}, {"qid": 2525, "question": "How many users/clicks does their search engine have? in Context-aware Deep Model for Entity Recommendation in Search Engine at Alibaba", "answer": ["No"], "top_k_doc_id": [4318, 4320, 4321, 4317, 4967, 6088, 5800, 130, 302, 6980, 303, 6635, 2986, 4424, 3483], "orig_top_k_doc_id": [4317, 4320, 4318, 4321, 4967, 6088, 5800, 130, 302, 6980, 303, 6635, 2986, 4424, 3483]}]}
{"group_id": 803, "group_size": 2, "items": [{"qid": 2295, "question": "Which are the sequence model architectures this method can be transferred across? in Simplify the Usage of Lexicon in Chinese NER", "answer": ["The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models"], "top_k_doc_id": [436, 2466, 3642, 3643, 3646, 3647, 4749, 4974, 1781, 165, 4584, 4583, 6028, 4859, 6029], "orig_top_k_doc_id": [3642, 3647, 3646, 3643, 2466, 436, 4974, 1781, 165, 4584, 4583, 6028, 4859, 6029, 4749]}, {"qid": 2296, "question": " What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods? in Simplify the Usage of Lexicon in Chinese NER", "answer": ["Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)"], "top_k_doc_id": [436, 2466, 3642, 3643, 3646, 3647, 4749, 2679, 1033, 1782, 7818, 3406, 6609, 4750, 3505], "orig_top_k_doc_id": [3647, 3642, 3646, 2679, 1033, 1782, 7818, 3643, 3406, 4749, 6609, 436, 4750, 2466, 3505]}]}
{"group_id": 804, "group_size": 2, "items": [{"qid": 2343, "question": "Could you tell me more about the metrics used for performance evaluation? in Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "answer": ["BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy"], "top_k_doc_id": [20, 398, 3743, 5678, 7285, 2189, 4442, 5573, 266, 7223, 7224, 3810, 7157, 135, 6355], "orig_top_k_doc_id": [3743, 7285, 20, 2189, 4442, 398, 5573, 266, 7223, 7224, 5678, 3810, 7157, 135, 6355]}, {"qid": 2344, "question": "which tasks are used in BLUE benchmark? in Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "answer": ["Inference task\nThe aim of the inference task is to predict whether the premise sentence entails or contradicts the hypothesis sentence, Document multilabel classification\nThe multilabel classification task predicts multiple labels from the texts., Relation extraction\nThe aim of the relation extraction task is to predict relations and their types between the two entities mentioned in the sentences., Named entity recognition\nThe aim of the named entity recognition task is to predict mention spans given in the text , Sentence similarity\nThe sentence similarity task is to predict similarity scores based on sentence pairs"], "top_k_doc_id": [20, 398, 3743, 5678, 7285, 2886, 4412, 6367, 4529, 6368, 5133, 2885, 6135, 873, 1144], "orig_top_k_doc_id": [3743, 7285, 5678, 2886, 398, 4412, 6367, 20, 4529, 6368, 5133, 2885, 6135, 873, 1144]}]}
{"group_id": 805, "group_size": 2, "items": [{"qid": 2459, "question": "Can the model add new relations to the knowledge graph, or just new entities? in Open-World Knowledge Graph Completion", "answer": ["The model does not add new relations to the knowledge graph."], "top_k_doc_id": [342, 562, 1353, 3533, 4098, 4104, 4490, 4720, 6002, 337, 4160, 5212, 6587, 4978, 466], "orig_top_k_doc_id": [4490, 4098, 342, 337, 4104, 4720, 3533, 4160, 1353, 5212, 6587, 562, 4978, 6002, 466]}, {"qid": 2562, "question": "what was the evaluation metrics studied in this work? in An Open-World Extension to Knowledge Graph Completion Models", "answer": ["mean rank (MR), mean reciprocal rank (MRR), as well as Hits@1, Hits@3, and Hits@10"], "top_k_doc_id": [342, 562, 1353, 3533, 4098, 4104, 4490, 4720, 6002, 4493, 4492, 4103, 4719, 1822, 1605], "orig_top_k_doc_id": [4490, 4098, 4720, 562, 1353, 3533, 4493, 4492, 4103, 4719, 6002, 1822, 4104, 1605, 342]}]}
{"group_id": 806, "group_size": 2, "items": [{"qid": 2479, "question": "Does their solution involve connecting images and text? in SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy", "answer": ["Yes"], "top_k_doc_id": [276, 4166, 4167, 4168, 4169, 4170, 6686, 7146, 283, 7800, 6688, 279, 282, 4428, 3737], "orig_top_k_doc_id": [4166, 4170, 4168, 4169, 4167, 283, 276, 7146, 7800, 6688, 279, 282, 6686, 4428, 3737]}, {"qid": 2480, "question": "Which model do they use to generate key messages? in SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy", "answer": ["ontology-based knowledge tree, heuristics-based, n-grams model"], "top_k_doc_id": [276, 4166, 4167, 4168, 4169, 4170, 6686, 7146, 6075, 5428, 2828, 6347, 3245, 6076, 3367], "orig_top_k_doc_id": [4166, 4168, 4170, 4169, 4167, 6075, 5428, 2828, 276, 6347, 3245, 7146, 6686, 6076, 3367]}]}
{"group_id": 807, "group_size": 2, "items": [{"qid": 2490, "question": "How large is the dataset? in Unsupervised Learning of Style-sensitive Word Vectors", "answer": ["30M utterances"], "top_k_doc_id": [1370, 4190, 4191, 4192, 4193, 5174, 5303, 6817, 7275, 7444, 1864, 7688, 5175, 6082, 7443], "orig_top_k_doc_id": [4190, 4191, 4193, 4192, 6817, 5174, 7275, 1370, 5303, 7444, 1864, 7688, 5175, 6082, 7443]}, {"qid": 2491, "question": "How is the dataset created? in Unsupervised Learning of Style-sensitive Word Vectors", "answer": ["We collected Japanese fictional stories from the Web"], "top_k_doc_id": [1370, 4190, 4191, 4192, 4193, 5174, 5303, 6817, 7275, 7852, 4131, 5845, 6661, 6858, 7853], "orig_top_k_doc_id": [4193, 4190, 4191, 4192, 7275, 6817, 5174, 7852, 4131, 1370, 5845, 6661, 6858, 5303, 7853]}]}
{"group_id": 808, "group_size": 2, "items": [{"qid": 2496, "question": "So this paper turns unstructured text inputs to parameters that GNNs can read? in Graph Neural Networks with Generated Parameters for Relation Extraction", "answer": ["Yes"], "top_k_doc_id": [5226, 5228, 5230, 4216, 4217, 4218, 4220, 2548, 3534, 5894, 301, 1154, 5827, 4099, 2868], "orig_top_k_doc_id": [4216, 4217, 4218, 4220, 2548, 3534, 5894, 5226, 301, 5228, 1154, 5827, 5230, 4099, 2868]}, {"qid": 3078, "question": "How they extract \"structured answer-relevant relation\"? in Improving Question Generation With to the Point Context", "answer": ["Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.", "off-the-shelf toolbox of OpenIE"], "top_k_doc_id": [5226, 5228, 5230, 5229, 5227, 559, 1170, 4463, 347, 3805, 3190, 7610, 7514, 7072, 1121], "orig_top_k_doc_id": [5226, 5229, 5227, 559, 5230, 5228, 1170, 4463, 347, 3805, 3190, 7610, 7514, 7072, 1121]}]}
{"group_id": 809, "group_size": 2, "items": [{"qid": 2527, "question": "Was any variation in results observed based on language typology? in Information-Theoretic Probing for Linguistic Structure", "answer": ["It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information."], "top_k_doc_id": [4328, 4329, 4330, 4331, 6448, 6870, 12, 3066, 14, 18, 13, 4333, 4754, 1273, 3944], "orig_top_k_doc_id": [4331, 4328, 12, 3066, 14, 18, 13, 4330, 4333, 4329, 4754, 1273, 6448, 6870, 3944]}, {"qid": 2528, "question": "Does the work explicitly study the relationship between model complexity and linguistic structure encoding? in Information-Theoretic Probing for Linguistic Structure", "answer": ["No"], "top_k_doc_id": [4328, 4329, 4330, 4331, 6448, 6870, 4447, 4470, 2494, 1560, 237, 4473, 6828, 3368, 3938], "orig_top_k_doc_id": [4328, 4331, 4329, 4447, 4470, 2494, 6448, 1560, 4330, 237, 4473, 6828, 6870, 3368, 3938]}]}
{"group_id": 810, "group_size": 2, "items": [{"qid": 2530, "question": "Does the training dataset provide logical form supervision? in A Sketch-Based System for Semantic Parsing", "answer": ["Yes"], "top_k_doc_id": [4362, 4363, 4364, 4365, 4366, 3912, 3916, 7734, 2691, 3914, 2818, 3489, 4555, 1213, 2821], "orig_top_k_doc_id": [4362, 4365, 4366, 3912, 4364, 3916, 4363, 7734, 2691, 3914, 2818, 3489, 4555, 1213, 2821]}, {"qid": 2531, "question": "What is the difference between the full test set and the hard test set? in A Sketch-Based System for Semantic Parsing", "answer": ["3000 hard samples are selected from the test set"], "top_k_doc_id": [4362, 4363, 4364, 4365, 4366, 1216, 7830, 5603, 6853, 980, 4558, 1586, 3220, 4023, 6421], "orig_top_k_doc_id": [4365, 4362, 4366, 1216, 4364, 7830, 4363, 5603, 6853, 980, 4558, 1586, 3220, 4023, 6421]}]}
{"group_id": 811, "group_size": 2, "items": [{"qid": 2541, "question": "Which regions of the United States do they consider? in English verb regularization in books and tweets", "answer": ["all regions except those that are colored black"], "top_k_doc_id": [2090, 4404, 4405, 4406, 4407, 4409, 2534, 6526, 4003, 6896, 3066, 5783, 7307, 7533, 2541], "orig_top_k_doc_id": [4405, 4406, 4404, 4407, 2534, 6526, 4003, 4409, 6896, 3066, 5783, 7307, 7533, 2541, 2090]}, {"qid": 2542, "question": "Why did they only consider six years of published books? in English verb regularization in books and tweets", "answer": ["No"], "top_k_doc_id": [2090, 4404, 4405, 4406, 4407, 4409, 124, 6451, 5907, 2226, 2093, 5362, 6110, 3125, 126], "orig_top_k_doc_id": [4405, 4404, 4406, 2090, 124, 6451, 5907, 4409, 2226, 2093, 4407, 5362, 6110, 3125, 126]}]}
{"group_id": 812, "group_size": 2, "items": [{"qid": 2554, "question": "What is the multi-instance learning? in Bootstrapping Generators from Noisy Data", "answer": ["No"], "top_k_doc_id": [6375, 6376, 6377, 6379, 4460, 4459, 4456, 4727, 2015, 6661, 4731, 3004, 7187, 5850, 6113], "orig_top_k_doc_id": [4460, 4459, 4456, 6375, 6377, 6379, 4727, 6376, 2015, 6661, 4731, 3004, 7187, 5850, 6113]}, {"qid": 3952, "question": "Did they try Roberta? in Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised Two-path Bootstrapping Approach", "answer": ["No", "No", "No"], "top_k_doc_id": [6375, 6376, 6377, 6379, 6380, 6378, 1077, 5144, 5292, 3574, 4278, 5295, 7256, 4633, 1809], "orig_top_k_doc_id": [6375, 6379, 6376, 6380, 6378, 6377, 1077, 5144, 5292, 3574, 4278, 5295, 7256, 4633, 1809]}]}
{"group_id": 813, "group_size": 2, "items": [{"qid": 2600, "question": "what are the topics pulled from Reddit? in Identifying Dogmatism in Social Media: Signals and Models", "answer": ["politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. ", "training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit"], "top_k_doc_id": [4577, 4578, 4579, 4580, 4581, 4582, 1701, 1172, 446, 5908, 3450, 4130, 441, 1174, 5907], "orig_top_k_doc_id": [4581, 4577, 4580, 4579, 4582, 4578, 1701, 1172, 446, 5908, 3450, 4130, 441, 1174, 5907]}, {"qid": 2601, "question": "What predictive model do they build? in Identifying Dogmatism in Social Media: Signals and Models", "answer": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "top_k_doc_id": [4577, 4578, 4579, 4580, 4581, 4582, 521, 6005, 2405, 6623, 7743, 4781, 4784, 804, 5191], "orig_top_k_doc_id": [4581, 4579, 4582, 4577, 4580, 4578, 521, 6005, 2405, 6623, 7743, 4781, 4784, 804, 5191]}]}
{"group_id": 814, "group_size": 2, "items": [{"qid": 2646, "question": "How was this data collected? in Data Collection for Interactive Learning through the Dialog", "answer": ["CrowdFlower", "The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English."], "top_k_doc_id": [495, 4548, 4656, 4657, 4658, 4659, 4925, 6585, 6587, 6880, 6588, 4719, 900, 7758, 4720], "orig_top_k_doc_id": [4656, 4657, 4658, 4659, 4548, 495, 6880, 6588, 4719, 4925, 6587, 900, 7758, 6585, 4720]}, {"qid": 2647, "question": "What is the average length of dialog? in Data Collection for Interactive Learning through the Dialog", "answer": ["4.49 turns", "4.5 turns per dialog (8533 turns / 1900 dialogs)"], "top_k_doc_id": [495, 4548, 4656, 4657, 4658, 4659, 4925, 6585, 6587, 6880, 7842, 899, 1632, 7843, 4442], "orig_top_k_doc_id": [4656, 4657, 4658, 4659, 4925, 6587, 7842, 6585, 899, 1632, 495, 6880, 7843, 4442, 4548]}]}
{"group_id": 815, "group_size": 2, "items": [{"qid": 2775, "question": "What are the models used for the baseline of the three NLP tasks? in A Resource for Computational Experiments on Mapudungun", "answer": ["state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer", "For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15."], "top_k_doc_id": [1430, 4861, 4862, 4863, 4864, 1782, 4341, 2154, 2886, 626, 1431, 5044, 6768, 1429, 6310], "orig_top_k_doc_id": [4862, 4861, 4864, 4863, 1430, 1782, 4341, 2154, 2886, 626, 1431, 5044, 6768, 1429, 6310]}, {"qid": 2776, "question": "How is non-standard pronunciation identified? in A Resource for Computational Experiments on Mapudungun", "answer": ["No", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "top_k_doc_id": [1430, 4861, 4862, 4863, 4864, 4617, 1286, 4615, 4616, 2633, 6, 5538, 1069, 2634, 7028], "orig_top_k_doc_id": [4862, 4863, 4864, 4861, 4617, 1286, 4615, 4616, 2633, 6, 5538, 1069, 1430, 2634, 7028]}]}
{"group_id": 816, "group_size": 2, "items": [{"qid": 2807, "question": "How many tweets were collected? in An Iterative Approach for Identifying Complaint Based Tweets in Social Media Platforms", "answer": ["$19,300$, added 2500 randomly sampled tweets", "$19,300$ tweets"], "top_k_doc_id": [770, 1758, 4916, 4917, 4948, 6131, 6140, 7029, 4136, 5470, 5085, 7773, 2402, 4895, 5783], "orig_top_k_doc_id": [4916, 4917, 770, 6140, 7029, 4136, 5470, 5085, 7773, 1758, 2402, 4895, 4948, 5783, 6131]}, {"qid": 2808, "question": "What language is explored in this paper? in An Iterative Approach for Identifying Complaint Based Tweets in Social Media Platforms", "answer": ["No", "English language"], "top_k_doc_id": [770, 1758, 4916, 4917, 4948, 6131, 3730, 4739, 520, 1839, 4322, 447, 4284, 6623, 234], "orig_top_k_doc_id": [4916, 4917, 770, 3730, 4739, 520, 1758, 1839, 4948, 4322, 447, 6131, 4284, 6623, 234]}]}
{"group_id": 817, "group_size": 2, "items": [{"qid": 2830, "question": "What word embeddings do they test? in Equation Embeddings", "answer": ["Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model", "Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings"], "top_k_doc_id": [1182, 4954, 4955, 3048, 6552, 6852, 4490, 5568, 498, 3049, 463, 930, 3208, 7763, 4969], "orig_top_k_doc_id": [3048, 4954, 6552, 1182, 6852, 4490, 5568, 498, 3049, 463, 930, 3208, 7763, 4969, 4955]}, {"qid": 2831, "question": "How do they define similar equations? in Equation Embeddings", "answer": ["By using Euclidean distance computed between the context vector representations of the equations", "Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B."], "top_k_doc_id": [1182, 4954, 4955, 4957, 6890, 4956, 3682, 3986, 3676, 258, 7395, 4714, 6551, 425, 2934], "orig_top_k_doc_id": [4955, 4954, 4957, 6890, 4956, 3682, 3986, 1182, 3676, 258, 7395, 4714, 6551, 425, 2934]}]}
{"group_id": 818, "group_size": 2, "items": [{"qid": 2867, "question": "What downstream tasks are evaluated? in Impact of Batch Size on Stopping Active Learning for Text Classification", "answer": ["No", "text classification"], "top_k_doc_id": [2372, 5024, 5025, 6140, 6143, 7220, 6697, 511, 3619, 3620, 621, 534, 6448, 2681, 2709], "orig_top_k_doc_id": [5024, 5025, 2372, 6140, 7220, 6697, 511, 3619, 6143, 3620, 621, 534, 6448, 2681, 2709]}, {"qid": 2868, "question": "What is active learning? in Impact of Batch Size on Stopping Active Learning for Text Classification", "answer": ["A process of training a model when selected unlabeled samples are annotated on each iteration.", "Active learning is a process that selectively determines which unlabeled samples for a machine learning model should be annotated."], "top_k_doc_id": [2372, 5024, 5025, 6140, 6143, 7220, 165, 166, 3794, 2590, 145, 4690, 2587, 4771, 3789], "orig_top_k_doc_id": [5024, 5025, 7220, 6143, 6140, 165, 166, 3794, 2590, 145, 4690, 2587, 2372, 4771, 3789]}]}
{"group_id": 819, "group_size": 2, "items": [{"qid": 2873, "question": "How larger are the training sets of these versions of ELMo compared to the previous ones? in High Quality ELMo Embeddings for Seven Less-Resourced Languages", "answer": ["By 14 times.", "up to 1.95 times larger"], "top_k_doc_id": [45, 50, 4695, 5048, 5049, 5050, 5051, 5341, 2099, 5344, 7009, 4607, 5757, 3420, 4562], "orig_top_k_doc_id": [5048, 5051, 5049, 5341, 50, 4695, 2099, 5344, 45, 7009, 5050, 4607, 5757, 3420, 4562]}, {"qid": 2874, "question": "What is the improvement in performance for Estonian in the NER task? in High Quality ELMo Embeddings for Seven Less-Resourced Languages", "answer": ["5 percent points.", "0.05 F1"], "top_k_doc_id": [45, 50, 4695, 5048, 5049, 5050, 5051, 2326, 6297, 7285, 51, 48, 5845, 5844, 6698], "orig_top_k_doc_id": [5048, 5051, 5049, 5050, 50, 2326, 6297, 45, 7285, 51, 4695, 48, 5845, 5844, 6698]}]}
{"group_id": 820, "group_size": 2, "items": [{"qid": 3280, "question": "What metrics are used in evaluation? in Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data", "answer": ["precision, recall , F1 score"], "top_k_doc_id": [488, 1886, 5498, 5499, 5501, 6448, 6625, 6632, 6917, 4116, 138, 6633, 135, 4786, 137], "orig_top_k_doc_id": [5501, 5499, 5498, 6632, 4116, 1886, 138, 6917, 6448, 6633, 135, 488, 6625, 4786, 137]}, {"qid": 3281, "question": "Which natural language(s) are studied in this paper? in Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data", "answer": ["No", "English"], "top_k_doc_id": [488, 1886, 5498, 5499, 5501, 6448, 6625, 6632, 6917, 2265, 4609, 2661, 489, 1789, 4603], "orig_top_k_doc_id": [5501, 5499, 5498, 2265, 4609, 6632, 488, 2661, 1886, 489, 1789, 4603, 6448, 6917, 6625]}]}
{"group_id": 821, "group_size": 2, "items": [{"qid": 3321, "question": "What datasets do they use? in Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP", "answer": ["three datasets based on IMDB reviews and Yelp reviews", "1 IMDB dataset and 2 Yelp datasets"], "top_k_doc_id": [329, 5246, 5251, 5558, 5562, 6169, 5157, 4583, 787, 4199, 6451, 3562, 4207, 4205, 4478], "orig_top_k_doc_id": [5562, 5558, 5251, 5157, 4583, 787, 4199, 6451, 3562, 6169, 4207, 329, 4205, 5246, 4478]}, {"qid": 3323, "question": "What are the benchmark attacking methods? in Elephant in the Room: An Evaluation Framework for Assessing Adversarial Examples in NLP", "answer": ["FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4", "FGM, FGVM, DeepFool, HotFlip, TYC"], "top_k_doc_id": [329, 5246, 5251, 5558, 5562, 6169, 5559, 5563, 5560, 5561, 539, 1893, 494, 6865, 6864], "orig_top_k_doc_id": [5562, 5558, 5559, 5563, 5560, 5561, 5251, 539, 6169, 5246, 1893, 494, 329, 6865, 6864]}]}
{"group_id": 822, "group_size": 2, "items": [{"qid": 3393, "question": "Do they report results only on English dataset? in Distilling Translations with Visual Awareness", "answer": ["No", "No"], "top_k_doc_id": [83, 84, 4280, 5618, 5619, 1238, 3657, 7064, 1111, 1237, 80, 3655, 5671, 1457, 1778], "orig_top_k_doc_id": [83, 1238, 3657, 4280, 5618, 7064, 1111, 1237, 5619, 80, 3655, 84, 5671, 1457, 1778]}, {"qid": 3394, "question": "What dataset does this approach achieve state of the art results on? in Distilling Translations with Visual Awareness", "answer": ["the English-German dataset"], "top_k_doc_id": [83, 84, 4280, 5618, 5619, 868, 2922, 1217, 1527, 5732, 5620, 871, 4178, 2737, 5477], "orig_top_k_doc_id": [83, 4280, 868, 2922, 5618, 1217, 1527, 84, 5732, 5620, 871, 5619, 4178, 2737, 5477]}]}
{"group_id": 823, "group_size": 2, "items": [{"qid": 3401, "question": "what are the existing approaches? in Embedding Geographic Locations for Modelling the Natural Environment using Flickr Tags and Structured Data", "answer": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "top_k_doc_id": [558, 637, 638, 5217, 5626, 5627, 5628, 5629, 5630, 5631, 6622, 1945, 6005, 7113, 6796], "orig_top_k_doc_id": [5626, 5631, 5627, 5628, 5630, 5629, 638, 558, 6622, 5217, 637, 1945, 6005, 7113, 6796]}, {"qid": 3402, "question": "what dataset is used in this paper? in Embedding Geographic Locations for Modelling the Natural Environment using Flickr Tags and Structured Data", "answer": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "top_k_doc_id": [558, 637, 638, 5217, 5626, 5627, 5628, 5629, 5630, 5631, 6622, 6833, 6020, 6854, 3799], "orig_top_k_doc_id": [5626, 5631, 5627, 5628, 5630, 5629, 638, 6833, 558, 6020, 637, 6854, 5217, 3799, 6622]}]}
{"group_id": 824, "group_size": 2, "items": [{"qid": 3421, "question": "What is the McGurk effect? in A Surprising Density of Illusionable Natural Speech", "answer": ["a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard", "When the perception of what we hear is influenced by what we see."], "top_k_doc_id": [610, 617, 5664, 5665, 5666, 5667, 609, 4005, 6835, 6498, 3197, 3392, 6079, 6836, 2663], "orig_top_k_doc_id": [5664, 5667, 5665, 5666, 609, 4005, 610, 6835, 6498, 3197, 617, 3392, 6079, 6836, 2663]}, {"qid": 3422, "question": "Are humans and machine learning systems fooled by the same kinds of illusions? in A Surprising Density of Illusionable Natural Speech", "answer": ["No"], "top_k_doc_id": [610, 617, 5664, 5665, 5666, 5667, 6863, 3562, 898, 1920, 5474, 1161, 3445, 2701, 33], "orig_top_k_doc_id": [5664, 5667, 5666, 5665, 610, 6863, 3562, 617, 898, 1920, 5474, 1161, 3445, 2701, 33]}]}
{"group_id": 825, "group_size": 2, "items": [{"qid": 3467, "question": "Which dataset has been used in this work? in Text Classification based on Word Subspace with Term-Frequency", "answer": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "top_k_doc_id": [1800, 2652, 4534, 5750, 5752, 5753, 5754, 5755, 5756, 6962, 420, 5757, 304, 2351, 1661], "orig_top_k_doc_id": [5755, 5750, 5754, 5753, 6962, 5756, 420, 5752, 2652, 4534, 5757, 304, 2351, 1661, 1800]}, {"qid": 3468, "question": "What can word subspace represent? in Text Classification based on Word Subspace with Term-Frequency", "answer": ["Word vectors, usually in the context of others within the same class"], "top_k_doc_id": [1800, 2652, 4534, 5750, 5752, 5753, 5754, 5755, 5756, 6962, 1801, 422, 421, 4531, 4533], "orig_top_k_doc_id": [5755, 5750, 5753, 5754, 1800, 5752, 6962, 5756, 1801, 422, 4534, 421, 4531, 4533, 2652]}]}
{"group_id": 826, "group_size": 2, "items": [{"qid": 3481, "question": "What did the best systems use for their model? in Applying a Pre-trained Language Model to Spanish Twitter Humor Prediction", "answer": ["No"], "top_k_doc_id": [2083, 5272, 5696, 5769, 5770, 6176, 6313, 6732, 2789, 2875, 2329, 1640, 4302, 3750, 1639], "orig_top_k_doc_id": [5769, 6176, 5272, 6732, 2789, 2875, 2329, 1640, 6313, 4302, 2083, 5770, 3750, 1639, 5696]}, {"qid": 3482, "question": "What were their results on the classification and regression tasks in Applying a Pre-trained Language Model to Spanish Twitter Humor Prediction", "answer": ["F1 of 0.8099", "F1 score result of 0.8099"], "top_k_doc_id": [2083, 5272, 5696, 5769, 5770, 6176, 6313, 7048, 5292, 770, 6240, 7755, 7115, 7132, 5008], "orig_top_k_doc_id": [5769, 5770, 2083, 6176, 7048, 5696, 5272, 5292, 770, 6240, 7755, 6313, 7115, 7132, 5008]}]}
{"group_id": 827, "group_size": 2, "items": [{"qid": 3529, "question": "IS the graph representation supervised? in Pre-training of Graph Augmented Transformers for Medication Recommendation", "answer": ["The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)"], "top_k_doc_id": [5827, 5828, 5829, 5830, 5831, 7834, 1395, 4996, 3020, 1394, 4318, 5002, 4321, 1401, 1155], "orig_top_k_doc_id": [5827, 5831, 5829, 5828, 1395, 4996, 5830, 3020, 1394, 4318, 5002, 4321, 1401, 7834, 1155]}, {"qid": 3530, "question": "Is the G-BERT model useful beyond the task considered? in Pre-training of Graph Augmented Transformers for Medication Recommendation", "answer": ["There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.", "It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records."], "top_k_doc_id": [5827, 5828, 5829, 5830, 5831, 7834, 97, 4647, 4646, 2052, 4650, 1839, 1139, 4652, 4413], "orig_top_k_doc_id": [5827, 5831, 5828, 5830, 5829, 97, 4647, 4646, 2052, 4650, 1839, 1139, 4652, 7834, 4413]}]}
{"group_id": 828, "group_size": 2, "items": [{"qid": 3646, "question": "What are the opportunities presented by the use of Semantic Web technologies in Machine Translation? in Semantic Web for Machine Translation: Challenges and Directions", "answer": ["disambiguation, Named Entities, Non-standard speech, Translating KBs", "disambiguation, NERD,  non-standard language, translating KBs", "Disambiguation, Named Entities, Non-standard speech, Translating KBs", "SWT can be applied to support the semantic disambiguation in MT: to  recognize ambiguous words before translation and  as a post-editing technique applied to  the output language. SWT may be used for translating KBs."], "top_k_doc_id": [34, 993, 2201, 2548, 2818, 3416, 5982, 7194, 637, 4222, 4864, 6534, 2188, 447, 3216], "orig_top_k_doc_id": [5982, 2201, 2818, 7194, 637, 4222, 34, 993, 4864, 6534, 2188, 447, 3416, 2548, 3216]}, {"qid": 3647, "question": "What are the challenges associated with the use of Semantic Web technologies in Machine Translation? in Semantic Web for Machine Translation: Challenges and Directions", "answer": ["syntactic disambiguation problem which as yet lacks good solutions, directly related to the ambiguity problem and therefore has to be resolved in that wider context, In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open", "reordering errors,  lexical and syntactic ambiguity", "SWT are hard to implement"], "top_k_doc_id": [34, 993, 2201, 2548, 2818, 3416, 5982, 117, 3124, 5983, 2986, 3679, 987, 1309, 5736], "orig_top_k_doc_id": [5982, 2818, 2201, 993, 117, 3124, 5983, 2986, 3679, 34, 987, 1309, 3416, 5736, 2548]}]}
{"group_id": 829, "group_size": 2, "items": [{"qid": 3650, "question": "what were the baselines? in Advancing Speech Recognition With No Speech Or With Noisy Speech", "answer": ["No", "No", "No"], "top_k_doc_id": [416, 1228, 4875, 5765, 5987, 5990, 6314, 7534, 4373, 4921, 812, 811, 7537, 2292, 6036], "orig_top_k_doc_id": [5987, 416, 6314, 1228, 4373, 4921, 7534, 812, 5990, 811, 7537, 4875, 2292, 5765, 6036]}, {"qid": 3651, "question": "what dataset was used? in Advancing Speech Recognition With No Speech Or With Noisy Speech", "answer": [" two types of simultaneous speech EEG recording databases ", "The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment.", "Speech EEG recording collected from male and female subjects under different background noises", "For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment."], "top_k_doc_id": [416, 1228, 4875, 5765, 5987, 5990, 6314, 7534, 1336, 6479, 381, 1296, 844, 973, 2002], "orig_top_k_doc_id": [416, 5987, 6314, 7534, 4875, 1336, 6479, 381, 1296, 5990, 5765, 844, 973, 2002, 1228]}]}
{"group_id": 830, "group_size": 2, "items": [{"qid": 3674, "question": "what dataset was used for training? in Neural Language Modeling with Visual Features", "answer": ["64M segments from YouTube videos", "YouCook2 , sth-sth", "64M segments from YouTube videos", "About 64M segments from YouTube videos comprising a total of 1.2B tokens."], "top_k_doc_id": [511, 521, 1058, 2737, 4553, 5966, 6020, 7147, 2900, 3799, 3175, 2414, 7148, 1517, 790], "orig_top_k_doc_id": [2737, 6020, 7147, 511, 1058, 2900, 3799, 521, 3175, 2414, 7148, 1517, 4553, 790, 5966]}, {"qid": 3675, "question": "what is the size of the training data? in Neural Language Modeling with Visual Features", "answer": ["64M video segments with 1.2B tokens", "64M", "64M segments from YouTube videos, INLINEFORM0 B tokens, vocabulary of 66K wordpieces"], "top_k_doc_id": [511, 521, 1058, 2737, 4553, 5966, 6020, 7147, 5967, 4033, 6014, 7138, 3789, 6441, 6021], "orig_top_k_doc_id": [6020, 1058, 5967, 4033, 2737, 4553, 6014, 511, 5966, 7138, 7147, 3789, 521, 6441, 6021]}]}
{"group_id": 831, "group_size": 2, "items": [{"qid": 3844, "question": "Do they use skip-gram word2vec? in Improving Word Representations: A Sub-sampled Unigram Distribution for Negative Sampling", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [1203, 4260, 4261, 4872, 6054, 6221, 6222, 6223, 6225, 6917, 5302, 6635, 5709, 4259, 6240], "orig_top_k_doc_id": [6221, 6222, 4260, 6225, 1203, 4261, 5302, 6635, 6054, 5709, 4259, 4872, 6223, 6917, 6240]}, {"qid": 3845, "question": "How is quality of the word vectors measured? in Improving Word Representations: A Sub-sampled Unigram Distribution for Negative Sampling", "answer": ["correlation between the word similarity scores by human judgment and the word distances in vector space, select the semantically closest word, from the candidate answers", "They evaluate it on the word analogy, word similarity and synonym selection tasks using Pearson correlation coefficient as the metric.", "No"], "top_k_doc_id": [1203, 4260, 4261, 4872, 6054, 6221, 6222, 6223, 6225, 6917, 6224, 3700, 5303, 3699, 138], "orig_top_k_doc_id": [6221, 6222, 6225, 6223, 6224, 4260, 3700, 1203, 5303, 3699, 138, 4872, 4261, 6917, 6054]}]}
{"group_id": 832, "group_size": 2, "items": [{"qid": 3863, "question": "What dicrimating features are discovered? in A Low Dimensionality Representation for Language Variety Identification", "answer": ["Highest gain is obtained by average, maximum, minimum, and standard deviation. Probability and proportionality features have low information gain", "average, maximum and minimum, standard deviation", "a document's terms' minimum, maximum, average (relative to all terms and to in-vocabulary terms), and standard deviation of weights; and proportion of terms that are in-vocabulary"], "top_k_doc_id": [1434, 2784, 2785, 3116, 3422, 5104, 6240, 6242, 6243, 5822, 7687, 4004, 4673, 4782, 6961], "orig_top_k_doc_id": [6243, 6242, 6240, 1434, 3422, 5822, 7687, 4004, 2785, 2784, 4673, 4782, 5104, 6961, 3116]}, {"qid": 3864, "question": "What results are obtained on the alternate datasets? in A Low Dimensionality Representation for Language Variety Identification", "answer": ["Accuracy results range from 74.4 to 100 ", " three representations obtained comparative results and support the robustness of the low dimensionality representation", "Comparable to state-of-the-art"], "top_k_doc_id": [1434, 2784, 2785, 3116, 3422, 5104, 6240, 6242, 6243, 6241, 6837, 2216, 2065, 6372, 6579], "orig_top_k_doc_id": [6243, 6242, 6240, 1434, 3422, 3116, 6241, 6837, 2785, 2216, 2065, 2784, 6372, 5104, 6579]}]}
{"group_id": 833, "group_size": 2, "items": [{"qid": 3872, "question": "What languages do they apply the model to? in A Joint Model for Word Embedding and Word Morphology", "answer": ["English", "English", "English", "English"], "top_k_doc_id": [923, 5962, 6148, 6251, 7318, 6280, 655, 4471, 4510, 4289, 2448, 395, 6149, 6279, 922], "orig_top_k_doc_id": [6280, 6251, 7318, 923, 6148, 655, 4471, 4510, 4289, 2448, 395, 6149, 6279, 5962, 922]}, {"qid": 3873, "question": "How are the embeddings evaluated in the human judgement comparison? in A Joint Model for Word Embedding and Word Morphology", "answer": ["human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments", "human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments", "Using cosine similarity between the embeddings which is then correlated with human judgement"], "top_k_doc_id": [923, 5962, 6148, 6251, 7318, 3208, 3207, 5341, 5344, 2225, 1943, 6860, 6254, 3837, 2914], "orig_top_k_doc_id": [3208, 3207, 5341, 5344, 2225, 6251, 923, 1943, 6860, 6254, 5962, 7318, 3837, 6148, 2914]}]}
{"group_id": 834, "group_size": 2, "items": [{"qid": 3947, "question": "What downstream tasks are analyzed? in DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "answer": ["sentiment classification, question answering", "General Language Understanding, question answering task (SQuAD v1.1 - BIBREF14), classification task (IMDb sentiment classification - BIBREF13)", "a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14)."], "top_k_doc_id": [105, 4289, 4414, 4415, 5995, 6367, 6368, 6369, 6660, 4627, 440, 4626, 4628, 439, 2414], "orig_top_k_doc_id": [6368, 6367, 4414, 4415, 6369, 105, 4627, 440, 4626, 6660, 4628, 5995, 439, 4289, 2414]}, {"qid": 3948, "question": "How much time takes the training of DistilBERT? in DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "answer": ["on 8 16GB V100 GPUs for approximately 90 hours", "90 hours", "No"], "top_k_doc_id": [105, 4289, 4414, 4415, 5995, 6367, 6368, 6369, 6660, 2826, 715, 714, 5991, 1150, 1010], "orig_top_k_doc_id": [6367, 6368, 4414, 4415, 6369, 6660, 2826, 715, 5995, 714, 105, 5991, 4289, 1150, 1010]}]}
{"group_id": 835, "group_size": 2, "items": [{"qid": 3956, "question": "How many parameters does their noisy channel model have? in Simple and Effective Noisy Channel Modeling for Neural Machine Translation", "answer": ["No", "No", "No"], "top_k_doc_id": [6344, 6389, 6390, 6391, 7403, 2824, 922, 1812, 2825, 4370, 5109, 7642, 923, 1262, 2058], "orig_top_k_doc_id": [6389, 6391, 6390, 7403, 2824, 922, 1812, 2825, 4370, 5109, 7642, 6344, 923, 1262, 2058]}, {"qid": 3957, "question": "Which language pairs do they evaluate on? in Simple and Effective Noisy Channel Modeling for Neural Machine Translation", "answer": ["English-German, Chinese-English", "English-German; Chinese-English; German-English", "En-De, De-En, Zh-En, Englsh-Russian and Russian-English"], "top_k_doc_id": [6344, 6389, 6390, 6391, 4875, 3830, 774, 3004, 4561, 2053, 6424, 5835, 5053, 4812, 2547], "orig_top_k_doc_id": [6389, 6391, 4875, 6390, 6344, 3830, 774, 3004, 4561, 2053, 6424, 5835, 5053, 4812, 2547]}]}
{"group_id": 836, "group_size": 2, "items": [{"qid": 3967, "question": "What is reordering in the context of the paper? in Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder", "answer": ["changing the order of the word-by-word translation so it matches the target language", "Changing the word order of the translation so it is in the right order of the target language.", "Re-arranging translated words so that they are in the correct order in the target language"], "top_k_doc_id": [309, 3301, 3403, 4396, 6291, 6294, 6424, 6425, 6426, 3920, 3917, 100, 1886, 4857, 6314], "orig_top_k_doc_id": [6426, 6425, 6424, 3920, 309, 3917, 4396, 3301, 100, 3403, 1886, 6291, 6294, 4857, 6314]}, {"qid": 3968, "question": "How does the paper use language model for context aware search? in Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder", "answer": ["the language model is combined with cross-lingual word embedding to obtain context information in the word-by-word translation", "combining a language model (LM) with cross-lingual word embedding, Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:\n\n$ L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h) $\n\nHere, $q(f,e)$ is a lexical score defined as:\n\n$ q(f,e) = \\frac{d(f,e) + 1}{2} $\n\nwhere $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ ., Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses.", "It is used to calculate the probability of a possible target word given the history of target words that come before it."], "top_k_doc_id": [309, 3301, 3403, 4396, 6291, 6294, 6424, 6425, 6426, 7825, 1014, 7848, 6370, 96, 3562], "orig_top_k_doc_id": [6426, 6424, 309, 6425, 6294, 7825, 3301, 4396, 1014, 7848, 3403, 6291, 6370, 96, 3562]}]}
{"group_id": 837, "group_size": 2, "items": [{"qid": 4111, "question": "How exactly do they weigh between different statistical models? in Using Statistical and Semantic Models for Multi-Document Summarization", "answer": ["They define cWeight as weight obtained for each sentence using all the models where the sentences is in the summary of predicted by each model.", "by training on field-specific corpora", "after training on corpus, we assign weights among the different techniques"], "top_k_doc_id": [5997, 6571, 6573, 6574, 7242, 7280, 6572, 1132, 456, 7524, 1969, 7215, 2106, 7762, 4829], "orig_top_k_doc_id": [6571, 7242, 6574, 7280, 6572, 6573, 5997, 1132, 456, 7524, 1969, 7215, 2106, 7762, 4829]}, {"qid": 4113, "question": "What showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score? in Using Statistical and Semantic Models for Multi-Document Summarization", "answer": ["Combination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model", "Jaccard/Cosine Similarity Matrix+TextRank\n+InferSent Based Model", "Best result was obtained by using combination of: Jaccard/Cosine Similarity Matrix, TextRank and InferSent Based Model"], "top_k_doc_id": [5997, 6571, 6573, 6574, 6925, 459, 460, 5150, 4828, 458, 5079, 4481, 4649, 5152, 7243], "orig_top_k_doc_id": [6574, 6571, 6925, 459, 460, 5150, 4828, 458, 5079, 6573, 5997, 4481, 4649, 5152, 7243]}]}
{"group_id": 838, "group_size": 2, "items": [{"qid": 4153, "question": "what are the recent models they compare with? in On the State of the Art of Evaluation in Neural Language Models", "answer": ["Recurrent Highway Networks, NAS, BIBREF5", "BIBREF1, Neural Cache BIBREF6, BIBREF0", "Recurrent Highway Networks, NAS "], "top_k_doc_id": [3069, 7363, 4561, 2584, 6448, 1033, 1798, 2733, 6583, 2970, 2335, 2086, 2739, 3150, 872], "orig_top_k_doc_id": [3069, 4561, 2584, 6448, 1033, 7363, 1798, 2733, 6583, 2970, 2335, 2086, 2739, 3150, 872]}, {"qid": 4510, "question": "what state of the art models do they compare to? in Recurrent Neural Network Grammars", "answer": ["Vinyals et al (2015) for English parsing, Wang et al (2015) for Chinese parsing, and LSTM LM for Language modeling both in English and Chinese ", "IKN 5-gram, LSTM LM"], "top_k_doc_id": [3069, 7363, 7054, 7367, 5780, 2136, 7050, 2135, 686, 5429, 655, 7053, 2752, 7584, 2717], "orig_top_k_doc_id": [7054, 7367, 5780, 3069, 2136, 7050, 2135, 686, 7363, 5429, 655, 7053, 2752, 7584, 2717]}]}
{"group_id": 839, "group_size": 2, "items": [{"qid": 4154, "question": "what were their results on the hutter prize dataset? in On the State of the Art of Evaluation in Neural Language Models", "answer": ["slightly off the state of the art", "1.30 and 1.31", "1.30 BPC is their best result"], "top_k_doc_id": [2874, 2875, 6612, 1168, 6108, 6547, 1170, 4631, 872, 4441, 7562, 2330, 1560, 982, 4915], "orig_top_k_doc_id": [6612, 1168, 6108, 6547, 1170, 4631, 872, 2874, 4441, 7562, 2330, 1560, 2875, 982, 4915]}, {"qid": 4156, "question": "what regularisation methods did they look at? in On the State of the Art of Evaluation in Neural Language Models", "answer": ["No", "dropout, variational dropout, recurrent dropout"], "top_k_doc_id": [2874, 2875, 6612, 3069, 6614, 7039, 3739, 2752, 6613, 6583, 3416, 3741, 6362, 7605, 875], "orig_top_k_doc_id": [3069, 2874, 6614, 7039, 3739, 2752, 6613, 6583, 3416, 6612, 3741, 2875, 6362, 7605, 875]}]}
{"group_id": 840, "group_size": 2, "items": [{"qid": 4155, "question": "what was their newly established state of the art results? in On the State of the Art of Evaluation in Neural Language Models", "answer": ["58.3 perplexity in PTB, and 65.9 perplexity in Wikitext-2", "At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4, our best result, exp(4.188)"], "top_k_doc_id": [436, 1773, 2739, 4915, 4982, 7780, 7287, 494, 2875, 7285, 4047, 686, 4048, 1329, 5546], "orig_top_k_doc_id": [4915, 2739, 4982, 7780, 7287, 1773, 494, 2875, 7285, 4047, 686, 436, 4048, 1329, 5546]}, {"qid": 4157, "question": "what architectures were reevaluated? in On the State of the Art of Evaluation in Neural Language Models", "answer": ["LSTMs, Recurrent Highway Networks, NAS", "Answer with content missing: (Architecture section missing) The Long Short-Term Memory, Recurrent Highway Network and NAS", "LSTM, RHN and NAS."], "top_k_doc_id": [436, 1773, 2739, 1560, 2330, 7375, 3069, 2301, 2188, 4412, 5429, 5252, 7391, 7116, 2584], "orig_top_k_doc_id": [1560, 2330, 7375, 436, 3069, 2301, 2188, 2739, 1773, 4412, 5429, 5252, 7391, 7116, 2584]}]}
{"group_id": 841, "group_size": 2, "items": [{"qid": 4195, "question": "what were the length constraints they set? in On NMT Search Errors and Model Errors: Cat Got Your Tongue?", "answer": ["search to translations longer than 0.25 times the source sentence length, search to either the length of the best Beam-10 hypothesis or the reference length", "They set translation length longer than minimum 0.25 times the source sentence length"], "top_k_doc_id": [357, 593, 774, 2763, 2931, 4152, 6668, 6669, 6670, 7693, 3980, 4204, 6776, 7158, 5748], "orig_top_k_doc_id": [6669, 6670, 6668, 3980, 4152, 2763, 774, 4204, 357, 6776, 7158, 593, 7693, 2931, 5748]}, {"qid": 4196, "question": "what is the test set size? in On NMT Search Errors and Model Errors: Cat Got Your Tongue?", "answer": ["2,169 sentences", "2,169 sentences", "2,169 sentences"], "top_k_doc_id": [357, 593, 774, 2763, 2931, 4152, 6668, 6669, 6670, 7693, 5233, 6618, 4182, 6413, 2494], "orig_top_k_doc_id": [6668, 6669, 5233, 2763, 6618, 2931, 4152, 593, 7693, 4182, 774, 6670, 6413, 357, 2494]}]}
{"group_id": 842, "group_size": 2, "items": [{"qid": 4422, "question": "Did they use the same dataset as Skip-gram to train? in SubGram: Extending Skip-gram Word Representation with Substrings", "answer": ["Yes", "No"], "top_k_doc_id": [2775, 5298, 6240, 6635, 6952, 6953, 6954, 1182, 1936, 4839, 1184, 1183, 1935, 1449, 1203], "orig_top_k_doc_id": [6954, 6952, 6953, 1182, 1936, 5298, 4839, 2775, 1184, 1183, 1935, 1449, 6635, 1203, 6240]}, {"qid": 4423, "question": "How much were the gains they obtained? in SubGram: Extending Skip-gram Word Representation with Substrings", "answer": ["between 21-57% in several morpho-syntactic questions", "Only 0.2% accuracy gain in morpho-sintactic questions in original test set, and 12.7% accuracy gain on their test set"], "top_k_doc_id": [2775, 5298, 6240, 6635, 6952, 6953, 6954, 7140, 5302, 2876, 1662, 6242, 6241, 5627, 5297], "orig_top_k_doc_id": [6954, 6952, 6953, 6635, 5298, 2775, 7140, 6240, 5302, 2876, 1662, 6242, 6241, 5627, 5297]}]}
{"group_id": 843, "group_size": 2, "items": [{"qid": 4424, "question": "What is the extractive technique used for summarization? in Plain English Summarization of Contracts", "answer": ["Answer with content missing: (baseline list) TextRank, KLSum, Lead-1, Lead-K and Random-K", "TextRank, KLSum, Lead-1, Lead-K, Random-K"], "top_k_doc_id": [1863, 6923, 6955, 6956, 6957, 2334, 4826, 6570, 3715, 6566, 1253, 1567, 7241, 4825, 1132], "orig_top_k_doc_id": [6955, 6957, 6923, 6956, 1863, 2334, 4826, 6570, 3715, 6566, 1253, 1567, 7241, 4825, 1132]}, {"qid": 4425, "question": "How big is the dataset? in Plain English Summarization of Contracts", "answer": ["446", "446 sets of parallel text"], "top_k_doc_id": [1863, 6923, 6955, 6956, 6957, 4030, 2448, 3857, 5277, 4425, 1864, 2447, 4907, 7617, 1694], "orig_top_k_doc_id": [6955, 6957, 6956, 4030, 6923, 1863, 2448, 3857, 5277, 4425, 1864, 2447, 4907, 7617, 1694]}]}
{"group_id": 844, "group_size": 2, "items": [{"qid": 4450, "question": "How are experiments designed to measure impact on performance by different choices? in Applying Cyclical Learning Rate to Neural Machine Translation", "answer": ["CLR is selected by the range test, Shrink strategy is applied when examining the effects of CLR in training NMT, The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as \u201cnshrink\"); 2) with shrink at a rate of 0.5 (\u201cyshrink\")", "The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as \u201cnshrink\"); 2) with shrink at a rate of 0.5 (\u201cyshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5."], "top_k_doc_id": [1681, 1682, 6991, 6992, 6994, 5569, 1777, 2041, 7472, 7863, 5871, 2494, 244, 717, 6295], "orig_top_k_doc_id": [6991, 6994, 1681, 6992, 1682, 5569, 1777, 2041, 7472, 7863, 5871, 2494, 244, 717, 6295]}, {"qid": 4451, "question": "What impact on performance is shown for different choices of optimizers and learning rate policies? in Applying Cyclical Learning Rate to Neural Machine Translation", "answer": ["The training takes fewer epochs to converge to reach a local minimum with better BLEU scores", "Applying CLR has positive impacts on NMT training for both Adam and SGD, it can be observed that the effects of applying CLR to Adam are more significant than those of SGD, we see that the trend of CLR with a larger batch size for NMT training does indeed lead to better performance., The benefit of a larger batch size afforded by CLR means that training time can be cut down considerably."], "top_k_doc_id": [1681, 1682, 6991, 6992, 6994, 6993, 3691, 3188, 7646, 5660, 7648, 3775, 3692, 7647, 6589], "orig_top_k_doc_id": [6991, 6994, 6992, 6993, 3691, 3188, 7646, 5660, 1681, 1682, 7648, 3775, 3692, 7647, 6589]}]}
{"group_id": 845, "group_size": 2, "items": [{"qid": 4499, "question": "What dimensions do the considered embeddings have? in Extrapolation in NLP", "answer": ["Answer with content missing: (Models sections) 100, 200 and 400", "100, 200, 400"], "top_k_doc_id": [3083, 4885, 7039, 7040, 7041, 3207, 3208, 3215, 1860, 3994, 3614, 4931, 2044, 5305, 3795], "orig_top_k_doc_id": [7039, 7040, 3207, 3083, 3208, 4885, 3215, 1860, 7041, 3994, 3614, 4931, 2044, 5305, 3795]}, {"qid": 4500, "question": "How are global structures considered? in Extrapolation in NLP", "answer": ["No", "global structure in the learned embeddings is related to a linearity in the training objective"], "top_k_doc_id": [3083, 4885, 7039, 7040, 7041, 2937, 3086, 2734, 2058, 2190, 7382, 1899, 2701, 1533, 2182], "orig_top_k_doc_id": [7040, 7041, 7039, 3083, 2937, 3086, 2734, 2058, 2190, 7382, 1899, 2701, 1533, 4885, 2182]}]}
{"group_id": 846, "group_size": 2, "items": [{"qid": 4577, "question": "What other model inference optimization schemes authors explore? in SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units for speech recognition", "answer": [" shortcut connections, batch normalization (BN), self-normalizing neural networks (SNNs)", " frame-skipping, multi-threaded lazy computation"], "top_k_doc_id": [381, 1175, 5411, 5475, 5477, 7150, 7151, 7152, 7687, 4367, 3426, 380, 2136, 4370, 1618], "orig_top_k_doc_id": [7150, 7152, 7151, 5475, 5411, 7687, 4367, 3426, 381, 380, 5477, 2136, 1175, 4370, 1618]}, {"qid": 4578, "question": "On what dataset is model trained/tested? in SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units for speech recognition", "answer": ["Siri internal datasets (en_US and zh_CN)", "Siri internal datasets (en_US and zh_CN)"], "top_k_doc_id": [381, 1175, 5411, 5475, 5477, 7150, 7151, 7152, 4544, 4543, 2783, 7621, 6894, 3781, 2217], "orig_top_k_doc_id": [7150, 7152, 7151, 4544, 5475, 1175, 381, 4543, 2783, 5477, 5411, 7621, 6894, 3781, 2217]}]}
{"group_id": 847, "group_size": 2, "items": [{"qid": 4690, "question": "What is the computational complexity of old method in Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts", "answer": ["O(2**N)", "No"], "top_k_doc_id": [3670, 6103, 7312, 321, 6126, 5750, 1013, 2781, 2772, 2173, 551, 6594, 6802, 1034, 2771], "orig_top_k_doc_id": [7312, 6103, 321, 6126, 5750, 1013, 2781, 3670, 2772, 2173, 551, 6594, 6802, 1034, 2771]}, {"qid": 4691, "question": "Could you tell me more about the old method? in Efficient Calculation of Bigram Frequencies in a Corpus of Short Texts", "answer": ["freq(*, word) = freq(word, *) = freq(word)", "$$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1)"], "top_k_doc_id": [3670, 6103, 7312, 7157, 7223, 7158, 7224, 908, 4138, 7775, 46, 6084, 7226, 1958, 3894], "orig_top_k_doc_id": [7157, 7223, 7312, 7158, 3670, 7224, 908, 4138, 7775, 46, 6084, 7226, 1958, 3894, 6103]}]}
{"group_id": 848, "group_size": 2, "items": [{"qid": 4827, "question": "What are the three SOTA models evaluated? in Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction", "answer": ["BIBREF3, BIBREF4, BIBREF5 ", "BIBREF3, BIBREF4, BIBREF5"], "top_k_doc_id": [80, 84, 1046, 2162, 7509, 7513, 2164, 2584, 7510, 3748, 2021, 5600, 2972, 1141, 1389], "orig_top_k_doc_id": [7509, 7513, 84, 1046, 2162, 80, 2164, 2584, 7510, 3748, 2021, 5600, 2972, 1141, 1389]}, {"qid": 4828, "question": "What is the morphological constraint added? in Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction", "answer": ["Aligned words must share the same morphosyntactic category", "each iteration they can align two words only if they share the same morphosyntactic category"], "top_k_doc_id": [80, 84, 1046, 2162, 7509, 7513, 1000, 6782, 624, 7686, 3746, 919, 6251, 4848, 6855], "orig_top_k_doc_id": [7509, 7513, 1046, 80, 2162, 1000, 6782, 624, 84, 7686, 3746, 919, 6251, 4848, 6855]}]}
{"group_id": 849, "group_size": 2, "items": [{"qid": 4867, "question": "What simplification of the architecture is performed that resulted in same performance? in SEPT: Improving Scientific Named Entity Recognition with Span Representation", "answer": ["randomly sampling them rather than enumerate them all, simple max-pooling to extract span representation because those features are implicitly included in self-attention layers of transformers", " we simplify the origin network architecture and extract span representation by a simple pooling layer"], "top_k_doc_id": [22, 1256, 4750, 7553, 7554, 7555, 404, 1090, 403, 387, 5184, 1262, 2755, 6825, 4668], "orig_top_k_doc_id": [7553, 7554, 7555, 404, 1090, 403, 387, 1256, 22, 4750, 5184, 1262, 2755, 6825, 4668]}, {"qid": 4868, "question": "How much better is performance of SEPT compared to previous state-of-the-art? in SEPT: Improving Scientific Named Entity Recognition with Span Representation", "answer": ["SEPT have improvement for Recall  3.9%  and F1 1.3%  over the best performing baseline (SCIIE(SciBERT))", "In ELMo model, SCIIE achieves almost 3.0% F1 higher than BiLSTM,  in SciBERT, the performance becomes similar, which is only a 0.5% gap"], "top_k_doc_id": [22, 1256, 4750, 7553, 7554, 7555, 4551, 5775, 1419, 3617, 23, 2320, 501, 4669, 926], "orig_top_k_doc_id": [7553, 7554, 7555, 22, 4551, 5775, 4750, 1256, 1419, 3617, 23, 2320, 501, 4669, 926]}]}
{"group_id": 850, "group_size": 2, "items": [{"qid": 4927, "question": "What type of attention is used in the recognition system? in Towards better decoding and language model integration in sequence to sequence models", "answer": ["find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context", "No"], "top_k_doc_id": [1013, 2995, 2996, 4377, 3338, 4376, 4756, 3339, 380, 3173, 7642, 1791, 1822, 4918, 7693], "orig_top_k_doc_id": [4377, 3338, 4376, 2995, 4756, 3339, 2996, 380, 1013, 3173, 7642, 1791, 1822, 4918, 7693]}, {"qid": 4928, "question": "What are the solutions proposed for the seq2seq shortcomings? in Towards better decoding and language model integration in sequence to sequence models", "answer": ["forbids emitting the EOS token, beam search criterion can be extended to promote long transcripts, coverage criterion prevents looping over the utterance, ground-truth label distribution is smoothed", "label smoothing, use of coverage"], "top_k_doc_id": [1013, 2995, 2996, 4377, 493, 7455, 494, 705, 6067, 6390, 5638, 2024, 2027, 7594, 1138], "orig_top_k_doc_id": [2995, 1013, 493, 7455, 494, 705, 2996, 6067, 4377, 6390, 5638, 2024, 2027, 7594, 1138]}]}
{"group_id": 851, "group_size": 2, "items": [{"qid": 4990, "question": "Which downstream tasks are considered? in Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding", "answer": ["semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13", "SICK, MSRP, TREC, MR, SST, CR, SUBJ, MPQA, STS14, SNLI"], "top_k_doc_id": [1234, 1235, 1768, 2146, 2494, 3659, 7766, 7767, 7768, 7770, 3781, 620, 1975, 6061, 6532], "orig_top_k_doc_id": [7767, 3659, 7768, 7766, 2494, 1768, 7770, 3781, 620, 1975, 2146, 1235, 6061, 1234, 6532]}, {"qid": 4991, "question": "How long are the two unlabelled corpora? in Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding", "answer": ["Amazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus", "71000000, 142000000"], "top_k_doc_id": [1234, 1235, 1768, 2146, 2494, 3659, 7766, 7767, 7768, 7770, 2495, 4049, 2637, 610, 2148], "orig_top_k_doc_id": [3659, 7770, 1235, 7767, 7766, 1768, 7768, 2146, 2494, 2495, 4049, 1234, 2637, 610, 2148]}]}
{"group_id": 852, "group_size": 2, "items": [{"qid": 4999, "question": "What languages are used for the experiments? in A Study on Neural Network Language Modeling", "answer": ["English, French", "No"], "top_k_doc_id": [3069, 6828, 7367, 7687, 659, 6034, 3406, 4186, 7070, 7066, 6190, 627, 5711, 5106, 7688], "orig_top_k_doc_id": [6828, 659, 6034, 3406, 4186, 7070, 7066, 6190, 3069, 7687, 627, 5711, 7367, 5106, 7688]}, {"qid": 5001, "question": "What language model architectures are examined? in A Study on Neural Network Language Modeling", "answer": ["FNNLM, RNNLM, BiRNN, LSTM", "RNNLM, LSTM-RNN, FNNLM"], "top_k_doc_id": [3069, 6828, 7367, 7687, 3301, 5053, 1560, 4901, 4878, 4095, 1256, 6165, 2301, 6697, 1651], "orig_top_k_doc_id": [3301, 3069, 5053, 1560, 4901, 6828, 4878, 4095, 1256, 7687, 7367, 6165, 2301, 6697, 1651]}]}
{"group_id": 853, "group_size": 1, "items": [{"qid": 176, "question": "what amounts of size were used on german-english? in Revisiting Low-Resource Neural Machine Translation: A Case Study", "answer": ["Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development", "ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)"], "top_k_doc_id": [231, 232, 5276, 6539, 7342, 6944, 1111, 7434, 7850, 1722, 6110, 3618, 6538, 233, 650], "orig_top_k_doc_id": [231, 232, 5276, 6539, 7342, 6944, 1111, 7434, 7850, 1722, 6110, 3618, 6538, 233, 650]}]}
{"group_id": 854, "group_size": 1, "items": [{"qid": 294, "question": "Which retrieval system was used for baselines? in Quasar: Datasets for Question Answering by Search and Reading", "answer": ["The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system."], "top_k_doc_id": [352, 354, 355, 356, 2464, 353, 5736, 302, 5119, 3167, 3851, 1405, 7359, 7541, 6449], "orig_top_k_doc_id": [352, 354, 355, 356, 2464, 353, 5736, 302, 5119, 3167, 3851, 1405, 7359, 7541, 6449]}]}
{"group_id": 855, "group_size": 1, "items": [{"qid": 576, "question": "What datasets did they use? in Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation", "answer": ["The E2E NLG challenge dataset BIBREF21"], "top_k_doc_id": [705, 4900, 490, 3984, 898, 703, 2331, 1500, 3368, 1643, 5495, 706, 7700, 4461, 5493], "orig_top_k_doc_id": [705, 4900, 490, 3984, 898, 703, 2331, 1500, 3368, 1643, 5495, 706, 7700, 4461, 5493]}]}
{"group_id": 856, "group_size": 1, "items": [{"qid": 674, "question": "What languages do they look at? in On the coexistence of competing languages", "answer": ["No"], "top_k_doc_id": [839, 832, 840, 833, 834, 836, 835, 837, 4506, 4332, 3750, 6882, 6190, 6881, 6280], "orig_top_k_doc_id": [839, 832, 840, 833, 834, 836, 835, 837, 4506, 4332, 3750, 6882, 6190, 6881, 6280]}]}
{"group_id": 857, "group_size": 1, "items": [{"qid": 790, "question": "Which Twitter corpus was used to train the word vectors? in Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task?", "answer": ["They collected tweets in Russian language using a heuristic query specific to Russian"], "top_k_doc_id": [982, 983, 985, 984, 4712, 5313, 3703, 2105, 5314, 2344, 155, 2104, 1685, 6176, 1517], "orig_top_k_doc_id": [982, 983, 985, 984, 4712, 5313, 3703, 2105, 5314, 2344, 155, 2104, 1685, 6176, 1517]}]}
{"group_id": 858, "group_size": 1, "items": [{"qid": 828, "question": "What state-of-the-art models are compared against? in Spectral decomposition method of dialog state tracking via collective matrix factorization", "answer": ["a deep neural network (DNN) architecture proposed in BIBREF24 , maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model"], "top_k_doc_id": [1034, 1033, 1031, 1029, 1032, 5991, 5992, 7839, 7842, 1030, 2523, 3193, 4545, 4872, 7840], "orig_top_k_doc_id": [1034, 1033, 1031, 1029, 1032, 5991, 5992, 7839, 7842, 1030, 2523, 3193, 4545, 4872, 7840]}]}
{"group_id": 859, "group_size": 1, "items": [{"qid": 834, "question": "How does this compare to traditional calibration methods like Platt Scaling? in Improving Open Information Extraction via Iterative Rank-Aware Learning", "answer": ["No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods."], "top_k_doc_id": [1055, 1081, 4277, 854, 1697, 4555, 5422, 3423, 2335, 5423, 2420, 2792, 4263, 2917, 3341], "orig_top_k_doc_id": [1055, 1081, 4277, 854, 1697, 4555, 5422, 3423, 2335, 5423, 2420, 2792, 4263, 2917, 3341]}]}
{"group_id": 860, "group_size": 1, "items": [{"qid": 836, "question": "What statistics on the VIST dataset are reported? in Character-Centric Storytelling", "answer": ["In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."], "top_k_doc_id": [1058, 4976, 4977, 1059, 1958, 6519, 1256, 1692, 7352, 5941, 1081, 370, 5794, 5793, 1158], "orig_top_k_doc_id": [1058, 4976, 4977, 1059, 1958, 6519, 1256, 1692, 7352, 5941, 1081, 370, 5794, 5793, 1158]}]}
{"group_id": 861, "group_size": 1, "items": [{"qid": 906, "question": "what standard speech transcription pipeline was used? in Comparing Human and Machine Errors in Conversational Speech Transcription", "answer": ["pipeline that is used at Microsoft for production data"], "top_k_doc_id": [1161, 1162, 1164, 1163, 6776, 4862, 7793, 5767, 899, 5766, 150, 7661, 6480, 5391, 4372], "orig_top_k_doc_id": [1161, 1162, 1164, 1163, 6776, 4862, 7793, 5767, 899, 5766, 150, 7661, 6480, 5391, 4372]}]}
{"group_id": 862, "group_size": 1, "items": [{"qid": 1254, "question": "What are all machine learning approaches compared in this work? in Corporate IT-Support Help-Desk Process Hybrid-Automation Solution with Machine Learning Approach", "answer": ["Feature selection, Random forest, XGBoost, Hierarchical Model"], "top_k_doc_id": [1706, 4669, 1709, 3432, 4663, 7514, 4819, 5201, 4545, 4734, 6401, 3685, 3634, 5316, 462], "orig_top_k_doc_id": [1706, 4669, 1709, 3432, 4663, 7514, 4819, 5201, 4545, 4734, 6401, 3685, 3634, 5316, 462]}]}
{"group_id": 863, "group_size": 1, "items": [{"qid": 1343, "question": "How many CNNs and LSTMs were ensembled? in BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs", "answer": ["10 CNNs and 10 LSTMs"], "top_k_doc_id": [1843, 1844, 1842, 1841, 7472, 7361, 6894, 5813, 727, 450, 7420, 5008, 7260, 1329, 7362], "orig_top_k_doc_id": [1843, 1844, 1842, 1841, 7472, 7361, 6894, 5813, 727, 450, 7420, 5008, 7260, 1329, 7362]}]}
{"group_id": 864, "group_size": 1, "items": [{"qid": 1393, "question": "Which two datasets does the resource come from? in Meteorologists and Students: A resource for language grounding of geographical descriptors", "answer": ["two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor"], "top_k_doc_id": [1929, 1930, 7066, 5716, 2634, 7070, 5699, 2270, 4615, 4617, 7029, 6946, 4619, 5704, 6488], "orig_top_k_doc_id": [1929, 1930, 7066, 5716, 2634, 7070, 5699, 2270, 4615, 4617, 7029, 6946, 4619, 5704, 6488]}]}
{"group_id": 865, "group_size": 1, "items": [{"qid": 1595, "question": "What is current state-of-the-art model? in Multi-domain Dialogue State Tracking as Dynamic Knowledge Graph Enhanced Question Answering", "answer": ["SUMBT BIBREF17 is the current state-of-the-art model on WOZ 2.0, TRADE BIBREF3 is the current published state-of-the-art model"], "top_k_doc_id": [2234, 2227, 2228, 2233, 3679, 3508, 2232, 2276, 3507, 7805, 2231, 3193, 7800, 5257, 164], "orig_top_k_doc_id": [2234, 2227, 2228, 2233, 3679, 3508, 2232, 2276, 3507, 7805, 2231, 3193, 7800, 5257, 164]}]}
{"group_id": 866, "group_size": 1, "items": [{"qid": 1658, "question": "What features did they train on? in Luminoso at SemEval-2018 Task 10: Distinguishing Attributes Using Text Corpora and Relational Knowledge", "answer": ["direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset"], "top_k_doc_id": [2359, 3412, 1330, 5698, 2874, 4278, 3634, 3414, 2112, 3409, 5175, 3633, 2110, 2115, 7008], "orig_top_k_doc_id": [2359, 3412, 1330, 5698, 2874, 4278, 3634, 3414, 2112, 3409, 5175, 3633, 2110, 2115, 7008]}]}
{"group_id": 867, "group_size": 1, "items": [{"qid": 1720, "question": "What baseline approaches do they compare against? in Revealing the Importance of Semantic Retrieval for Machine Reading at Scale", "answer": ["HotspotQA: Yang, Ding, Muppet\nFever: Hanselowski, Yoneda, Nie"], "top_k_doc_id": [2455, 490, 5914, 6651, 352, 2836, 4467, 5719, 217, 1017, 1474, 4320, 1405, 3175, 1244], "orig_top_k_doc_id": [2455, 490, 5914, 6651, 352, 2836, 4467, 5719, 217, 1017, 1474, 4320, 1405, 3175, 1244]}]}
{"group_id": 868, "group_size": 1, "items": [{"qid": 1770, "question": "Do sluice networks outperform non-transfer learning approaches? in Latent Multi-task Architecture Learning", "answer": ["Yes"], "top_k_doc_id": [2556, 2557, 5980, 1005, 626, 3328, 575, 5979, 6073, 6703, 2868, 627, 5417, 4288, 7259], "orig_top_k_doc_id": [2556, 2557, 5980, 1005, 626, 3328, 575, 5979, 6073, 6703, 2868, 627, 5417, 4288, 7259]}]}
{"group_id": 869, "group_size": 1, "items": [{"qid": 1775, "question": "what bottlenecks were identified? in Evaluating the Performance of a Speech Recognition based System", "answer": ["Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System."], "top_k_doc_id": [2576, 2577, 3007, 2292, 4716, 5765, 2313, 3481, 4120, 4718, 1796, 6604, 2633, 5393, 4373], "orig_top_k_doc_id": [2576, 2577, 3007, 2292, 4716, 5765, 2313, 3481, 4120, 4718, 1796, 6604, 2633, 5393, 4373]}]}
{"group_id": 870, "group_size": 1, "items": [{"qid": 1792, "question": "Are recurrent neural networks trained on perturbed data? in On the Robustness of Projection Neural Networks For Efficient Text Representation: An Empirical Study", "answer": ["No"], "top_k_doc_id": [2615, 2617, 7233, 95, 3564, 627, 3562, 1618, 6016, 2940, 2700, 6864, 3175, 6314, 7363], "orig_top_k_doc_id": [2615, 2617, 7233, 95, 3564, 627, 3562, 1618, 6016, 2940, 2700, 6864, 3175, 6314, 7363]}]}
{"group_id": 871, "group_size": 1, "items": [{"qid": 1858, "question": "Is their model fine-tuned also on all available data, what are results? in Effectiveness of self-supervised pre-training for speech recognition", "answer": ["No"], "top_k_doc_id": [3842, 3839, 2709, 2713, 2268, 3688, 4653, 534, 1560, 2238, 4368, 4530, 622, 5841, 4590], "orig_top_k_doc_id": [3842, 3839, 2709, 2713, 2268, 3688, 4653, 534, 1560, 2238, 4368, 4530, 622, 5841, 4590]}]}
{"group_id": 872, "group_size": 1, "items": [{"qid": 1924, "question": "What domains do they experiment with? in Domain Agnostic Real-Valued Specificity Prediction", "answer": ["Twitter, Yelp reviews and movie reviews"], "top_k_doc_id": [2861, 2865, 2864, 2866, 2863, 2862, 4727, 7739, 3144, 7282, 854, 853, 1717, 7272, 3291], "orig_top_k_doc_id": [2861, 2865, 2864, 2866, 2863, 2862, 4727, 7739, 3144, 7282, 854, 853, 1717, 7272, 3291]}]}
{"group_id": 873, "group_size": 1, "items": [{"qid": 1938, "question": "It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained? in Learning to Paraphrase for Question Answering", "answer": ["using multiple pivot sentences"], "top_k_doc_id": [2893, 2898, 2897, 2896, 2894, 1189, 2691, 1188, 7664, 5736, 2519, 2692, 2442, 2895, 510], "orig_top_k_doc_id": [2893, 2898, 2897, 2896, 2894, 1189, 2691, 1188, 7664, 5736, 2519, 2692, 2442, 2895, 510]}]}
{"group_id": 874, "group_size": 1, "items": [{"qid": 1989, "question": "Why is this work different from text-only UNMT? in Unsupervised Multi-modal Neural Machine Translation", "answer": ["the image can play the role of a pivot \u201clanguage\" to bridge the two languages without paralleled corpus"], "top_k_doc_id": [2998, 3002, 2999, 4568, 6424, 3001, 3003, 4569, 6425, 5027, 7138, 5028, 575, 2413, 6405], "orig_top_k_doc_id": [2998, 3002, 2999, 4568, 6424, 3001, 3003, 4569, 6425, 5027, 7138, 5028, 575, 2413, 6405]}]}
{"group_id": 875, "group_size": 1, "items": [{"qid": 1996, "question": "What evaluation metrics are used to measure diversity? in Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems", "answer": ["No"], "top_k_doc_id": [3012, 2435, 1771, 6320, 1673, 7300, 3014, 404, 1170, 7479, 4960, 1171, 2865, 7371, 5744], "orig_top_k_doc_id": [3012, 2435, 1771, 6320, 1673, 7300, 3014, 404, 1170, 7479, 4960, 1171, 2865, 7371, 5744]}]}
{"group_id": 876, "group_size": 1, "items": [{"qid": 2005, "question": "What datasets are used for experiments on three other tasks? in Normalized and Geometry-Aware Self-Attention Network for Image Captioning", "answer": ["VATEX, WMT 2014 English-to-German, and VQA-v2 datasets"], "top_k_doc_id": [3026, 3031, 3029, 3027, 4034, 3030, 7580, 4035, 3028, 4756, 2414, 2917, 6995, 3175, 3799], "orig_top_k_doc_id": [3026, 3031, 3029, 3027, 4034, 3030, 7580, 4035, 3028, 4756, 2414, 2917, 6995, 3175, 3799]}]}
{"group_id": 877, "group_size": 1, "items": [{"qid": 2167, "question": "Which matching features do they employ? in Multi-turn Inference Matching Network for Natural Language Inference", "answer": ["Matching features from matching sentences from various perspectives."], "top_k_doc_id": [3324, 3322, 3323, 7542, 3326, 7541, 3325, 2234, 1738, 4364, 7567, 2820, 280, 2821, 2850], "orig_top_k_doc_id": [3324, 3322, 3323, 7542, 3326, 7541, 3325, 2234, 1738, 4364, 7567, 2820, 280, 2821, 2850]}]}
{"group_id": 878, "group_size": 1, "items": [{"qid": 2251, "question": "Does the Agent ask for a value of a variable using natural language generated text? in e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations", "answer": ["Yes"], "top_k_doc_id": [3530, 3531, 3532, 3680, 7841, 7840, 7842, 201, 325, 2278, 3681, 1817, 7843, 202, 5745], "orig_top_k_doc_id": [3530, 3531, 3532, 3680, 7841, 7840, 7842, 201, 325, 2278, 3681, 1817, 7843, 202, 5745]}]}
{"group_id": 879, "group_size": 1, "items": [{"qid": 2329, "question": "How are the synthetic examples generated? in BLEURT: Learning Robust Metrics for Text Generation", "answer": ["Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out"], "top_k_doc_id": [3723, 3721, 3719, 3722, 3720, 3724, 4744, 3725, 286, 287, 6868, 276, 6864, 3805, 6689], "orig_top_k_doc_id": [3723, 3721, 3719, 3722, 3720, 3724, 4744, 3725, 286, 287, 6868, 276, 6864, 3805, 6689]}]}
{"group_id": 880, "group_size": 1, "items": [{"qid": 2376, "question": "How were their results compared to state-of-the-art? in Self-attention based end-to-end Hindi-English Neural Machine Translation", "answer": ["transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model"], "top_k_doc_id": [3824, 2838, 2917, 3823, 3357, 4592, 3685, 5835, 5655, 6291, 3358, 6399, 1884, 3602, 3687], "orig_top_k_doc_id": [3824, 2838, 2917, 3823, 3357, 4592, 3685, 5835, 5655, 6291, 3358, 6399, 1884, 3602, 3687]}]}
{"group_id": 881, "group_size": 1, "items": [{"qid": 2397, "question": "how did they ask if a tweet was racist? in A Dictionary-based Approach to Racism Detection in Dutch Social Media", "answer": ["if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture."], "top_k_doc_id": [3893, 3898, 3894, 3896, 3897, 5812, 3895, 6131, 6520, 5085, 6132, 7260, 6624, 413, 6628], "orig_top_k_doc_id": [3893, 3898, 3894, 3896, 3897, 5812, 3895, 6131, 6520, 5085, 6132, 7260, 6624, 413, 6628]}]}
{"group_id": 882, "group_size": 1, "items": [{"qid": 2400, "question": "How many human subjects were used in the study? in That and There: Judging the Intent of Pointing Actions with Robotic Arms", "answer": ["No"], "top_k_doc_id": [3906, 3905, 3909, 3908, 3907, 3911, 3910, 2125, 7638, 1706, 1296, 7156, 3773, 4548, 5917], "orig_top_k_doc_id": [3906, 3905, 3909, 3908, 3907, 3911, 3910, 2125, 7638, 1706, 1296, 7156, 3773, 4548, 5917]}]}
{"group_id": 883, "group_size": 1, "items": [{"qid": 2401, "question": "How does the model compute the likelihood of executing to the correction semantic denotation? in Weakly-supervised Neural Semantic Parsing with a Generative Ranker", "answer": ["By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x."], "top_k_doc_id": [3912, 3916, 3914, 3913, 3915, 4555, 118, 2370, 3495, 4556, 4257, 7054, 2371, 4557, 1988], "orig_top_k_doc_id": [3912, 3916, 3914, 3913, 3915, 4555, 118, 2370, 3495, 4556, 4257, 7054, 2371, 4557, 1988]}]}
{"group_id": 884, "group_size": 1, "items": [{"qid": 2540, "question": "What is the source of the tables? in Putting Self-Supervised Token Embedding on the Tables", "answer": ["The Online Retail Data Set consists of a clean list of 25873 invoices, totaling 541909 rows and 8 columns."], "top_k_doc_id": [4399, 4400, 4403, 3814, 332, 633, 4539, 2819, 3158, 4402, 2717, 2178, 5028, 6553, 4432], "orig_top_k_doc_id": [4399, 4400, 4403, 3814, 332, 633, 4539, 2819, 3158, 4402, 2717, 2178, 5028, 6553, 4432]}]}
{"group_id": 885, "group_size": 1, "items": [{"qid": 2546, "question": "Is there any human evaluation involved in evaluating this famework? in Language-Based Image Editing with Recurrent Attentive Models", "answer": ["Yes"], "top_k_doc_id": [4430, 4427, 7146, 3686, 4428, 287, 4744, 1245, 2212, 7145, 5850, 491, 3116, 1140, 1250], "orig_top_k_doc_id": [4430, 4427, 7146, 3686, 4428, 287, 4744, 1245, 2212, 7145, 5850, 491, 3116, 1140, 1250]}]}
{"group_id": 886, "group_size": 1, "items": [{"qid": 2552, "question": "What dataset is used for train/test of this method? in Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven Acoustic Embedding Selection", "answer": ["Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset."], "top_k_doc_id": [4450, 4447, 4451, 4449, 2292, 4448, 2530, 2528, 285, 7504, 3950, 2531, 2627, 4863, 6468], "orig_top_k_doc_id": [4450, 4447, 4451, 4449, 2292, 4448, 2530, 2528, 285, 7504, 3950, 2531, 2627, 4863, 6468]}]}
{"group_id": 887, "group_size": 1, "items": [{"qid": 2557, "question": "what is the practical application for this paper? in Comparing morphological complexity of Spanish, Otomi and Nahuatl", "answer": ["Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools."], "top_k_doc_id": [4472, 4473, 4471, 4470, 4474, 1431, 1430, 4593, 1225, 5962, 1000, 5965, 7509, 1984, 1048], "orig_top_k_doc_id": [4472, 4473, 4471, 4470, 4474, 1431, 1430, 4593, 1225, 5962, 1000, 5965, 7509, 1984, 1048]}]}
{"group_id": 888, "group_size": 1, "items": [{"qid": 2560, "question": "By how much does their method outperform state-of-the-art OOD detection? in Contextual Out-of-Domain Utterance Handling With Counterfeit Data Augmentation", "answer": ["AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average"], "top_k_doc_id": [4484, 4486, 4485, 488, 485, 3416, 5410, 6925, 2209, 400, 1989, 1331, 6671, 5965, 7847], "orig_top_k_doc_id": [4484, 4486, 4485, 488, 485, 3416, 5410, 6925, 2209, 400, 1989, 1331, 6671, 5965, 7847]}]}
{"group_id": 889, "group_size": 1, "items": [{"qid": 2567, "question": "How many parameters does the model have? in Improving Pre-Trained Multilingual Models with Vocabulary Expansion", "answer": ["No"], "top_k_doc_id": [5621, 4693, 50, 3241, 4511, 5105, 7342, 6870, 6167, 5115, 4571, 6871, 4590, 5172, 7596], "orig_top_k_doc_id": [5621, 4693, 50, 3241, 4511, 5105, 7342, 6870, 6167, 5115, 4571, 6871, 4590, 5172, 7596]}]}
{"group_id": 890, "group_size": 1, "items": [{"qid": 2574, "question": "What is the difference in findings of Buck et al? It looks like the same conclusion was mentioned in Buck et al.. in Analyzing Language Learned by an Active Question Answering Agent", "answer": ["AQA diverges from well structured language in favour of less fluent, but more effective, classic information retrieval (IR) query operations"], "top_k_doc_id": [6575, 4680, 5231, 7104, 6585, 2343, 1665, 6305, 7072, 4511, 1706, 1726, 6584, 19, 1643], "orig_top_k_doc_id": [6575, 4680, 5231, 7104, 6585, 2343, 1665, 6305, 7072, 4511, 1706, 1726, 6584, 19, 1643]}]}
{"group_id": 891, "group_size": 1, "items": [{"qid": 2702, "question": "Did they use a crowdsourcing platform for annotations? in How big is big enough? Unsupervised word sense disambiguation using a very large corpus", "answer": ["No", "No"], "top_k_doc_id": [608, 5911, 1518, 4585, 414, 4738, 690, 5038, 5910, 4475, 68, 4012, 5360, 5320, 4734], "orig_top_k_doc_id": [608, 5911, 1518, 4585, 414, 4738, 690, 5038, 5910, 4475, 68, 4012, 5360, 5320, 4734]}]}
{"group_id": 892, "group_size": 1, "items": [{"qid": 2767, "question": "Do the authors also try the model on other datasets? in Character-Level Question Answering with Attention", "answer": ["No", "No"], "top_k_doc_id": [3033, 3034, 7163, 2910, 1651, 4842, 7688, 1424, 1332, 259, 1547, 6864, 5472, 5048, 4204], "orig_top_k_doc_id": [3033, 3034, 7163, 2910, 1651, 4842, 7688, 1424, 1332, 259, 1547, 6864, 5472, 5048, 4204]}]}
{"group_id": 893, "group_size": 1, "items": [{"qid": 2771, "question": "Did they only experiment with one language pair? in To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation", "answer": ["Yes", "Yes"], "top_k_doc_id": [4853, 4857, 4856, 4854, 4855, 4527, 6426, 5027, 2616, 4044, 7437, 2761, 7825, 244, 5351], "orig_top_k_doc_id": [4853, 4857, 4856, 4854, 4855, 4527, 6426, 5027, 2616, 4044, 7437, 2761, 7825, 244, 5351]}]}
{"group_id": 894, "group_size": 1, "items": [{"qid": 2811, "question": "How long of dialog history is captured? in Dialog Context Language Modeling with Recurrent Neural Networks", "answer": ["two previous turns", "160"], "top_k_doc_id": [848, 4546, 4545, 4926, 2253, 6794, 3467, 4553, 4924, 6793, 6588, 6587, 1527, 7476, 849], "orig_top_k_doc_id": [848, 4546, 4545, 4926, 2253, 6794, 3467, 4553, 4924, 6793, 6588, 6587, 1527, 7476, 849]}]}
{"group_id": 895, "group_size": 1, "items": [{"qid": 3602, "question": "What is the accuracy of this model compared to sota? in OpenTapioca: Lightweight Entity Linking for Wikidata", "answer": ["No", "The model improves the state of the art performance for the ISTEX dataset (F1 micro: 0.870, F1 macro: 0.858) and for the Microposts 2016 dataset (F1 micro:  0.087).", "The micro and macro f1-scores of this model are 0.482 and 0.399 on the AIDA-CoNLL dataset, 0.087 and 0.515 on the Microposts 2016 dataset, 0.870 and 0.858 on the ISTEX-1000 dataset, 0.335 and 0.310 on the RSS-500 dataset", "The accuracy ", "No"], "top_k_doc_id": [5923, 5926, 4859, 5924, 5925, 2675, 6297, 1423, 7245, 6950, 1121, 6049, 1294, 2096, 6048], "orig_top_k_doc_id": [5923, 5926, 4859, 5924, 5925, 2675, 6297, 1423, 7245, 6950, 1121, 6049, 1294, 2096, 6048]}]}
{"group_id": 896, "group_size": 1, "items": [{"qid": 3938, "question": "What data is used in this work? in Tie-breaker: Using language models to quantify gender bias in sports journalism", "answer": ["Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data", "post-game interviews from ASAP Sport's website", "tennis post-match interview transcripts, live text play-by-play commentaries"], "top_k_doc_id": [6341, 5949, 6342, 1443, 52, 5153, 5006, 1444, 5252, 6343, 7772, 3941, 6964, 3942, 5191], "orig_top_k_doc_id": [6341, 5949, 6342, 1443, 52, 5153, 5006, 1444, 5252, 6343, 7772, 3941, 6964, 3942, 5191]}]}
{"group_id": 897, "group_size": 1, "items": [{"qid": 3958, "question": "How large the improvement margin is? in Zero-Shot Adaptive Transfer for Conversational Language Understanding", "answer": ["+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500", "Average F1 improvement of 5.07", "+7.24, +11.03, +14.67, +5.07 for 2000, 1000, 500 and zero training instances respectively"], "top_k_doc_id": [6395, 6394, 6392, 4414, 3572, 4571, 7299, 4030, 6034, 4752, 5621, 6063, 6033, 5870, 4031], "orig_top_k_doc_id": [6395, 6394, 6392, 4414, 3572, 4571, 7299, 4030, 6034, 4752, 5621, 6063, 6033, 5870, 4031]}]}
{"group_id": 898, "group_size": 1, "items": [{"qid": 3962, "question": "Are the two paragraphs encoded independently? in Recognizing Arrow Of Time In The Short Stories", "answer": ["Yes", "Yes", "Yes"], "top_k_doc_id": [6409, 6067, 6065, 803, 6066, 2664, 4636, 804, 4976, 3878, 7408, 671, 3879, 2008, 1912], "orig_top_k_doc_id": [6409, 6067, 6065, 803, 6066, 2664, 4636, 804, 4976, 3878, 7408, 671, 3879, 2008, 1912]}]}
{"group_id": 899, "group_size": 1, "items": [{"qid": 4369, "question": "Are reddit and twitter datasets, which are fairly prevalent, not effective in addressing these problems? in What to do about non-standard (or non-canonical) language in NLP", "answer": ["Yes", "No"], "top_k_doc_id": [6285, 6207, 6883, 3598, 6881, 5907, 5913, 7599, 5011, 6286, 4212, 6805, 3582, 6616, 5641], "orig_top_k_doc_id": [6285, 6207, 6883, 3598, 6881, 5907, 5913, 7599, 5011, 6286, 4212, 6805, 3582, 6616, 5641]}]}
{"group_id": 900, "group_size": 1, "items": [{"qid": 4437, "question": "Did the authors collect new data for evaluation? in Sentiment Analysis of Czech Texts: An Algorithmic Survey", "answer": ["No", "No"], "top_k_doc_id": [6971, 1957, 7172, 6640, 277, 6974, 7855, 1250, 56, 5405, 57, 7186, 6973, 5526, 2142], "orig_top_k_doc_id": [6971, 1957, 7172, 6640, 277, 6974, 7855, 1250, 56, 5405, 57, 7186, 6973, 5526, 2142]}]}
{"group_id": 901, "group_size": 1, "items": [{"qid": 4542, "question": "What existing techniques do the authors compare against? in Morphology-based Entity and Relational Entity Extraction Framework for Arabic", "answer": ["ANGE, ATEEMA, GENTREE, and NUMNORM", "ANGE, ATEEMA, GENTREE "], "top_k_doc_id": [7091, 7097, 7093, 7092, 7096, 131, 4216, 7098, 7055, 3628, 559, 3414, 2285, 3412, 3632], "orig_top_k_doc_id": [7091, 7097, 7093, 7092, 7096, 131, 4216, 7098, 7055, 3628, 559, 3414, 2285, 3412, 3632]}]}
{"group_id": 902, "group_size": 1, "items": [{"qid": 4774, "question": "What set of semantic tags did they use? in What can we learn from Semantic Tagging?", "answer": ["Tags categories ranging from anaphoric (definite, possessive pronoun), attribute (colour, concrete quantity, intersective, relation), unnamed entity (concept), logical (alternative, disjunction), discourse (subordinate relation), events (present simple, past simple), etc.", "tagset originated in the Parallel Meaning Bank (PMB) project BIBREF9"], "top_k_doc_id": [7439, 7441, 2906, 7440, 6146, 4351, 6251, 5365, 863, 2348, 3207, 5603, 1945, 4345, 4678], "orig_top_k_doc_id": [7439, 7441, 2906, 7440, 6146, 4351, 6251, 5365, 863, 2348, 3207, 5603, 1945, 4345, 4678]}]}
{"group_id": 903, "group_size": 1, "items": [{"qid": 5000, "question": "What is the caching mechanism? in A Study on Neural Network Language Modeling", "answer": ["The cache language models are based on the assumption that the word in recent history are more likely to appear again, conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching", "store the outputs and states of language models for future prediction given the same contextual history"], "top_k_doc_id": [7784, 7780, 7783, 7787, 4415, 5724, 6847, 7472, 1560, 1389, 4095, 527, 709, 3069, 7246], "orig_top_k_doc_id": [7784, 7780, 7783, 7787, 4415, 5724, 6847, 7472, 1560, 1389, 4095, 527, 709, 3069, 7246]}]}
{"group_id": 904, "group_size": 1, "items": [{"qid": 5002, "question": "What directions are suggested to improve language models? in A Study on Neural Network Language Modeling", "answer": ["Improved architecture for ANN, use of linguistical properties of words or sentences as features.", "gate recurrent unit (GRU) RNNLM, dropout strategy for addressing overfitting, character level neural network language model and ect."], "top_k_doc_id": [720, 1651, 1653, 5710, 3507, 7688, 7785, 3207, 1521, 4186, 7183, 5711, 6092, 4095, 3368], "orig_top_k_doc_id": [720, 1651, 1653, 5710, 3507, 7688, 7785, 3207, 1521, 4186, 7183, 5711, 6092, 4095, 3368]}]}
